{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m7KBpffWzlxH"
   },
   "source": [
    "### Import TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Sequential\" model lets us to define a stack of neural network layers\n",
    "from keras.models import Sequential\n",
    "\n",
    "# import the \"core\" layers:\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "\n",
    "# CNN\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "\n",
    "# import some utilities to transform our data\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jRFxccghyMVo"
   },
   "source": [
    "### Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    " %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = 'Covid19-dataset/train'\n",
    "DATADIR2 = 'Covid19-dataset/test'\n",
    "CATEGORIES = ['Covid','Normal','Viral Pneumonia']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for category in CATEGORIES:\n",
    "    path = os.path.join(DATADIR,category)\n",
    "    for img in os.listdir(path):\n",
    "        img_array = cv2.imread(os.path.join(path,img),cv2.IMREAD_COLOR)\n",
    "        plt.imshow(img_array)\n",
    "        plt.show()\n",
    "        break\n",
    "    break\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3480, 4248, 3)\n"
     ]
    }
   ],
   "source": [
    "print(img_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "IMG_SIZE = 100\n",
    "new_array = cv2.resize(img_array,(IMG_SIZE,IMG_SIZE))\n",
    "plt.imshow(new_array,cmap = 'gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = []\n",
    "def create_training_data():\n",
    "    for category in CATEGORIES:\n",
    "        path = os.path.join(DATADIR,category)\n",
    "        class_num = CATEGORIES.index(category)\n",
    "        for img in os.listdir(path):\n",
    "            img_array = cv2.imread(os.path.join(path,img),cv2. IMREAD_GRAYSCALE)\n",
    "            new_array = cv2.resize(img_array,(IMG_SIZE,IMG_SIZE))\n",
    "            training_data.append([new_array,class_num])\n",
    "            \n",
    "create_training_data()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "251\n",
      "[[array([[ 6,  4,  3, ...,  6,  4,  8],\n",
      "       [ 2,  1,  1, ...,  4,  2,  7],\n",
      "       [ 8,  8,  6, ...,  1,  5,  7],\n",
      "       ...,\n",
      "       [ 1, 11, 19, ...,  1,  1,  4],\n",
      "       [ 0,  8, 16, ...,  3,  5,  5],\n",
      "       [ 0,  0, 18, ...,  5,  8,  7]], dtype=uint8), 0], [array([[ 53,  53,  53, ...,   2,   2,   2],\n",
      "       [ 57,  59,  67, ...,   2,   2,   2],\n",
      "       [ 65,  64,  80, ...,   2,   2,   2],\n",
      "       ...,\n",
      "       [116, 124, 140, ...,  97,  90,  83],\n",
      "       [118, 126, 138, ...,  95,  89,  81],\n",
      "       [116, 128, 138, ...,  93,  86,  81]], dtype=uint8), 0], [array([[  4, 177,   0, ..., 199, 192, 188],\n",
      "       [  0,   0,   0, ..., 212, 214, 220],\n",
      "       [  0,   0,   0, ..., 216, 212, 219],\n",
      "       ...,\n",
      "       [  0,   0,   0, ..., 124,  75,  33],\n",
      "       [  0,   0,   0, ..., 177,  75,  44],\n",
      "       [  0,   0,   0, ..., 134, 165,  63]], dtype=uint8), 0], [array([[ 0,  0,  0, ...,  0,  0,  0],\n",
      "       [ 0,  0,  0, ...,  0,  0,  0],\n",
      "       [ 0,  0,  0, ..., 14,  0,  0],\n",
      "       ...,\n",
      "       [ 0,  0,  0, ..., 13, 11, 11],\n",
      "       [ 0,  0,  0, ..., 11, 11, 11],\n",
      "       [ 0,  0,  0, ..., 10, 12, 11]], dtype=uint8), 0], [array([[13, 13, 13, ..., 13, 13, 13],\n",
      "       [13, 13, 13, ..., 24, 16, 13],\n",
      "       [13, 13, 13, ..., 33, 25, 25],\n",
      "       ...,\n",
      "       [13, 13, 13, ..., 13, 13, 13],\n",
      "       [13, 13, 13, ..., 13, 13, 13],\n",
      "       [13, 13, 13, ..., 13, 13, 13]], dtype=uint8), 0], [array([[2, 4, 4, ..., 0, 0, 0],\n",
      "       [4, 4, 5, ..., 4, 5, 5],\n",
      "       [5, 4, 5, ..., 5, 5, 5],\n",
      "       ...,\n",
      "       [3, 1, 0, ..., 2, 3, 4],\n",
      "       [4, 0, 0, ..., 3, 4, 4],\n",
      "       [4, 1, 0, ..., 4, 4, 4]], dtype=uint8), 0], [array([[ 1,  1,  1, ...,  1,  1,  1],\n",
      "       [ 1,  1,  1, ...,  1,  1,  1],\n",
      "       [ 1,  1,  1, ...,  1,  1,  1],\n",
      "       ...,\n",
      "       [ 1,  1,  1, ...,  1,  1,  1],\n",
      "       [ 1,  1,  1, ...,  1,  1,  1],\n",
      "       [ 1,  1,  1, ...,  1,  1, 66]], dtype=uint8), 0], [array([[  3,   1,   2, ...,   0,   0,   0],\n",
      "       [  2,   1,   3, ..., 254,   2,   0],\n",
      "       [  2,   2,   2, ..., 255, 174,   0],\n",
      "       ...,\n",
      "       [134, 156, 173, ...,  18,  13,   0],\n",
      "       [131, 171, 178, ...,  16,  12,   2],\n",
      "       [127, 167, 183, ...,  13,   9,   1]], dtype=uint8), 0], [array([[ 87,  71,  55, ...,  42,  44,  44],\n",
      "       [ 76,  57,  54, ...,  51,  60,  46],\n",
      "       [ 70,  60,  46, ...,  40,  24,  64],\n",
      "       ...,\n",
      "       [227, 227, 206, ..., 171, 142, 174],\n",
      "       [230, 233, 220, ..., 177, 173, 174],\n",
      "       [235, 233, 223, ..., 177, 191, 185]], dtype=uint8), 0], [array([[  1,   2,   1, ...,   0,   0,   0],\n",
      "       [  3,   2,   2, ...,   1,   1,   0],\n",
      "       [  2,   2,   3, ...,   1,   1,   1],\n",
      "       ...,\n",
      "       [166, 166, 172, ...,  33,  26,   4],\n",
      "       [152, 167, 191, ...,  34,  23,  15],\n",
      "       [137, 174, 187, ...,  36,  22,   7]], dtype=uint8), 0], [array([[110, 128,  88, ...,  49,  48,  35],\n",
      "       [122, 126, 108, ...,  55,  52,  47],\n",
      "       [121, 130,  90, ...,  54,  44,  56],\n",
      "       ...,\n",
      "       [252, 253, 251, ..., 191, 183, 150],\n",
      "       [255, 254, 254, ..., 191, 184, 153],\n",
      "       [254, 255, 254, ..., 198, 177, 160]], dtype=uint8), 0], [array([[ 19,  18,  18, ..., 190, 247, 254],\n",
      "       [ 22,  18,  16, ..., 197, 247, 254],\n",
      "       [ 30,  39,  32, ..., 196, 248, 253],\n",
      "       ...,\n",
      "       [203, 202, 206, ..., 254, 255, 255],\n",
      "       [206, 207, 208, ..., 254, 254, 253],\n",
      "       [212, 209, 207, ..., 255, 255, 255]], dtype=uint8), 0], [array([[  0,   0,   0, ...,   5,   5,  18],\n",
      "       [  0,   0,   0, ...,   4,   5,  17],\n",
      "       [  0,   0,   2, ...,   9,  12,  32],\n",
      "       ...,\n",
      "       [  1,   2,   7, ...,  99,  95, 116],\n",
      "       [  0,   3,   7, ..., 104,  86, 111],\n",
      "       [  0,   2,   6, ..., 107,  90, 106]], dtype=uint8), 0], [array([[ 7,  7, 55, ...,  7,  7,  7],\n",
      "       [ 8,  9, 57, ...,  6,  8,  9],\n",
      "       [ 6,  8, 54, ...,  5,  6,  6],\n",
      "       ...,\n",
      "       [10, 10, 11, ..., 20,  3,  3],\n",
      "       [11, 10, 10, ..., 20,  3,  3],\n",
      "       [11, 11, 10, ..., 24, 12,  3]], dtype=uint8), 0], [array([[ 2,  1,  1, ...,  0,  0,  0],\n",
      "       [ 1,  1,  1, ...,  2,  2,  1],\n",
      "       [ 1,  1,  3, ...,  3,  2,  3],\n",
      "       ...,\n",
      "       [41, 53, 64, ...,  0,  0,  2],\n",
      "       [52, 63, 65, ...,  1,  0,  2],\n",
      "       [56, 65, 86, ...,  1,  0,  2]], dtype=uint8), 0], [array([[ 76,  19,  23, ...,   0,   1,   2],\n",
      "       [ 77,  36,  28, ...,   0,   0,   0],\n",
      "       [ 70,  89,  39, ...,   1,   1,   1],\n",
      "       ...,\n",
      "       [200, 194, 206, ..., 194, 178, 169],\n",
      "       [195, 201, 207, ..., 196, 183, 170],\n",
      "       [197, 209, 206, ..., 199, 188, 162]], dtype=uint8), 0], [array([[137, 105,  93, ...,  10,  10,   9],\n",
      "       [135, 131, 120, ..., 107, 103, 101],\n",
      "       [131, 131, 120, ...,  97,  93,  96],\n",
      "       ...,\n",
      "       [ 76, 124,  91, ..., 108,  95,  88],\n",
      "       [ 59,  94,  76, ..., 107, 101,  96],\n",
      "       [ 30,  56,  73, ..., 111,  97,  89]], dtype=uint8), 0], [array([[  2,   3,   4, ...,  13,   8,   2],\n",
      "       [  0,   0,   0, ...,   1,   1,   0],\n",
      "       [  0,   0,   0, ...,   2,   2,   1],\n",
      "       ...,\n",
      "       [  1,   2,   5, ..., 111, 101,  83],\n",
      "       [  1,   1,   5, ..., 129, 104,  97],\n",
      "       [  1,   2,   5, ..., 130, 113,  99]], dtype=uint8), 0], [array([[  1,   1,   1, ...,   1,   1,   0],\n",
      "       [  1,   1,   1, ...,   1,   1,   1],\n",
      "       [  1,   1,   1, ...,   1,   1,   1],\n",
      "       ...,\n",
      "       [  1,   1,   1, ..., 184, 182,  57],\n",
      "       [  1,   1,   1, ..., 181, 169,  58],\n",
      "       [  1,   1,   1, ..., 191,  81,  72]], dtype=uint8), 0], [array([[  1,   1,   1, ...,   0,   1,   1],\n",
      "       [  2,   2,   2, ...,   1,   1,   1],\n",
      "       [ 17,  15,  22, ...,   1,   1,   1],\n",
      "       ...,\n",
      "       [  9,  55,  91, ..., 110,  70,  26],\n",
      "       [ 11,  53,  86, ..., 107,  57,  23],\n",
      "       [  8,  46,  86, ..., 103,  50,  25]], dtype=uint8), 0], [array([[ 4,  4,  4, ...,  4,  4,  4],\n",
      "       [ 4,  4, 53, ...,  4,  4,  4],\n",
      "       [ 4,  4,  3, ...,  4,  4,  4],\n",
      "       ...,\n",
      "       [ 4,  4,  5, ...,  4,  4,  4],\n",
      "       [ 4,  4,  4, ...,  4,  4,  4],\n",
      "       [ 4,  4,  4, ...,  4,  4,  4]], dtype=uint8), 0], [array([[ 1,  1,  1, ...,  2,  2,  2],\n",
      "       [ 1,  2,  1, ...,  2,  2,  2],\n",
      "       [ 0,  1,  0, ...,  2,  2,  2],\n",
      "       ...,\n",
      "       [ 5, 14, 31, ..., 57, 24, 14],\n",
      "       [ 5, 10, 28, ..., 51, 24, 15],\n",
      "       [ 5,  8, 21, ..., 46, 20, 11]], dtype=uint8), 0], [array([[  1,   1,   0, ...,   1,   1,   1],\n",
      "       [  1, 203,   0, ...,   1,   1,   1],\n",
      "       [  0,   0,   0, ...,   2,   1,   1],\n",
      "       ...,\n",
      "       [202, 198, 209, ..., 217, 202, 214],\n",
      "       [185, 199, 210, ..., 219, 208, 209],\n",
      "       [198, 198, 212, ..., 219, 211, 224]], dtype=uint8), 0], [array([[  2,   0,   0, ...,  17,  22, 225],\n",
      "       [  0,   0,   0, ..., 119,  48,  23],\n",
      "       [  0,   0,   0, ..., 147, 153, 142],\n",
      "       ...,\n",
      "       [181, 178, 195, ...,   0,   0,   0],\n",
      "       [187, 179, 200, ...,   0,   0,   0],\n",
      "       [188, 188, 212, ...,   0,   0,   1]], dtype=uint8), 0], [array([[ 0,  0,  6, ...,  7,  7,  7],\n",
      "       [ 0,  7,  6, ...,  6,  6,  7],\n",
      "       [ 0,  5,  6, ...,  6,  6,  6],\n",
      "       ...,\n",
      "       [23, 17, 12, ...,  9, 13, 18],\n",
      "       [20, 16, 12, ...,  8, 12, 18],\n",
      "       [19, 15, 12, ...,  8, 12, 16]], dtype=uint8), 0], [array([[  0,   0,   0, ...,   0,   0,   0],\n",
      "       [  1,   1,   1, ...,   0,   0,   0],\n",
      "       [  2,   2,   2, ...,   0,   0,   0],\n",
      "       ...,\n",
      "       [250, 245, 243, ..., 142, 127, 110],\n",
      "       [238, 242, 253, ..., 141, 137, 122],\n",
      "       [251, 246, 244, ..., 143, 130, 119]], dtype=uint8), 0], [array([[ 24,  16,  21, ...,  36,  38,  36],\n",
      "       [  3,   3,   4, ...,  23,  26,  27],\n",
      "       [  0,   0,   0, ...,  16,  17,  17],\n",
      "       ...,\n",
      "       [221, 209, 224, ..., 172, 166, 150],\n",
      "       [225, 210, 226, ..., 176, 171, 158],\n",
      "       [224, 209, 222, ..., 178, 174, 165]], dtype=uint8), 0], [array([[  0,   0,   0, ...,   0,   0,   0],\n",
      "       [  0,   0,   0, ...,   1,   0,   0],\n",
      "       [  0,   1,   1, ...,   1,   1,   1],\n",
      "       ...,\n",
      "       [191, 196, 204, ..., 202, 204, 207],\n",
      "       [199, 198, 193, ..., 205, 207, 215],\n",
      "       [193, 194, 200, ..., 208, 209, 205]], dtype=uint8), 0], [array([[ 61,  38,  22, ...,   0,   0,   0],\n",
      "       [ 54,  75,  71, ...,  31,  12,   8],\n",
      "       [ 58,  66,  78, ...,  21,  20,   9],\n",
      "       ...,\n",
      "       [254, 254, 253, ..., 200, 198, 194],\n",
      "       [254, 255, 254, ..., 205, 199, 193],\n",
      "       [255, 255, 255, ..., 210, 209, 197]], dtype=uint8), 0], [array([[  0,   0,   0, ...,  35,  33,  28],\n",
      "       [  0,   0,   0, ...,  54,  47,  34],\n",
      "       [  0,   0,   0, ...,  66,  58,  47],\n",
      "       ...,\n",
      "       [ 80,  97, 106, ..., 119,  76,  10],\n",
      "       [ 73,  93, 109, ..., 135,  72,   6],\n",
      "       [ 69,  92, 106, ..., 108,  64,   2]], dtype=uint8), 0], [array([[  4,   4,   4, ...,   1,   1,   1],\n",
      "       [  5,   5,   5, ...,   1,   1,   1],\n",
      "       [  8,   7,   7, ...,   1,   1,   1],\n",
      "       ...,\n",
      "       [225, 231, 231, ..., 150, 146, 137],\n",
      "       [230, 230, 240, ..., 147, 141, 125],\n",
      "       [228, 238, 237, ..., 152, 144, 130]], dtype=uint8), 0], [array([[  0,   0,   0, ..., 154, 167, 184],\n",
      "       [  2,   2,   2, ..., 147, 158, 171],\n",
      "       [  4,   4,   4, ..., 142, 154, 167],\n",
      "       ...,\n",
      "       [210, 213, 218, ..., 196, 209, 190],\n",
      "       [215, 212, 223, ..., 189, 202, 176],\n",
      "       [232, 224, 230, ..., 188, 198, 162]], dtype=uint8), 0], [array([[  1,   1,   1, ...,   0,   0,   0],\n",
      "       [  1,   2,   2, ...,   0,   0,   0],\n",
      "       [  2,   2,   3, ...,   1,   0,   0],\n",
      "       ...,\n",
      "       [254, 255, 255, ..., 249, 253, 250],\n",
      "       [255, 255, 255, ..., 248, 253, 251],\n",
      "       [255, 255, 255, ..., 250, 253, 254]], dtype=uint8), 0], [array([[ 29,   8,   9, ...,  46,  48,  53],\n",
      "       [ 77, 105,  99, ...,  69,  69,  64],\n",
      "       [ 80,  89,  82, ...,  79, 101, 138],\n",
      "       ...,\n",
      "       [146, 164, 179, ..., 179, 158, 117],\n",
      "       [133, 162, 174, ..., 186, 139, 100],\n",
      "       [130, 151, 168, ..., 162, 130, 102]], dtype=uint8), 0], [array([[  0,   0,   0, ...,  70,  68,  85],\n",
      "       [  0,   0,   0, ...,  98,  84,  49],\n",
      "       [  0,   0,   1, ...,  64,  53,  50],\n",
      "       ...,\n",
      "       [243, 242, 247, ..., 255, 255, 255],\n",
      "       [247, 249, 253, ..., 255, 255, 255],\n",
      "       [248, 248, 251, ..., 255, 255, 255]], dtype=uint8), 0], [array([[35, 52, 65, ..., 32, 32, 33],\n",
      "       [34, 55, 70, ..., 33, 32, 35],\n",
      "       [36, 55, 64, ..., 33, 32, 34],\n",
      "       ...,\n",
      "       [33, 32, 45, ..., 76, 32, 33],\n",
      "       [31, 32, 46, ..., 40, 54, 35],\n",
      "       [33, 34, 47, ..., 55, 34, 33]], dtype=uint8), 0], [array([[255, 255, 254, ..., 254, 252, 254],\n",
      "       [254, 163, 103, ..., 115, 127, 112],\n",
      "       [201, 121, 113, ...,  94,  65,  71],\n",
      "       ...,\n",
      "       [104,  92,  97, ..., 109,  74,  47],\n",
      "       [ 93,  91,  99, ..., 103,  68,  47],\n",
      "       [ 84,  83,  95, ...,  99,  61,  45]], dtype=uint8), 0], [array([[ 48,  50,  57, ...,  57,  77,  49],\n",
      "       [ 42,  55,  60, ...,  44,  51,  27],\n",
      "       [ 47,  85,  80, ...,  57,  40,  42],\n",
      "       ...,\n",
      "       [126, 130, 128, ..., 146, 138, 117],\n",
      "       [111, 116, 119, ..., 140, 135, 130],\n",
      "       [112, 116, 115, ..., 149, 138, 118]], dtype=uint8), 0], [array([[ 30,  20,  19, ...,   0,   0,   0],\n",
      "       [ 17,  34,  35, ...,   0,   0,   0],\n",
      "       [ 14,  16,  37, ...,   0,   0,   0],\n",
      "       ...,\n",
      "       [174, 181, 182, ..., 191, 185, 186],\n",
      "       [180, 180, 185, ..., 195, 187, 193],\n",
      "       [184, 183, 189, ..., 193, 185, 186]], dtype=uint8), 0], [array([[25, 25, 24, ..., 10, 10, 10],\n",
      "       [25, 25, 24, ..., 10, 10, 10],\n",
      "       [25, 25, 25, ..., 10, 10, 10],\n",
      "       ...,\n",
      "       [28, 28, 18, ..., 26,  1,  3],\n",
      "       [28, 27,  3, ..., 26,  1,  3],\n",
      "       [28, 25, 10, ..., 26,  1, 11]], dtype=uint8), 0], [array([[11,  9,  9, ...,  7,  8,  8],\n",
      "       [ 5,  3,  3, ...,  3,  3,  3],\n",
      "       [ 5,  3,  3, ...,  1,  3,  3],\n",
      "       ...,\n",
      "       [ 9,  8,  7, ...,  5,  6,  7],\n",
      "       [ 9,  8,  7, ...,  5,  6,  7],\n",
      "       [ 9,  7,  7, ...,  5,  6,  7]], dtype=uint8), 0], [array([[  1,   0,   0, ...,   0,   0,   0],\n",
      "       [  1, 254, 252, ...,   0,   0,   0],\n",
      "       [  1, 254,   0, ...,   0,   0,   0],\n",
      "       ...,\n",
      "       [  0,   7,  23, ...,   0,   0,   0],\n",
      "       [  0,  12,  20, ...,   0,   0,   0],\n",
      "       [  0,  12,  21, ...,   0,   0,   0]], dtype=uint8), 0], [array([[14, 14, 14, ..., 14, 14, 14],\n",
      "       [14, 14, 14, ..., 14, 14, 14],\n",
      "       [14, 14, 14, ..., 14, 14, 14],\n",
      "       ...,\n",
      "       [14, 14, 14, ..., 14, 14, 14],\n",
      "       [14, 14, 14, ..., 14, 14, 14],\n",
      "       [14, 14, 14, ..., 14, 14, 14]], dtype=uint8), 0], [array([[  1, 254,   0, ...,   0,   0,   1],\n",
      "       [  1, 255,   0, ...,   0,   0,   0],\n",
      "       [  1, 255, 252, ...,   0,   0,   0],\n",
      "       ...,\n",
      "       [  0,   0,  20, ...,   0,   0,   0],\n",
      "       [  0,   0,  20, ...,   0,   0,   0],\n",
      "       [  0,   0,  22, ...,   0,   0,   0]], dtype=uint8), 0], [array([[254, 254, 254, ..., 254, 254, 254],\n",
      "       [115,  99,  84, ..., 128, 134, 123],\n",
      "       [110,  98,  94, ..., 114, 123, 115],\n",
      "       ...,\n",
      "       [  1,   2,  18, ...,   2,   1,   0],\n",
      "       [  0,   2,  20, ...,   3,   1,   0],\n",
      "       [252,  28,  19, ...,   3,   1,   1]], dtype=uint8), 0], [array([[  0,   0,   0, ...,   0,   0,   0],\n",
      "       [  0,   0,   0, ...,   0,   0,   0],\n",
      "       [  0,   0,   0, ...,   0,   0,   0],\n",
      "       ...,\n",
      "       [142, 139, 144, ...,   0,   0,   0],\n",
      "       [148, 147, 148, ...,   0,   0,   0],\n",
      "       [161, 156, 154, ...,   0,   0,   0]], dtype=uint8), 0], [array([[  0,   0,   0, ...,   0,   0,   0],\n",
      "       [  1,   3,   0, ...,   0,   0,   0],\n",
      "       [  0,   1,   0, ...,   0,   0,   0],\n",
      "       ...,\n",
      "       [  0, 253,   0, ...,   0,   0,   0],\n",
      "       [  1,   2,   0, ...,   0,   0,   0],\n",
      "       [  0,   0,   0, ...,   0,   0,   0]], dtype=uint8), 0], [array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8), 0], [array([[  1,   1,   1, ...,  50,   4,   9],\n",
      "       [  1,   1,   1, ...,   8,  13,  15],\n",
      "       [  1,   1,   2, ...,  31,  26,  36],\n",
      "       ...,\n",
      "       [120, 135, 148, ..., 199, 205, 186],\n",
      "       [124, 136, 140, ..., 198, 207, 189],\n",
      "       [122, 136, 144, ..., 202, 197, 193]], dtype=uint8), 0], [array([[204,   0,   1, ...,   1,   1,   1],\n",
      "       [255,  32,   1, ...,  28,  29,  30],\n",
      "       [  1,   3,  18, ...,  44,  41,  43],\n",
      "       ...,\n",
      "       [103,  97, 253, ...,   0,   1,  24],\n",
      "       [102, 119, 122, ...,   1,   1,  26],\n",
      "       [103, 115, 127, ...,   1,   1,  25]], dtype=uint8), 0], [array([[ 3,  3,  3, ...,  3,  3,  3],\n",
      "       [ 3,  3,  3, ...,  3,  3,  3],\n",
      "       [ 3,  3,  3, ...,  3,  3,  3],\n",
      "       ...,\n",
      "       [ 3,  3,  0, ..., 68, 51, 22],\n",
      "       [ 3,  3,  3, ..., 67, 49, 21],\n",
      "       [ 3,  3,  3, ..., 61, 36, 21]], dtype=uint8), 0], [array([[  1,   1,   1, ...,   1,   1,   1],\n",
      "       [  1,   1,   1, ...,   5,   1,  11],\n",
      "       [  1,   3,   1, ...,  43,  42,  32],\n",
      "       ...,\n",
      "       [  1,  58, 253, ...,  94,  80,  71],\n",
      "       [  1,   3,  81, ...,  93,  86,  73],\n",
      "       [  1,   1,  51, ...,  93,  89,  73]], dtype=uint8), 0], [array([[ 27,  32,  28, ...,   7,   4,   8],\n",
      "       [ 30,  41,  37, ...,   1,   1,   1],\n",
      "       [ 49,  51,  47, ...,   1,   1,   1],\n",
      "       ...,\n",
      "       [ 63,  72, 252, ...,  19,  17,  14],\n",
      "       [ 68,  87, 103, ...,   0,  15,  26],\n",
      "       [ 60,  92, 109, ...,  10,  15,   8]], dtype=uint8), 0], [array([[  1,   1,   1, ...,   1,   1,   1],\n",
      "       [  1,   1,   1, ...,   1,   1,   1],\n",
      "       [  1,   3,   1, ...,   1,   1,   1],\n",
      "       ...,\n",
      "       [  1,   1, 254, ...,   1,   1,   1],\n",
      "       [  1,   1,  60, ...,   1,   1,   1],\n",
      "       [  1,   1,  54, ...,   1,   1,   1]], dtype=uint8), 0], [array([[255, 241, 188, ..., 207, 203, 182],\n",
      "       [208, 131,  84, ...,  78,  81, 102],\n",
      "       [205, 132, 127, ...,  98,  84,  79],\n",
      "       ...,\n",
      "       [101,  93,  98, ..., 122, 107,  75],\n",
      "       [ 96,  90,  98, ..., 118, 105,  70],\n",
      "       [ 85,  84,  97, ..., 116, 101,  60]], dtype=uint8), 0], [array([[ 8,  8, 10, ...,  1,  1,  1],\n",
      "       [22, 18, 26, ...,  1,  1,  1],\n",
      "       [72, 65, 38, ..., 12, 18, 13],\n",
      "       ...,\n",
      "       [19, 46, 39, ...,  1,  1,  1],\n",
      "       [32, 38, 45, ...,  1,  1,  1],\n",
      "       [22, 33, 44, ...,  1,  1,  1]], dtype=uint8), 0], [array([[  0,   0,   0, ...,   0,   0,   0],\n",
      "       [  0,   0,   0, ...,   0,   0,   0],\n",
      "       [  0,   1,   1, ...,   1,   1,   1],\n",
      "       ...,\n",
      "       [ 29,  62,  98, ...,  96,  87,  53],\n",
      "       [ 63,  65, 107, ..., 117, 101,  74],\n",
      "       [ 67,  82, 103, ..., 120, 112,  93]], dtype=uint8), 0], [array([[128, 126, 123, ...,  89,  91,  90],\n",
      "       [127, 128, 135, ...,  91,  95,  93],\n",
      "       [128, 132, 131, ...,  97,  96,  97],\n",
      "       ...,\n",
      "       [166, 185, 199, ...,  42,  33,  32],\n",
      "       [168, 184, 202, ...,  46,  34,  32],\n",
      "       [168, 185, 201, ...,  48,  35,  32]], dtype=uint8), 0], [array([[ 0,  5,  5, ...,  0,  0,  0],\n",
      "       [ 0, 33, 42, ...,  1,  0,  0],\n",
      "       [ 0, 71, 66, ...,  3,  2,  1],\n",
      "       ...,\n",
      "       [ 0,  0,  0, ...,  0,  0,  0],\n",
      "       [ 0,  0,  0, ...,  0,  0,  0],\n",
      "       [ 0,  0,  0, ...,  0,  0,  0]], dtype=uint8), 0], [array([[ 12,  16, 148, ...,  13,  13,  11],\n",
      "       [ 13, 255,  14, ...,  13,  13,  13],\n",
      "       [  1, 201, 254, ...,  12,  12,  13],\n",
      "       ...,\n",
      "       [ 10,  10,  10, ...,  10,  11,  11],\n",
      "       [ 10,  10,  10, ...,  10,  11,  11],\n",
      "       [ 10,  10,  10, ...,  11,  11,  11]], dtype=uint8), 0], [array([[ 0,  0,  0, ...,  0,  0,  0],\n",
      "       [ 0,  0,  0, ...,  0,  0,  0],\n",
      "       [ 1,  2,  2, ...,  0,  0,  0],\n",
      "       ...,\n",
      "       [32, 33, 39, ...,  0,  0,  0],\n",
      "       [31, 37, 37, ...,  0,  0,  0],\n",
      "       [32, 39, 32, ...,  0,  0,  0]], dtype=uint8), 0], [array([[ 5,  5,  5, ...,  5,  5,  5],\n",
      "       [ 5,  5,  5, ...,  5,  5,  5],\n",
      "       [ 5,  5,  5, ...,  5,  5,  5],\n",
      "       ...,\n",
      "       [ 5,  5, 10, ...,  5,  5,  5],\n",
      "       [ 5,  5,  8, ...,  5,  5,  5],\n",
      "       [ 5,  5,  6, ...,  5,  5,  5]], dtype=uint8), 0], [array([[ 1,  1,  0, ...,  0,  0,  0],\n",
      "       [ 0,  1,  0, ...,  0,  0,  0],\n",
      "       [ 0,  0,  7, ...,  0,  0,  0],\n",
      "       ...,\n",
      "       [11, 26, 51, ...,  0,  0,  0],\n",
      "       [10, 27, 44, ...,  0,  0,  0],\n",
      "       [10, 26, 39, ...,  0,  0,  0]], dtype=uint8), 0], [array([[  1,   0,   0, ...,  14,  14,  14],\n",
      "       [  1,   0,   0, ...,  14,  14,  15],\n",
      "       [  1,   4, 251, ...,  15,  15,  16],\n",
      "       ...,\n",
      "       [  6,  20,  28, ...,  11,  11,  11],\n",
      "       [  3,  20,  28, ...,  11,  11,  11],\n",
      "       [  0,  21,  31, ...,  11,  11,  11]], dtype=uint8), 0], [array([[ 8,  8,  8, ...,  8,  8,  8],\n",
      "       [ 8,  8,  8, ...,  8,  8,  8],\n",
      "       [ 8,  8, 63, ...,  8,  8,  8],\n",
      "       ...,\n",
      "       [11, 18, 25, ...,  9,  8,  8],\n",
      "       [11, 11, 20, ...,  8,  8,  8],\n",
      "       [10, 10, 47, ...,  7,  8,  8]], dtype=uint8), 0], [array([[  0,   0,   0, ..., 110, 164, 158],\n",
      "       [  0,   0,   0, ...,  84, 152, 154],\n",
      "       [  0,   0,   0, ...,  65, 144, 148],\n",
      "       ...,\n",
      "       [  0,   0,   0, ..., 157, 146, 122],\n",
      "       [  0,   0,   0, ..., 162, 145, 125],\n",
      "       [  0,   0,   0, ..., 163, 149, 132]], dtype=uint8), 0], [array([[21, 24, 27, ..., 74, 66, 51],\n",
      "       [21, 22, 22, ..., 25, 23, 23],\n",
      "       [22, 20, 24, ..., 43, 37, 27],\n",
      "       ...,\n",
      "       [22, 22, 22, ..., 33, 25, 22],\n",
      "       [22, 22, 22, ..., 73, 47, 23],\n",
      "       [22, 22, 22, ..., 80, 61, 24]], dtype=uint8), 0], [array([[ 4,  4,  4, ...,  4,  4,  4],\n",
      "       [ 4,  4,  5, ...,  4,  4,  4],\n",
      "       [ 4,  4, 23, ...,  4,  4,  4],\n",
      "       ...,\n",
      "       [ 4,  6, 21, ...,  4,  4,  4],\n",
      "       [ 4,  5, 11, ...,  4,  4,  4],\n",
      "       [ 4,  5, 13, ...,  4,  4,  4]], dtype=uint8), 0], [array([[ 7,  6,  6, ...,  6,  7,  7],\n",
      "       [ 7,  7,  7, ...,  6,  7,  7],\n",
      "       [ 7,  7,  7, ...,  6,  7,  7],\n",
      "       ...,\n",
      "       [ 6,  6,  6, ...,  6,  7,  7],\n",
      "       [ 6,  6,  6, ..., 10,  7,  7],\n",
      "       [ 6,  6,  6, ..., 19,  7,  7]], dtype=uint8), 0], [array([[  0,   0,   0, ...,  87,  89, 102],\n",
      "       [  1,   1,   2, ..., 100,  98, 107],\n",
      "       [ 15,  19,  20, ..., 109, 115, 118],\n",
      "       ...,\n",
      "       [  0,   0,   0, ..., 162, 149, 134],\n",
      "       [  0,   0,   0, ..., 172, 152, 136],\n",
      "       [  0,   0,   0, ..., 169, 162, 137]], dtype=uint8), 0], [array([[ 26,  14,   8, ...,   8,   0,   6],\n",
      "       [ 16,  12,   8, ...,   1,   0,   1],\n",
      "       [ 18,  11,   1, ...,   0,   1,   0],\n",
      "       ...,\n",
      "       [162, 155, 163, ..., 182, 178, 181],\n",
      "       [165, 153, 153, ..., 188, 175, 174],\n",
      "       [155, 144, 151, ..., 191, 187, 187]], dtype=uint8), 0], [array([[ 54,  55,  68, ...,   4,   0,   4],\n",
      "       [ 63,  63,  63, ...,  17,  21,  21],\n",
      "       [ 71,  73,  66, ...,  58,  16,  29],\n",
      "       ...,\n",
      "       [ 81, 104, 119, ..., 135, 142, 144],\n",
      "       [ 81, 106, 127, ..., 146, 144, 144],\n",
      "       [ 76, 103, 123, ..., 139, 139, 145]], dtype=uint8), 0], [array([[ 19,  16,  18, ...,  19,  19,  48],\n",
      "       [ 17,  16,  18, ...,  19,  17,  36],\n",
      "       [ 13,  17,  18, ...,  19,  17,  35],\n",
      "       ...,\n",
      "       [172, 176, 181, ..., 115, 117, 160],\n",
      "       [168, 175, 182, ..., 109, 116, 142],\n",
      "       [163, 179, 179, ..., 107, 113, 116]], dtype=uint8), 0], [array([[ 17,  20,  18, ...,  48,  51,  23],\n",
      "       [ 19,  19,  17, ...,  14,  22,  16],\n",
      "       [ 12,  12,  13, ...,  19,  30,  31],\n",
      "       ...,\n",
      "       [ 41,  67,  84, ...,  10,  10,  11],\n",
      "       [ 57,  83, 101, ...,  10,  12,  13],\n",
      "       [ 84, 110, 128, ...,  14,  18,  20]], dtype=uint8), 0], [array([[ 10,   8,   8, ...,  10,  16,  84],\n",
      "       [  8,   7,   8, ...,  20,  11,  74],\n",
      "       [  7,  18,  28, ...,  37,  64, 145],\n",
      "       ...,\n",
      "       [ 13,  13,  10, ..., 123, 121, 152],\n",
      "       [ 31,   8, 111, ..., 138, 143, 140],\n",
      "       [181,  18,  18, ..., 176, 176, 180]], dtype=uint8), 0], [array([[ 30,  30,  23, ...,  48,  48,  48],\n",
      "       [ 35,  26,  28, ...,  46,  47,  49],\n",
      "       [ 32,  24,  21, ...,  54,  37, 222],\n",
      "       ...,\n",
      "       [ 28,  28,  34, ...,  82,  57,  31],\n",
      "       [ 26,  26,  28, ...,  86,  54,  32],\n",
      "       [ 37,  25,  32, ...,  87,  62,  29]], dtype=uint8), 0], [array([[ 11,  11,   9, ...,  10,  12,   9],\n",
      "       [  9,  10,   6, ...,   8,  10,   9],\n",
      "       [ 13,  11,  12, ...,   5,   9,  10],\n",
      "       ...,\n",
      "       [102, 112, 135, ..., 122, 101,  91],\n",
      "       [106, 118, 130, ..., 123,  93,  91],\n",
      "       [103, 119, 133, ..., 117,  97,  86]], dtype=uint8), 0], [array([[ 48,  51,  55, ...,  52,  50,  53],\n",
      "       [ 48,  43,  70, ...,  49,  50,  47],\n",
      "       [ 50,  89, 102, ...,  50,  47,  49],\n",
      "       ...,\n",
      "       [152, 158, 167, ..., 191, 178, 138],\n",
      "       [151, 155, 174, ..., 187, 166, 135],\n",
      "       [139, 161, 158, ..., 189, 158, 106]], dtype=uint8), 0], [array([[ 64, 247,  52, ...,  39,  51,  52],\n",
      "       [ 65,  62,  51, ...,  84,  52,  55],\n",
      "       [ 51,  46,  41, ...,  54,  52,  51],\n",
      "       ...,\n",
      "       [181, 187, 190, ..., 187, 178, 172],\n",
      "       [189, 188, 181, ..., 187, 190, 183],\n",
      "       [187, 195, 190, ..., 194, 189, 187]], dtype=uint8), 0], [array([[ 65,  63,  62, ...,  98,  96, 100],\n",
      "       [ 69,  63,  61, ..., 102,  95,  97],\n",
      "       [ 64,  66,  58, ...,  97,  96, 101],\n",
      "       ...,\n",
      "       [174, 175, 180, ...,  48,  39,  35],\n",
      "       [171, 169, 177, ...,  43,  29,  26],\n",
      "       [172, 179, 178, ...,  39,  30,  31]], dtype=uint8), 0], [array([[ 60,  63,  69, ...,  68,  66,  67],\n",
      "       [ 62,  60,  65, ...,  80,  62,  66],\n",
      "       [ 60,  62,  62, ..., 254,  64,  65],\n",
      "       ...,\n",
      "       [ 87,  79,  80, ..., 109, 116,  83],\n",
      "       [ 87,  80,  73, ..., 126, 116,  89],\n",
      "       [ 78,  77,  72, ..., 116, 120,  98]], dtype=uint8), 0], [array([[  1,   1,   1, ...,  20,  22,  24],\n",
      "       [  1,   1,   1, ...,   3,   3,   2],\n",
      "       [  1,   1,   1, ...,   1,   1,   1],\n",
      "       ...,\n",
      "       [196, 193, 196, ..., 198, 195, 188],\n",
      "       [193, 193, 195, ..., 195, 193, 184],\n",
      "       [198, 194, 194, ..., 195, 193, 179]], dtype=uint8), 0], [array([[  1,   1,   3, ...,  31,  19,  95],\n",
      "       [ 11,  10,  15, ...,  85,  56, 254],\n",
      "       [ 21,  21,   4, ...,   8,   3,  12],\n",
      "       ...,\n",
      "       [136, 136, 152, ..., 179, 182, 180],\n",
      "       [117, 128, 142, ..., 176, 178, 177],\n",
      "       [116, 139, 145, ..., 183, 184, 184]], dtype=uint8), 0], [array([[ 35,  40,  44, ...,  37,  30,  37],\n",
      "       [ 30,  36,  33, ...,  14,  16,  20],\n",
      "       [ 37,  29,  30, ...,  17,  12,  23],\n",
      "       ...,\n",
      "       [128, 160, 160, ..., 121,  91,  67],\n",
      "       [115, 157, 161, ...,  93,  85,  64],\n",
      "       [146, 155, 157, ...,  85,  75,  71]], dtype=uint8), 0], [array([[ 15,  18,  13, ...,  17,  16,  16],\n",
      "       [ 24,  12,  16, ...,   7,  22,  12],\n",
      "       [ 19,  18,   9, ...,  13,  14,  14],\n",
      "       ...,\n",
      "       [164, 172, 176, ..., 126,  92, 109],\n",
      "       [169, 178, 183, ..., 120,  89, 104],\n",
      "       [176, 180, 181, ..., 114,  87, 106]], dtype=uint8), 0], [array([[  3,   2,   2, ...,  34,  26,  22],\n",
      "       [  2,   1,   1, ...,  45,  42,  33],\n",
      "       [  1,   1,   1, ...,  55,  50,  42],\n",
      "       ...,\n",
      "       [176, 199, 252, ..., 200, 193, 189],\n",
      "       [178, 186, 200, ..., 211, 188, 180],\n",
      "       [191, 189, 206, ..., 217, 201, 197]], dtype=uint8), 0], [array([[ 64,  63,  71, ...,  59,  52,  41],\n",
      "       [ 85,  68,  69, ...,  23,  53,  57],\n",
      "       [ 97,  95,  90, ...,  53,  52,  46],\n",
      "       ...,\n",
      "       [182, 189, 196, ...,  71,  46,  29],\n",
      "       [204, 206, 207, ...,  80,  61,  37],\n",
      "       [212, 215, 216, ..., 103,  86,  52]], dtype=uint8), 0], [array([[ 10,  13,  16, ...,   3,   3,   4],\n",
      "       [ 15,  22,  31, ...,   4,   4,   4],\n",
      "       [ 36,  31,  32, ...,   4,   4,   4],\n",
      "       ...,\n",
      "       [158, 173, 187, ..., 160, 169, 178],\n",
      "       [174, 187, 193, ..., 178, 179, 173],\n",
      "       [180, 189, 199, ..., 185, 191, 190]], dtype=uint8), 0], [array([[ 51,  51,  48, ...,  35,  37,  36],\n",
      "       [ 48,  49,  48, ...,  35,  36,  35],\n",
      "       [ 47,  47,  48, ...,  37,  35,  37],\n",
      "       ...,\n",
      "       [191, 203, 204, ..., 195, 187, 188],\n",
      "       [196, 199, 200, ..., 188, 188, 186],\n",
      "       [201, 202, 198, ..., 185, 188, 188]], dtype=uint8), 0], [array([[ 31,  34,  35, ...,  18,  16,  20],\n",
      "       [ 38,  34,  33, ...,  18,  19,  21],\n",
      "       [ 35,  32,  32, ...,  21,  21,  19],\n",
      "       ...,\n",
      "       [ 18,  20,  43, ..., 142, 141, 136],\n",
      "       [ 19,  22,  41, ..., 137, 130, 144],\n",
      "       [ 20,  27,  41, ..., 139, 137, 138]], dtype=uint8), 0], [array([[ 7,  7,  7, ...,  5,  6,  6],\n",
      "       [ 2,  2,  1, ...,  1,  1,  1],\n",
      "       [ 2,  4,  2, ...,  0,  0,  1],\n",
      "       ...,\n",
      "       [ 4,  0,  0, ..., 93,  8,  0],\n",
      "       [ 6,  1,  0, ..., 81,  0,  0],\n",
      "       [ 8,  1,  1, ...,  4,  0,  0]], dtype=uint8), 0], [array([[  1,   1,   0, ...,  87,  87, 148],\n",
      "       [  1,   1,   1, ...,  86,  72, 125],\n",
      "       [  1,   1,   1, ...,  77,  84, 130],\n",
      "       ...,\n",
      "       [ 14,  30,  64, ...,   8,   4,   1],\n",
      "       [ 19,  26,  56, ...,   3,   3,   1],\n",
      "       [ 11,  22,  52, ...,   5,   1,   1]], dtype=uint8), 0], [array([[254,  19,  18, ...,  15,  10,   6],\n",
      "       [254,  33,  18, ...,  15,  10,   4],\n",
      "       [252,  18,  81, ...,  16,   5,   4],\n",
      "       ...,\n",
      "       [  0,   6,  18, ...,   0,   0,   0],\n",
      "       [  0,   6,  19, ...,   0,   0,   0],\n",
      "       [  0,   7,  21, ...,   0,   0,   0]], dtype=uint8), 0], [array([[  1,   1,   1, ...,   1,   1,   1],\n",
      "       [  0,   0,   1, ...,  16,   9,   6],\n",
      "       [  1,   3,   7, ...,  27,  21,  18],\n",
      "       ...,\n",
      "       [197, 206, 207, ..., 229, 218, 219],\n",
      "       [201, 199, 204, ..., 229, 226, 221],\n",
      "       [194, 201, 209, ..., 226, 226, 220]], dtype=uint8), 0], [array([[ 3,  2,  2, ...,  0,  2,  2],\n",
      "       [ 1,  1,  1, ...,  1,  1,  1],\n",
      "       [ 1,  1,  1, ...,  0,  1,  3],\n",
      "       ...,\n",
      "       [ 1,  1,  1, ..., 48, 26,  6],\n",
      "       [ 1,  1,  1, ..., 45, 24,  6],\n",
      "       [ 1,  1,  1, ..., 37, 21,  2]], dtype=uint8), 0], [array([[136, 105, 103, ..., 161, 153, 155],\n",
      "       [147, 173, 115, ..., 146, 150, 147],\n",
      "       [139, 141, 134, ..., 143, 143, 142],\n",
      "       ...,\n",
      "       [163, 166, 169, ...,   4,   9,  12],\n",
      "       [163, 167, 169, ...,   3,   7,  11],\n",
      "       [165, 168, 171, ...,   3,   8,  14]], dtype=uint8), 0], [array([[20, 32, 18, ...,  7, 10, 10],\n",
      "       [11, 26, 39, ..., 12,  7, 21],\n",
      "       [19, 28, 28, ..., 30, 28, 40],\n",
      "       ...,\n",
      "       [45, 66, 94, ..., 80, 69, 68],\n",
      "       [48, 64, 93, ..., 81, 72, 66],\n",
      "       [48, 68, 93, ..., 75, 75, 61]], dtype=uint8), 0], [array([[14, 13, 12, ...,  0,  0,  0],\n",
      "       [13, 12, 11, ...,  0,  0,  0],\n",
      "       [13, 11, 11, ...,  0,  0,  0],\n",
      "       ...,\n",
      "       [ 0,  0,  0, ...,  0,  0,  0],\n",
      "       [ 0,  0,  0, ...,  0,  0,  0],\n",
      "       [ 0,  0,  0, ...,  0,  0,  0]], dtype=uint8), 0], [array([[  0,   0,   0, ...,   0,   0,   0],\n",
      "       [  0,   0,   0, ...,   0,   0,   0],\n",
      "       [  0,   0,   0, ...,   0,   0,   0],\n",
      "       ...,\n",
      "       [  0,   0,   0, ..., 121, 113, 108],\n",
      "       [  0,   0,   0, ..., 123, 119, 107],\n",
      "       [  0,   0,   0, ..., 127, 122, 110]], dtype=uint8), 0], [array([[ 33,  31,  42, ..., 121, 120, 113],\n",
      "       [ 71,  77,  73, ..., 121, 121, 122],\n",
      "       [ 53,  68,  58, ..., 119, 121, 122],\n",
      "       ...,\n",
      "       [ 25,  31,  48, ..., 118,  88,  64],\n",
      "       [ 25,  29,  42, ..., 118,  90,  61],\n",
      "       [ 26,  28,  40, ..., 114,  89,  61]], dtype=uint8), 0], [array([[ 0,  0,  0, ...,  0,  0,  0],\n",
      "       [ 0,  0,  0, ...,  0,  0,  0],\n",
      "       [ 0,  0,  0, ...,  0,  0,  0],\n",
      "       ...,\n",
      "       [ 0,  0,  0, ...,  0,  0,  0],\n",
      "       [ 0,  0,  0, ...,  0,  0,  0],\n",
      "       [ 5,  0,  0, ...,  2,  2, 18]], dtype=uint8), 0], [array([[ 55,  67, 136, ...,  71,  61,  55],\n",
      "       [ 68, 109, 100, ...,  88,  96,  77],\n",
      "       [ 76, 118, 122, ..., 110, 111, 108],\n",
      "       ...,\n",
      "       [ 81,  46,  21, ..., 165, 161, 156],\n",
      "       [ 75,  38,  14, ..., 163, 164, 161],\n",
      "       [ 74,  33,   2, ..., 169, 168, 165]], dtype=uint8), 0], [array([[ 65,  70,  96, ...,  97, 112, 101],\n",
      "       [ 76,  88,  93, ...,  99, 118, 113],\n",
      "       [112, 122, 130, ...,  94, 106, 118],\n",
      "       ...,\n",
      "       [  0,   0,  49, ..., 125, 128,  92],\n",
      "       [  0,   0,  67, ..., 128, 125,  96],\n",
      "       [  0,   1,  60, ..., 129, 127,  99]], dtype=uint8), 0], [array([[  3,  14,  19, ..., 146, 130, 113],\n",
      "       [  8,  17,  31, ..., 122, 111, 117],\n",
      "       [ 20,  28,  48, ..., 143, 141, 143],\n",
      "       ...,\n",
      "       [  3,   0,   5, ...,   0,   0,   0],\n",
      "       [ 64,   0,  11, ...,   0,   0,   0],\n",
      "       [207,   5,  20, ...,   0,   0,   1]], dtype=uint8), 0], [array([[  7,  29,  42, ...,   1,   1,   0],\n",
      "       [ 18,  41,  46, ...,  12,  10,  12],\n",
      "       [ 29,  43,  49, ...,  21,  20,  22],\n",
      "       ...,\n",
      "       [ 39,  67,  85, ...,  38,   0,   0],\n",
      "       [ 42,  78,  90, ...,  31,   0,   0],\n",
      "       [ 51,  81, 126, ...,  24,   0,   0]], dtype=uint8), 0], [array([[  0,   0,   0, ...,   0,   0,   0],\n",
      "       [  0,   0,   0, ...,   0,   0,   0],\n",
      "       [  0,   0,   0, ...,   0,   3,   1],\n",
      "       ...,\n",
      "       [ 79, 115, 144, ..., 110,  91,  88],\n",
      "       [ 79, 111, 144, ..., 102,  93,  84],\n",
      "       [ 78, 111, 139, ..., 103,  97,  84]], dtype=uint8), 0], [array([[  1,   1,   1, ...,   1,   1,   1],\n",
      "       [  1,   1,   1, ...,   1,   1,   1],\n",
      "       [  1,   1,   1, ...,   0,   1,   1],\n",
      "       ...,\n",
      "       [ 41,  31,  18, ...,  50,  63,  65],\n",
      "       [ 39,  29,  16, ..., 107, 129, 141],\n",
      "       [ 38,  26,  15, ..., 206, 214, 215]], dtype=uint8), 0], [array([[ 0,  0,  0, ...,  0,  0,  0],\n",
      "       [ 0,  0,  0, ...,  0,  0,  0],\n",
      "       [ 0,  0,  0, ...,  0,  0,  0],\n",
      "       ...,\n",
      "       [38, 31,  1, ..., 60, 17, 59],\n",
      "       [21, 37,  7, ..., 54, 31, 36],\n",
      "       [17, 33, 19, ..., 50, 52, 11]], dtype=uint8), 0], [array([[36, 36, 35, ..., 20, 20, 20],\n",
      "       [21, 21, 20, ..., 20, 20, 20],\n",
      "       [15, 15, 15, ..., 20, 20, 20],\n",
      "       ...,\n",
      "       [ 7, 12,  2, ..., 82, 73, 50],\n",
      "       [ 7,  8,  3, ..., 84, 71, 46],\n",
      "       [ 7,  7,  3, ..., 83, 69, 42]], dtype=uint8), 0], [array([[  0,   0,   0, ...,   0,   0,   0],\n",
      "       [  0,   0,   0, ...,   0,   0,   0],\n",
      "       [  0,   0,   0, ...,   0,   3,   1],\n",
      "       ...,\n",
      "       [ 80, 114, 143, ..., 113,  93,  88],\n",
      "       [ 75, 110, 143, ..., 101,  94,  84],\n",
      "       [ 79, 112, 142, ..., 105,  98,  84]], dtype=uint8), 0], [array([[ 57,  69,  83, ...,  19,  19,  19],\n",
      "       [ 66,  80,  94, ...,  17,  18,  18],\n",
      "       [ 72,  93, 122, ...,  14,  15,  16],\n",
      "       ...,\n",
      "       [ 75,  40,   0, ...,  55,  71,  89],\n",
      "       [ 80,  42,   1, ...,  49,  67,  87],\n",
      "       [ 82,  35,   0, ...,  48,  63,  90]], dtype=uint8), 0], [array([[ 24,  25,  36, ...,  85, 125,  86],\n",
      "       [ 21,  27,  34, ..., 134, 104,  87],\n",
      "       [ 20,  26,  33, ..., 130,  93,  96],\n",
      "       ...,\n",
      "       [  0,   0,   0, ...,   6,   5,   5],\n",
      "       [  0,   0,   0, ...,   5,   4,   5],\n",
      "       [  0,   0,   0, ...,   6,   6,   6]], dtype=uint8), 1], [array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 3, 2, 1],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 5, 0, 0],\n",
      "       [0, 0, 0, ..., 7, 0, 0],\n",
      "       [0, 0, 0, ..., 6, 0, 0]], dtype=uint8), 1], [array([[13, 10,  4, ..., 34, 33, 43],\n",
      "       [12,  8,  6, ..., 39, 39, 42],\n",
      "       [ 8,  8,  2, ..., 39, 41, 43],\n",
      "       ...,\n",
      "       [22, 14, 24, ..., 19, 23, 30],\n",
      "       [18, 17, 20, ..., 27, 27, 25],\n",
      "       [21, 20, 18, ..., 29, 17, 37]], dtype=uint8), 1], [array([[19, 12, 13, ..., 28, 29, 26],\n",
      "       [17, 14, 14, ..., 28, 22, 30],\n",
      "       [18, 14, 16, ..., 27, 27, 30],\n",
      "       ...,\n",
      "       [11,  9, 14, ..., 28, 32, 31],\n",
      "       [18, 10, 14, ..., 28, 26, 28],\n",
      "       [ 9, 11,  9, ..., 17, 21, 31]], dtype=uint8), 1], [array([[ 5,  7,  6, ...,  9,  9,  8],\n",
      "       [ 5,  3,  4, ...,  5,  8,  8],\n",
      "       [ 3,  3,  2, ...,  5,  7,  6],\n",
      "       ...,\n",
      "       [ 7,  6,  6, ..., 10,  9, 10],\n",
      "       [ 7,  5,  5, ...,  9,  9,  9],\n",
      "       [ 6,  7,  6, ..., 10,  9,  9]], dtype=uint8), 1], [array([[22, 22, 22, ..., 10, 14, 11],\n",
      "       [22, 23, 23, ...,  5,  5,  7],\n",
      "       [22, 22, 17, ...,  1,  8,  4],\n",
      "       ...,\n",
      "       [20, 17, 20, ..., 13, 13, 15],\n",
      "       [22, 20, 21, ..., 10, 13, 14],\n",
      "       [20, 20, 22, ..., 13, 16, 12]], dtype=uint8), 1], [array([[15, 22, 28, ..., 62, 48, 40],\n",
      "       [12, 20, 27, ..., 53, 45, 32],\n",
      "       [11, 18, 27, ..., 54, 45, 36],\n",
      "       ...,\n",
      "       [ 0,  1,  0, ..., 10, 11, 11],\n",
      "       [ 0,  1,  0, ..., 10, 11, 11],\n",
      "       [ 0,  1,  0, ..., 10, 11, 11]], dtype=uint8), 1], [array([[  0,   0,   0, ...,   0,   0,   0],\n",
      "       [  0,   0,   0, ...,   0,   0,   0],\n",
      "       [  0,   0,   0, ...,   0,   0,   0],\n",
      "       ...,\n",
      "       [ 42,  64,  91, ..., 103,  87,  65],\n",
      "       [ 50,  64,  87, ...,  94,  76,  59],\n",
      "       [ 50,  57,  78, ...,  91,  82,  54]], dtype=uint8), 1], [array([[16, 15, 14, ...,  9, 11, 12],\n",
      "       [14, 14, 10, ...,  8,  5,  7],\n",
      "       [13, 11, 10, ...,  3,  6,  4],\n",
      "       ...,\n",
      "       [11, 11, 10, ..., 14, 15, 15],\n",
      "       [11, 10, 12, ..., 15, 15, 14],\n",
      "       [12, 12, 12, ..., 12, 14, 15]], dtype=uint8), 1], [array([[23,  9,  5, ..., 17, 20,  0],\n",
      "       [21,  4,  3, ..., 18, 18,  0],\n",
      "       [19,  8,  1, ..., 18, 18,  0],\n",
      "       ...,\n",
      "       [ 0, 19,  8, ..., 24, 21, 20],\n",
      "       [ 0, 17,  7, ..., 23, 22, 19],\n",
      "       [ 0, 19, 15, ..., 23, 20, 19]], dtype=uint8), 1], [array([[ 4,  3,  4, ...,  4,  4,  0],\n",
      "       [ 4,  4,  4, ...,  4,  4,  0],\n",
      "       [ 5,  4,  4, ...,  3,  3,  0],\n",
      "       ...,\n",
      "       [ 0,  5,  5, ...,  7, 10, 10],\n",
      "       [ 0,  5,  5, ..., 10, 10, 10],\n",
      "       [ 0,  5,  6, ...,  6, 10, 10]], dtype=uint8), 1], [array([[10, 14, 13, ..., 20, 18, 18],\n",
      "       [12, 14, 12, ..., 20, 18, 17],\n",
      "       [12, 10, 16, ..., 21, 19, 20],\n",
      "       ...,\n",
      "       [ 3,  4,  6, ..., 16, 16, 16],\n",
      "       [ 1,  4,  6, ..., 15, 16, 13],\n",
      "       [ 2,  4,  4, ..., 17, 18, 16]], dtype=uint8), 1], [array([[ 4,  3,  4, ...,  4,  4,  0],\n",
      "       [ 4,  4,  4, ...,  4,  4,  0],\n",
      "       [ 5,  4,  4, ...,  3,  3,  0],\n",
      "       ...,\n",
      "       [ 0,  5,  5, ...,  7, 10, 10],\n",
      "       [ 0,  5,  5, ..., 10, 10, 10],\n",
      "       [ 0,  5,  6, ...,  6, 10, 10]], dtype=uint8), 1], [array([[4, 3, 4, ..., 5, 5, 8],\n",
      "       [4, 4, 1, ..., 5, 4, 8],\n",
      "       [5, 4, 3, ..., 5, 5, 8],\n",
      "       ...,\n",
      "       [4, 3, 0, ..., 7, 7, 5],\n",
      "       [4, 2, 2, ..., 7, 7, 4],\n",
      "       [5, 4, 1, ..., 7, 7, 5]], dtype=uint8), 1], [array([[10,  8,  8, ...,  2,  3,  3],\n",
      "       [ 8, 10, 10, ...,  8,  1,  2],\n",
      "       [ 9,  9, 10, ..., 18, 17, 16],\n",
      "       ...,\n",
      "       [ 9, 10, 12, ..., 14, 14, 13],\n",
      "       [10, 12,  7, ..., 13, 14, 14],\n",
      "       [13, 11,  8, ..., 13, 15, 15]], dtype=uint8), 1], [array([[10, 13, 16, ..., 17, 20, 22],\n",
      "       [ 9, 13, 16, ..., 20, 19, 20],\n",
      "       [ 8, 11, 19, ..., 16, 19, 21],\n",
      "       ...,\n",
      "       [26, 25, 24, ..., 14, 20, 17],\n",
      "       [27, 26, 26, ...,  9, 19, 18],\n",
      "       [27, 26, 26, ..., 25, 25, 26]], dtype=uint8), 1], [array([[ 8,  9,  8, ...,  4,  5,  5],\n",
      "       [ 8,  7,  7, ...,  1,  2,  2],\n",
      "       [ 5,  5,  5, ..., 11, 10, 10],\n",
      "       ...,\n",
      "       [ 6,  5,  5, ...,  7,  9,  9],\n",
      "       [ 6,  7,  5, ...,  5,  8, 10],\n",
      "       [ 8,  5,  4, ...,  7,  9,  9]], dtype=uint8), 1], [array([[25, 24, 22, ..., 17, 22, 16],\n",
      "       [26, 23, 20, ...,  9, 17, 12],\n",
      "       [25, 22, 22, ..., 12,  8, 19],\n",
      "       ...,\n",
      "       [15, 17, 15, ..., 20, 21, 23],\n",
      "       [17, 15, 12, ..., 23, 22, 24],\n",
      "       [15, 13, 15, ..., 25, 23, 25]], dtype=uint8), 1], [array([[ 4,  8, 11, ..., 24, 13,  8],\n",
      "       [12, 14, 12, ..., 26, 19, 12],\n",
      "       [ 1, 11, 12, ..., 23, 20, 12],\n",
      "       ...,\n",
      "       [ 2,  3,  4, ..., 20, 17, 20],\n",
      "       [ 0,  2,  4, ..., 20, 17, 21],\n",
      "       [ 0,  2,  1, ..., 20, 20, 12]], dtype=uint8), 1], [array([[ 21,  26,  31, ...,  40,  40,  61],\n",
      "       [ 27,  34,  36, ...,  52,  83,  92],\n",
      "       [ 37,  42,  49, ...,  97, 106,  96],\n",
      "       ...,\n",
      "       [  0,   0,   0, ...,   0,   0,   0],\n",
      "       [  0,   0,   0, ...,   0,   0,   0],\n",
      "       [  0,   0,   0, ...,   0,   0,   0]], dtype=uint8), 1], [array([[ 27,  29,  33, ...,  26,  26,  25],\n",
      "       [ 33,  35,  42, ...,  97, 115,  60],\n",
      "       [ 38,  49,  63, ...,  98,  81,  91],\n",
      "       ...,\n",
      "       [  4,   5,   6, ...,  18,  17,  17],\n",
      "       [  7,   5,   7, ...,  19,  17,  15],\n",
      "       [  7,   6,   4, ...,   6,  17,  18]], dtype=uint8), 1], [array([[22, 19, 16, ..., 18, 21, 18],\n",
      "       [18, 15, 14, ..., 19, 19, 22],\n",
      "       [16, 14, 11, ..., 21, 21, 18],\n",
      "       ...,\n",
      "       [15, 20, 14, ..., 22, 21, 22],\n",
      "       [17, 18, 19, ..., 21, 21, 21],\n",
      "       [17, 18, 18, ..., 21, 20, 22]], dtype=uint8), 1], [array([[12,  8,  8, ..., 20, 21, 22],\n",
      "       [ 8, 11, 10, ..., 20, 23, 22],\n",
      "       [11,  9, 10, ..., 19, 22, 20],\n",
      "       ...,\n",
      "       [20, 17, 16, ..., 29, 27, 30],\n",
      "       [18, 18, 18, ..., 29, 25, 32],\n",
      "       [20, 24, 17, ..., 30, 31, 32]], dtype=uint8), 1], [array([[17, 22, 18, ..., 11, 21, 13],\n",
      "       [24, 23, 21, ..., 15, 18, 12],\n",
      "       [23, 21, 18, ..., 10, 12, 12],\n",
      "       ...,\n",
      "       [18, 20, 14, ..., 23, 21, 24],\n",
      "       [18, 16, 22, ..., 20, 22, 23],\n",
      "       [21, 21, 19, ..., 22, 22, 25]], dtype=uint8), 1], [array([[ 7,  8,  7, ..., 17, 15, 12],\n",
      "       [ 7,  8,  8, ..., 43, 31, 31],\n",
      "       [ 7,  7,  8, ..., 54, 59, 54],\n",
      "       ...,\n",
      "       [ 5,  6,  6, ...,  8,  9,  9],\n",
      "       [ 5,  6,  6, ...,  8,  9, 10],\n",
      "       [ 6,  5,  6, ...,  8, 10,  9]], dtype=uint8), 1], [array([[24, 24, 24, ..., 26, 26,  0],\n",
      "       [24, 24, 24, ..., 26, 26,  0],\n",
      "       [24, 24, 24, ..., 26, 26,  0],\n",
      "       ...,\n",
      "       [ 6, 10, 10, ..., 19, 24, 24],\n",
      "       [ 6, 10, 10, ..., 20, 20, 24],\n",
      "       [ 6, 13, 10, ..., 24, 24, 24]], dtype=uint8), 1], [array([[0, 0, 0, ..., 5, 0, 0],\n",
      "       [0, 0, 0, ..., 3, 0, 0],\n",
      "       [0, 0, 0, ..., 3, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8), 1], [array([[ 3,  3,  3, ...,  3,  4,  0],\n",
      "       [ 4,  3,  4, ...,  0,  3,  0],\n",
      "       [ 5,  5,  5, ...,  3,  4,  0],\n",
      "       ...,\n",
      "       [ 0,  8, 13, ..., 21, 21, 21],\n",
      "       [ 0, 13, 13, ..., 21, 20, 21],\n",
      "       [ 0, 13, 13, ..., 21, 20, 21]], dtype=uint8), 1], [array([[ 55,  74,  86, ...,  95,  99,  86],\n",
      "       [ 51,  74,  84, ..., 100,  94,  90],\n",
      "       [ 49,  69,  78, ...,  98,  91,  83],\n",
      "       ...,\n",
      "       [ 35,  35,  35, ...,  32,  32,  33],\n",
      "       [ 36,  36,  35, ...,  31,  34,  34],\n",
      "       [ 35,  35,  35, ...,  35,  33,  31]], dtype=uint8), 1], [array([[ 8,  9,  7, ..., 10,  9, 10],\n",
      "       [ 9, 10,  8, ...,  9, 10, 10],\n",
      "       [ 3,  2,  7, ...,  7,  8, 10],\n",
      "       ...,\n",
      "       [10,  9,  8, ..., 10, 10, 11],\n",
      "       [ 9,  9,  9, ...,  7, 11, 10],\n",
      "       [ 8,  8,  8, ..., 11, 10, 11]], dtype=uint8), 1], [array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8), 1], [array([[24, 18, 12, ..., 32, 26, 24],\n",
      "       [29, 18, 11, ..., 22, 25, 26],\n",
      "       [27,  8,  5, ..., 28, 19, 36],\n",
      "       ...,\n",
      "       [10, 16, 11, ..., 29, 33, 29],\n",
      "       [14, 12, 15, ..., 33, 30, 37],\n",
      "       [ 8, 12, 11, ..., 31, 34, 30]], dtype=uint8), 1], [array([[14, 15, 13, ..., 23, 26, 40],\n",
      "       [14, 14, 11, ..., 24, 28, 43],\n",
      "       [14, 15, 14, ..., 23, 27, 41],\n",
      "       ...,\n",
      "       [13, 11, 10, ..., 18, 19, 25],\n",
      "       [14, 12,  8, ..., 18, 18, 25],\n",
      "       [15, 14, 12, ..., 18, 18, 25]], dtype=uint8), 1], [array([[ 20,   6,  36, ..., 101,  87,  75],\n",
      "       [ 31,   4,  36, ..., 100,  86,  64],\n",
      "       [ 26,   9,  14, ...,  96,  88,  59],\n",
      "       ...,\n",
      "       [ 29,  35,  30, ...,  13,   9,  12],\n",
      "       [ 30,  32,  36, ...,  19,  14,  12],\n",
      "       [ 33,  25,  27, ...,  14,  20,  16]], dtype=uint8), 1], [array([[  0,  42,  51, ..., 103,   0,   0],\n",
      "       [  0,  40,  50, ..., 101,   0,   0],\n",
      "       [  0,  38,  47, ..., 100,   0,   0],\n",
      "       ...,\n",
      "       [ 16,  25,  16, ...,   0,   0,   0],\n",
      "       [ 25,  25,  25, ...,   0,   0,   0],\n",
      "       [ 25,  25,  20, ...,   0,   0,   0]], dtype=uint8), 1], [array([[ 78,  99, 112, ...,  44,  90, 203],\n",
      "       [ 85,  99, 111, ...,  56,  95, 206],\n",
      "       [ 87,  99, 107, ...,  48,  93, 209],\n",
      "       ...,\n",
      "       [ 11,  12,  18, ...,  62, 219, 237],\n",
      "       [ 16,  13,  16, ...,  49, 229, 237],\n",
      "       [ 12,  12,  19, ...,  47, 231, 241]], dtype=uint8), 1], [array([[ 72,  77,  79, ..., 102, 236, 253],\n",
      "       [105,  68,  76, ..., 127, 225, 255],\n",
      "       [127, 136, 105, ..., 133, 227, 178],\n",
      "       ...,\n",
      "       [  0,   0,   0, ...,   0,   1,   2],\n",
      "       [  0,   0,   0, ...,   1, 252, 253],\n",
      "       [  0,   0,   0, ...,   0,   2,   2]], dtype=uint8), 1], [array([[14, 13, 13, ..., 16, 16, 16],\n",
      "       [12, 11, 13, ..., 17, 16, 16],\n",
      "       [12, 12, 10, ..., 15, 16, 16],\n",
      "       ...,\n",
      "       [12, 12, 11, ..., 16, 17, 17],\n",
      "       [11, 11, 13, ..., 16, 16, 17],\n",
      "       [10,  9, 14, ..., 16, 17, 17]], dtype=uint8), 1], [array([[  0,   0,   0, ..., 134,  65,  55],\n",
      "       [  0,   0,   0, ...,  71,  70,  62],\n",
      "       [  0,   0, 255, ...,  76,  70,  68],\n",
      "       ...,\n",
      "       [  0,   0,   0, ...,   0,   0,   0],\n",
      "       [  0,   0,   0, ...,   0,   0,   0],\n",
      "       [  0,   0,   0, ...,   0,   0,   0]], dtype=uint8), 1], [array([[ 0,  0,  0, ...,  0,  0,  0],\n",
      "       [ 0,  0,  0, ...,  0,  0,  0],\n",
      "       [ 0,  0,  0, ...,  0,  0,  0],\n",
      "       ...,\n",
      "       [ 0, 23, 26, ...,  0,  0,  0],\n",
      "       [ 0, 23, 26, ...,  0,  0,  0],\n",
      "       [ 0, 23, 26, ...,  0,  0,  0]], dtype=uint8), 1], [array([[ 0,  0,  0, ..., 13, 13,  0],\n",
      "       [ 0,  0,  0, ..., 13, 13,  0],\n",
      "       [ 0,  0,  0, ..., 13, 13,  0],\n",
      "       ...,\n",
      "       [ 0,  0, 25, ..., 25, 25, 26],\n",
      "       [ 0,  0, 25, ..., 25, 25, 26],\n",
      "       [ 0,  0, 25, ..., 25, 25, 26]], dtype=uint8), 1], [array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8), 1], [array([[ 0,  0,  0, ..., 52, 41, 42],\n",
      "       [ 0,  0,  0, ..., 40, 39, 35],\n",
      "       [ 0,  0,  0, ..., 41, 36, 36],\n",
      "       ...,\n",
      "       [ 0,  0,  0, ...,  0,  0,  0],\n",
      "       [ 0,  0,  0, ...,  0,  0,  0],\n",
      "       [ 0,  0,  0, ...,  0,  0,  0]], dtype=uint8), 1], [array([[18, 19, 17, ..., 19, 21, 20],\n",
      "       [16, 16, 13, ..., 21, 21, 19],\n",
      "       [16, 16, 16, ..., 20, 20, 20],\n",
      "       ...,\n",
      "       [16, 13, 11, ..., 15, 19, 19],\n",
      "       [15, 10, 14, ..., 12, 18, 16],\n",
      "       [13, 13, 16, ..., 16, 19, 18]], dtype=uint8), 1], [array([[ 9,  8,  9, ..., 72, 55, 48],\n",
      "       [10,  9,  6, ..., 88, 85, 77],\n",
      "       [ 8,  7,  8, ..., 85, 88, 70],\n",
      "       ...,\n",
      "       [ 6,  6,  3, ...,  8,  9,  8],\n",
      "       [ 6,  5,  3, ...,  7,  9, 10],\n",
      "       [ 6,  5,  3, ...,  7,  9,  9]], dtype=uint8), 1], [array([[ 66,  33,   4, ...,   0,   0,   0],\n",
      "       [ 66,  33,   4, ...,   0,   0,   0],\n",
      "       [ 69,  33,   4, ...,   0,   0,   0],\n",
      "       ...,\n",
      "       [238,  44,  11, ...,   0,   0,   0],\n",
      "       [236,  45,  12, ...,   0,   0,   0],\n",
      "       [242,  45,  17, ...,   0,   0,   0]], dtype=uint8), 1], [array([[  5,   4,   4, ..., 104, 244, 255],\n",
      "       [  6,   6,   6, ..., 102, 245, 255],\n",
      "       [ 15,  17,  10, ..., 118, 252, 255],\n",
      "       ...,\n",
      "       [  0,   0,   0, ..., 147, 201, 246],\n",
      "       [  0,   0,   0, ..., 154, 230, 245],\n",
      "       [  0,   0,   0, ..., 163, 204, 241]], dtype=uint8), 1], [array([[58, 66, 75, ..., 96, 93, 86],\n",
      "       [50, 60, 71, ..., 97, 95, 87],\n",
      "       [48, 59, 67, ..., 91, 84, 89],\n",
      "       ...,\n",
      "       [17, 20, 21, ...,  0,  0,  0],\n",
      "       [16, 21, 19, ...,  0,  0,  0],\n",
      "       [14, 20, 20, ...,  0,  1,  4]], dtype=uint8), 1], [array([[ 7,  4,  5, ...,  5, 10,  7],\n",
      "       [ 4,  6,  4, ...,  3,  5, 11],\n",
      "       [ 4,  3,  2, ...,  2,  4,  4],\n",
      "       ...,\n",
      "       [14, 12, 14, ..., 20, 23, 22],\n",
      "       [14, 15, 13, ..., 22, 21, 20],\n",
      "       [14, 13, 11, ..., 21, 17, 21]], dtype=uint8), 1], [array([[ 11,  11,  11, ...,  11,  11,  11],\n",
      "       [ 11,  11,  11, ...,  10,  11,  11],\n",
      "       [ 11,  11,  11, ...,   8,   9,  10],\n",
      "       ...,\n",
      "       [ 14,  14,  14, ...,  45,   8, 161],\n",
      "       [ 14,  14,  14, ...,  11, 100, 192],\n",
      "       [ 14,  14,  14, ...,   9,  11,   8]], dtype=uint8), 1], [array([[ 37,  54,  70, ...,  76, 154, 255],\n",
      "       [ 40,  58,  73, ...,  77, 156, 255],\n",
      "       [ 40,  58,  65, ...,  72, 145, 255],\n",
      "       ...,\n",
      "       [  0,   0,   0, ...,   1,   0,   0],\n",
      "       [  0,   0,   0, ...,   2,   0,   0],\n",
      "       [  0,   0,   0, ...,   1,   0,   0]], dtype=uint8), 1], [array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8), 1], [array([[ 0,  0,  0, ...,  2,  1,  0],\n",
      "       [ 0,  0,  0, ...,  2,  2,  1],\n",
      "       [ 0,  0,  0, ..., 25,  6,  3],\n",
      "       ...,\n",
      "       [ 0,  0,  0, ...,  0,  0,  0],\n",
      "       [ 0,  0,  0, ...,  0,  0,  0],\n",
      "       [ 0,  0,  0, ...,  0,  0,  0]], dtype=uint8), 1], [array([[ 0,  0,  0, ...,  6,  0,  0],\n",
      "       [ 0,  0,  0, ...,  8,  0,  0],\n",
      "       [ 0,  0,  0, ..., 10,  0,  0],\n",
      "       ...,\n",
      "       [ 0,  0,  0, ...,  4,  0,  0],\n",
      "       [ 0,  0,  0, ...,  4,  0,  0],\n",
      "       [ 0,  0,  0, ...,  4,  0,  0]], dtype=uint8), 1], [array([[  0, 121,  32, ...,  13,   0,   0],\n",
      "       [  0, 118,  32, ...,  13,   0,   0],\n",
      "       [  0, 113,  32, ...,  13,   0,   0],\n",
      "       ...,\n",
      "       [  0,  29,  20, ...,   6,   0,   0],\n",
      "       [  0,  29,  20, ...,   6,   0,   0],\n",
      "       [  0,  29,  20, ...,   8,   0,   0]], dtype=uint8), 1], [array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8), 1], [array([[54, 53, 53, ..., 26, 26,  0],\n",
      "       [60, 64, 62, ..., 23, 26,  0],\n",
      "       [64, 68, 66, ..., 30, 29,  0],\n",
      "       ...,\n",
      "       [23, 23, 23, ..., 18, 18,  0],\n",
      "       [23, 23,  7, ..., 18, 18,  0],\n",
      "       [23, 23, 18, ..., 18, 18,  0]], dtype=uint8), 1], [array([[ 15,  17,  18, ..., 160, 122,  84],\n",
      "       [ 16,  17,  10, ..., 176,  95,  88],\n",
      "       [ 17,  17, 255, ..., 185,  91,  91],\n",
      "       ...,\n",
      "       [  0,   0,   0, ...,   4,  12,  12],\n",
      "       [  0,   0,   0, ...,   6,  16,  12],\n",
      "       [  0,   0,   0, ...,   7,  13,  15]], dtype=uint8), 1], [array([[  8,  28,  58, ..., 196, 254, 255],\n",
      "       [  6,  25,  50, ..., 174, 253, 255],\n",
      "       [  5,  22,  44, ..., 205, 253, 255],\n",
      "       ...,\n",
      "       [  0,   0,   0, ...,   0,   0,  27],\n",
      "       [  0,   0,   0, ...,   0,   0,  22],\n",
      "       [  0,   0,   0, ...,   0,   0,  22]], dtype=uint8), 1], [array([[131, 126, 129, ...,  93,  94,  89],\n",
      "       [120, 119, 116, ..., 101,  95,  88],\n",
      "       [117, 117, 112, ...,  96,  89, 104],\n",
      "       ...,\n",
      "       [  0,   0,   0, ...,   0,   0,   0],\n",
      "       [  0,   0,   0, ...,   0,   0,   0],\n",
      "       [  0,   0,   0, ...,   0,   0,   0]], dtype=uint8), 1], [array([[9, 9, 9, ..., 5, 5, 7],\n",
      "       [8, 9, 9, ..., 4, 4, 6],\n",
      "       [9, 9, 9, ..., 2, 4, 6],\n",
      "       ...,\n",
      "       [7, 4, 6, ..., 3, 6, 6],\n",
      "       [6, 5, 7, ..., 6, 6, 6],\n",
      "       [5, 4, 5, ..., 4, 6, 7]], dtype=uint8), 1], [array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8), 1], [array([[ 5,  2,  9, ..., 16, 16, 10],\n",
      "       [ 6,  3,  8, ..., 17, 17, 16],\n",
      "       [ 7,  6,  5, ..., 17, 16, 17],\n",
      "       ...,\n",
      "       [21, 18,  8, ..., 15, 15, 16],\n",
      "       [22, 18, 10, ..., 14, 13, 14],\n",
      "       [21, 17,  6, ..., 15, 15, 17]], dtype=uint8), 1], [array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8), 1], [array([[25, 44, 55, ..., 43, 32, 12],\n",
      "       [24, 43, 62, ..., 40, 29, 11],\n",
      "       [22, 38, 56, ..., 40, 21,  9],\n",
      "       ...,\n",
      "       [ 0,  0,  0, ...,  0,  0,  0],\n",
      "       [ 0,  0,  0, ...,  0,  0,  0],\n",
      "       [ 0,  0,  0, ...,  0,  0,  0]], dtype=uint8), 1], [array([[49, 49, 49, ..., 49, 49, 49],\n",
      "       [49, 49, 49, ..., 49, 49, 49],\n",
      "       [49, 49, 49, ..., 49, 49, 31],\n",
      "       ...,\n",
      "       [ 0, 12, 49, ..., 49, 49, 49],\n",
      "       [ 0, 12, 49, ..., 49, 49, 49],\n",
      "       [ 0,  0, 49, ..., 49, 49, 49]], dtype=uint8), 1], [array([[49, 49, 49, ..., 49, 49, 49],\n",
      "       [49, 49, 49, ..., 49, 49, 49],\n",
      "       [49, 49, 49, ..., 49, 49, 31],\n",
      "       ...,\n",
      "       [ 0, 12, 49, ..., 49, 49, 49],\n",
      "       [ 0, 12, 49, ..., 49, 49, 49],\n",
      "       [ 0,  0, 49, ..., 49, 49, 49]], dtype=uint8), 1], [array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 9, ..., 0, 0, 0],\n",
      "       [0, 0, 9, ..., 0, 0, 0],\n",
      "       [0, 0, 9, ..., 0, 0, 0]], dtype=uint8), 1], [array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8), 1], [array([[  0,   7,  23, ..., 147, 194, 199],\n",
      "       [  0,   5,  18, ..., 184, 167, 150],\n",
      "       [  0,   3,  14, ..., 189, 175, 150],\n",
      "       ...,\n",
      "       [  0,   0,   0, ...,   0,   0,   0],\n",
      "       [  0,   0,   0, ...,   0,   0,   0],\n",
      "       [  0,   0,   0, ...,   0,   0,   0]], dtype=uint8), 1], [array([[0, 0, 0, ..., 4, 4, 0],\n",
      "       [0, 0, 0, ..., 6, 7, 0],\n",
      "       [1, 1, 1, ..., 7, 6, 0],\n",
      "       ...,\n",
      "       [0, 0, 3, ..., 6, 3, 1],\n",
      "       [0, 0, 3, ..., 6, 3, 1],\n",
      "       [0, 0, 3, ..., 5, 3, 1]], dtype=uint8), 2], [array([[127, 141, 157, ..., 158, 122, 255],\n",
      "       [127, 143, 147, ..., 149, 156,  95],\n",
      "       [116, 148, 157, ..., 130, 136, 167],\n",
      "       ...,\n",
      "       [ 12,  17,  12, ...,   0,   0,   0],\n",
      "       [ 24,  19,  13, ...,   0,   0,   0],\n",
      "       [253, 215,   4, ...,   0,   0,   0]], dtype=uint8), 2], [array([[ 0, 13,  7, ...,  6,  6,  6],\n",
      "       [ 0, 13,  6, ...,  6,  6,  6],\n",
      "       [ 0, 19, 12, ...,  6,  6,  6],\n",
      "       ...,\n",
      "       [51, 26, 56, ...,  3,  4,  0],\n",
      "       [42, 27, 38, ...,  4,  4,  0],\n",
      "       [49, 33, 26, ...,  3,  3,  0]], dtype=uint8), 2], [array([[ 38,  74, 103, ...,  17,   2, 148],\n",
      "       [ 37,  77,  88, ...,  10,   4,   0],\n",
      "       [ 33,  70,  94, ...,   6,   3,   0],\n",
      "       ...,\n",
      "       [ 51,  75, 103, ...,  45,  12,   2],\n",
      "       [ 45,  82, 101, ...,  34,  16,   3],\n",
      "       [ 52, 252,  98, ...,  27,  17,   3]], dtype=uint8), 2], [array([[  0,   0,   0, ...,  48,  33,  22],\n",
      "       [  0,   0,   0, ...,  45,  29,  20],\n",
      "       [  0,   0,   0, ...,  45,  32,  20],\n",
      "       ...,\n",
      "       [ 39,  86,  81, ...,   0,   0,   0],\n",
      "       [ 49,  94,  78, ...,   0,   0,   0],\n",
      "       [ 41, 109,  88, ...,   0,   0,   0]], dtype=uint8), 2], [array([[ 36, 103, 113, ...,  15,  18, 169],\n",
      "       [ 32,  43,  96, ...,  23,  34,  45],\n",
      "       [ 30,  37,  46, ..., 201,  75, 254],\n",
      "       ...,\n",
      "       [  7,  11,  17, ...,   0,   0,   0],\n",
      "       [  7,  12,  17, ...,   0,   0,   0],\n",
      "       [ 14,   7,  15, ...,   0,   0,   0]], dtype=uint8), 2], [array([[ 29,  39,  40, ...,  60, 184, 200],\n",
      "       [ 30,  36,  41, ...,  65,  57,  41],\n",
      "       [ 35,  39,  41, ...,  75, 184, 226],\n",
      "       ...,\n",
      "       [  5,   3,   4, ...,   0,   0,   0],\n",
      "       [  0,   0,   0, ...,   0,   0,   0],\n",
      "       [199,   3,   0, ...,   0,   0,   0]], dtype=uint8), 2], [array([[25, 25, 39, ...,  1,  2,  2],\n",
      "       [26, 29, 34, ...,  0,  1,  0],\n",
      "       [43, 50, 69, ...,  0,  0,  4],\n",
      "       ...,\n",
      "       [17, 19, 17, ..., 10, 10,  9],\n",
      "       [19, 19, 16, ..., 10, 10, 10],\n",
      "       [19, 19, 17, ...,  9, 10, 10]], dtype=uint8), 2], [array([[ 11,  19,  28, ...,  12, 128,   2],\n",
      "       [ 12,  21,  26, ...,  11,   5,   3],\n",
      "       [ 11,  23,  27, ...,   8,   8,   6],\n",
      "       ...,\n",
      "       [ 12,  12,  10, ...,   0,   0,   0],\n",
      "       [  7,   8,   7, ...,   0,   0,   0],\n",
      "       [157,   3,   2, ...,   0,   0,   0]], dtype=uint8), 2], [array([[25, 25, 39, ...,  1,  2,  2],\n",
      "       [26, 29, 34, ...,  0,  1,  0],\n",
      "       [43, 50, 69, ...,  0,  0,  4],\n",
      "       ...,\n",
      "       [17, 19, 17, ..., 10, 10,  9],\n",
      "       [19, 19, 16, ..., 10, 10, 10],\n",
      "       [19, 19, 17, ...,  9, 10, 10]], dtype=uint8), 2], [array([[  4,   8,  14, ...,  13,   5, 255],\n",
      "       [  4,   7,  12, ...,  10,   2,   3],\n",
      "       [  2,   7,  12, ...,   3, 168, 255],\n",
      "       ...,\n",
      "       [ 12,  10,   9, ...,   0,   0,   0],\n",
      "       [  2,   4,   3, ...,   0,   0,   0],\n",
      "       [255,  10,   5, ...,   0,   0,   8]], dtype=uint8), 2], [array([[  1,   5,  11, ...,  45, 254, 173],\n",
      "       [  1,   5,  11, ...,  38,  36,  36],\n",
      "       [  5,   5,  11, ..., 168, 252, 174],\n",
      "       ...,\n",
      "       [ 17,  27,  36, ...,   0,   0,   0],\n",
      "       [ 29,  53,  35, ...,   0,   0,   0],\n",
      "       [168,   8,  16, ...,   0,   0,   0]], dtype=uint8), 2], [array([[  0,   8,  11, ...,  57, 169, 245],\n",
      "       [  0,   7,  11, ...,  56,  37,  26],\n",
      "       [  4,   7,  10, ..., 186, 169, 254],\n",
      "       ...,\n",
      "       [  7,   8,   8, ...,   4,   5,   5],\n",
      "       [  5,   7,   6, ...,   4,   5,   5],\n",
      "       [ 46,  90,   6, ...,   4,   4,   7]], dtype=uint8), 2], [array([[ 35,  40,  48, ...,  18, 133, 128],\n",
      "       [ 33,  41,  48, ...,  18,  19,  11],\n",
      "       [ 32,  36,  57, ..., 173, 254, 132],\n",
      "       ...,\n",
      "       [  9,  10,   7, ...,   0,   0,   0],\n",
      "       [  2,   2,   2, ...,   0,   0,   1],\n",
      "       [131,   3,   3, ...,   0,   0,   0]], dtype=uint8), 2], [array([[ 12,  14,  17, ...,  78, 201,  86],\n",
      "       [ 12,  14,  16, ...,  61,  78,  69],\n",
      "       [ 10,  15,  16, ..., 252, 198,  90],\n",
      "       ...,\n",
      "       [  6,   6,   7, ...,   0,   0,   0],\n",
      "       [  1,   1,   1, ...,   0,   0,   0],\n",
      "       [ 69,  58,   1, ...,   0,   0,   0]], dtype=uint8), 2], [array([[ 27,  26, 117, ...,  25,  23,  21],\n",
      "       [ 26,  26,  35, ..., 253,  70, 118],\n",
      "       [ 25,  26,  30, ...,  22,  15, 131],\n",
      "       ...,\n",
      "       [  0,   0,   0, ..., 253, 159,   1],\n",
      "       [  0,   0,   0, ...,   2, 151,   1],\n",
      "       [  0,   0,   0, ...,   0,   0,   0]], dtype=uint8), 2], [array([[ 19,  18,  25, ..., 184,  13,   4],\n",
      "       [ 17,  21,  26, ...,  18,  13,   1],\n",
      "       [ 18,  20,  26, ...,  16,   5,   4],\n",
      "       ...,\n",
      "       [  5,  10,  13, ...,   0,   0,   0],\n",
      "       [  4,   5,   7, ...,   1,   0,   0],\n",
      "       [254,   2,   7, ...,   1,   0,   0]], dtype=uint8), 2], [array([[29, 55, 48, ...,  3,  3,  2],\n",
      "       [53, 57, 76, ...,  1,  2,  1],\n",
      "       [62, 70, 78, ...,  5,  0,  4],\n",
      "       ...,\n",
      "       [14, 13, 10, ...,  8, 11, 11],\n",
      "       [14, 14,  8, ..., 10, 11, 10],\n",
      "       [14, 13, 10, ..., 11, 11, 11]], dtype=uint8), 2], [array([[  8,  13,  14, ...,   5,   2, 254],\n",
      "       [  6,  12,  15, ...,   6,   1,   7],\n",
      "       [  7,  10,  15, ..., 254, 255, 111],\n",
      "       ...,\n",
      "       [  5,   8,   9, ...,   0,   0,   0],\n",
      "       [  0,   0,   0, ...,   0,   0,   0],\n",
      "       [254, 219,   5, ...,   0,   0,   1]], dtype=uint8), 2], [array([[ 63,  54,  54, ...,  29, 254,  55],\n",
      "       [ 61,  59,  61, ...,  37,  32,  28],\n",
      "       [128,  67,  61, ..., 232, 169,  76],\n",
      "       ...,\n",
      "       [  5,   4,   6, ...,   0,   0,   0],\n",
      "       [  2,   1,   3, ...,   0,   0,   0],\n",
      "       [ 47, 118,   0, ...,   0,   0,   0]], dtype=uint8), 2], [array([[  0,   0,   0, ...,  15, 254, 120],\n",
      "       [  0,   0,   0, ...,  17,   7,   6],\n",
      "       [  8,   0,   0, ..., 254, 209,  84],\n",
      "       ...,\n",
      "       [  5,   5,   2, ...,   0,   0,   0],\n",
      "       [  0,   0,   0, ...,   0,   0,   0],\n",
      "       [119,   4,   5, ...,   0,   0,   0]], dtype=uint8), 2], [array([[ 20,  40,  36, ...,  83, 131,  62],\n",
      "       [ 21,  26,  42, ...,  71,  76,  71],\n",
      "       [ 17,  23,  36, ...,  79,  84,  67],\n",
      "       ...,\n",
      "       [  6,   4,   4, ...,   0,   0,   1],\n",
      "       [  6,   3,   2, ...,   0,   0,   0],\n",
      "       [ 37, 149,   0, ...,   0,   0,   0]], dtype=uint8), 2], [array([[31, 32, 34, ..., 43, 40, 38],\n",
      "       [31, 33, 36, ..., 46, 43, 40],\n",
      "       [34, 36, 38, ..., 52, 47, 45],\n",
      "       ...,\n",
      "       [30, 30, 29, ..., 28, 28, 28],\n",
      "       [30, 30, 30, ..., 28, 28, 28],\n",
      "       [16, 30, 30, ..., 28, 28, 13]], dtype=uint8), 2], [array([[ 43,  40,  44, ...,  49,  49,  45],\n",
      "       [ 47,  46,  54, ...,  55,  56,  52],\n",
      "       [ 57, 111, 120, ..., 119, 129, 136],\n",
      "       ...,\n",
      "       [  8,   7,  11, ...,  21,  21,  13],\n",
      "       [  9,   6,  10, ...,  25,  19,  15],\n",
      "       [  9,   5,  10, ...,  22,  19,  17]], dtype=uint8), 2], [array([[107, 114, 124, ..., 129,  93, 110],\n",
      "       [ 92, 108, 112, ..., 101, 125, 131],\n",
      "       [102, 117, 102, ...,  63,  41,  56],\n",
      "       ...,\n",
      "       [  6,   9,  10, ...,   0,   0,   0],\n",
      "       [  3,   4,   2, ...,   0,   0,   0],\n",
      "       [ 26, 252,   0, ...,   1,   0,   0]], dtype=uint8), 2], [array([[ 16,  19,  19, ...,  30, 254, 255],\n",
      "       [ 17,  19,  19, ...,  29,  41,  37],\n",
      "       [ 20,  18,  19, ..., 175, 253, 137],\n",
      "       ...,\n",
      "       [ 44,  32,  40, ...,  11,  13,   8],\n",
      "       [ 27,  16,  22, ...,  11,  13,   9],\n",
      "       [127,  14,  20, ...,  14,  13,   4]], dtype=uint8), 2], [array([[  2,   7,  14, ...,   4, 178,  10],\n",
      "       [  2,   7,  14, ...,   1,   5,   2],\n",
      "       [  3,   6,  13, ...,   1,   4,   7],\n",
      "       ...,\n",
      "       [  5,   9,   9, ...,   0,   0,   1],\n",
      "       [  7,   4,   2, ...,   0,   0,   0],\n",
      "       [  2, 253,   3, ...,   0,   0,   0]], dtype=uint8), 2], [array([[ 10,  16,  26, ...,   4, 254,  21],\n",
      "       [ 11,  16,  24, ...,   2,   2,   4],\n",
      "       [ 12,  16,  22, ...,  40,  28,  12],\n",
      "       ...,\n",
      "       [ 10,  10,  11, ...,   4,   4,   4],\n",
      "       [  9,   8,   7, ...,   4,   4,   4],\n",
      "       [ 23, 209,   2, ...,   4,   4,   4]], dtype=uint8), 2], [array([[ 14,  22,  27, ...,   2,   1,   0],\n",
      "       [ 15,  21,  29, ..., 255, 255,   0],\n",
      "       [ 12,  23,  28, ...,   5, 225, 167],\n",
      "       ...,\n",
      "       [  0,   0,   5, ...,   4, 222,  17],\n",
      "       [  2,   0,   5, ..., 254, 250,   1],\n",
      "       [  0,   0,   5, ...,   0,   0,   0]], dtype=uint8), 2], [array([[36, 36, 36, ...,  7,  2,  1],\n",
      "       [30, 30, 30, ..., 44, 11,  6],\n",
      "       [43, 25, 25, ..., 58, 38, 49],\n",
      "       ...,\n",
      "       [48, 46, 46, ..., 36, 36, 34],\n",
      "       [49, 46, 44, ..., 41, 36, 30],\n",
      "       [49, 46, 46, ..., 36, 36, 32]], dtype=uint8), 2], [array([[ 42,  44,  46, ...,  14,  58, 239],\n",
      "       [ 12,  44,  44, ...,  23,  38,  30],\n",
      "       [  6,  42,  45, ...,  20,  14,  40],\n",
      "       ...,\n",
      "       [ 43,  43,  44, ...,   8,  14,  22],\n",
      "       [ 34,  39,  40, ...,   8,  14,  14],\n",
      "       [  2, 255,  13, ...,   8,  14,  14]], dtype=uint8), 2], [array([[ 7,  5,  3, ..., 36, 17,  1],\n",
      "       [ 4,  3,  2, ..., 36, 35, 32],\n",
      "       [ 5,  2,  1, ..., 36, 35, 31],\n",
      "       ...,\n",
      "       [16, 20, 25, ..., 23, 31, 25],\n",
      "       [19, 20, 27, ..., 24, 29, 22],\n",
      "       [17, 23, 26, ..., 22, 24, 25]], dtype=uint8), 2], [array([[  7,  16,  20, ...,  12,   5,   0],\n",
      "       [  8,  15,  20, ...,  10,  53,   1],\n",
      "       [  8,  15,  20, ...,   6, 254, 252],\n",
      "       ...,\n",
      "       [  0,   0,   0, ...,   3,   4,  55],\n",
      "       [  0,   0,   0, ..., 254, 149,   0],\n",
      "       [  0,   0,   0, ...,   0,   0,   0]], dtype=uint8), 2], [array([[  1,   2,   6, ...,  59,  47,  27],\n",
      "       [  1,   2,   7, ...,  62,  33,  38],\n",
      "       [  1,   3,   6, ...,  53,  40,  30],\n",
      "       ...,\n",
      "       [  5,  13,  24, ...,   0,   0,   1],\n",
      "       [  1,   9,  19, ...,   0,   0,   0],\n",
      "       [  0, 253,  14, ...,   0,   0,   0]], dtype=uint8), 2], [array([[ 19,  27,  23, ...,  79, 121,  21],\n",
      "       [ 19,  26,  24, ...,  79, 103,  24],\n",
      "       [ 18,  30,  25, ...,  80, 117,  29],\n",
      "       ...,\n",
      "       [  1,   0,   0, ...,   1,   6,   1],\n",
      "       [  0,   0,   0, ...,   3,   1, 166],\n",
      "       [  0,   0,   0, ...,   0,   0,   0]], dtype=uint8), 2], [array([[  1,   2,   2, ...,  10, 137,  54],\n",
      "       [  2,   3,   4, ...,  10,  13,   3],\n",
      "       [  4,   5,   5, ...,  15,   7,   9],\n",
      "       ...,\n",
      "       [  6,   4,   2, ...,   0,   0,   0],\n",
      "       [  1,   0,   2, ...,   0,   0,   0],\n",
      "       [ 92,  90,   4, ...,   0,   0,   0]], dtype=uint8), 2], [array([[12, 26, 35, ...,  0,  0,  0],\n",
      "       [16, 21, 32, ...,  0,  0,  0],\n",
      "       [17, 21, 31, ...,  0,  0,  0],\n",
      "       ...,\n",
      "       [ 0,  0,  0, ...,  0,  0,  0],\n",
      "       [ 0,  0,  0, ...,  0,  0,  0],\n",
      "       [ 0,  0,  0, ...,  0,  0,  0]], dtype=uint8), 2], [array([[  6,   8,  10, ..., 254,  97,  44],\n",
      "       [  6,   8,  10, ..., 123, 120,  46],\n",
      "       [  8,   8,   9, ..., 225,  86,  51],\n",
      "       ...,\n",
      "       [  9,  10,  15, ...,   6,   6,   6],\n",
      "       [  6,  10,  12, ...,   6,   6,   6],\n",
      "       [  6, 236,   7, ...,   6,   6,   6]], dtype=uint8), 2], [array([[79, 59, 67, ..., 23, 19, 37],\n",
      "       [67, 65, 77, ..., 26, 13, 24],\n",
      "       [77, 71, 79, ..., 24, 11, 19],\n",
      "       ...,\n",
      "       [ 0,  0,  0, ...,  0,  0,  0],\n",
      "       [ 0,  0,  0, ...,  0,  0,  0],\n",
      "       [ 0,  0,  0, ...,  0,  0,  0]], dtype=uint8), 2], [array([[ 55,  72,  74, ...,  45,  46,  41],\n",
      "       [ 71,  65,  70, ..., 132, 254,  35],\n",
      "       [ 55,  68,  77, ..., 135, 129, 254],\n",
      "       ...,\n",
      "       [ 14,  16,  17, ..., 254, 254,  14],\n",
      "       [ 14,   0,  18, ..., 191,  14,  14],\n",
      "       [ 14,  14,   7, ...,  14,  14,  14]], dtype=uint8), 2], [array([[  0,   0,   0, ...,   4, 224, 102],\n",
      "       [  0,   0,   0, ...,   1,   4,   3],\n",
      "       [  0,   0,   0, ...,   5,   3,   0],\n",
      "       ...,\n",
      "       [ 17,  21,  14, ...,   0,   0,   0],\n",
      "       [ 23,  25,  10, ...,   0,   0,   0],\n",
      "       [172,   7,   4, ...,   0,   0,   6]], dtype=uint8), 2], [array([[ 18,  22,  25, ...,   2,   5,  11],\n",
      "       [ 17,  22,  29, ...,   5,   3,   6],\n",
      "       [ 18,  21,  27, ...,  11,   1,   0],\n",
      "       ...,\n",
      "       [ 11,   2,   8, ...,   0,   0,   6],\n",
      "       [  1,   0,   0, ...,   0,   0,   0],\n",
      "       [ 29, 199,  12, ...,   0,   0,   0]], dtype=uint8), 2], [array([[  4,   5,   7, ...,  45,  27,  20],\n",
      "       [  3,   4,   6, ...,  32,  29,  20],\n",
      "       [  1,   4,   6, ...,  31,  23,  18],\n",
      "       ...,\n",
      "       [  1,   5,   7, ...,   0,   0,   0],\n",
      "       [  0,   2,   2, ...,   0,   0,   0],\n",
      "       [  3, 254,   2, ...,   0,   0,   0]], dtype=uint8), 2], [array([[31, 30, 30, ..., 12, 12, 10],\n",
      "       [30, 30, 28, ..., 10, 10,  9],\n",
      "       [30, 28, 27, ...,  7,  6,  7],\n",
      "       ...,\n",
      "       [30, 30, 30, ..., 21, 21, 21],\n",
      "       [30, 30, 30, ..., 21, 21, 22],\n",
      "       [30, 30, 30, ..., 22, 22, 23]], dtype=uint8), 2], [array([[  5,  10,  14, ...,  19,  16,  12],\n",
      "       [  5,   9,  14, ..., 255, 252,  10],\n",
      "       [ 11,   8,  11, ..., 238, 179,  54],\n",
      "       ...,\n",
      "       [  0,   0,   0, ..., 254,   2,   0],\n",
      "       [  0,   0,   0, ..., 253, 141,   0],\n",
      "       [  1,   0,   0, ...,   0,   0,   1]], dtype=uint8), 2], [array([[  0,   2,  10, ...,  17,  17,  16],\n",
      "       [  0,   2,  10, ...,  24,   4,   6],\n",
      "       [  0,   0,   8, ...,  14,  12,   7],\n",
      "       ...,\n",
      "       [  6,   8,   8, ...,   0,   0,   0],\n",
      "       [  0,   0,   0, ...,   0,   0,   0],\n",
      "       [ 60, 195,   9, ...,   0,   0,   7]], dtype=uint8), 2], [array([[ 20,   4,  14, ...,   9,  10,  10],\n",
      "       [ 41,   9,  14, ...,  12,  10,  16],\n",
      "       [ 68,  15,  15, ...,  12,  15,  14],\n",
      "       ...,\n",
      "       [  8,  14,  28, ...,   1,   0,   0],\n",
      "       [  5,   9,  23, ...,   1,   0,   0],\n",
      "       [  4,   3, 175, ...,   1,   0,   0]], dtype=uint8), 2], [array([[ 0,  0,  0, ..., 29, 13,  0],\n",
      "       [ 0,  0,  0, ..., 26,  0,  0],\n",
      "       [ 0,  0,  0, ..., 36,  0,  0],\n",
      "       ...,\n",
      "       [ 0,  0,  0, ...,  0,  0,  0],\n",
      "       [ 0,  0,  0, ...,  0,  0,  0],\n",
      "       [ 0,  0,  0, ...,  0,  0,  0]], dtype=uint8), 2], [array([[ 29,  31,  38, ...,  70,  28,  31],\n",
      "       [ 26,  31,  34, ...,  31,  27,  30],\n",
      "       [ 18,  18,  18, ...,  32,  33,  28],\n",
      "       ...,\n",
      "       [  8,   7,   8, ...,   0,   0,   0],\n",
      "       [  6,   9,   6, ...,   0,   0,   0],\n",
      "       [ 11, 239,   2, ...,   0,   0,   0]], dtype=uint8), 2], [array([[  0,   0,   0, ...,   6,   4,   2],\n",
      "       [  0,   0,   0, ...,   5, 160,  60],\n",
      "       [  1,   0,   0, ..., 254,   7,   4],\n",
      "       ...,\n",
      "       [  0,   0,   0, ...,   1,   1,  59],\n",
      "       [  0,   0,   0, ..., 254, 254,   0],\n",
      "       [  0,   0,   0, ...,   0,   0,   0]], dtype=uint8), 2], [array([[  3,   4,   7, ...,   7,   9,   6],\n",
      "       [  2,   3,   6, ...,   5,   3,   6],\n",
      "       [  2,   3,   7, ...,  11,   3,   2],\n",
      "       ...,\n",
      "       [ 15,  19,  14, ...,   0,   0,   0],\n",
      "       [ 21,  40,   8, ...,   0,   0,   0],\n",
      "       [ 31, 178,   8, ...,   0,   0,   9]], dtype=uint8), 2], [array([[18, 14, 20, ..., 33, 19, 15],\n",
      "       [16, 19, 21, ..., 34, 18, 15],\n",
      "       [14, 17, 20, ..., 30, 23, 15],\n",
      "       ...,\n",
      "       [ 0,  0,  0, ...,  0,  0,  0],\n",
      "       [ 0,  0,  0, ...,  0,  0,  0],\n",
      "       [ 0,  0,  1, ...,  0,  0,  0]], dtype=uint8), 2], [array([[  9,  12,  14, ...,   3,   6,   7],\n",
      "       [ 10,  13,  16, ...,   2,   3,   6],\n",
      "       [ 11,  16,  20, ...,   7,   2,   4],\n",
      "       ...,\n",
      "       [ 96, 124, 136, ...,   3,   6,   7],\n",
      "       [115, 126, 142, ...,   4,   6,   9],\n",
      "       [113, 134, 146, ...,   5,   5,   6]], dtype=uint8), 2], [array([[ 0,  0,  0, ...,  8,  0,  0],\n",
      "       [ 0,  0,  0, ...,  9,  0,  0],\n",
      "       [ 0,  0,  0, ..., 11,  2,  0],\n",
      "       ...,\n",
      "       [ 0,  0,  6, ...,  0,  0,  0],\n",
      "       [ 0,  0, 10, ...,  0,  0,  0],\n",
      "       [ 0,  0, 12, ...,  0,  0,  0]], dtype=uint8), 2], [array([[10, 12, 14, ...,  4,  0,  0],\n",
      "       [ 8, 10, 14, ...,  2,  0,  0],\n",
      "       [ 8, 11, 14, ...,  0,  0,  0],\n",
      "       ...,\n",
      "       [ 0,  0,  0, ...,  0,  0,  0],\n",
      "       [ 0,  0,  0, ...,  0,  0,  0],\n",
      "       [ 0,  0,  0, ...,  0,  0,  0]], dtype=uint8), 2], [array([[ 46,  56,  51, ...,   8,   8,   9],\n",
      "       [ 37,  36,  39, ...,   6,  10,  17],\n",
      "       [ 34,  39,  40, ...,  12,  16,  17],\n",
      "       ...,\n",
      "       [  1,   3,   4, ...,   0,   0,   0],\n",
      "       [  0,   0,   0, ...,   0,   0,   0],\n",
      "       [  6,   2, 252, ...,   0,   0,  12]], dtype=uint8), 2], [array([[  0,   0,   0, ...,   0,   0,   0],\n",
      "       [  0,   0,   0, ..., 253, 245,   0],\n",
      "       [  0,   0,   0, ...,   3,   7,   0],\n",
      "       ...,\n",
      "       [  0,   0,   0, ...,   2,   1,  15],\n",
      "       [  0,   0,   0, ..., 253, 210,   0],\n",
      "       [  0,   0,   0, ...,   0,   0,   0]], dtype=uint8), 2], [array([[24, 32, 48, ..., 40, 38, 20],\n",
      "       [25, 29, 44, ..., 47, 27, 19],\n",
      "       [26, 30, 42, ..., 37, 21, 19],\n",
      "       ...,\n",
      "       [ 2,  0,  0, ...,  0,  0,  2],\n",
      "       [ 0,  0,  0, ...,  0,  0,  0],\n",
      "       [ 0,  0,  0, ...,  0,  0,  0]], dtype=uint8), 2], [array([[  3,   8,  11, ...,   0,   0,   0],\n",
      "       [  3,   6,   9, ..., 253,   1,   0],\n",
      "       [  0,   5,   7, ...,   1, 254, 254],\n",
      "       ...,\n",
      "       [  0,   0,   0, ...,   2,   2,  25],\n",
      "       [  0,   0,   0, ..., 253, 149,   0],\n",
      "       [  0,   0,   0, ...,   0,   0,   0]], dtype=uint8), 2], [array([[ 0,  0,  0, ..., 76, 73, 66],\n",
      "       [ 0,  0,  0, ..., 62, 50, 45],\n",
      "       [ 0,  0,  0, ..., 46, 41, 32],\n",
      "       ...,\n",
      "       [ 0,  0,  0, ...,  0,  0,  0],\n",
      "       [ 0,  0,  0, ...,  0,  0,  0],\n",
      "       [ 0,  0,  0, ...,  0,  0,  0]], dtype=uint8), 2], [array([[ 16,  16,   0, ...,   2, 255,  96],\n",
      "       [ 16,  16,   0, ...,  21,  15,  19],\n",
      "       [ 16,  16,   0, ...,  18,  18,   0],\n",
      "       ...,\n",
      "       [ 28,  19,  20, ...,   0,   0,  13],\n",
      "       [ 18,  16,  16, ...,   0,   0,  16],\n",
      "       [ 90,  43,  26, ...,   0,   0,   0]], dtype=uint8), 2], [array([[  7,  17,  28, ...,  18,  17, 254],\n",
      "       [  7,  15,  26, ...,  18,  14,  11],\n",
      "       [  7,  14,  26, ...,  22, 193, 102],\n",
      "       ...,\n",
      "       [ 12,  22,  43, ...,   0,   0,   0],\n",
      "       [ 10,  22,  46, ...,   0,   0,   0],\n",
      "       [166, 221,  74, ...,   0,   0,   0]], dtype=uint8), 2], [array([[32, 29, 29, ..., 58, 44, 41],\n",
      "       [86, 85, 86, ..., 42, 22, 16],\n",
      "       [67, 67, 47, ..., 23, 19, 16],\n",
      "       ...,\n",
      "       [ 6,  2, 21, ...,  8,  9,  9],\n",
      "       [ 7,  1, 20, ...,  8,  9,  9],\n",
      "       [17,  1, 21, ...,  8, 10, 15]], dtype=uint8), 2], [array([[ 76,  80,  89, ..., 153, 163, 183],\n",
      "       [ 85,  86,  80, ..., 128, 128, 193],\n",
      "       [120,  85,  96, ..., 130, 145, 205],\n",
      "       ...,\n",
      "       [ 19,  16,  13, ...,  10,  10,  11],\n",
      "       [ 18,  16,  12, ...,  11,  11,  11],\n",
      "       [ 18,  16,  12, ...,  11,  11,  11]], dtype=uint8), 2], [array([[19, 19, 25, ...,  0,  0,  0],\n",
      "       [20, 21, 25, ...,  0,  0,  0],\n",
      "       [19, 18, 24, ...,  0,  0,  0],\n",
      "       ...,\n",
      "       [ 0,  2, 11, ...,  0,  0,  0],\n",
      "       [ 0,  2, 10, ...,  0,  0,  0],\n",
      "       [ 0,  1,  8, ...,  0,  0,  0]], dtype=uint8), 2], [array([[ 77, 118, 121, ...,  13,  13,  11],\n",
      "       [ 65,  84, 107, ..., 228, 255,  13],\n",
      "       [ 52,  59, 105, ...,  19,  18,   7],\n",
      "       ...,\n",
      "       [  0,   0,   0, ..., 229,   4,   0],\n",
      "       [  0,   0,   0, ...,  12, 255,   0],\n",
      "       [  0,   0,   0, ...,  10,   5,   0]], dtype=uint8), 2], [array([[ 40,  38,  32, ...,  22,  26,  29],\n",
      "       [ 63,  46,  37, ...,  25,  29,  34],\n",
      "       [ 23,  63,  45, ...,  36,  35, 254],\n",
      "       ...,\n",
      "       [  5,  14,  23, ...,   2,   0, 253],\n",
      "       [  5,  17,  25, ...,   4, 255, 176],\n",
      "       [  6,  17,  27, ...,   3,   0,   0]], dtype=uint8), 2], [array([[14, 22, 89, ..., 77, 18, 15],\n",
      "       [14, 14, 24, ..., 14, 14, 16],\n",
      "       [17, 12, 18, ..., 14, 16, 15],\n",
      "       ...,\n",
      "       [ 0,  0,  0, ...,  0,  0,  0],\n",
      "       [ 0,  0,  0, ...,  0,  0,  0],\n",
      "       [ 0,  0,  0, ...,  0,  0,  0]], dtype=uint8), 2], [array([[7, 7, 7, ..., 0, 0, 1],\n",
      "       [8, 7, 7, ..., 0, 0, 0],\n",
      "       [7, 8, 7, ..., 0, 0, 3],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8), 2], [array([[ 48,  45,  43, ..., 255, 255, 255],\n",
      "       [ 47,  43,  51, ..., 254, 255, 255],\n",
      "       [ 22,  46,  43, ..., 255, 255, 255],\n",
      "       ...,\n",
      "       [ 22,   9,  78, ...,  16,  16,  21],\n",
      "       [ 18,  14,  71, ...,  13,  18,  18],\n",
      "       [ 18,  16,  59, ...,  16,  18,  18]], dtype=uint8), 2]]\n"
     ]
    }
   ],
   "source": [
    "print(len(training_data))\n",
    "print(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "for features,label in training_data:\n",
    "    X_train.append(features)\n",
    "    y_train.append(label)\n",
    "X_train = np.array(X_train).reshape(-1,IMG_SIZE,IMG_SIZE,1)\n",
    "y_train = np.array(y_train)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = []\n",
    "def create_test_data():\n",
    "    for category in CATEGORIES:\n",
    "        path = os.path.join(DATADIR2,category)\n",
    "        class_num = CATEGORIES.index(category)\n",
    "        for img in os.listdir(path):\n",
    "            img_array = cv2.imread(os.path.join(path,img),cv2. IMREAD_GRAYSCALE)\n",
    "            new_array = cv2.resize(img_array,(IMG_SIZE,IMG_SIZE))\n",
    "            test_data.append([new_array,class_num])\n",
    "            \n",
    "create_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66\n"
     ]
    }
   ],
   "source": [
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = []\n",
    "y_test = []\n",
    "for features,label in test_data:\n",
    "    X_test.append(features)\n",
    "    y_test.append(label)\n",
    "X_test = np.array(X_test).reshape(-1,IMG_SIZE,IMG_SIZE,1)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T05:11:34.747153Z",
     "iopub.status.busy": "2022-01-26T05:11:34.746577Z",
     "iopub.status.idle": "2022-01-26T05:11:48.763399Z",
     "shell.execute_reply": "2022-01-26T05:11:48.763818Z"
    },
    "id": "JWoEqyMuXFF4"
   },
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = (X_train, y_train), (X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7wArwCTJJlUa"
   },
   "source": [
    "### Verify the data\n",
    "\n",
    "To verify that the dataset looks correct, let's plot the first 25 images from the training set and display the class name below each image:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T05:11:48.812571Z",
     "iopub.status.busy": "2022-01-26T05:11:48.780279Z",
     "iopub.status.idle": "2022-01-26T05:11:49.701268Z",
     "shell.execute_reply": "2022-01-26T05:11:49.701645Z"
    },
    "id": "K3PAELE2eSU9"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 25 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class_names = ['Covid', 'Normal', 'Viral Pneumonia']\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(train_images[i])\n",
    "    plt.xlabel(class_names[train_labels[i]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize pixel values to be between 0 and 1\n",
    "train_images = train_images.astype('float32')\n",
    "test_images = test_images.astype('float32')\n",
    "train_images /= 255\n",
    "test_images /= 255\n",
    "\n",
    "# OneHotEncoding for the output label:\n",
    "train_labels = np_utils.to_categorical(y_train, 3)\n",
    "test_labels = np_utils.to_categorical(y_test, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oewp-wYg31t9"
   },
   "source": [
    "### Create the convolutional base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare Sequential model fro our network:\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 100, 100, 32)\n"
     ]
    }
   ],
   "source": [
    "# CNN first layer (with 32 3x3 filter):\n",
    "model.add(Convolution2D(32, (3, 3), activation='relu', input_shape=(100,100,1), padding=\"same\"))\n",
    "\n",
    "print(model.output_shape) # with no zero padding -> (None, 248, 248, 32)\n",
    "\n",
    "# If your image batch is of N images of HxW size with C channels: \n",
    "# theano uses the NCHW ordering while tensorflow uses the NHWC ordering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 100, 100, 32)\n",
      "(None, 50, 50, 32)\n"
     ]
    }
   ],
   "source": [
    "# more hidden layers:\n",
    "model.add(Convolution2D(32, (3, 3), activation='relu', padding=\"same\"))\n",
    "print(model.output_shape)\n",
    "\n",
    "# Pooling Layer:\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "print(model.output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 50, 50, 64)\n",
      "(None, 25, 25, 64)\n"
     ]
    }
   ],
   "source": [
    "# more hidden layers:\n",
    "model.add(Convolution2D(64, (3, 3), activation='relu', padding=\"same\"))\n",
    "print(model.output_shape)\n",
    "\n",
    "# Pooling Layer:\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "print(model.output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 25, 25, 128)\n",
      "(None, 12, 12, 128)\n"
     ]
    }
   ],
   "source": [
    "# more hidden layers:\n",
    "model.add(Convolution2D(128, (3, 3), activation='relu', padding=\"same\"))\n",
    "print(model.output_shape)\n",
    "\n",
    "# Pooling Layer:\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "print(model.output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 18432)\n",
      "(None, 128)\n",
      "(None, 3)\n"
     ]
    }
   ],
   "source": [
    "# output Fully connected Dense layers:\n",
    "model.add(Flatten())\n",
    "print(model.output_shape)\n",
    "\n",
    "model.add(Dense(128, activation='relu'))\n",
    "print(model.output_shape)\n",
    "\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "print(model.output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 100, 100, 32)      320       \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 100, 100, 32)      9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 50, 50, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 50, 50, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 25, 25, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 25, 25, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 18432)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               2359424   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 2,461,731\n",
      "Trainable params: 2,461,731\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy',metrics=['accuracy'],\n",
    "              optimizer='adam')\n",
    "# more info about loss functions: https://keras.io/losses\n",
    "# more infor about Optimizers: https://keras.io/optimizers\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "9/9 [==============================] - 1s 88ms/step - loss: 0.9547 - accuracy: 0.5896 - val_loss: 0.6818 - val_accuracy: 0.6212\n",
      "Epoch 2/10\n",
      "9/9 [==============================] - 1s 68ms/step - loss: 0.3595 - accuracy: 0.8566 - val_loss: 0.4351 - val_accuracy: 0.8485\n",
      "Epoch 3/10\n",
      "9/9 [==============================] - 1s 69ms/step - loss: 0.3045 - accuracy: 0.8964 - val_loss: 0.5180 - val_accuracy: 0.8485\n",
      "Epoch 4/10\n",
      "9/9 [==============================] - 1s 67ms/step - loss: 0.2917 - accuracy: 0.8725 - val_loss: 0.4559 - val_accuracy: 0.7576\n",
      "Epoch 5/10\n",
      "9/9 [==============================] - 1s 66ms/step - loss: 0.2181 - accuracy: 0.9044 - val_loss: 0.3562 - val_accuracy: 0.8485\n",
      "Epoch 6/10\n",
      "9/9 [==============================] - 1s 65ms/step - loss: 0.1381 - accuracy: 0.9283 - val_loss: 0.2996 - val_accuracy: 0.8788\n",
      "Epoch 7/10\n",
      "9/9 [==============================] - 1s 66ms/step - loss: 0.0866 - accuracy: 0.9721 - val_loss: 0.2806 - val_accuracy: 0.8636\n",
      "Epoch 8/10\n",
      "9/9 [==============================] - 1s 66ms/step - loss: 0.0517 - accuracy: 0.9801 - val_loss: 0.2492 - val_accuracy: 0.8939\n",
      "Epoch 9/10\n",
      "9/9 [==============================] - 1s 66ms/step - loss: 0.0341 - accuracy: 0.9841 - val_loss: 0.2386 - val_accuracy: 0.8939\n",
      "Epoch 10/10\n",
      "9/9 [==============================] - 1s 65ms/step - loss: 0.0180 - accuracy: 0.9920 - val_loss: 0.3026 - val_accuracy: 0.9242\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_images, train_labels, batch_size=30, epochs=10, verbose=1, validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_classes = model.predict(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 16ms/step - loss: 0.3026 - accuracy: 0.9242\n",
      "The accuracy is:  0.9242424368858337\n"
     ]
    }
   ],
   "source": [
    "# Testing:\n",
    "score = model.evaluate(test_images, test_labels, verbose=1)\n",
    "print('The accuracy is: ', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "accuracy = history.history['accuracy']\n",
    "val_accuracy = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(len(accuracy))\n",
    "plt.plot(epochs, accuracy, 'r-', label='Training accuracy')\n",
    "plt.plot(epochs, val_accuracy, 'b', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'r-', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape each image pixels into a row of feature table with 100*100=10000 features (each pixel is a feature):\n",
    "\n",
    "train_images = train_images.reshape(train_images.shape[0], 10000)\n",
    "\n",
    "test_images = test_images.reshape(test_images.shape[0], 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "k = 100  #  k  is the number of components (new features) after dimensionality reduction\n",
    "\n",
    "my_pca = PCA(n_components = k)\n",
    "\n",
    "# X_Train is feature matrix of training set before dimensionality reduction, \n",
    "\n",
    "# X_Train_New is feature matrix of training set after dimensionality reduction:\n",
    "\n",
    "train_images_new = my_pca.fit_transform(train_images)\n",
    "\n",
    "test_images_new = my_pca.transform(test_images)\n",
    "\n",
    "print(len(train_images_new[0]))\n",
    "print(len(test_images_new[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(251, 100)\n",
      "(66, 100)\n"
     ]
    }
   ],
   "source": [
    "print(train_images_new.shape)\n",
    "print(test_images_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_creator():\n",
    "    \"Define the Network Architecture (model)\"\n",
    "    # define:\n",
    "    model = Sequential()\n",
    "    input_size = 100\n",
    "    hidden_neurons = 100\n",
    "    out_size = 3\n",
    "\n",
    "    # design the structure:\n",
    "    # second layer: hidden layer:\n",
    "    model.add(Dense(hidden_neurons, input_dim = input_size))  # Nuerons\n",
    "    model.add(Activation('relu')) # Activation\n",
    "\n",
    "    # third layer: output layer:\n",
    "    model.add(Dense(out_size, input_dim = hidden_neurons))  # Nuerons\n",
    "    model.add(Activation('softmax')) # Activation  \n",
    "\n",
    "    # compile:\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam',  metrics=['accuracy'])\n",
    "\n",
    "    # return: \n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 2\n",
    "np.random.seed(seed=2)\n",
    "#I think you meant to make the seed = 2\n",
    "\n",
    "# Use KerasClassifier class to wrap your model as an object:\n",
    "ann_model = KerasClassifier(build_fn = model_creator, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': [30, 50, 100], 'epochs': [10, 20, 30, 40, 50]} \n",
      "\n",
      "Epoch 1/10\n",
      "8/8 [==============================] - 1s 7ms/step - loss: 1.8726 - accuracy: 0.3111\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1.0048 - accuracy: 0.5600\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.5398 - accuracy: 0.7689\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.3317 - accuracy: 0.8933\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.2319 - accuracy: 0.9378\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1748 - accuracy: 0.9511\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1373 - accuracy: 0.9644\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1116 - accuracy: 0.9911\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0925 - accuracy: 0.9911\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0782 - accuracy: 0.9956\n",
      "Epoch 1/10\n",
      "8/8 [==============================] - 1s 6ms/step - loss: 2.2461 - accuracy: 0.2920\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1.3351 - accuracy: 0.5752\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.7563 - accuracy: 0.6991\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.4077 - accuracy: 0.8540\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.2470 - accuracy: 0.9425\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1690 - accuracy: 0.9558\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1272 - accuracy: 0.9779\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0996 - accuracy: 0.9956\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0807 - accuracy: 0.9956\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0678 - accuracy: 1.0000\n",
      "Epoch 1/10\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 2.1150 - accuracy: 0.3274\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.1360 - accuracy: 0.5354\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.6032 - accuracy: 0.7566\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.3618 - accuracy: 0.8938\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.2312 - accuracy: 0.9602\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1627 - accuracy: 0.9823\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1225 - accuracy: 0.9867\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0982 - accuracy: 0.9867\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0804 - accuracy: 0.9867\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0666 - accuracy: 1.0000\n",
      "WARNING:tensorflow:5 out of the last 21 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000149544A5040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/10\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1.3833 - accuracy: 0.4292\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.6683 - accuracy: 0.7212\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.3603 - accuracy: 0.8894\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.2209 - accuracy: 0.9513\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1521 - accuracy: 0.9558\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1122 - accuracy: 0.9735\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0893 - accuracy: 0.9867\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0720 - accuracy: 0.9912\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0586 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0499 - accuracy: 1.0000\n",
      "WARNING:tensorflow:6 out of the last 22 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000014990E9A790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/10\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 1.1740 - accuracy: 0.4469\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.6020 - accuracy: 0.7345\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.3176 - accuracy: 0.9115\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1937 - accuracy: 0.9513\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1349 - accuracy: 0.9735\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1016 - accuracy: 0.9912: 0s - loss: 0.1016 - accuracy: 0.99\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0803 - accuracy: 0.9956\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0656 - accuracy: 0.9956\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0545 - accuracy: 0.9956\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0468 - accuracy: 0.9956\n",
      "Epoch 1/10\n",
      "8/8 [==============================] - 1s 6ms/step - loss: 1.3388 - accuracy: 0.4469\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.6926 - accuracy: 0.7434\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.3872 - accuracy: 0.8761\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.2481 - accuracy: 0.9248\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1681 - accuracy: 0.9513\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1214 - accuracy: 0.9735\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0928 - accuracy: 0.9779\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0747 - accuracy: 0.9956\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0608 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0517 - accuracy: 1.0000\n",
      "Epoch 1/10\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.1689 - accuracy: 0.5088\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.5240 - accuracy: 0.8053\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.2885 - accuracy: 0.9381\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1820 - accuracy: 0.9646\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1255 - accuracy: 0.9735\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0946 - accuracy: 0.9779\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0743 - accuracy: 0.9912\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0592 - accuracy: 0.9956\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0491 - accuracy: 0.9956\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0416 - accuracy: 0.9956\n",
      "Epoch 1/10\n",
      "8/8 [==============================] - 1s 7ms/step - loss: 2.3023 - accuracy: 0.2920\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.3240 - accuracy: 0.5221\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.7003 - accuracy: 0.7257\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.3731 - accuracy: 0.8805\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.2285 - accuracy: 0.9469\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1572 - accuracy: 0.9690\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1176 - accuracy: 0.9912\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0923 - accuracy: 0.9956\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0746 - accuracy: 0.9956\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0631 - accuracy: 0.9956\n",
      "Epoch 1/10\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2.2172 - accuracy: 0.3319\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.2410 - accuracy: 0.5442\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.6621 - accuracy: 0.7478\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3913 - accuracy: 0.8894\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.2670 - accuracy: 0.9204\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1940 - accuracy: 0.9513\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1480 - accuracy: 0.9646\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1167 - accuracy: 0.9779\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0951 - accuracy: 0.9912\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0791 - accuracy: 0.9956\n",
      "Epoch 1/10\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1.4671 - accuracy: 0.4292\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.7842 - accuracy: 0.6770\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.4320 - accuracy: 0.8319\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.2673 - accuracy: 0.9027\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1803 - accuracy: 0.9646\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1309 - accuracy: 0.9690\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1008 - accuracy: 0.9823\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0802 - accuracy: 0.9867\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0658 - accuracy: 0.9912\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0547 - accuracy: 0.9956\n",
      "Epoch 1/20\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 2.0833 - accuracy: 0.2889\n",
      "Epoch 2/20\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.2371 - accuracy: 0.5378\n",
      "Epoch 3/20\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.7547 - accuracy: 0.6933\n",
      "Epoch 4/20\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.4670 - accuracy: 0.8222\n",
      "Epoch 5/20\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3046 - accuracy: 0.8933\n",
      "Epoch 6/20\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.2147 - accuracy: 0.9289\n",
      "Epoch 7/20\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1607 - accuracy: 0.9600\n",
      "Epoch 8/20\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1260 - accuracy: 0.9689\n",
      "Epoch 9/20\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1021 - accuracy: 0.9778\n",
      "Epoch 10/20\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0837 - accuracy: 0.9867\n",
      "Epoch 11/20\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0706 - accuracy: 0.9867\n",
      "Epoch 12/20\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0603 - accuracy: 1.0000\n",
      "Epoch 13/20\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0526 - accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0462 - accuracy: 1.0000\n",
      "Epoch 15/20\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0411 - accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0370 - accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0334 - accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0301 - accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0272 - accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0250 - accuracy: 1.0000\n",
      "Epoch 1/20\n",
      "8/8 [==============================] - 1s 8ms/step - loss: 1.4324 - accuracy: 0.4027\n",
      "Epoch 2/20\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.7377 - accuracy: 0.6903\n",
      "Epoch 3/20\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.3842 - accuracy: 0.8673\n",
      "Epoch 4/20\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.2405 - accuracy: 0.9425\n",
      "Epoch 5/20\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1623 - accuracy: 0.9646\n",
      "Epoch 6/20\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1202 - accuracy: 0.9867\n",
      "Epoch 7/20\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0928 - accuracy: 0.9956\n",
      "Epoch 8/20\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0762 - accuracy: 0.9956\n",
      "Epoch 9/20\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0642 - accuracy: 0.9956\n",
      "Epoch 10/20\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0548 - accuracy: 1.0000\n",
      "Epoch 11/20\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0475 - accuracy: 1.0000\n",
      "Epoch 12/20\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0416 - accuracy: 1.0000\n",
      "Epoch 13/20\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0372 - accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0332 - accuracy: 1.0000\n",
      "Epoch 15/20\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0297 - accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0271 - accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0246 - accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0225 - accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0208 - accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0191 - accuracy: 1.0000\n",
      "Epoch 1/20\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1.2429 - accuracy: 0.5088\n",
      "Epoch 2/20\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.5963 - accuracy: 0.7478\n",
      "Epoch 3/20\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.3038 - accuracy: 0.8982\n",
      "Epoch 4/20\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1935 - accuracy: 0.9425\n",
      "Epoch 5/20\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1289 - accuracy: 0.9735\n",
      "Epoch 6/20\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0959 - accuracy: 0.9912\n",
      "Epoch 7/20\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0735 - accuracy: 0.9956\n",
      "Epoch 8/20\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0593 - accuracy: 0.9956\n",
      "Epoch 9/20\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0496 - accuracy: 0.9956\n",
      "Epoch 10/20\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0415 - accuracy: 1.0000\n",
      "Epoch 11/20\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0366 - accuracy: 1.0000\n",
      "Epoch 12/20\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0318 - accuracy: 1.0000\n",
      "Epoch 13/20\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0284 - accuracy: 1.0000\n",
      "Epoch 14/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0254 - accuracy: 1.0000\n",
      "Epoch 15/20\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0230 - accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0209 - accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0191 - accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0176 - accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0162 - accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0150 - accuracy: 1.0000\n",
      "Epoch 1/20\n",
      "8/8 [==============================] - 1s 5ms/step - loss: 3.5835 - accuracy: 0.1681\n",
      "Epoch 2/20\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2.1841 - accuracy: 0.2301\n",
      "Epoch 3/20\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.1578 - accuracy: 0.4867\n",
      "Epoch 4/20\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.5808 - accuracy: 0.7522\n",
      "Epoch 5/20\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.3224 - accuracy: 0.9071\n",
      "Epoch 6/20\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.2093 - accuracy: 0.9602\n",
      "Epoch 7/20\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1464 - accuracy: 0.9867\n",
      "Epoch 8/20\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1152 - accuracy: 0.9956\n",
      "Epoch 9/20\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0903 - accuracy: 1.0000\n",
      "Epoch 10/20\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0766 - accuracy: 1.0000\n",
      "Epoch 11/20\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0641 - accuracy: 1.0000\n",
      "Epoch 12/20\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0556 - accuracy: 1.0000\n",
      "Epoch 13/20\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0490 - accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0439 - accuracy: 1.0000\n",
      "Epoch 15/20\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0390 - accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0355 - accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0322 - accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0295 - accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0271 - accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0250 - accuracy: 1.0000\n",
      "Epoch 1/20\n",
      "8/8 [==============================] - 1s 5ms/step - loss: 1.4613 - accuracy: 0.4027\n",
      "Epoch 2/20\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.7841 - accuracy: 0.6903\n",
      "Epoch 3/20\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4534 - accuracy: 0.8451\n",
      "Epoch 4/20\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.2865 - accuracy: 0.9115\n",
      "Epoch 5/20\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1912 - accuracy: 0.9469\n",
      "Epoch 6/20\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1386 - accuracy: 0.9690\n",
      "Epoch 7/20\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1057 - accuracy: 0.9779\n",
      "Epoch 8/20\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 0.0839 - accuracy: 0.9867\n",
      "Epoch 9/20\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0674 - accuracy: 0.9956\n",
      "Epoch 10/20\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0569 - accuracy: 1.0000\n",
      "Epoch 11/20\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0484 - accuracy: 1.0000\n",
      "Epoch 12/20\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0421 - accuracy: 1.0000\n",
      "Epoch 13/20\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0366 - accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0326 - accuracy: 1.0000\n",
      "Epoch 15/20\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0291 - accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0263 - accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0239 - accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0219 - accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0200 - accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0185 - accuracy: 1.0000\n",
      "Epoch 1/20\n",
      "8/8 [==============================] - 1s 8ms/step - loss: 1.3299 - accuracy: 0.3894\n",
      "Epoch 2/20\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.6903 - accuracy: 0.7212\n",
      "Epoch 3/20\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.4060 - accuracy: 0.8850\n",
      "Epoch 4/20\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.2535 - accuracy: 0.9381\n",
      "Epoch 5/20\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1782 - accuracy: 0.9646\n",
      "Epoch 6/20\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1323 - accuracy: 0.9823\n",
      "Epoch 7/20\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1044 - accuracy: 0.9912\n",
      "Epoch 8/20\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0836 - accuracy: 0.9956\n",
      "Epoch 9/20\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0693 - accuracy: 0.9956\n",
      "Epoch 10/20\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0594 - accuracy: 0.9956\n",
      "Epoch 11/20\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0507 - accuracy: 0.9956\n",
      "Epoch 12/20\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0440 - accuracy: 0.9956\n",
      "Epoch 13/20\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0387 - accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0346 - accuracy: 1.0000\n",
      "Epoch 15/20\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0308 - accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0276 - accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0250 - accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0228 - accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0208 - accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0191 - accuracy: 1.0000\n",
      "Epoch 1/20\n",
      "8/8 [==============================] - 1s 7ms/step - loss: 1.7564 - accuracy: 0.3363\n",
      "Epoch 2/20\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.9564 - accuracy: 0.6018\n",
      "Epoch 3/20\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.5135 - accuracy: 0.8142\n",
      "Epoch 4/20\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.3117 - accuracy: 0.8982\n",
      "Epoch 5/20\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.2085 - accuracy: 0.9336\n",
      "Epoch 6/20\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1511 - accuracy: 0.9646\n",
      "Epoch 7/20\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1166 - accuracy: 0.9735\n",
      "Epoch 8/20\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0941 - accuracy: 0.9867\n",
      "Epoch 9/20\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0789 - accuracy: 0.9912\n",
      "Epoch 10/20\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 0.0653 - accuracy: 1.0000\n",
      "Epoch 11/20\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0557 - accuracy: 1.0000\n",
      "Epoch 12/20\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0490 - accuracy: 1.0000\n",
      "Epoch 13/20\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0432 - accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0383 - accuracy: 1.0000\n",
      "Epoch 15/20\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0344 - accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0309 - accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0282 - accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0256 - accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0235 - accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0215 - accuracy: 1.0000\n",
      "Epoch 1/20\n",
      "8/8 [==============================] - 2s 8ms/step - loss: 0.9943 - accuracy: 0.5929\n",
      "Epoch 2/20\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.5442 - accuracy: 0.7965\n",
      "Epoch 3/20\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.3232 - accuracy: 0.8938\n",
      "Epoch 4/20\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.2152 - accuracy: 0.9558\n",
      "Epoch 5/20\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1485 - accuracy: 0.9646\n",
      "Epoch 6/20\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1132 - accuracy: 0.9735\n",
      "Epoch 7/20\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0899 - accuracy: 0.9867\n",
      "Epoch 8/20\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0723 - accuracy: 0.9912\n",
      "Epoch 9/20\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0597 - accuracy: 0.9956\n",
      "Epoch 10/20\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0503 - accuracy: 1.0000\n",
      "Epoch 11/20\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0440 - accuracy: 1.0000\n",
      "Epoch 12/20\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0376 - accuracy: 1.0000\n",
      "Epoch 13/20\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0332 - accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0296 - accuracy: 1.0000\n",
      "Epoch 15/20\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0263 - accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0237 - accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0214 - accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0195 - accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0178 - accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0164 - accuracy: 1.0000\n",
      "Epoch 1/20\n",
      "8/8 [==============================] - 2s 67ms/step - loss: 1.1038 - accuracy: 0.5177\n",
      "Epoch 2/20\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 0.5233 - accuracy: 0.8142\n",
      "Epoch 3/20\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 0.3053 - accuracy: 0.8938\n",
      "Epoch 4/20\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1999 - accuracy: 0.9558\n",
      "Epoch 5/20\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1475 - accuracy: 0.9823\n",
      "Epoch 6/20\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1149 - accuracy: 0.9823\n",
      "Epoch 7/20\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0894 - accuracy: 0.9867\n",
      "Epoch 8/20\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0741 - accuracy: 0.9867\n",
      "Epoch 9/20\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0611 - accuracy: 0.9912\n",
      "Epoch 10/20\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0514 - accuracy: 1.0000\n",
      "Epoch 11/20\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0451 - accuracy: 1.0000\n",
      "Epoch 12/20\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0391 - accuracy: 1.0000\n",
      "Epoch 13/20\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0344 - accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0302 - accuracy: 1.0000\n",
      "Epoch 15/20\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0270 - accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0244 - accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0221 - accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0201 - accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0184 - accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0169 - accuracy: 1.0000\n",
      "Epoch 1/20\n",
      "8/8 [==============================] - 1s 6ms/step - loss: 1.5068 - accuracy: 0.5133\n",
      "Epoch 2/20\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.7729 - accuracy: 0.6814\n",
      "Epoch 3/20\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.3980 - accuracy: 0.8628\n",
      "Epoch 4/20\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.2403 - accuracy: 0.9204\n",
      "Epoch 5/20\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1607 - accuracy: 0.9690\n",
      "Epoch 6/20\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1136 - accuracy: 0.9779\n",
      "Epoch 7/20\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0851 - accuracy: 0.9823\n",
      "Epoch 8/20\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0651 - accuracy: 0.9956\n",
      "Epoch 9/20\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0534 - accuracy: 0.9956\n",
      "Epoch 10/20\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0437 - accuracy: 1.0000\n",
      "Epoch 11/20\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0378 - accuracy: 1.0000\n",
      "Epoch 12/20\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0330 - accuracy: 1.0000\n",
      "Epoch 13/20\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0289 - accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0256 - accuracy: 1.0000\n",
      "Epoch 15/20\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0232 - accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0210 - accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0190 - accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0174 - accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0160 - accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0149 - accuracy: 1.0000\n",
      "Epoch 1/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.8924 - accuracy: 0.2222\n",
      "Epoch 2/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.9965 - accuracy: 0.5289\n",
      "Epoch 3/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.5424 - accuracy: 0.7956\n",
      "Epoch 4/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3440 - accuracy: 0.8800\n",
      "Epoch 5/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.2289 - accuracy: 0.9467\n",
      "Epoch 6/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1674 - accuracy: 0.9644\n",
      "Epoch 7/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1315 - accuracy: 0.9689\n",
      "Epoch 8/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1067 - accuracy: 0.9822\n",
      "Epoch 9/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0878 - accuracy: 0.9867\n",
      "Epoch 10/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0748 - accuracy: 0.9956\n",
      "Epoch 11/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0641 - accuracy: 1.0000\n",
      "Epoch 12/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0558 - accuracy: 1.0000\n",
      "Epoch 13/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0488 - accuracy: 1.0000\n",
      "Epoch 14/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0434 - accuracy: 1.0000\n",
      "Epoch 15/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0390 - accuracy: 1.0000\n",
      "Epoch 16/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0350 - accuracy: 1.0000\n",
      "Epoch 17/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0316 - accuracy: 1.0000\n",
      "Epoch 18/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0290 - accuracy: 1.0000\n",
      "Epoch 19/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0264 - accuracy: 1.0000\n",
      "Epoch 20/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0243 - accuracy: 1.0000\n",
      "Epoch 21/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0224 - accuracy: 1.0000\n",
      "Epoch 22/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0208 - accuracy: 1.0000\n",
      "Epoch 23/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0193 - accuracy: 1.0000\n",
      "Epoch 24/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0181 - accuracy: 1.0000\n",
      "Epoch 25/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0168 - accuracy: 1.0000\n",
      "Epoch 26/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0158 - accuracy: 1.0000\n",
      "Epoch 27/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0149 - accuracy: 1.0000\n",
      "Epoch 28/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0140 - accuracy: 1.0000\n",
      "Epoch 29/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0132 - accuracy: 1.0000\n",
      "Epoch 30/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0125 - accuracy: 1.0000\n",
      "Epoch 1/30\n",
      "8/8 [==============================] - 1s 6ms/step - loss: 1.4104 - accuracy: 0.4292\n",
      "Epoch 2/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.6749 - accuracy: 0.7345\n",
      "Epoch 3/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.3476 - accuracy: 0.9115\n",
      "Epoch 4/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.2173 - accuracy: 0.9469\n",
      "Epoch 5/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1517 - accuracy: 0.9646\n",
      "Epoch 6/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1167 - accuracy: 0.9690\n",
      "Epoch 7/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0923 - accuracy: 0.9779\n",
      "Epoch 8/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0744 - accuracy: 0.9912\n",
      "Epoch 9/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0629 - accuracy: 0.9912\n",
      "Epoch 10/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0519 - accuracy: 0.9912\n",
      "Epoch 11/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0450 - accuracy: 1.0000\n",
      "Epoch 12/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0392 - accuracy: 1.0000\n",
      "Epoch 13/30\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0343 - accuracy: 1.0000\n",
      "Epoch 14/30\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0305 - accuracy: 1.0000\n",
      "Epoch 15/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0273 - accuracy: 1.0000\n",
      "Epoch 16/30\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0247 - accuracy: 1.0000\n",
      "Epoch 17/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0224 - accuracy: 1.0000\n",
      "Epoch 18/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0205 - accuracy: 1.0000\n",
      "Epoch 19/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0188 - accuracy: 1.0000\n",
      "Epoch 20/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0174 - accuracy: 1.0000\n",
      "Epoch 21/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0161 - accuracy: 1.0000\n",
      "Epoch 22/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0149 - accuracy: 1.0000\n",
      "Epoch 23/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0139 - accuracy: 1.0000\n",
      "Epoch 24/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0130 - accuracy: 1.0000\n",
      "Epoch 25/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0122 - accuracy: 1.0000\n",
      "Epoch 26/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0114 - accuracy: 1.0000\n",
      "Epoch 27/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0107 - accuracy: 1.0000\n",
      "Epoch 28/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0101 - accuracy: 1.0000\n",
      "Epoch 29/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0096 - accuracy: 1.0000\n",
      "Epoch 30/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0090 - accuracy: 1.0000\n",
      "Epoch 1/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.9762 - accuracy: 0.2655\n",
      "Epoch 2/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.1467 - accuracy: 0.5000\n",
      "Epoch 3/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.6568 - accuracy: 0.7301\n",
      "Epoch 4/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3885 - accuracy: 0.8628\n",
      "Epoch 5/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.2471 - accuracy: 0.9513\n",
      "Epoch 6/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1749 - accuracy: 0.9690\n",
      "Epoch 7/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1274 - accuracy: 0.9867\n",
      "Epoch 8/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1004 - accuracy: 0.9956\n",
      "Epoch 9/30\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0804 - accuracy: 1.0000\n",
      "Epoch 10/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0677 - accuracy: 1.0000\n",
      "Epoch 11/30\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0581 - accuracy: 1.0000\n",
      "Epoch 12/30\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0506 - accuracy: 1.0000\n",
      "Epoch 13/30\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0444 - accuracy: 1.0000\n",
      "Epoch 14/30\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0395 - accuracy: 1.0000\n",
      "Epoch 15/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0353 - accuracy: 1.0000\n",
      "Epoch 16/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0320 - accuracy: 1.0000\n",
      "Epoch 17/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0291 - accuracy: 1.0000\n",
      "Epoch 18/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0264 - accuracy: 1.0000\n",
      "Epoch 19/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0244 - accuracy: 1.0000\n",
      "Epoch 20/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0224 - accuracy: 1.0000\n",
      "Epoch 21/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0207 - accuracy: 1.0000\n",
      "Epoch 22/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0193 - accuracy: 1.0000\n",
      "Epoch 23/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0179 - accuracy: 1.0000\n",
      "Epoch 24/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0167 - accuracy: 1.0000\n",
      "Epoch 25/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0157 - accuracy: 1.0000\n",
      "Epoch 26/30\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0147 - accuracy: 1.0000\n",
      "Epoch 27/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0138 - accuracy: 1.0000\n",
      "Epoch 28/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0130 - accuracy: 1.0000\n",
      "Epoch 29/30\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0123 - accuracy: 1.0000\n",
      "Epoch 30/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0116 - accuracy: 1.0000\n",
      "Epoch 1/30\n",
      "8/8 [==============================] - 1s 7ms/step - loss: 2.1761 - accuracy: 0.3407\n",
      "Epoch 2/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.1598 - accuracy: 0.5398\n",
      "Epoch 3/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.6064 - accuracy: 0.7566\n",
      "Epoch 4/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.3553 - accuracy: 0.9027\n",
      "Epoch 5/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.2357 - accuracy: 0.9381\n",
      "Epoch 6/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1634 - accuracy: 0.9735\n",
      "Epoch 7/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1209 - accuracy: 0.9823\n",
      "Epoch 8/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0937 - accuracy: 0.9912\n",
      "Epoch 9/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0763 - accuracy: 0.9956\n",
      "Epoch 10/30\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0650 - accuracy: 0.9956\n",
      "Epoch 11/30\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0552 - accuracy: 1.0000\n",
      "Epoch 12/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0486 - accuracy: 1.0000\n",
      "Epoch 13/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0432 - accuracy: 1.0000\n",
      "Epoch 14/30\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0385 - accuracy: 1.0000\n",
      "Epoch 15/30\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0349 - accuracy: 1.0000\n",
      "Epoch 16/30\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0317 - accuracy: 1.0000\n",
      "Epoch 17/30\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0290 - accuracy: 1.0000\n",
      "Epoch 18/30\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0266 - accuracy: 1.0000\n",
      "Epoch 19/30\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0244 - accuracy: 1.0000\n",
      "Epoch 20/30\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0226 - accuracy: 1.0000\n",
      "Epoch 21/30\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0210 - accuracy: 1.0000\n",
      "Epoch 22/30\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0195 - accuracy: 1.0000\n",
      "Epoch 23/30\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0182 - accuracy: 1.0000\n",
      "Epoch 24/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0171 - accuracy: 1.0000\n",
      "Epoch 25/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0160 - accuracy: 1.0000\n",
      "Epoch 26/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0151 - accuracy: 1.0000\n",
      "Epoch 27/30\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0142 - accuracy: 1.0000\n",
      "Epoch 28/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0134 - accuracy: 1.0000\n",
      "Epoch 29/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0126 - accuracy: 1.0000\n",
      "Epoch 30/30\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0120 - accuracy: 1.0000\n",
      "Epoch 1/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2.2811 - accuracy: 0.3319\n",
      "Epoch 2/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.2328 - accuracy: 0.5442\n",
      "Epoch 3/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.6054 - accuracy: 0.7566\n",
      "Epoch 4/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.3198 - accuracy: 0.9071\n",
      "Epoch 5/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1954 - accuracy: 0.9690\n",
      "Epoch 6/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1387 - accuracy: 0.9823\n",
      "Epoch 7/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1036 - accuracy: 0.9867\n",
      "Epoch 8/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0836 - accuracy: 0.9912\n",
      "Epoch 9/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0693 - accuracy: 0.9912\n",
      "Epoch 10/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0589 - accuracy: 0.9956\n",
      "Epoch 11/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0520 - accuracy: 0.9956\n",
      "Epoch 12/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0458 - accuracy: 1.0000\n",
      "Epoch 13/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0410 - accuracy: 1.0000\n",
      "Epoch 14/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0367 - accuracy: 1.0000\n",
      "Epoch 15/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0333 - accuracy: 1.0000\n",
      "Epoch 16/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0302 - accuracy: 1.0000\n",
      "Epoch 17/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0276 - accuracy: 1.0000\n",
      "Epoch 18/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0254 - accuracy: 1.0000\n",
      "Epoch 19/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0234 - accuracy: 1.0000\n",
      "Epoch 20/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0216 - accuracy: 1.0000\n",
      "Epoch 21/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0201 - accuracy: 1.0000\n",
      "Epoch 22/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0187 - accuracy: 1.0000\n",
      "Epoch 23/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0175 - accuracy: 1.0000\n",
      "Epoch 24/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0164 - accuracy: 1.0000\n",
      "Epoch 25/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0154 - accuracy: 1.0000\n",
      "Epoch 26/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0145 - accuracy: 1.0000\n",
      "Epoch 27/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0137 - accuracy: 1.0000\n",
      "Epoch 28/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0129 - accuracy: 1.0000\n",
      "Epoch 29/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0122 - accuracy: 1.0000\n",
      "Epoch 30/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0116 - accuracy: 1.0000\n",
      "Epoch 1/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.1181 - accuracy: 0.5088\n",
      "Epoch 2/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.5193 - accuracy: 0.7965\n",
      "Epoch 3/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3020 - accuracy: 0.8894\n",
      "Epoch 4/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.2024 - accuracy: 0.9469\n",
      "Epoch 5/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1397 - accuracy: 0.9690\n",
      "Epoch 6/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1019 - accuracy: 0.9779\n",
      "Epoch 7/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0800 - accuracy: 0.9867\n",
      "Epoch 8/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0630 - accuracy: 0.9956\n",
      "Epoch 9/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0519 - accuracy: 0.9956\n",
      "Epoch 10/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0439 - accuracy: 1.0000\n",
      "Epoch 11/30\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0379 - accuracy: 1.0000\n",
      "Epoch 12/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0331 - accuracy: 1.0000\n",
      "Epoch 13/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0294 - accuracy: 1.0000\n",
      "Epoch 14/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0261 - accuracy: 1.0000\n",
      "Epoch 15/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0235 - accuracy: 1.0000\n",
      "Epoch 16/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0213 - accuracy: 1.0000\n",
      "Epoch 17/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0195 - accuracy: 1.0000\n",
      "Epoch 18/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0177 - accuracy: 1.0000\n",
      "Epoch 19/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0163 - accuracy: 1.0000\n",
      "Epoch 20/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0151 - accuracy: 1.0000\n",
      "Epoch 21/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0140 - accuracy: 1.0000\n",
      "Epoch 22/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0130 - accuracy: 1.0000\n",
      "Epoch 23/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0122 - accuracy: 1.0000\n",
      "Epoch 24/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0114 - accuracy: 1.0000\n",
      "Epoch 25/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0106 - accuracy: 1.0000\n",
      "Epoch 26/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0100 - accuracy: 1.0000\n",
      "Epoch 27/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0094 - accuracy: 1.0000\n",
      "Epoch 28/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0089 - accuracy: 1.0000\n",
      "Epoch 29/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0084 - accuracy: 1.0000\n",
      "Epoch 30/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0080 - accuracy: 1.0000\n",
      "Epoch 1/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 2.2635 - accuracy: 0.3230\n",
      "Epoch 2/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.2776 - accuracy: 0.5133\n",
      "Epoch 3/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.6859 - accuracy: 0.7124\n",
      "Epoch 4/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3829 - accuracy: 0.8673\n",
      "Epoch 5/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.2397 - accuracy: 0.9381\n",
      "Epoch 6/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1611 - accuracy: 0.9735\n",
      "Epoch 7/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1208 - accuracy: 0.9823\n",
      "Epoch 8/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0964 - accuracy: 0.9912\n",
      "Epoch 9/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0790 - accuracy: 0.9956\n",
      "Epoch 10/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0674 - accuracy: 0.9956\n",
      "Epoch 11/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0571 - accuracy: 1.0000\n",
      "Epoch 12/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0498 - accuracy: 1.0000\n",
      "Epoch 13/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0442 - accuracy: 1.0000\n",
      "Epoch 14/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0394 - accuracy: 1.0000\n",
      "Epoch 15/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0353 - accuracy: 1.0000\n",
      "Epoch 16/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0320 - accuracy: 1.0000\n",
      "Epoch 17/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0292 - accuracy: 1.0000\n",
      "Epoch 18/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0267 - accuracy: 1.0000\n",
      "Epoch 19/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0245 - accuracy: 1.0000\n",
      "Epoch 20/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0226 - accuracy: 1.0000\n",
      "Epoch 21/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0209 - accuracy: 1.0000\n",
      "Epoch 22/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0195 - accuracy: 1.0000\n",
      "Epoch 23/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0181 - accuracy: 1.0000\n",
      "Epoch 24/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0169 - accuracy: 1.0000\n",
      "Epoch 25/30\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0158 - accuracy: 1.0000\n",
      "Epoch 26/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0149 - accuracy: 1.0000\n",
      "Epoch 27/30\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0140 - accuracy: 1.0000\n",
      "Epoch 28/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0132 - accuracy: 1.0000\n",
      "Epoch 29/30\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0125 - accuracy: 1.0000\n",
      "Epoch 30/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0118 - accuracy: 1.0000\n",
      "Epoch 1/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.2635 - accuracy: 0.3496\n",
      "Epoch 2/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2.1174 - accuracy: 0.4735\n",
      "Epoch 3/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1.3177 - accuracy: 0.6106\n",
      "Epoch 4/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.7728 - accuracy: 0.7434\n",
      "Epoch 5/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4452 - accuracy: 0.8407\n",
      "Epoch 6/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.2745 - accuracy: 0.9248\n",
      "Epoch 7/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1897 - accuracy: 0.9646\n",
      "Epoch 8/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1424 - accuracy: 0.9779\n",
      "Epoch 9/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1135 - accuracy: 0.9867\n",
      "Epoch 10/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0922 - accuracy: 0.9867\n",
      "Epoch 11/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0779 - accuracy: 0.9956\n",
      "Epoch 12/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0662 - accuracy: 1.0000\n",
      "Epoch 13/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0581 - accuracy: 1.0000\n",
      "Epoch 14/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0501 - accuracy: 1.0000\n",
      "Epoch 15/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0446 - accuracy: 1.0000\n",
      "Epoch 16/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0400 - accuracy: 1.0000\n",
      "Epoch 17/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0362 - accuracy: 1.0000\n",
      "Epoch 18/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0328 - accuracy: 1.0000\n",
      "Epoch 19/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0302 - accuracy: 1.0000\n",
      "Epoch 20/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0276 - accuracy: 1.0000\n",
      "Epoch 21/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0254 - accuracy: 1.0000\n",
      "Epoch 22/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0235 - accuracy: 1.0000\n",
      "Epoch 23/30\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0219 - accuracy: 1.0000\n",
      "Epoch 24/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0204 - accuracy: 1.0000\n",
      "Epoch 25/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0190 - accuracy: 1.0000\n",
      "Epoch 26/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0179 - accuracy: 1.0000\n",
      "Epoch 27/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0168 - accuracy: 1.0000\n",
      "Epoch 28/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0158 - accuracy: 1.0000\n",
      "Epoch 29/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0149 - accuracy: 1.0000\n",
      "Epoch 30/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0141 - accuracy: 1.0000\n",
      "Epoch 1/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.0057 - accuracy: 0.5354\n",
      "Epoch 2/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.5096 - accuracy: 0.8186\n",
      "Epoch 3/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.3010 - accuracy: 0.9159\n",
      "Epoch 4/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.2004 - accuracy: 0.9558\n",
      "Epoch 5/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1467 - accuracy: 0.9646\n",
      "Epoch 6/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1115 - accuracy: 0.9735\n",
      "Epoch 7/30\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0872 - accuracy: 0.9867\n",
      "Epoch 8/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0699 - accuracy: 0.9912\n",
      "Epoch 9/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0587 - accuracy: 0.9912\n",
      "Epoch 10/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0492 - accuracy: 0.9956\n",
      "Epoch 11/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0424 - accuracy: 1.0000\n",
      "Epoch 12/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0368 - accuracy: 1.0000\n",
      "Epoch 13/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0322 - accuracy: 1.0000\n",
      "Epoch 14/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0288 - accuracy: 1.0000\n",
      "Epoch 15/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0257 - accuracy: 1.0000\n",
      "Epoch 16/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0232 - accuracy: 1.0000\n",
      "Epoch 17/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0209 - accuracy: 1.0000\n",
      "Epoch 18/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0190 - accuracy: 1.0000\n",
      "Epoch 19/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0173 - accuracy: 1.0000\n",
      "Epoch 20/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0160 - accuracy: 1.0000\n",
      "Epoch 21/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0147 - accuracy: 1.0000\n",
      "Epoch 22/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0137 - accuracy: 1.0000\n",
      "Epoch 23/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0126 - accuracy: 1.0000\n",
      "Epoch 24/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0118 - accuracy: 1.0000\n",
      "Epoch 25/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0110 - accuracy: 1.0000\n",
      "Epoch 26/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0104 - accuracy: 1.0000\n",
      "Epoch 27/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0097 - accuracy: 1.0000\n",
      "Epoch 28/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0092 - accuracy: 1.0000\n",
      "Epoch 29/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0086 - accuracy: 1.0000\n",
      "Epoch 30/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0081 - accuracy: 1.0000\n",
      "Epoch 1/30\n",
      "8/8 [==============================] - 1s 6ms/step - loss: 1.7486 - accuracy: 0.1991\n",
      "Epoch 2/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.9001 - accuracy: 0.6018\n",
      "Epoch 3/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.4954 - accuracy: 0.8230\n",
      "Epoch 4/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.2912 - accuracy: 0.9336\n",
      "Epoch 5/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.2017 - accuracy: 0.9558\n",
      "Epoch 6/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1476 - accuracy: 0.9735\n",
      "Epoch 7/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1132 - accuracy: 0.9823\n",
      "Epoch 8/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0926 - accuracy: 0.9867\n",
      "Epoch 9/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0748 - accuracy: 0.9956\n",
      "Epoch 10/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0629 - accuracy: 1.0000\n",
      "Epoch 11/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0537 - accuracy: 1.0000\n",
      "Epoch 12/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0471 - accuracy: 1.0000\n",
      "Epoch 13/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0416 - accuracy: 1.0000\n",
      "Epoch 14/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0372 - accuracy: 1.0000\n",
      "Epoch 15/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0332 - accuracy: 1.0000\n",
      "Epoch 16/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0302 - accuracy: 1.0000\n",
      "Epoch 17/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0274 - accuracy: 1.0000\n",
      "Epoch 18/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0250 - accuracy: 1.0000\n",
      "Epoch 19/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0230 - accuracy: 1.0000\n",
      "Epoch 20/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0211 - accuracy: 1.0000\n",
      "Epoch 21/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0196 - accuracy: 1.0000\n",
      "Epoch 22/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0182 - accuracy: 1.0000\n",
      "Epoch 23/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0169 - accuracy: 1.0000\n",
      "Epoch 24/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0159 - accuracy: 1.0000\n",
      "Epoch 25/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0148 - accuracy: 1.0000\n",
      "Epoch 26/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0139 - accuracy: 1.0000\n",
      "Epoch 27/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0130 - accuracy: 1.0000\n",
      "Epoch 28/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0123 - accuracy: 1.0000\n",
      "Epoch 29/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0116 - accuracy: 1.0000\n",
      "Epoch 30/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0110 - accuracy: 1.0000\n",
      "Epoch 1/40\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 1.4851 - accuracy: 0.4356\n",
      "Epoch 2/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.7486 - accuracy: 0.6667\n",
      "Epoch 3/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.4399 - accuracy: 0.8311\n",
      "Epoch 4/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.2684 - accuracy: 0.9067\n",
      "Epoch 5/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1956 - accuracy: 0.9422\n",
      "Epoch 6/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1435 - accuracy: 0.9689\n",
      "Epoch 7/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1086 - accuracy: 0.9733\n",
      "Epoch 8/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0863 - accuracy: 0.9867\n",
      "Epoch 9/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0702 - accuracy: 0.9867\n",
      "Epoch 10/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0594 - accuracy: 0.9956\n",
      "Epoch 11/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0507 - accuracy: 1.0000\n",
      "Epoch 12/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0435 - accuracy: 1.0000\n",
      "Epoch 13/40\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0381 - accuracy: 1.0000\n",
      "Epoch 14/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0340 - accuracy: 1.0000\n",
      "Epoch 15/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0305 - accuracy: 1.0000\n",
      "Epoch 16/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0274 - accuracy: 1.0000\n",
      "Epoch 17/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0247 - accuracy: 1.0000\n",
      "Epoch 18/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0227 - accuracy: 1.0000\n",
      "Epoch 19/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0207 - accuracy: 1.0000\n",
      "Epoch 20/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0189 - accuracy: 1.0000\n",
      "Epoch 21/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0175 - accuracy: 1.0000\n",
      "Epoch 22/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0162 - accuracy: 1.0000\n",
      "Epoch 23/40\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0152 - accuracy: 1.0000\n",
      "Epoch 24/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0141 - accuracy: 1.0000\n",
      "Epoch 25/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0132 - accuracy: 1.0000\n",
      "Epoch 26/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0124 - accuracy: 1.0000\n",
      "Epoch 27/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0116 - accuracy: 1.0000\n",
      "Epoch 28/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0110 - accuracy: 1.0000\n",
      "Epoch 29/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0104 - accuracy: 1.0000\n",
      "Epoch 30/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0098 - accuracy: 1.0000\n",
      "Epoch 31/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0093 - accuracy: 1.0000\n",
      "Epoch 32/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0088 - accuracy: 1.0000\n",
      "Epoch 33/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0084 - accuracy: 1.0000\n",
      "Epoch 34/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0080 - accuracy: 1.0000\n",
      "Epoch 35/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0076 - accuracy: 1.0000\n",
      "Epoch 36/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0072 - accuracy: 1.0000\n",
      "Epoch 37/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0069 - accuracy: 1.0000\n",
      "Epoch 38/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0066 - accuracy: 1.0000\n",
      "Epoch 39/40\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0063 - accuracy: 1.0000\n",
      "Epoch 40/40\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0061 - accuracy: 1.0000\n",
      "Epoch 1/40\n",
      "8/8 [==============================] - 1s 6ms/step - loss: 1.3579 - accuracy: 0.4071\n",
      "Epoch 2/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.7409 - accuracy: 0.6593\n",
      "Epoch 3/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.4321 - accuracy: 0.8451\n",
      "Epoch 4/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.2798 - accuracy: 0.9159\n",
      "Epoch 5/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1973 - accuracy: 0.9425\n",
      "Epoch 6/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1455 - accuracy: 0.9602\n",
      "Epoch 7/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1154 - accuracy: 0.9779\n",
      "Epoch 8/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0899 - accuracy: 0.9867\n",
      "Epoch 9/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0746 - accuracy: 0.9912\n",
      "Epoch 10/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0626 - accuracy: 0.9956\n",
      "Epoch 11/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0535 - accuracy: 1.0000\n",
      "Epoch 12/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0464 - accuracy: 1.0000\n",
      "Epoch 13/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0409 - accuracy: 1.0000\n",
      "Epoch 14/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0363 - accuracy: 1.0000\n",
      "Epoch 15/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0326 - accuracy: 1.0000\n",
      "Epoch 16/40\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0294 - accuracy: 1.0000\n",
      "Epoch 17/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0265 - accuracy: 1.0000\n",
      "Epoch 18/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0241 - accuracy: 1.0000\n",
      "Epoch 19/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0221 - accuracy: 1.0000\n",
      "Epoch 20/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0203 - accuracy: 1.0000\n",
      "Epoch 21/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0188 - accuracy: 1.0000\n",
      "Epoch 22/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0174 - accuracy: 1.0000\n",
      "Epoch 23/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0162 - accuracy: 1.0000\n",
      "Epoch 24/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0151 - accuracy: 1.0000\n",
      "Epoch 25/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0141 - accuracy: 1.0000\n",
      "Epoch 26/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0132 - accuracy: 1.0000\n",
      "Epoch 27/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0124 - accuracy: 1.0000\n",
      "Epoch 28/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0117 - accuracy: 1.0000\n",
      "Epoch 29/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0109 - accuracy: 1.0000\n",
      "Epoch 30/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0104 - accuracy: 1.0000\n",
      "Epoch 31/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0098 - accuracy: 1.0000\n",
      "Epoch 32/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0093 - accuracy: 1.0000\n",
      "Epoch 33/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0088 - accuracy: 1.0000\n",
      "Epoch 34/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0084 - accuracy: 1.0000\n",
      "Epoch 35/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0080 - accuracy: 1.0000\n",
      "Epoch 36/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0076 - accuracy: 1.0000\n",
      "Epoch 37/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0072 - accuracy: 1.0000\n",
      "Epoch 38/40\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0069 - accuracy: 1.0000\n",
      "Epoch 39/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0066 - accuracy: 1.0000\n",
      "Epoch 40/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0063 - accuracy: 1.0000\n",
      "Epoch 1/40\n",
      "8/8 [==============================] - 1s 5ms/step - loss: 1.3559 - accuracy: 0.4071\n",
      "Epoch 2/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.6704 - accuracy: 0.7212\n",
      "Epoch 3/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3871 - accuracy: 0.8496\n",
      "Epoch 4/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.2508 - accuracy: 0.9336\n",
      "Epoch 5/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1727 - accuracy: 0.9690\n",
      "Epoch 6/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1311 - accuracy: 0.9735\n",
      "Epoch 7/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1004 - accuracy: 0.9779\n",
      "Epoch 8/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0805 - accuracy: 0.9823\n",
      "Epoch 9/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0660 - accuracy: 0.9912\n",
      "Epoch 10/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0555 - accuracy: 1.0000\n",
      "Epoch 11/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0472 - accuracy: 1.0000\n",
      "Epoch 12/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0405 - accuracy: 1.0000\n",
      "Epoch 13/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0358 - accuracy: 1.0000\n",
      "Epoch 14/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0317 - accuracy: 1.0000\n",
      "Epoch 15/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0284 - accuracy: 1.0000\n",
      "Epoch 16/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0255 - accuracy: 1.0000\n",
      "Epoch 17/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0231 - accuracy: 1.0000\n",
      "Epoch 18/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0209 - accuracy: 1.0000\n",
      "Epoch 19/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0193 - accuracy: 1.0000\n",
      "Epoch 20/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0177 - accuracy: 1.0000\n",
      "Epoch 21/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0164 - accuracy: 1.0000\n",
      "Epoch 22/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0151 - accuracy: 1.0000\n",
      "Epoch 23/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0141 - accuracy: 1.0000\n",
      "Epoch 24/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0131 - accuracy: 1.0000\n",
      "Epoch 25/40\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0123 - accuracy: 1.0000\n",
      "Epoch 26/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0115 - accuracy: 1.0000\n",
      "Epoch 27/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0109 - accuracy: 1.0000\n",
      "Epoch 28/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0102 - accuracy: 1.0000\n",
      "Epoch 29/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0097 - accuracy: 1.0000\n",
      "Epoch 30/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0091 - accuracy: 1.0000\n",
      "Epoch 31/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0086 - accuracy: 1.0000\n",
      "Epoch 32/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0082 - accuracy: 1.0000\n",
      "Epoch 33/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0078 - accuracy: 1.0000\n",
      "Epoch 34/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0074 - accuracy: 1.0000\n",
      "Epoch 35/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0070 - accuracy: 1.0000\n",
      "Epoch 36/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0067 - accuracy: 1.0000\n",
      "Epoch 37/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0064 - accuracy: 1.0000\n",
      "Epoch 38/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0062 - accuracy: 1.0000\n",
      "Epoch 39/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0059 - accuracy: 1.0000\n",
      "Epoch 40/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0056 - accuracy: 1.0000\n",
      "Epoch 1/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.3631 - accuracy: 0.3982\n",
      "Epoch 2/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.6655 - accuracy: 0.7743\n",
      "Epoch 3/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3694 - accuracy: 0.8805\n",
      "Epoch 4/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.2259 - accuracy: 0.9381\n",
      "Epoch 5/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1573 - accuracy: 0.9602\n",
      "Epoch 6/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1104 - accuracy: 0.9779\n",
      "Epoch 7/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0865 - accuracy: 0.9867\n",
      "Epoch 8/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0684 - accuracy: 0.9912\n",
      "Epoch 9/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0568 - accuracy: 1.0000\n",
      "Epoch 10/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0476 - accuracy: 1.0000\n",
      "Epoch 11/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0410 - accuracy: 1.0000\n",
      "Epoch 12/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0354 - accuracy: 1.0000\n",
      "Epoch 13/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0314 - accuracy: 1.0000\n",
      "Epoch 14/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0279 - accuracy: 1.0000\n",
      "Epoch 15/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0251 - accuracy: 1.0000\n",
      "Epoch 16/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0226 - accuracy: 1.0000\n",
      "Epoch 17/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0206 - accuracy: 1.0000\n",
      "Epoch 18/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0188 - accuracy: 1.0000\n",
      "Epoch 19/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0173 - accuracy: 1.0000\n",
      "Epoch 20/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0160 - accuracy: 1.0000\n",
      "Epoch 21/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0148 - accuracy: 1.0000\n",
      "Epoch 22/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0138 - accuracy: 1.0000\n",
      "Epoch 23/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0128 - accuracy: 1.0000\n",
      "Epoch 24/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0120 - accuracy: 1.0000\n",
      "Epoch 25/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0112 - accuracy: 1.0000\n",
      "Epoch 26/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0105 - accuracy: 1.0000\n",
      "Epoch 27/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0099 - accuracy: 1.0000\n",
      "Epoch 28/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0094 - accuracy: 1.0000\n",
      "Epoch 29/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0088 - accuracy: 1.0000\n",
      "Epoch 30/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0084 - accuracy: 1.0000\n",
      "Epoch 31/40\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0079 - accuracy: 1.0000\n",
      "Epoch 32/40\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0076 - accuracy: 1.0000\n",
      "Epoch 33/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0072 - accuracy: 1.0000\n",
      "Epoch 34/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0068 - accuracy: 1.0000\n",
      "Epoch 35/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0065 - accuracy: 1.0000\n",
      "Epoch 36/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0062 - accuracy: 1.0000\n",
      "Epoch 37/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0060 - accuracy: 1.0000\n",
      "Epoch 38/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0057 - accuracy: 1.0000\n",
      "Epoch 39/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0055 - accuracy: 1.0000\n",
      "Epoch 40/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0052 - accuracy: 1.0000\n",
      "Epoch 1/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.6945 - accuracy: 0.3628\n",
      "Epoch 2/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.8912 - accuracy: 0.6549\n",
      "Epoch 3/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4565 - accuracy: 0.8628\n",
      "Epoch 4/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.2661 - accuracy: 0.9381\n",
      "Epoch 5/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1621 - accuracy: 0.9690\n",
      "Epoch 6/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1172 - accuracy: 0.9779\n",
      "Epoch 7/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0882 - accuracy: 0.9912\n",
      "Epoch 8/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0702 - accuracy: 0.9956\n",
      "Epoch 9/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0576 - accuracy: 1.0000\n",
      "Epoch 10/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0485 - accuracy: 1.0000\n",
      "Epoch 11/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0419 - accuracy: 1.0000\n",
      "Epoch 12/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0365 - accuracy: 1.0000\n",
      "Epoch 13/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0324 - accuracy: 1.0000\n",
      "Epoch 14/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0291 - accuracy: 1.0000\n",
      "Epoch 15/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0262 - accuracy: 1.0000\n",
      "Epoch 16/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0236 - accuracy: 1.0000\n",
      "Epoch 17/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0216 - accuracy: 1.0000\n",
      "Epoch 18/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0198 - accuracy: 1.0000\n",
      "Epoch 19/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0183 - accuracy: 1.0000\n",
      "Epoch 20/40\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0169 - accuracy: 1.0000\n",
      "Epoch 21/40\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0156 - accuracy: 1.0000\n",
      "Epoch 22/40\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0146 - accuracy: 1.0000\n",
      "Epoch 23/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0136 - accuracy: 1.0000\n",
      "Epoch 24/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0128 - accuracy: 1.0000\n",
      "Epoch 25/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0120 - accuracy: 1.0000\n",
      "Epoch 26/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0113 - accuracy: 1.0000\n",
      "Epoch 27/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0106 - accuracy: 1.0000\n",
      "Epoch 28/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0100 - accuracy: 1.0000\n",
      "Epoch 29/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0095 - accuracy: 1.0000\n",
      "Epoch 30/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0090 - accuracy: 1.0000\n",
      "Epoch 31/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0086 - accuracy: 1.0000\n",
      "Epoch 32/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0081 - accuracy: 1.0000\n",
      "Epoch 33/40\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0077 - accuracy: 1.0000\n",
      "Epoch 34/40\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0074 - accuracy: 1.0000\n",
      "Epoch 35/40\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0071 - accuracy: 1.0000\n",
      "Epoch 36/40\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0067 - accuracy: 1.0000\n",
      "Epoch 37/40\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0064 - accuracy: 1.0000\n",
      "Epoch 38/40\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0062 - accuracy: 1.0000\n",
      "Epoch 39/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0059 - accuracy: 1.0000\n",
      "Epoch 40/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0057 - accuracy: 1.0000\n",
      "Epoch 1/40\n",
      "8/8 [==============================] - 1s 6ms/step - loss: 1.0766 - accuracy: 0.5088\n",
      "Epoch 2/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.5441 - accuracy: 0.7920\n",
      "Epoch 3/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.3099 - accuracy: 0.8982\n",
      "Epoch 4/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1994 - accuracy: 0.9381\n",
      "Epoch 5/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1385 - accuracy: 0.9735\n",
      "Epoch 6/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1021 - accuracy: 0.9779\n",
      "Epoch 7/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0754 - accuracy: 0.9912\n",
      "Epoch 8/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0621 - accuracy: 0.9956\n",
      "Epoch 9/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0501 - accuracy: 0.9956\n",
      "Epoch 10/40\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0423 - accuracy: 0.9956\n",
      "Epoch 11/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0363 - accuracy: 1.0000\n",
      "Epoch 12/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0316 - accuracy: 1.0000\n",
      "Epoch 13/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0277 - accuracy: 1.0000\n",
      "Epoch 14/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0249 - accuracy: 1.0000\n",
      "Epoch 15/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0223 - accuracy: 1.0000\n",
      "Epoch 16/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0201 - accuracy: 1.0000\n",
      "Epoch 17/40\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0183 - accuracy: 1.0000\n",
      "Epoch 18/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0167 - accuracy: 1.0000\n",
      "Epoch 19/40\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0153 - accuracy: 1.0000\n",
      "Epoch 20/40\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0141 - accuracy: 1.0000\n",
      "Epoch 21/40\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0131 - accuracy: 1.0000\n",
      "Epoch 22/40\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0121 - accuracy: 1.0000\n",
      "Epoch 23/40\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0113 - accuracy: 1.0000\n",
      "Epoch 24/40\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0106 - accuracy: 1.0000\n",
      "Epoch 25/40\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0099 - accuracy: 1.0000\n",
      "Epoch 26/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0093 - accuracy: 1.0000\n",
      "Epoch 27/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0088 - accuracy: 1.0000\n",
      "Epoch 28/40\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0082 - accuracy: 1.0000\n",
      "Epoch 29/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0078 - accuracy: 1.0000\n",
      "Epoch 30/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0074 - accuracy: 1.0000\n",
      "Epoch 31/40\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0070 - accuracy: 1.0000\n",
      "Epoch 32/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0066 - accuracy: 1.0000\n",
      "Epoch 33/40\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0063 - accuracy: 1.0000\n",
      "Epoch 34/40\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0060 - accuracy: 1.0000\n",
      "Epoch 35/40\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0057 - accuracy: 1.0000\n",
      "Epoch 36/40\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 0.0055 - accuracy: 1.0000\n",
      "Epoch 37/40\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0052 - accuracy: 1.0000\n",
      "Epoch 38/40\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.0051 - accuracy: 1.00 - 0s 26ms/step - loss: 0.0050 - accuracy: 1.0000\n",
      "Epoch 39/40\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0048 - accuracy: 1.0000\n",
      "Epoch 40/40\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0046 - accuracy: 1.0000\n",
      "Epoch 1/40\n",
      "8/8 [==============================] - 1s 13ms/step - loss: 1.9322 - accuracy: 0.2345\n",
      "Epoch 2/40\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1.0198 - accuracy: 0.5088\n",
      "Epoch 3/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.5536 - accuracy: 0.8053\n",
      "Epoch 4/40\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.3322 - accuracy: 0.8982\n",
      "Epoch 5/40\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.2215 - accuracy: 0.9425\n",
      "Epoch 6/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1599 - accuracy: 0.9646\n",
      "Epoch 7/40\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1220 - accuracy: 0.9690\n",
      "Epoch 8/40\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0975 - accuracy: 0.9912\n",
      "Epoch 9/40\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0807 - accuracy: 0.9956\n",
      "Epoch 10/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0682 - accuracy: 0.9956\n",
      "Epoch 11/40\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0586 - accuracy: 0.9956\n",
      "Epoch 12/40\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0513 - accuracy: 0.9956\n",
      "Epoch 13/40\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0453 - accuracy: 0.9956\n",
      "Epoch 14/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0404 - accuracy: 0.9956\n",
      "Epoch 15/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0359 - accuracy: 1.0000\n",
      "Epoch 16/40\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0327 - accuracy: 1.0000\n",
      "Epoch 17/40\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0292 - accuracy: 1.0000\n",
      "Epoch 18/40\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0267 - accuracy: 1.0000\n",
      "Epoch 19/40\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0244 - accuracy: 1.0000\n",
      "Epoch 20/40\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0225 - accuracy: 1.0000\n",
      "Epoch 21/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0207 - accuracy: 1.0000\n",
      "Epoch 22/40\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0193 - accuracy: 1.0000\n",
      "Epoch 23/40\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0180 - accuracy: 1.0000\n",
      "Epoch 24/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0167 - accuracy: 1.0000\n",
      "Epoch 25/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0157 - accuracy: 1.0000\n",
      "Epoch 26/40\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0147 - accuracy: 1.0000\n",
      "Epoch 27/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0138 - accuracy: 1.0000\n",
      "Epoch 28/40\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0130 - accuracy: 1.0000\n",
      "Epoch 29/40\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0123 - accuracy: 1.0000\n",
      "Epoch 30/40\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0116 - accuracy: 1.0000\n",
      "Epoch 31/40\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0110 - accuracy: 1.0000\n",
      "Epoch 32/40\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0105 - accuracy: 1.0000\n",
      "Epoch 33/40\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0099 - accuracy: 1.0000\n",
      "Epoch 34/40\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0095 - accuracy: 1.0000\n",
      "Epoch 35/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0090 - accuracy: 1.0000\n",
      "Epoch 36/40\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0086 - accuracy: 1.0000\n",
      "Epoch 37/40\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0082 - accuracy: 1.0000\n",
      "Epoch 38/40\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0079 - accuracy: 1.0000\n",
      "Epoch 39/40\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0075 - accuracy: 1.0000\n",
      "Epoch 40/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0072 - accuracy: 1.0000\n",
      "Epoch 1/40\n",
      "8/8 [==============================] - 1s 7ms/step - loss: 1.2511 - accuracy: 0.6018\n",
      "Epoch 2/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.6853 - accuracy: 0.7389\n",
      "Epoch 3/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.3723 - accuracy: 0.8850\n",
      "Epoch 4/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.2251 - accuracy: 0.9513\n",
      "Epoch 5/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1485 - accuracy: 0.9867\n",
      "Epoch 6/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1086 - accuracy: 0.9912\n",
      "Epoch 7/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0816 - accuracy: 0.9956\n",
      "Epoch 8/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0651 - accuracy: 0.9956\n",
      "Epoch 9/40\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0534 - accuracy: 1.0000\n",
      "Epoch 10/40\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0451 - accuracy: 1.0000\n",
      "Epoch 11/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0387 - accuracy: 1.0000\n",
      "Epoch 12/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0338 - accuracy: 1.0000\n",
      "Epoch 13/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0300 - accuracy: 1.0000\n",
      "Epoch 14/40\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0268 - accuracy: 1.0000\n",
      "Epoch 15/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0240 - accuracy: 1.0000\n",
      "Epoch 16/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0218 - accuracy: 1.0000\n",
      "Epoch 17/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0199 - accuracy: 1.0000\n",
      "Epoch 18/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0182 - accuracy: 1.0000\n",
      "Epoch 19/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0167 - accuracy: 1.0000\n",
      "Epoch 20/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0155 - accuracy: 1.0000\n",
      "Epoch 21/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0143 - accuracy: 1.0000\n",
      "Epoch 22/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0133 - accuracy: 1.0000\n",
      "Epoch 23/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0124 - accuracy: 1.0000\n",
      "Epoch 24/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0116 - accuracy: 1.0000\n",
      "Epoch 25/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0109 - accuracy: 1.0000\n",
      "Epoch 26/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0102 - accuracy: 1.0000\n",
      "Epoch 27/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0096 - accuracy: 1.0000\n",
      "Epoch 28/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0091 - accuracy: 1.0000\n",
      "Epoch 29/40\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0086 - accuracy: 1.0000\n",
      "Epoch 30/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0081 - accuracy: 1.0000\n",
      "Epoch 31/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0077 - accuracy: 1.0000\n",
      "Epoch 32/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0073 - accuracy: 1.0000\n",
      "Epoch 33/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0069 - accuracy: 1.0000\n",
      "Epoch 34/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0066 - accuracy: 1.0000\n",
      "Epoch 35/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0063 - accuracy: 1.0000\n",
      "Epoch 36/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0060 - accuracy: 1.0000\n",
      "Epoch 37/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0057 - accuracy: 1.0000\n",
      "Epoch 38/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0055 - accuracy: 1.0000\n",
      "Epoch 39/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0052 - accuracy: 1.0000\n",
      "Epoch 40/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0050 - accuracy: 1.0000\n",
      "Epoch 1/40\n",
      "8/8 [==============================] - 1s 7ms/step - loss: 2.2988 - accuracy: 0.1726\n",
      "Epoch 2/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.2562 - accuracy: 0.4381\n",
      "Epoch 3/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.6739 - accuracy: 0.7434\n",
      "Epoch 4/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3898 - accuracy: 0.8938\n",
      "Epoch 5/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.2520 - accuracy: 0.9425\n",
      "Epoch 6/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1783 - accuracy: 0.9646\n",
      "Epoch 7/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1330 - accuracy: 0.9779\n",
      "Epoch 8/40\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1046 - accuracy: 0.9867\n",
      "Epoch 9/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0853 - accuracy: 1.0000\n",
      "Epoch 10/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0709 - accuracy: 1.0000\n",
      "Epoch 11/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0603 - accuracy: 1.0000\n",
      "Epoch 12/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0519 - accuracy: 1.0000\n",
      "Epoch 13/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0454 - accuracy: 1.0000\n",
      "Epoch 14/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0404 - accuracy: 1.0000\n",
      "Epoch 15/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0363 - accuracy: 1.0000\n",
      "Epoch 16/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0328 - accuracy: 1.0000\n",
      "Epoch 17/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0299 - accuracy: 1.0000\n",
      "Epoch 18/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0273 - accuracy: 1.0000\n",
      "Epoch 19/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0251 - accuracy: 1.0000\n",
      "Epoch 20/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0231 - accuracy: 1.0000\n",
      "Epoch 21/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0214 - accuracy: 1.0000\n",
      "Epoch 22/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0199 - accuracy: 1.0000\n",
      "Epoch 23/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0185 - accuracy: 1.0000\n",
      "Epoch 24/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0173 - accuracy: 1.0000\n",
      "Epoch 25/40\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0162 - accuracy: 1.0000\n",
      "Epoch 26/40\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0152 - accuracy: 1.0000\n",
      "Epoch 27/40\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0143 - accuracy: 1.0000\n",
      "Epoch 28/40\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0135 - accuracy: 1.0000\n",
      "Epoch 29/40\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0128 - accuracy: 1.0000\n",
      "Epoch 30/40\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0121 - accuracy: 1.0000\n",
      "Epoch 31/40\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0115 - accuracy: 1.0000\n",
      "Epoch 32/40\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0109 - accuracy: 1.0000\n",
      "Epoch 33/40\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0104 - accuracy: 1.0000\n",
      "Epoch 34/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0099 - accuracy: 1.0000\n",
      "Epoch 35/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0094 - accuracy: 1.0000\n",
      "Epoch 36/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0090 - accuracy: 1.0000\n",
      "Epoch 37/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0086 - accuracy: 1.0000\n",
      "Epoch 38/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0082 - accuracy: 1.0000\n",
      "Epoch 39/40\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0079 - accuracy: 1.0000\n",
      "Epoch 40/40\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0076 - accuracy: 1.0000\n",
      "Epoch 1/40\n",
      "8/8 [==============================] - 1s 14ms/step - loss: 1.7682 - accuracy: 0.3009\n",
      "Epoch 2/40\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.8958 - accuracy: 0.5973\n",
      "Epoch 3/40\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.4699 - accuracy: 0.8407\n",
      "Epoch 4/40\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.2784 - accuracy: 0.9336\n",
      "Epoch 5/40\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1820 - accuracy: 0.9690\n",
      "Epoch 6/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1316 - accuracy: 0.9823\n",
      "Epoch 7/40\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1020 - accuracy: 0.9912\n",
      "Epoch 8/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0805 - accuracy: 0.9912\n",
      "Epoch 9/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0665 - accuracy: 0.9956\n",
      "Epoch 10/40\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0565 - accuracy: 1.0000\n",
      "Epoch 11/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0489 - accuracy: 1.0000\n",
      "Epoch 12/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0429 - accuracy: 1.0000\n",
      "Epoch 13/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0378 - accuracy: 1.0000\n",
      "Epoch 14/40\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0337 - accuracy: 1.0000\n",
      "Epoch 15/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0305 - accuracy: 1.0000\n",
      "Epoch 16/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0278 - accuracy: 1.0000\n",
      "Epoch 17/40\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0252 - accuracy: 1.0000\n",
      "Epoch 18/40\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0231 - accuracy: 1.0000\n",
      "Epoch 19/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0213 - accuracy: 1.0000\n",
      "Epoch 20/40\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0196 - accuracy: 1.0000\n",
      "Epoch 21/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0181 - accuracy: 1.0000\n",
      "Epoch 22/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0170 - accuracy: 1.0000\n",
      "Epoch 23/40\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0158 - accuracy: 1.0000\n",
      "Epoch 24/40\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0148 - accuracy: 1.0000\n",
      "Epoch 25/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0139 - accuracy: 1.0000\n",
      "Epoch 26/40\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0130 - accuracy: 1.0000\n",
      "Epoch 27/40\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0123 - accuracy: 1.0000\n",
      "Epoch 28/40\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0116 - accuracy: 1.0000\n",
      "Epoch 29/40\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0109 - accuracy: 1.0000\n",
      "Epoch 30/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0103 - accuracy: 1.0000\n",
      "Epoch 31/40\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0098 - accuracy: 1.0000\n",
      "Epoch 32/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0093 - accuracy: 1.0000\n",
      "Epoch 33/40\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0089 - accuracy: 1.0000\n",
      "Epoch 34/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0084 - accuracy: 1.0000\n",
      "Epoch 35/40\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0080 - accuracy: 1.0000\n",
      "Epoch 36/40\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0077 - accuracy: 1.0000\n",
      "Epoch 37/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0073 - accuracy: 1.0000\n",
      "Epoch 38/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0070 - accuracy: 1.0000\n",
      "Epoch 39/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0067 - accuracy: 1.0000\n",
      "Epoch 40/40\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0065 - accuracy: 1.0000\n",
      "Epoch 1/50\n",
      "8/8 [==============================] - 1s 6ms/step - loss: 1.6799 - accuracy: 0.3689\n",
      "Epoch 2/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.8826 - accuracy: 0.6089\n",
      "Epoch 3/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.4527 - accuracy: 0.8267\n",
      "Epoch 4/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.2643 - accuracy: 0.9378\n",
      "Epoch 5/50\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1745 - accuracy: 0.9644\n",
      "Epoch 6/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1262 - accuracy: 0.9733\n",
      "Epoch 7/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0967 - accuracy: 0.9911\n",
      "Epoch 8/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0760 - accuracy: 0.9956\n",
      "Epoch 9/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0636 - accuracy: 1.0000\n",
      "Epoch 10/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0540 - accuracy: 1.0000\n",
      "Epoch 11/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0467 - accuracy: 1.0000\n",
      "Epoch 12/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0408 - accuracy: 1.0000\n",
      "Epoch 13/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0362 - accuracy: 1.0000\n",
      "Epoch 14/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0325 - accuracy: 1.0000\n",
      "Epoch 15/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0291 - accuracy: 1.0000\n",
      "Epoch 16/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0264 - accuracy: 1.0000\n",
      "Epoch 17/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0242 - accuracy: 1.0000\n",
      "Epoch 18/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0221 - accuracy: 1.0000\n",
      "Epoch 19/50\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.0224 - accuracy: 1.00 - 0s 5ms/step - loss: 0.0204 - accuracy: 1.0000\n",
      "Epoch 20/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0189 - accuracy: 1.0000\n",
      "Epoch 21/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0175 - accuracy: 1.0000\n",
      "Epoch 22/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0163 - accuracy: 1.0000\n",
      "Epoch 23/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0152 - accuracy: 1.0000\n",
      "Epoch 24/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0143 - accuracy: 1.0000\n",
      "Epoch 25/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0134 - accuracy: 1.0000\n",
      "Epoch 26/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0126 - accuracy: 1.0000\n",
      "Epoch 27/50\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0119 - accuracy: 1.0000\n",
      "Epoch 28/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0112 - accuracy: 1.0000\n",
      "Epoch 29/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0106 - accuracy: 1.0000\n",
      "Epoch 30/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0100 - accuracy: 1.0000\n",
      "Epoch 31/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0095 - accuracy: 1.0000\n",
      "Epoch 32/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0090 - accuracy: 1.0000\n",
      "Epoch 33/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0086 - accuracy: 1.0000\n",
      "Epoch 34/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0082 - accuracy: 1.0000\n",
      "Epoch 35/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0078 - accuracy: 1.0000\n",
      "Epoch 36/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0074 - accuracy: 1.0000\n",
      "Epoch 37/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0071 - accuracy: 1.0000\n",
      "Epoch 38/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0068 - accuracy: 1.0000\n",
      "Epoch 39/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0065 - accuracy: 1.0000\n",
      "Epoch 40/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0063 - accuracy: 1.0000\n",
      "Epoch 41/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0060 - accuracy: 1.0000\n",
      "Epoch 42/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0058 - accuracy: 1.0000\n",
      "Epoch 43/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0056 - accuracy: 1.0000\n",
      "Epoch 44/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0054 - accuracy: 1.0000\n",
      "Epoch 45/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0052 - accuracy: 1.0000\n",
      "Epoch 46/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0050 - accuracy: 1.0000\n",
      "Epoch 47/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0048 - accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0046 - accuracy: 1.0000\n",
      "Epoch 49/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0045 - accuracy: 1.0000\n",
      "Epoch 50/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0043 - accuracy: 1.0000\n",
      "Epoch 1/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.4093 - accuracy: 0.3628\n",
      "Epoch 2/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.7283 - accuracy: 0.6814\n",
      "Epoch 3/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.4083 - accuracy: 0.8628\n",
      "Epoch 4/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.2643 - accuracy: 0.9204\n",
      "Epoch 5/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1884 - accuracy: 0.9513\n",
      "Epoch 6/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1375 - accuracy: 0.9867\n",
      "Epoch 7/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1072 - accuracy: 0.9956\n",
      "Epoch 8/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0852 - accuracy: 0.9956\n",
      "Epoch 9/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0702 - accuracy: 1.0000\n",
      "Epoch 10/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0588 - accuracy: 1.0000\n",
      "Epoch 11/50\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0509 - accuracy: 1.0000\n",
      "Epoch 12/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0440 - accuracy: 1.0000\n",
      "Epoch 13/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0389 - accuracy: 1.0000\n",
      "Epoch 14/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0347 - accuracy: 1.0000\n",
      "Epoch 15/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0313 - accuracy: 1.0000\n",
      "Epoch 16/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0283 - accuracy: 1.0000\n",
      "Epoch 17/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0258 - accuracy: 1.0000\n",
      "Epoch 18/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0235 - accuracy: 1.0000\n",
      "Epoch 19/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0216 - accuracy: 1.0000\n",
      "Epoch 20/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0199 - accuracy: 1.0000\n",
      "Epoch 21/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0184 - accuracy: 1.0000\n",
      "Epoch 22/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0171 - accuracy: 1.0000\n",
      "Epoch 23/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0159 - accuracy: 1.0000\n",
      "Epoch 24/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0149 - accuracy: 1.0000\n",
      "Epoch 25/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0139 - accuracy: 1.0000\n",
      "Epoch 26/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0131 - accuracy: 1.0000\n",
      "Epoch 27/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0123 - accuracy: 1.0000\n",
      "Epoch 28/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0116 - accuracy: 1.0000\n",
      "Epoch 29/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0110 - accuracy: 1.0000\n",
      "Epoch 30/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0104 - accuracy: 1.0000\n",
      "Epoch 31/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0098 - accuracy: 1.0000\n",
      "Epoch 32/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0094 - accuracy: 1.0000\n",
      "Epoch 33/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0089 - accuracy: 1.0000\n",
      "Epoch 34/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0084 - accuracy: 1.0000\n",
      "Epoch 35/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0081 - accuracy: 1.0000\n",
      "Epoch 36/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0077 - accuracy: 1.0000\n",
      "Epoch 37/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0073 - accuracy: 1.0000\n",
      "Epoch 38/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0070 - accuracy: 1.0000\n",
      "Epoch 39/50\n",
      "8/8 [==============================] - 0s 41ms/step - loss: 0.0067 - accuracy: 1.0000\n",
      "Epoch 40/50\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0064 - accuracy: 1.0000\n",
      "Epoch 41/50\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0062 - accuracy: 1.0000\n",
      "Epoch 42/50\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0059 - accuracy: 1.0000\n",
      "Epoch 43/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0057 - accuracy: 1.0000\n",
      "Epoch 44/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0055 - accuracy: 1.0000\n",
      "Epoch 45/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0053 - accuracy: 1.0000\n",
      "Epoch 46/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0051 - accuracy: 1.0000\n",
      "Epoch 47/50\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0049 - accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0047 - accuracy: 1.0000\n",
      "Epoch 49/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0046 - accuracy: 1.0000\n",
      "Epoch 50/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0044 - accuracy: 1.0000\n",
      "Epoch 1/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.3884 - accuracy: 0.3850\n",
      "Epoch 2/50\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.7031 - accuracy: 0.7212\n",
      "Epoch 3/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.3760 - accuracy: 0.8938\n",
      "Epoch 4/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.2404 - accuracy: 0.9558\n",
      "Epoch 5/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1686 - accuracy: 0.9646\n",
      "Epoch 6/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1260 - accuracy: 0.9779\n",
      "Epoch 7/50\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0987 - accuracy: 0.9867\n",
      "Epoch 8/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0791 - accuracy: 0.9912\n",
      "Epoch 9/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0653 - accuracy: 0.9912\n",
      "Epoch 10/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0563 - accuracy: 0.9956\n",
      "Epoch 11/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0478 - accuracy: 1.0000\n",
      "Epoch 12/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0412 - accuracy: 1.0000\n",
      "Epoch 13/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0362 - accuracy: 1.0000\n",
      "Epoch 14/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0323 - accuracy: 1.0000\n",
      "Epoch 15/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0288 - accuracy: 1.0000\n",
      "Epoch 16/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0260 - accuracy: 1.0000\n",
      "Epoch 17/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0237 - accuracy: 1.0000\n",
      "Epoch 18/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0216 - accuracy: 1.0000\n",
      "Epoch 19/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0199 - accuracy: 1.0000\n",
      "Epoch 20/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0183 - accuracy: 1.0000\n",
      "Epoch 21/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0169 - accuracy: 1.0000\n",
      "Epoch 22/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0157 - accuracy: 1.0000\n",
      "Epoch 23/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0146 - accuracy: 1.0000\n",
      "Epoch 24/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0136 - accuracy: 1.0000\n",
      "Epoch 25/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0128 - accuracy: 1.0000\n",
      "Epoch 26/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0120 - accuracy: 1.0000\n",
      "Epoch 27/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0112 - accuracy: 1.0000\n",
      "Epoch 28/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0106 - accuracy: 1.0000\n",
      "Epoch 29/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0100 - accuracy: 1.0000\n",
      "Epoch 30/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0094 - accuracy: 1.0000\n",
      "Epoch 31/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0089 - accuracy: 1.0000\n",
      "Epoch 32/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0085 - accuracy: 1.0000\n",
      "Epoch 33/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0081 - accuracy: 1.0000\n",
      "Epoch 34/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0077 - accuracy: 1.0000\n",
      "Epoch 35/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0073 - accuracy: 1.0000\n",
      "Epoch 36/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0070 - accuracy: 1.0000\n",
      "Epoch 37/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0066 - accuracy: 1.0000\n",
      "Epoch 38/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0064 - accuracy: 1.0000\n",
      "Epoch 39/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0061 - accuracy: 1.0000\n",
      "Epoch 40/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0058 - accuracy: 1.0000\n",
      "Epoch 41/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0056 - accuracy: 1.0000\n",
      "Epoch 42/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0054 - accuracy: 1.0000\n",
      "Epoch 43/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0052 - accuracy: 1.0000\n",
      "Epoch 44/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0050 - accuracy: 1.0000\n",
      "Epoch 45/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0048 - accuracy: 1.0000\n",
      "Epoch 46/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0046 - accuracy: 1.0000\n",
      "Epoch 47/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0044 - accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0043 - accuracy: 1.0000\n",
      "Epoch 49/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0041 - accuracy: 1.0000\n",
      "Epoch 50/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0040 - accuracy: 1.0000\n",
      "Epoch 1/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.0862 - accuracy: 0.5398\n",
      "Epoch 2/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.5744 - accuracy: 0.7611\n",
      "Epoch 3/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3222 - accuracy: 0.8982\n",
      "Epoch 4/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1980 - accuracy: 0.9513\n",
      "Epoch 5/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1321 - accuracy: 0.9823\n",
      "Epoch 6/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0979 - accuracy: 0.9912\n",
      "Epoch 7/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0744 - accuracy: 0.9912\n",
      "Epoch 8/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0592 - accuracy: 1.0000\n",
      "Epoch 9/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0500 - accuracy: 1.0000\n",
      "Epoch 10/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0417 - accuracy: 1.0000\n",
      "Epoch 11/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0359 - accuracy: 1.0000\n",
      "Epoch 12/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0316 - accuracy: 1.0000\n",
      "Epoch 13/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0277 - accuracy: 1.0000\n",
      "Epoch 14/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0249 - accuracy: 1.0000\n",
      "Epoch 15/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0223 - accuracy: 1.0000\n",
      "Epoch 16/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0203 - accuracy: 1.0000\n",
      "Epoch 17/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0185 - accuracy: 1.0000\n",
      "Epoch 18/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0169 - accuracy: 1.0000\n",
      "Epoch 19/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0155 - accuracy: 1.0000\n",
      "Epoch 20/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0144 - accuracy: 1.0000\n",
      "Epoch 21/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0132 - accuracy: 1.0000\n",
      "Epoch 22/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0123 - accuracy: 1.0000\n",
      "Epoch 23/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0115 - accuracy: 1.0000\n",
      "Epoch 24/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0108 - accuracy: 1.0000\n",
      "Epoch 25/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0101 - accuracy: 1.0000\n",
      "Epoch 26/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0095 - accuracy: 1.0000\n",
      "Epoch 27/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0089 - accuracy: 1.0000\n",
      "Epoch 28/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0084 - accuracy: 1.0000\n",
      "Epoch 29/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0079 - accuracy: 1.0000\n",
      "Epoch 30/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0075 - accuracy: 1.0000\n",
      "Epoch 31/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0071 - accuracy: 1.0000\n",
      "Epoch 32/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0067 - accuracy: 1.0000\n",
      "Epoch 33/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0064 - accuracy: 1.0000\n",
      "Epoch 34/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0061 - accuracy: 1.0000\n",
      "Epoch 35/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0058 - accuracy: 1.0000\n",
      "Epoch 36/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0056 - accuracy: 1.0000\n",
      "Epoch 37/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0053 - accuracy: 1.0000\n",
      "Epoch 38/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0051 - accuracy: 1.0000\n",
      "Epoch 39/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0049 - accuracy: 1.0000\n",
      "Epoch 40/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0047 - accuracy: 1.0000\n",
      "Epoch 41/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0045 - accuracy: 1.0000\n",
      "Epoch 42/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0043 - accuracy: 1.0000\n",
      "Epoch 43/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0042 - accuracy: 1.0000\n",
      "Epoch 44/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0040 - accuracy: 1.0000\n",
      "Epoch 45/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0039 - accuracy: 1.0000\n",
      "Epoch 46/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0037 - accuracy: 1.0000\n",
      "Epoch 47/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 49/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 50/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 1/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.8042 - accuracy: 0.3850\n",
      "Epoch 2/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.9876 - accuracy: 0.6150\n",
      "Epoch 3/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5417 - accuracy: 0.8009\n",
      "Epoch 4/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3366 - accuracy: 0.8938\n",
      "Epoch 5/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.2239 - accuracy: 0.9381\n",
      "Epoch 6/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1620 - accuracy: 0.9602\n",
      "Epoch 7/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1245 - accuracy: 0.9779\n",
      "Epoch 8/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0981 - accuracy: 0.9823\n",
      "Epoch 9/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0780 - accuracy: 0.9867\n",
      "Epoch 10/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0638 - accuracy: 1.0000\n",
      "Epoch 11/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0540 - accuracy: 1.0000\n",
      "Epoch 12/50\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0463 - accuracy: 1.0000\n",
      "Epoch 13/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0405 - accuracy: 1.0000\n",
      "Epoch 14/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0356 - accuracy: 1.0000\n",
      "Epoch 15/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0315 - accuracy: 1.0000\n",
      "Epoch 16/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0285 - accuracy: 1.0000\n",
      "Epoch 17/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0256 - accuracy: 1.0000\n",
      "Epoch 18/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0232 - accuracy: 1.0000\n",
      "Epoch 19/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0213 - accuracy: 1.0000\n",
      "Epoch 20/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0195 - accuracy: 1.0000\n",
      "Epoch 21/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0180 - accuracy: 1.0000\n",
      "Epoch 22/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0166 - accuracy: 1.0000\n",
      "Epoch 23/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0155 - accuracy: 1.0000\n",
      "Epoch 24/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0144 - accuracy: 1.0000\n",
      "Epoch 25/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0135 - accuracy: 1.0000\n",
      "Epoch 26/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0126 - accuracy: 1.0000\n",
      "Epoch 27/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0118 - accuracy: 1.0000\n",
      "Epoch 28/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0111 - accuracy: 1.0000\n",
      "Epoch 29/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0105 - accuracy: 1.0000\n",
      "Epoch 30/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0099 - accuracy: 1.0000\n",
      "Epoch 31/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0094 - accuracy: 1.0000\n",
      "Epoch 32/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0089 - accuracy: 1.0000\n",
      "Epoch 33/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0085 - accuracy: 1.0000\n",
      "Epoch 34/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0081 - accuracy: 1.0000\n",
      "Epoch 35/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0077 - accuracy: 1.0000\n",
      "Epoch 36/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0073 - accuracy: 1.0000\n",
      "Epoch 37/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0070 - accuracy: 1.0000\n",
      "Epoch 38/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0067 - accuracy: 1.0000\n",
      "Epoch 39/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0064 - accuracy: 1.0000\n",
      "Epoch 40/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0061 - accuracy: 1.0000\n",
      "Epoch 41/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0059 - accuracy: 1.0000\n",
      "Epoch 42/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0057 - accuracy: 1.0000\n",
      "Epoch 43/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0054 - accuracy: 1.0000\n",
      "Epoch 44/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0052 - accuracy: 1.0000\n",
      "Epoch 45/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0050 - accuracy: 1.0000\n",
      "Epoch 46/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0049 - accuracy: 1.0000\n",
      "Epoch 47/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0047 - accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0045 - accuracy: 1.0000\n",
      "Epoch 49/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0044 - accuracy: 1.0000\n",
      "Epoch 50/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0042 - accuracy: 1.0000\n",
      "Epoch 1/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.7884 - accuracy: 0.3673\n",
      "Epoch 2/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.9433 - accuracy: 0.6062\n",
      "Epoch 3/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.5124 - accuracy: 0.8097\n",
      "Epoch 4/50\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.2927 - accuracy: 0.9292\n",
      "Epoch 5/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1906 - accuracy: 0.9469\n",
      "Epoch 6/50\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1336 - accuracy: 0.9823\n",
      "Epoch 7/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1008 - accuracy: 0.9867\n",
      "Epoch 8/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0817 - accuracy: 0.9912\n",
      "Epoch 9/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0674 - accuracy: 0.9956\n",
      "Epoch 10/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0561 - accuracy: 0.9956\n",
      "Epoch 11/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0482 - accuracy: 1.0000\n",
      "Epoch 12/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0421 - accuracy: 1.0000\n",
      "Epoch 13/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0372 - accuracy: 1.0000\n",
      "Epoch 14/50\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0331 - accuracy: 1.0000\n",
      "Epoch 15/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0297 - accuracy: 1.0000\n",
      "Epoch 16/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0269 - accuracy: 1.0000\n",
      "Epoch 17/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0244 - accuracy: 1.0000\n",
      "Epoch 18/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0224 - accuracy: 1.0000\n",
      "Epoch 19/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0204 - accuracy: 1.0000\n",
      "Epoch 20/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0189 - accuracy: 1.0000\n",
      "Epoch 21/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0174 - accuracy: 1.0000\n",
      "Epoch 22/50\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0161 - accuracy: 1.0000\n",
      "Epoch 23/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0151 - accuracy: 1.0000\n",
      "Epoch 24/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0141 - accuracy: 1.0000\n",
      "Epoch 25/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0132 - accuracy: 1.0000\n",
      "Epoch 26/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0123 - accuracy: 1.0000\n",
      "Epoch 27/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0116 - accuracy: 1.0000\n",
      "Epoch 28/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0109 - accuracy: 1.0000\n",
      "Epoch 29/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0103 - accuracy: 1.0000\n",
      "Epoch 30/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0097 - accuracy: 1.0000\n",
      "Epoch 31/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0092 - accuracy: 1.0000\n",
      "Epoch 32/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0088 - accuracy: 1.0000\n",
      "Epoch 33/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0083 - accuracy: 1.0000\n",
      "Epoch 34/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0079 - accuracy: 1.0000\n",
      "Epoch 35/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0076 - accuracy: 1.0000\n",
      "Epoch 36/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0072 - accuracy: 1.0000\n",
      "Epoch 37/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0069 - accuracy: 1.0000\n",
      "Epoch 38/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0066 - accuracy: 1.0000\n",
      "Epoch 39/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0063 - accuracy: 1.0000\n",
      "Epoch 40/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0060 - accuracy: 1.0000\n",
      "Epoch 41/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0058 - accuracy: 1.0000\n",
      "Epoch 42/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0056 - accuracy: 1.0000\n",
      "Epoch 43/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0054 - accuracy: 1.0000\n",
      "Epoch 44/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0051 - accuracy: 1.0000\n",
      "Epoch 45/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0050 - accuracy: 1.0000\n",
      "Epoch 46/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0048 - accuracy: 1.0000\n",
      "Epoch 47/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0046 - accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0044 - accuracy: 1.0000\n",
      "Epoch 49/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0043 - accuracy: 1.0000\n",
      "Epoch 50/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0041 - accuracy: 1.0000\n",
      "Epoch 1/50\n",
      "8/8 [==============================] - 1s 9ms/step - loss: 1.2609 - accuracy: 0.4513\n",
      "Epoch 2/50\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.7093 - accuracy: 0.6814\n",
      "Epoch 3/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.4339 - accuracy: 0.8142\n",
      "Epoch 4/50\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.2842 - accuracy: 0.9027\n",
      "Epoch 5/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1996 - accuracy: 0.9381\n",
      "Epoch 6/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1500 - accuracy: 0.9646\n",
      "Epoch 7/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1147 - accuracy: 0.9867\n",
      "Epoch 8/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0910 - accuracy: 0.9912\n",
      "Epoch 9/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0747 - accuracy: 0.9956\n",
      "Epoch 10/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0622 - accuracy: 0.9956\n",
      "Epoch 11/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0532 - accuracy: 0.9956\n",
      "Epoch 12/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0456 - accuracy: 1.0000\n",
      "Epoch 13/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0398 - accuracy: 1.0000\n",
      "Epoch 14/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0350 - accuracy: 1.0000\n",
      "Epoch 15/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0311 - accuracy: 1.0000\n",
      "Epoch 16/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0281 - accuracy: 1.0000\n",
      "Epoch 17/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0252 - accuracy: 1.0000\n",
      "Epoch 18/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0229 - accuracy: 1.0000\n",
      "Epoch 19/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0208 - accuracy: 1.0000\n",
      "Epoch 20/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0191 - accuracy: 1.0000\n",
      "Epoch 21/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0177 - accuracy: 1.0000\n",
      "Epoch 22/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0163 - accuracy: 1.0000\n",
      "Epoch 23/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0151 - accuracy: 1.0000\n",
      "Epoch 24/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0142 - accuracy: 1.0000\n",
      "Epoch 25/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0132 - accuracy: 1.0000\n",
      "Epoch 26/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0123 - accuracy: 1.0000\n",
      "Epoch 27/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0116 - accuracy: 1.0000\n",
      "Epoch 28/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0109 - accuracy: 1.0000\n",
      "Epoch 29/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0103 - accuracy: 1.0000\n",
      "Epoch 30/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0097 - accuracy: 1.0000\n",
      "Epoch 31/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0092 - accuracy: 1.0000\n",
      "Epoch 32/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0087 - accuracy: 1.0000\n",
      "Epoch 33/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0083 - accuracy: 1.0000\n",
      "Epoch 34/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0079 - accuracy: 1.0000\n",
      "Epoch 35/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0075 - accuracy: 1.0000\n",
      "Epoch 36/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0071 - accuracy: 1.0000\n",
      "Epoch 37/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0068 - accuracy: 1.0000\n",
      "Epoch 38/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0065 - accuracy: 1.0000\n",
      "Epoch 39/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0062 - accuracy: 1.0000\n",
      "Epoch 40/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0059 - accuracy: 1.0000\n",
      "Epoch 41/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0057 - accuracy: 1.0000\n",
      "Epoch 42/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0055 - accuracy: 1.0000\n",
      "Epoch 43/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0052 - accuracy: 1.0000\n",
      "Epoch 44/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0050 - accuracy: 1.0000\n",
      "Epoch 45/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0049 - accuracy: 1.0000\n",
      "Epoch 46/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0047 - accuracy: 1.0000\n",
      "Epoch 47/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0045 - accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0043 - accuracy: 1.0000\n",
      "Epoch 49/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0042 - accuracy: 1.0000\n",
      "Epoch 50/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0040 - accuracy: 1.0000\n",
      "Epoch 1/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.3436 - accuracy: 0.4381\n",
      "Epoch 2/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.7373 - accuracy: 0.7080\n",
      "Epoch 3/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.4348 - accuracy: 0.8230\n",
      "Epoch 4/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.2725 - accuracy: 0.9159\n",
      "Epoch 5/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1895 - accuracy: 0.9425\n",
      "Epoch 6/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1412 - accuracy: 0.9735\n",
      "Epoch 7/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1075 - accuracy: 0.9867\n",
      "Epoch 8/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0857 - accuracy: 0.9956\n",
      "Epoch 9/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0707 - accuracy: 1.0000\n",
      "Epoch 10/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0594 - accuracy: 1.0000\n",
      "Epoch 11/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0507 - accuracy: 1.0000\n",
      "Epoch 12/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0445 - accuracy: 1.0000\n",
      "Epoch 13/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0392 - accuracy: 1.0000\n",
      "Epoch 14/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0347 - accuracy: 1.0000\n",
      "Epoch 15/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0311 - accuracy: 1.0000\n",
      "Epoch 16/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0281 - accuracy: 1.0000\n",
      "Epoch 17/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0255 - accuracy: 1.0000\n",
      "Epoch 18/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0232 - accuracy: 1.0000\n",
      "Epoch 19/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0214 - accuracy: 1.0000\n",
      "Epoch 20/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0197 - accuracy: 1.0000\n",
      "Epoch 21/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0182 - accuracy: 1.0000\n",
      "Epoch 22/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0169 - accuracy: 1.0000\n",
      "Epoch 23/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0157 - accuracy: 1.0000\n",
      "Epoch 24/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0146 - accuracy: 1.0000\n",
      "Epoch 25/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0137 - accuracy: 1.0000\n",
      "Epoch 26/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0128 - accuracy: 1.0000\n",
      "Epoch 27/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0121 - accuracy: 1.0000\n",
      "Epoch 28/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0114 - accuracy: 1.0000\n",
      "Epoch 29/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0107 - accuracy: 1.0000\n",
      "Epoch 30/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0101 - accuracy: 1.0000\n",
      "Epoch 31/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0096 - accuracy: 1.0000\n",
      "Epoch 32/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0091 - accuracy: 1.0000\n",
      "Epoch 33/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0086 - accuracy: 1.0000\n",
      "Epoch 34/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0082 - accuracy: 1.0000\n",
      "Epoch 35/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0078 - accuracy: 1.0000\n",
      "Epoch 36/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0075 - accuracy: 1.0000\n",
      "Epoch 37/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0071 - accuracy: 1.0000\n",
      "Epoch 38/50\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.0067 - accuracy: 1.00 - 0s 5ms/step - loss: 0.0068 - accuracy: 1.0000\n",
      "Epoch 39/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0065 - accuracy: 1.0000\n",
      "Epoch 40/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0063 - accuracy: 1.0000\n",
      "Epoch 41/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0060 - accuracy: 1.0000\n",
      "Epoch 42/50\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0058 - accuracy: 1.0000\n",
      "Epoch 43/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0055 - accuracy: 1.0000\n",
      "Epoch 44/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0053 - accuracy: 1.0000\n",
      "Epoch 45/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0051 - accuracy: 1.0000\n",
      "Epoch 46/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0049 - accuracy: 1.0000\n",
      "Epoch 47/50\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0048 - accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0046 - accuracy: 1.0000\n",
      "Epoch 49/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0044 - accuracy: 1.0000\n",
      "Epoch 50/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0043 - accuracy: 1.0000\n",
      "Epoch 1/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.5789 - accuracy: 0.3761\n",
      "Epoch 2/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.8394 - accuracy: 0.6106\n",
      "Epoch 3/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.4616 - accuracy: 0.8274\n",
      "Epoch 4/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.2744 - accuracy: 0.9071\n",
      "Epoch 5/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1864 - accuracy: 0.9558\n",
      "Epoch 6/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1367 - accuracy: 0.9779\n",
      "Epoch 7/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1047 - accuracy: 0.9867\n",
      "Epoch 8/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0846 - accuracy: 0.9912\n",
      "Epoch 9/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0700 - accuracy: 0.9956\n",
      "Epoch 10/50\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0589 - accuracy: 0.9956\n",
      "Epoch 11/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0509 - accuracy: 0.9956\n",
      "Epoch 12/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0447 - accuracy: 1.0000\n",
      "Epoch 13/50\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0397 - accuracy: 1.0000\n",
      "Epoch 14/50\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0355 - accuracy: 1.0000\n",
      "Epoch 15/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0316 - accuracy: 1.0000\n",
      "Epoch 16/50\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0288 - accuracy: 1.0000\n",
      "Epoch 17/50\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0262 - accuracy: 1.0000\n",
      "Epoch 18/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0239 - accuracy: 1.0000\n",
      "Epoch 19/50\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0220 - accuracy: 1.0000\n",
      "Epoch 20/50\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0202 - accuracy: 1.0000\n",
      "Epoch 21/50\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0188 - accuracy: 1.0000\n",
      "Epoch 22/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0174 - accuracy: 1.0000\n",
      "Epoch 23/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0162 - accuracy: 1.0000\n",
      "Epoch 24/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0151 - accuracy: 1.0000\n",
      "Epoch 25/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0141 - accuracy: 1.0000\n",
      "Epoch 26/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0133 - accuracy: 1.0000\n",
      "Epoch 27/50\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0125 - accuracy: 1.0000\n",
      "Epoch 28/50\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.0126 - accuracy: 1.00 - 0s 7ms/step - loss: 0.0118 - accuracy: 1.0000\n",
      "Epoch 29/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0111 - accuracy: 1.0000\n",
      "Epoch 30/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0105 - accuracy: 1.0000\n",
      "Epoch 31/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0099 - accuracy: 1.0000\n",
      "Epoch 32/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0094 - accuracy: 1.0000\n",
      "Epoch 33/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0089 - accuracy: 1.0000\n",
      "Epoch 34/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0085 - accuracy: 1.0000\n",
      "Epoch 35/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0081 - accuracy: 1.0000\n",
      "Epoch 36/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0077 - accuracy: 1.0000\n",
      "Epoch 37/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0074 - accuracy: 1.0000\n",
      "Epoch 38/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0071 - accuracy: 1.0000\n",
      "Epoch 39/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0067 - accuracy: 1.0000\n",
      "Epoch 40/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0065 - accuracy: 1.0000\n",
      "Epoch 41/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0062 - accuracy: 1.0000\n",
      "Epoch 42/50\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0060 - accuracy: 1.0000\n",
      "Epoch 43/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0057 - accuracy: 1.0000\n",
      "Epoch 44/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0055 - accuracy: 1.0000\n",
      "Epoch 45/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0053 - accuracy: 1.0000\n",
      "Epoch 46/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0051 - accuracy: 1.0000\n",
      "Epoch 47/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0049 - accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0047 - accuracy: 1.0000\n",
      "Epoch 49/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0046 - accuracy: 1.0000\n",
      "Epoch 50/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0044 - accuracy: 1.0000\n",
      "Epoch 1/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.4403 - accuracy: 0.4159\n",
      "Epoch 2/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.7685 - accuracy: 0.7257\n",
      "Epoch 3/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.4276 - accuracy: 0.8407\n",
      "Epoch 4/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.2723 - accuracy: 0.9115\n",
      "Epoch 5/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1809 - accuracy: 0.9513\n",
      "Epoch 6/50\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1324 - accuracy: 0.9646\n",
      "Epoch 7/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1002 - accuracy: 0.9823\n",
      "Epoch 8/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0790 - accuracy: 0.9912\n",
      "Epoch 9/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0639 - accuracy: 1.0000\n",
      "Epoch 10/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0531 - accuracy: 1.0000\n",
      "Epoch 11/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0463 - accuracy: 1.0000\n",
      "Epoch 12/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0404 - accuracy: 1.0000\n",
      "Epoch 13/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0359 - accuracy: 1.0000\n",
      "Epoch 14/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0318 - accuracy: 1.0000\n",
      "Epoch 15/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0288 - accuracy: 1.0000\n",
      "Epoch 16/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0258 - accuracy: 1.0000\n",
      "Epoch 17/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0235 - accuracy: 1.0000\n",
      "Epoch 18/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0216 - accuracy: 1.0000\n",
      "Epoch 19/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0197 - accuracy: 1.0000\n",
      "Epoch 20/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0183 - accuracy: 1.0000\n",
      "Epoch 21/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0169 - accuracy: 1.0000\n",
      "Epoch 22/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0158 - accuracy: 1.0000\n",
      "Epoch 23/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0147 - accuracy: 1.0000\n",
      "Epoch 24/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0137 - accuracy: 1.0000\n",
      "Epoch 25/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0129 - accuracy: 1.0000\n",
      "Epoch 26/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0121 - accuracy: 1.0000\n",
      "Epoch 27/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0114 - accuracy: 1.0000\n",
      "Epoch 28/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0107 - accuracy: 1.0000\n",
      "Epoch 29/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0101 - accuracy: 1.0000\n",
      "Epoch 30/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0096 - accuracy: 1.0000\n",
      "Epoch 31/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0091 - accuracy: 1.0000\n",
      "Epoch 32/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0086 - accuracy: 1.0000\n",
      "Epoch 33/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0082 - accuracy: 1.0000\n",
      "Epoch 34/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0078 - accuracy: 1.0000\n",
      "Epoch 35/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0075 - accuracy: 1.0000\n",
      "Epoch 36/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0071 - accuracy: 1.0000\n",
      "Epoch 37/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0068 - accuracy: 1.0000\n",
      "Epoch 38/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0065 - accuracy: 1.0000\n",
      "Epoch 39/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0063 - accuracy: 1.0000\n",
      "Epoch 40/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0060 - accuracy: 1.0000\n",
      "Epoch 41/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0058 - accuracy: 1.0000\n",
      "Epoch 42/50\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0055 - accuracy: 1.0000\n",
      "Epoch 43/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0053 - accuracy: 1.0000\n",
      "Epoch 44/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0051 - accuracy: 1.0000\n",
      "Epoch 45/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0049 - accuracy: 1.0000\n",
      "Epoch 46/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0048 - accuracy: 1.0000\n",
      "Epoch 47/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0046 - accuracy: 1.0000\n",
      "Epoch 48/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0044 - accuracy: 1.0000\n",
      "Epoch 49/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0043 - accuracy: 1.0000\n",
      "Epoch 50/50\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0041 - accuracy: 1.0000\n",
      "Epoch 1/10\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 2.8189 - accuracy: 0.1111\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.9456 - accuracy: 0.2178\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.2907 - accuracy: 0.4133\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.8228 - accuracy: 0.6311\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.5412 - accuracy: 0.8000\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.3619 - accuracy: 0.8889\n",
      "Epoch 7/10\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.2546 - accuracy: 0.9511\n",
      "Epoch 8/10\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1904 - accuracy: 0.9556\n",
      "Epoch 9/10\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.1480 - accuracy: 0.9689\n",
      "Epoch 10/10\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.1201 - accuracy: 0.9867\n",
      "Epoch 1/10\n",
      "5/5 [==============================] - 1s 6ms/step - loss: 1.8272 - accuracy: 0.3982\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.2772 - accuracy: 0.5487\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.8831 - accuracy: 0.6372\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.6070 - accuracy: 0.7876\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.4252 - accuracy: 0.8496\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.3001 - accuracy: 0.9204\n",
      "Epoch 7/10\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.2194 - accuracy: 0.9425\n",
      "Epoch 8/10\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1678 - accuracy: 0.9646\n",
      "Epoch 9/10\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1364 - accuracy: 0.9779\n",
      "Epoch 10/10\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1126 - accuracy: 0.9867\n",
      "Epoch 1/10\n",
      "5/5 [==============================] - 1s 6ms/step - loss: 1.6160 - accuracy: 0.3451\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.0050 - accuracy: 0.5708\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.6451 - accuracy: 0.7389\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.4323 - accuracy: 0.8540\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.3047 - accuracy: 0.9027\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.2231 - accuracy: 0.9469\n",
      "Epoch 7/10\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1728 - accuracy: 0.9558\n",
      "Epoch 8/10\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1363 - accuracy: 0.9779\n",
      "Epoch 9/10\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1121 - accuracy: 0.9912\n",
      "Epoch 10/10\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0922 - accuracy: 0.9912\n",
      "Epoch 1/10\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1.6938 - accuracy: 0.3673\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.0743 - accuracy: 0.5708\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.6972 - accuracy: 0.7257\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.4565 - accuracy: 0.8142\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.3255 - accuracy: 0.8982\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.2387 - accuracy: 0.9425\n",
      "Epoch 7/10\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1831 - accuracy: 0.9690\n",
      "Epoch 8/10\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1470 - accuracy: 0.9690\n",
      "Epoch 9/10\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.1195 - accuracy: 0.9823\n",
      "Epoch 10/10\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0992 - accuracy: 0.9867\n",
      "Epoch 1/10\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.4641 - accuracy: 0.3407\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.8954 - accuracy: 0.6062\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.5593 - accuracy: 0.7920\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.3710 - accuracy: 0.8938\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.2573 - accuracy: 0.9469\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1926 - accuracy: 0.9646\n",
      "Epoch 7/10\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.1504 - accuracy: 0.9690\n",
      "Epoch 8/10\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1220 - accuracy: 0.9779\n",
      "Epoch 9/10\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0992 - accuracy: 0.9779\n",
      "Epoch 10/10\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0837 - accuracy: 0.9867\n",
      "Epoch 1/10\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1.7620 - accuracy: 0.2920\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.1427 - accuracy: 0.4823\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.7397 - accuracy: 0.6681\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.4855 - accuracy: 0.8053\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.3362 - accuracy: 0.8761\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.2419 - accuracy: 0.9204\n",
      "Epoch 7/10\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.1801 - accuracy: 0.9646\n",
      "Epoch 8/10\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1422 - accuracy: 0.9779\n",
      "Epoch 9/10\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1168 - accuracy: 0.9867\n",
      "Epoch 10/10\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0975 - accuracy: 0.9912\n",
      "Epoch 1/10\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.4729 - accuracy: 0.4602\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.9164 - accuracy: 0.6018\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.5645 - accuracy: 0.7920\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.3518 - accuracy: 0.9292\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.2380 - accuracy: 0.9513\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1763 - accuracy: 0.9646\n",
      "Epoch 7/10\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1378 - accuracy: 0.9735\n",
      "Epoch 8/10\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1090 - accuracy: 0.9823\n",
      "Epoch 9/10\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0893 - accuracy: 0.9867\n",
      "Epoch 10/10\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0748 - accuracy: 0.9956\n",
      "Epoch 1/10\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 2.3889 - accuracy: 0.1947\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.5968 - accuracy: 0.3761\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.0635 - accuracy: 0.5664\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.6742 - accuracy: 0.7434\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.4493 - accuracy: 0.8363\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.3197 - accuracy: 0.9204\n",
      "Epoch 7/10\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.2331 - accuracy: 0.9558\n",
      "Epoch 8/10\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.1766 - accuracy: 0.9735\n",
      "Epoch 9/10\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.1426 - accuracy: 0.9735\n",
      "Epoch 10/10\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.1140 - accuracy: 0.9912\n",
      "Epoch 1/10\n",
      "5/5 [==============================] - 1s 7ms/step - loss: 1.9148 - accuracy: 0.3053\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.2338 - accuracy: 0.4823\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.7768 - accuracy: 0.6283\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.4923 - accuracy: 0.8230\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.3336 - accuracy: 0.9159\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.2450 - accuracy: 0.9292\n",
      "Epoch 7/10\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1864 - accuracy: 0.9558\n",
      "Epoch 8/10\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1480 - accuracy: 0.9735\n",
      "Epoch 9/10\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1208 - accuracy: 0.9779\n",
      "Epoch 10/10\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1013 - accuracy: 0.9912\n",
      "Epoch 1/10\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.3555 - accuracy: 0.4912\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.8842 - accuracy: 0.6372\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.5822 - accuracy: 0.7920\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.3887 - accuracy: 0.8761\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.2751 - accuracy: 0.9204\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.2038 - accuracy: 0.9602\n",
      "Epoch 7/10\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.1583 - accuracy: 0.9735\n",
      "Epoch 8/10\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.1267 - accuracy: 0.9823\n",
      "Epoch 9/10\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.1023 - accuracy: 0.9867\n",
      "Epoch 10/10\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0864 - accuracy: 0.9912\n",
      "Epoch 1/20\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 3.1818 - accuracy: 0.2400\n",
      "Epoch 2/20\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 2.3827 - accuracy: 0.2844\n",
      "Epoch 3/20\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.7252 - accuracy: 0.3644\n",
      "Epoch 4/20\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 1.1941 - accuracy: 0.5378\n",
      "Epoch 5/20\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.7899 - accuracy: 0.6889\n",
      "Epoch 6/20\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.5272 - accuracy: 0.8133\n",
      "Epoch 7/20\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.3629 - accuracy: 0.9022\n",
      "Epoch 8/20\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.2700 - accuracy: 0.9378\n",
      "Epoch 9/20\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.2065 - accuracy: 0.9600\n",
      "Epoch 10/20\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.1669 - accuracy: 0.9689\n",
      "Epoch 11/20\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.1395 - accuracy: 0.9822\n",
      "Epoch 12/20\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1185 - accuracy: 0.9956\n",
      "Epoch 13/20\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.1041 - accuracy: 0.9956\n",
      "Epoch 14/20\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0914 - accuracy: 0.9956\n",
      "Epoch 15/20\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0822 - accuracy: 0.9956\n",
      "Epoch 16/20\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0738 - accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0668 - accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0611 - accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0560 - accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0517 - accuracy: 1.0000\n",
      "Epoch 1/20\n",
      "5/5 [==============================] - 1s 6ms/step - loss: 2.1684 - accuracy: 0.3142\n",
      "Epoch 2/20\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.4198 - accuracy: 0.4336\n",
      "Epoch 3/20\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.9223 - accuracy: 0.5796\n",
      "Epoch 4/20\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.5969 - accuracy: 0.7478\n",
      "Epoch 5/20\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.4058 - accuracy: 0.8584\n",
      "Epoch 6/20\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.2992 - accuracy: 0.9115\n",
      "Epoch 7/20\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.2350 - accuracy: 0.9248\n",
      "Epoch 8/20\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.1929 - accuracy: 0.9469\n",
      "Epoch 9/20\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1556 - accuracy: 0.9558\n",
      "Epoch 10/20\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1286 - accuracy: 0.9735\n",
      "Epoch 11/20\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.1091 - accuracy: 0.9779\n",
      "Epoch 12/20\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0924 - accuracy: 0.9867\n",
      "Epoch 13/20\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0788 - accuracy: 0.9867\n",
      "Epoch 14/20\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0708 - accuracy: 0.9956\n",
      "Epoch 15/20\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0634 - accuracy: 0.9956\n",
      "Epoch 16/20\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0565 - accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0512 - accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0466 - accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0425 - accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0391 - accuracy: 1.0000\n",
      "Epoch 1/20\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.6676 - accuracy: 0.3407\n",
      "Epoch 2/20\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.1033 - accuracy: 0.5487\n",
      "Epoch 3/20\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.7115 - accuracy: 0.7080\n",
      "Epoch 4/20\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.4672 - accuracy: 0.8451\n",
      "Epoch 5/20\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.3210 - accuracy: 0.9027\n",
      "Epoch 6/20\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.2376 - accuracy: 0.9248\n",
      "Epoch 7/20\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1812 - accuracy: 0.9558\n",
      "Epoch 8/20\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.1443 - accuracy: 0.9823\n",
      "Epoch 9/20\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.1178 - accuracy: 0.9823\n",
      "Epoch 10/20\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0986 - accuracy: 0.9823\n",
      "Epoch 11/20\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0828 - accuracy: 0.9823\n",
      "Epoch 12/20\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0721 - accuracy: 0.9867\n",
      "Epoch 13/20\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0633 - accuracy: 0.9956\n",
      "Epoch 14/20\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0561 - accuracy: 0.9956\n",
      "Epoch 15/20\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0503 - accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0454 - accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0412 - accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0375 - accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0346 - accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0318 - accuracy: 1.0000\n",
      "Epoch 1/20\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1.4406 - accuracy: 0.4735\n",
      "Epoch 2/20\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.9521 - accuracy: 0.6283\n",
      "Epoch 3/20\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.6479 - accuracy: 0.7389\n",
      "Epoch 4/20\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.4261 - accuracy: 0.8407\n",
      "Epoch 5/20\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.2900 - accuracy: 0.9027\n",
      "Epoch 6/20\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.2029 - accuracy: 0.9513\n",
      "Epoch 7/20\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1527 - accuracy: 0.9823\n",
      "Epoch 8/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 5ms/step - loss: 0.1186 - accuracy: 0.9823\n",
      "Epoch 9/20\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0942 - accuracy: 0.9867\n",
      "Epoch 10/20\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0771 - accuracy: 0.9956\n",
      "Epoch 11/20\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0649 - accuracy: 0.9956\n",
      "Epoch 12/20\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0550 - accuracy: 1.0000\n",
      "Epoch 13/20\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0476 - accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0425 - accuracy: 1.0000\n",
      "Epoch 15/20\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0377 - accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0338 - accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0307 - accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0282 - accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0260 - accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0240 - accuracy: 1.0000\n",
      "Epoch 1/20\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 2.1608 - accuracy: 0.2611\n",
      "Epoch 2/20\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.4449 - accuracy: 0.4469\n",
      "Epoch 3/20\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.9364 - accuracy: 0.5796\n",
      "Epoch 4/20\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.6072 - accuracy: 0.7434\n",
      "Epoch 5/20\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.4051 - accuracy: 0.8673\n",
      "Epoch 6/20\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.2839 - accuracy: 0.9204\n",
      "Epoch 7/20\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.2072 - accuracy: 0.9602\n",
      "Epoch 8/20\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1627 - accuracy: 0.9779\n",
      "Epoch 9/20\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.1310 - accuracy: 0.9912\n",
      "Epoch 10/20\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.1095 - accuracy: 0.9912\n",
      "Epoch 11/20\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0926 - accuracy: 0.9912\n",
      "Epoch 12/20\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0798 - accuracy: 0.9912\n",
      "Epoch 13/20\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0697 - accuracy: 0.9912\n",
      "Epoch 14/20\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0620 - accuracy: 0.9912\n",
      "Epoch 15/20\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0555 - accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0506 - accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0455 - accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0418 - accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0383 - accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0355 - accuracy: 1.0000\n",
      "Epoch 1/20\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.6106 - accuracy: 0.3673\n",
      "Epoch 2/20\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.0604 - accuracy: 0.5442\n",
      "Epoch 3/20\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.6708 - accuracy: 0.7168\n",
      "Epoch 4/20\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.4491 - accuracy: 0.8584\n",
      "Epoch 5/20\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.3121 - accuracy: 0.9159\n",
      "Epoch 6/20\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.2318 - accuracy: 0.9469\n",
      "Epoch 7/20\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.1800 - accuracy: 0.9558\n",
      "Epoch 8/20\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.1440 - accuracy: 0.9558\n",
      "Epoch 9/20\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.1171 - accuracy: 0.9867\n",
      "Epoch 10/20\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0991 - accuracy: 0.9867\n",
      "Epoch 11/20\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0850 - accuracy: 0.9956\n",
      "Epoch 12/20\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0746 - accuracy: 0.9956\n",
      "Epoch 13/20\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0652 - accuracy: 0.9956\n",
      "Epoch 14/20\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0589 - accuracy: 0.9956\n",
      "Epoch 15/20\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0525 - accuracy: 0.9956\n",
      "Epoch 16/20\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0473 - accuracy: 0.9956\n",
      "Epoch 17/20\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0432 - accuracy: 0.9956\n",
      "Epoch 18/20\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.0393 - accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.0363 - accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0335 - accuracy: 1.0000\n",
      "Epoch 1/20\n",
      "5/5 [==============================] - 1s 21ms/step - loss: 1.4105 - accuracy: 0.4115\n",
      "Epoch 2/20\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.9365 - accuracy: 0.6239\n",
      "Epoch 3/20\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.6204 - accuracy: 0.7478\n",
      "Epoch 4/20\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.4290 - accuracy: 0.8584\n",
      "Epoch 5/20\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.3043 - accuracy: 0.9336\n",
      "Epoch 6/20\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.2296 - accuracy: 0.9425\n",
      "Epoch 7/20\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.1794 - accuracy: 0.9646\n",
      "Epoch 8/20\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1436 - accuracy: 0.9690\n",
      "Epoch 9/20\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.1150 - accuracy: 0.9867\n",
      "Epoch 10/20\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0949 - accuracy: 0.9867\n",
      "Epoch 11/20\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0807 - accuracy: 0.9912\n",
      "Epoch 12/20\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0691 - accuracy: 0.9956\n",
      "Epoch 13/20\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0598 - accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0527 - accuracy: 1.0000\n",
      "Epoch 15/20\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0468 - accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0420 - accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0381 - accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0347 - accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0316 - accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0292 - accuracy: 1.0000\n",
      "Epoch 1/20\n",
      "5/5 [==============================] - 1s 9ms/step - loss: 2.0728 - accuracy: 0.2655\n",
      "Epoch 2/20\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.4039 - accuracy: 0.3805\n",
      "Epoch 3/20\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.9162 - accuracy: 0.5973\n",
      "Epoch 4/20\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.6047 - accuracy: 0.7566\n",
      "Epoch 5/20\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4068 - accuracy: 0.8761\n",
      "Epoch 6/20\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.2871 - accuracy: 0.9204\n",
      "Epoch 7/20\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.2180 - accuracy: 0.9336\n",
      "Epoch 8/20\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.1684 - accuracy: 0.9558\n",
      "Epoch 9/20\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.1383 - accuracy: 0.9823\n",
      "Epoch 10/20\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1137 - accuracy: 0.9867\n",
      "Epoch 11/20\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0957 - accuracy: 0.9912\n",
      "Epoch 12/20\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0819 - accuracy: 0.9956\n",
      "Epoch 13/20\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0727 - accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0644 - accuracy: 1.0000\n",
      "Epoch 15/20\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0582 - accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0525 - accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0480 - accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0440 - accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0406 - accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0376 - accuracy: 1.0000\n",
      "Epoch 1/20\n",
      "5/5 [==============================] - 1s 7ms/step - loss: 1.7541 - accuracy: 0.2832\n",
      "Epoch 2/20\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 1.1271 - accuracy: 0.4867\n",
      "Epoch 3/20\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.7132 - accuracy: 0.6903\n",
      "Epoch 4/20\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.4652 - accuracy: 0.8407\n",
      "Epoch 5/20\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.3219 - accuracy: 0.9204\n",
      "Epoch 6/20\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.2330 - accuracy: 0.9469\n",
      "Epoch 7/20\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1793 - accuracy: 0.9513\n",
      "Epoch 8/20\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.1352 - accuracy: 0.9646\n",
      "Epoch 9/20\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1084 - accuracy: 0.9735\n",
      "Epoch 10/20\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0898 - accuracy: 0.9867\n",
      "Epoch 11/20\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0756 - accuracy: 0.9912\n",
      "Epoch 12/20\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0656 - accuracy: 0.9956\n",
      "Epoch 13/20\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0575 - accuracy: 0.9956\n",
      "Epoch 14/20\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0521 - accuracy: 1.00 - 0s 8ms/step - loss: 0.0510 - accuracy: 0.9956\n",
      "Epoch 15/20\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0457 - accuracy: 0.9956\n",
      "Epoch 16/20\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0406 - accuracy: 0.9956\n",
      "Epoch 17/20\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0374 - accuracy: 0.9956\n",
      "Epoch 18/20\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0339 - accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0309 - accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0286 - accuracy: 1.0000\n",
      "Epoch 1/20\n",
      "5/5 [==============================] - 1s 8ms/step - loss: 1.5377 - accuracy: 0.3540\n",
      "Epoch 2/20\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1.0334 - accuracy: 0.5354\n",
      "Epoch 3/20\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6784 - accuracy: 0.7257\n",
      "Epoch 4/20\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.4596 - accuracy: 0.8274\n",
      "Epoch 5/20\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.3212 - accuracy: 0.9115\n",
      "Epoch 6/20\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.2329 - accuracy: 0.9646\n",
      "Epoch 7/20\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.1763 - accuracy: 0.9823\n",
      "Epoch 8/20\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.1381 - accuracy: 0.9912\n",
      "Epoch 9/20\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.1113 - accuracy: 0.9912\n",
      "Epoch 10/20\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0929 - accuracy: 0.9912\n",
      "Epoch 11/20\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0781 - accuracy: 0.9912\n",
      "Epoch 12/20\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0672 - accuracy: 1.0000\n",
      "Epoch 13/20\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0583 - accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0520 - accuracy: 1.0000\n",
      "Epoch 15/20\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0463 - accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0417 - accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0379 - accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0346 - accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0320 - accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0294 - accuracy: 1.0000\n",
      "Epoch 1/30\n",
      "5/5 [==============================] - 1s 9ms/step - loss: 0.9653 - accuracy: 0.5778\n",
      "Epoch 2/30\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.5910 - accuracy: 0.7511\n",
      "Epoch 3/30\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.3830 - accuracy: 0.8711\n",
      "Epoch 4/30\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.2517 - accuracy: 0.9244\n",
      "Epoch 5/30\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.1806 - accuracy: 0.9600\n",
      "Epoch 6/30\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.1352 - accuracy: 0.9733\n",
      "Epoch 7/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1063 - accuracy: 0.9822\n",
      "Epoch 8/30\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0849 - accuracy: 0.9822\n",
      "Epoch 9/30\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0697 - accuracy: 0.9867\n",
      "Epoch 10/30\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0572 - accuracy: 0.9867\n",
      "Epoch 11/30\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0474 - accuracy: 0.9956\n",
      "Epoch 12/30\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0407 - accuracy: 1.0000\n",
      "Epoch 13/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0359 - accuracy: 1.0000\n",
      "Epoch 14/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0314 - accuracy: 1.0000\n",
      "Epoch 15/30\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0281 - accuracy: 1.0000\n",
      "Epoch 16/30\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0254 - accuracy: 1.0000\n",
      "Epoch 17/30\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0229 - accuracy: 1.0000\n",
      "Epoch 18/30\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0209 - accuracy: 1.0000\n",
      "Epoch 19/30\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0193 - accuracy: 1.0000\n",
      "Epoch 20/30\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0179 - accuracy: 1.0000\n",
      "Epoch 21/30\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0166 - accuracy: 1.0000\n",
      "Epoch 22/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0154 - accuracy: 1.0000\n",
      "Epoch 23/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0144 - accuracy: 1.0000\n",
      "Epoch 24/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0135 - accuracy: 1.0000\n",
      "Epoch 25/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0127 - accuracy: 1.0000\n",
      "Epoch 26/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0120 - accuracy: 1.0000\n",
      "Epoch 27/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0114 - accuracy: 1.0000\n",
      "Epoch 28/30\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0107 - accuracy: 1.0000\n",
      "Epoch 29/30\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0102 - accuracy: 1.0000\n",
      "Epoch 30/30\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0097 - accuracy: 1.0000\n",
      "Epoch 1/30\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1.2661 - accuracy: 0.5442\n",
      "Epoch 2/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.8341 - accuracy: 0.6726\n",
      "Epoch 3/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.5480 - accuracy: 0.7611\n",
      "Epoch 4/30\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.3810 - accuracy: 0.8628\n",
      "Epoch 5/30\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.2684 - accuracy: 0.9292\n",
      "Epoch 6/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1990 - accuracy: 0.9513\n",
      "Epoch 7/30\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.1529 - accuracy: 0.9690\n",
      "Epoch 8/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1225 - accuracy: 0.9735\n",
      "Epoch 9/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0991 - accuracy: 0.9823\n",
      "Epoch 10/30\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0831 - accuracy: 0.9912\n",
      "Epoch 11/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0698 - accuracy: 0.9912\n",
      "Epoch 12/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0605 - accuracy: 0.9912\n",
      "Epoch 13/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0527 - accuracy: 0.9956\n",
      "Epoch 14/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0466 - accuracy: 0.9956\n",
      "Epoch 15/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0418 - accuracy: 0.9956\n",
      "Epoch 16/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0375 - accuracy: 1.0000\n",
      "Epoch 17/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0339 - accuracy: 1.0000\n",
      "Epoch 18/30\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0307 - accuracy: 1.0000\n",
      "Epoch 19/30\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0283 - accuracy: 1.0000\n",
      "Epoch 20/30\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0261 - accuracy: 1.0000\n",
      "Epoch 21/30\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0241 - accuracy: 1.0000\n",
      "Epoch 22/30\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0225 - accuracy: 1.0000\n",
      "Epoch 23/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0208 - accuracy: 1.0000\n",
      "Epoch 24/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0196 - accuracy: 1.0000\n",
      "Epoch 25/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0183 - accuracy: 1.0000\n",
      "Epoch 26/30\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0172 - accuracy: 1.0000\n",
      "Epoch 27/30\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0162 - accuracy: 1.0000\n",
      "Epoch 28/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0153 - accuracy: 1.0000\n",
      "Epoch 29/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0144 - accuracy: 1.0000\n",
      "Epoch 30/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0137 - accuracy: 1.0000\n",
      "Epoch 1/30\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1.1498 - accuracy: 0.4602\n",
      "Epoch 2/30\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6581 - accuracy: 0.7212\n",
      "Epoch 3/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.4085 - accuracy: 0.8540\n",
      "Epoch 4/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.2658 - accuracy: 0.9115\n",
      "Epoch 5/30\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.1891 - accuracy: 0.9646\n",
      "Epoch 6/30\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.1396 - accuracy: 0.9646\n",
      "Epoch 7/30\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.1100 - accuracy: 0.9646\n",
      "Epoch 8/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0892 - accuracy: 0.9735\n",
      "Epoch 9/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0745 - accuracy: 0.9912\n",
      "Epoch 10/30\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0628 - accuracy: 1.0000\n",
      "Epoch 11/30\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0537 - accuracy: 1.0000\n",
      "Epoch 12/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0474 - accuracy: 1.0000\n",
      "Epoch 13/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0419 - accuracy: 1.0000\n",
      "Epoch 14/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0374 - accuracy: 1.0000\n",
      "Epoch 15/30\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0334 - accuracy: 1.0000\n",
      "Epoch 16/30\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0304 - accuracy: 1.0000\n",
      "Epoch 17/30\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0278 - accuracy: 1.0000\n",
      "Epoch 18/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0255 - accuracy: 1.0000\n",
      "Epoch 19/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0236 - accuracy: 1.0000\n",
      "Epoch 20/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0218 - accuracy: 1.0000\n",
      "Epoch 21/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0203 - accuracy: 1.0000\n",
      "Epoch 22/30\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0190 - accuracy: 1.0000\n",
      "Epoch 23/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0177 - accuracy: 1.0000\n",
      "Epoch 24/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0167 - accuracy: 1.0000\n",
      "Epoch 25/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0157 - accuracy: 1.0000\n",
      "Epoch 26/30\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0148 - accuracy: 1.0000\n",
      "Epoch 27/30\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0139 - accuracy: 1.0000\n",
      "Epoch 28/30\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0132 - accuracy: 1.0000\n",
      "Epoch 29/30\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0125 - accuracy: 1.0000\n",
      "Epoch 30/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0119 - accuracy: 1.0000\n",
      "Epoch 1/30\n",
      "5/5 [==============================] - 1s 8ms/step - loss: 1.8113 - accuracy: 0.3496\n",
      "Epoch 2/30\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1.1858 - accuracy: 0.5265\n",
      "Epoch 3/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.7451 - accuracy: 0.6770\n",
      "Epoch 4/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.4844 - accuracy: 0.8142\n",
      "Epoch 5/30\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.3260 - accuracy: 0.8938\n",
      "Epoch 6/30\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.2376 - accuracy: 0.9425\n",
      "Epoch 7/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1802 - accuracy: 0.9690\n",
      "Epoch 8/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1414 - accuracy: 0.9823\n",
      "Epoch 9/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1112 - accuracy: 0.9912\n",
      "Epoch 10/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0925 - accuracy: 1.0000\n",
      "Epoch 11/30\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0782 - accuracy: 1.0000\n",
      "Epoch 12/30\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0680 - accuracy: 1.0000\n",
      "Epoch 13/30\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0595 - accuracy: 1.0000\n",
      "Epoch 14/30\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0534 - accuracy: 1.0000\n",
      "Epoch 15/30\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0479 - accuracy: 1.0000\n",
      "Epoch 16/30\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0436 - accuracy: 1.0000\n",
      "Epoch 17/30\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0399 - accuracy: 1.0000\n",
      "Epoch 18/30\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0365 - accuracy: 1.0000\n",
      "Epoch 19/30\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0336 - accuracy: 1.0000\n",
      "Epoch 20/30\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0312 - accuracy: 1.0000\n",
      "Epoch 21/30\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0291 - accuracy: 1.0000\n",
      "Epoch 22/30\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0272 - accuracy: 1.0000\n",
      "Epoch 23/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0254 - accuracy: 1.0000\n",
      "Epoch 24/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0239 - accuracy: 1.0000\n",
      "Epoch 25/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0225 - accuracy: 1.0000\n",
      "Epoch 26/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0212 - accuracy: 1.0000\n",
      "Epoch 27/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0200 - accuracy: 1.0000\n",
      "Epoch 28/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0189 - accuracy: 1.0000\n",
      "Epoch 29/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0180 - accuracy: 1.0000\n",
      "Epoch 30/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0171 - accuracy: 1.0000\n",
      "Epoch 1/30\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1.7490 - accuracy: 0.3894\n",
      "Epoch 2/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.1604 - accuracy: 0.5531\n",
      "Epoch 3/30\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.8029 - accuracy: 0.6947\n",
      "Epoch 4/30\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.5346 - accuracy: 0.8274\n",
      "Epoch 5/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.3846 - accuracy: 0.8761\n",
      "Epoch 6/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.2771 - accuracy: 0.9159\n",
      "Epoch 7/30\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.2095 - accuracy: 0.9469\n",
      "Epoch 8/30\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.1665 - accuracy: 0.9779\n",
      "Epoch 9/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1338 - accuracy: 0.9823\n",
      "Epoch 10/30\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.1111 - accuracy: 0.9867\n",
      "Epoch 11/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0914 - accuracy: 0.9912\n",
      "Epoch 12/30\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0781 - accuracy: 0.9912\n",
      "Epoch 13/30\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0676 - accuracy: 1.0000\n",
      "Epoch 14/30\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0589 - accuracy: 1.0000\n",
      "Epoch 15/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0526 - accuracy: 1.0000\n",
      "Epoch 16/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0470 - accuracy: 1.0000\n",
      "Epoch 17/30\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0424 - accuracy: 1.0000\n",
      "Epoch 18/30\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0385 - accuracy: 1.0000\n",
      "Epoch 19/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0353 - accuracy: 1.0000\n",
      "Epoch 20/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0326 - accuracy: 1.0000\n",
      "Epoch 21/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0300 - accuracy: 1.0000\n",
      "Epoch 22/30\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0279 - accuracy: 1.0000\n",
      "Epoch 23/30\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0261 - accuracy: 1.0000\n",
      "Epoch 24/30\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0243 - accuracy: 1.0000\n",
      "Epoch 25/30\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0229 - accuracy: 1.0000\n",
      "Epoch 26/30\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0214 - accuracy: 1.0000\n",
      "Epoch 27/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0202 - accuracy: 1.0000\n",
      "Epoch 28/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0191 - accuracy: 1.0000\n",
      "Epoch 29/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0181 - accuracy: 1.0000\n",
      "Epoch 30/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0172 - accuracy: 1.0000\n",
      "Epoch 1/30\n",
      "5/5 [==============================] - 1s 7ms/step - loss: 1.6619 - accuracy: 0.4425\n",
      "Epoch 2/30\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1.1090 - accuracy: 0.5487\n",
      "Epoch 3/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.7376 - accuracy: 0.7035\n",
      "Epoch 4/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.5010 - accuracy: 0.8053\n",
      "Epoch 5/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.3467 - accuracy: 0.8894\n",
      "Epoch 6/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.2570 - accuracy: 0.9425\n",
      "Epoch 7/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1972 - accuracy: 0.9513\n",
      "Epoch 8/30\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.1552 - accuracy: 0.9646\n",
      "Epoch 9/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1253 - accuracy: 0.9735\n",
      "Epoch 10/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1018 - accuracy: 0.9867\n",
      "Epoch 11/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0851 - accuracy: 0.9912\n",
      "Epoch 12/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0737 - accuracy: 0.9956\n",
      "Epoch 13/30\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0645 - accuracy: 0.9956\n",
      "Epoch 14/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0566 - accuracy: 0.9956\n",
      "Epoch 15/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0504 - accuracy: 0.9956\n",
      "Epoch 16/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0453 - accuracy: 0.9956\n",
      "Epoch 17/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0413 - accuracy: 0.9956\n",
      "Epoch 18/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0378 - accuracy: 1.0000\n",
      "Epoch 19/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0342 - accuracy: 1.0000\n",
      "Epoch 20/30\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0314 - accuracy: 1.0000\n",
      "Epoch 21/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0292 - accuracy: 1.0000\n",
      "Epoch 22/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0269 - accuracy: 1.0000\n",
      "Epoch 23/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0249 - accuracy: 1.0000\n",
      "Epoch 24/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0233 - accuracy: 1.0000\n",
      "Epoch 25/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0219 - accuracy: 1.0000\n",
      "Epoch 26/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0204 - accuracy: 1.0000\n",
      "Epoch 27/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0193 - accuracy: 1.0000\n",
      "Epoch 28/30\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0182 - accuracy: 1.0000\n",
      "Epoch 29/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0172 - accuracy: 1.0000\n",
      "Epoch 30/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0163 - accuracy: 1.0000\n",
      "Epoch 1/30\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1.0556 - accuracy: 0.5265\n",
      "Epoch 2/30\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6513 - accuracy: 0.7655\n",
      "Epoch 3/30\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.4284 - accuracy: 0.8628\n",
      "Epoch 4/30\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.2940 - accuracy: 0.9027\n",
      "Epoch 5/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.2099 - accuracy: 0.9558\n",
      "Epoch 6/30\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.1632 - accuracy: 0.9690\n",
      "Epoch 7/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1285 - accuracy: 0.9735\n",
      "Epoch 8/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1033 - accuracy: 0.9823\n",
      "Epoch 9/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0865 - accuracy: 0.9867\n",
      "Epoch 10/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0738 - accuracy: 0.9912\n",
      "Epoch 11/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0637 - accuracy: 0.9956\n",
      "Epoch 12/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0549 - accuracy: 0.9956\n",
      "Epoch 13/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0482 - accuracy: 0.9956\n",
      "Epoch 14/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0428 - accuracy: 0.9956\n",
      "Epoch 15/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0379 - accuracy: 1.0000\n",
      "Epoch 16/30\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0345 - accuracy: 1.0000\n",
      "Epoch 17/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0314 - accuracy: 1.0000\n",
      "Epoch 18/30\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0285 - accuracy: 1.0000\n",
      "Epoch 19/30\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0261 - accuracy: 1.0000\n",
      "Epoch 20/30\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0240 - accuracy: 1.0000\n",
      "Epoch 21/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0222 - accuracy: 1.0000\n",
      "Epoch 22/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0206 - accuracy: 1.0000\n",
      "Epoch 23/30\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0193 - accuracy: 1.0000\n",
      "Epoch 24/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0180 - accuracy: 1.0000\n",
      "Epoch 25/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0168 - accuracy: 1.0000\n",
      "Epoch 26/30\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0158 - accuracy: 1.0000\n",
      "Epoch 27/30\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0149 - accuracy: 1.0000\n",
      "Epoch 28/30\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0140 - accuracy: 1.0000\n",
      "Epoch 29/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0132 - accuracy: 1.0000\n",
      "Epoch 30/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0126 - accuracy: 1.0000\n",
      "Epoch 1/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.4667 - accuracy: 0.4646\n",
      "Epoch 2/30\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.9578 - accuracy: 0.6062\n",
      "Epoch 3/30\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.6206 - accuracy: 0.7522\n",
      "Epoch 4/30\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.4043 - accuracy: 0.8540\n",
      "Epoch 5/30\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.2826 - accuracy: 0.9071\n",
      "Epoch 6/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.2104 - accuracy: 0.9292\n",
      "Epoch 7/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1572 - accuracy: 0.9469\n",
      "Epoch 8/30\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.1261 - accuracy: 0.9690\n",
      "Epoch 9/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1027 - accuracy: 0.9779\n",
      "Epoch 10/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0870 - accuracy: 0.9867\n",
      "Epoch 11/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0731 - accuracy: 0.9956\n",
      "Epoch 12/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0638 - accuracy: 1.0000\n",
      "Epoch 13/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0559 - accuracy: 1.0000\n",
      "Epoch 14/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0499 - accuracy: 1.0000\n",
      "Epoch 15/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0443 - accuracy: 1.0000\n",
      "Epoch 16/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0402 - accuracy: 1.0000\n",
      "Epoch 17/30\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0366 - accuracy: 1.0000\n",
      "Epoch 18/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0334 - accuracy: 1.0000\n",
      "Epoch 19/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0308 - accuracy: 1.0000\n",
      "Epoch 20/30\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0287 - accuracy: 1.0000\n",
      "Epoch 21/30\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0265 - accuracy: 1.0000\n",
      "Epoch 22/30\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0247 - accuracy: 1.0000\n",
      "Epoch 23/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0231 - accuracy: 1.0000\n",
      "Epoch 24/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0216 - accuracy: 1.0000\n",
      "Epoch 25/30\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0204 - accuracy: 1.0000\n",
      "Epoch 26/30\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0192 - accuracy: 1.0000\n",
      "Epoch 27/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0181 - accuracy: 1.0000\n",
      "Epoch 28/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0171 - accuracy: 1.0000\n",
      "Epoch 29/30\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0162 - accuracy: 1.0000\n",
      "Epoch 30/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0153 - accuracy: 1.0000\n",
      "Epoch 1/30\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1.7803 - accuracy: 0.3319\n",
      "Epoch 2/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.1763 - accuracy: 0.4912\n",
      "Epoch 3/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.7624 - accuracy: 0.6726\n",
      "Epoch 4/30\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.4999 - accuracy: 0.8230\n",
      "Epoch 5/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.3451 - accuracy: 0.8850\n",
      "Epoch 6/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.2553 - accuracy: 0.9425\n",
      "Epoch 7/30\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.1945 - accuracy: 0.9646\n",
      "Epoch 8/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1541 - accuracy: 0.9779\n",
      "Epoch 9/30\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.1250 - accuracy: 0.9823\n",
      "Epoch 10/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1042 - accuracy: 0.9867\n",
      "Epoch 11/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0889 - accuracy: 0.9956\n",
      "Epoch 12/30\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0769 - accuracy: 0.9956\n",
      "Epoch 13/30\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0678 - accuracy: 0.9956\n",
      "Epoch 14/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0597 - accuracy: 0.9956\n",
      "Epoch 15/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0536 - accuracy: 0.9956\n",
      "Epoch 16/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0483 - accuracy: 0.9956\n",
      "Epoch 17/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0438 - accuracy: 1.0000\n",
      "Epoch 18/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0399 - accuracy: 1.0000\n",
      "Epoch 19/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0368 - accuracy: 1.0000\n",
      "Epoch 20/30\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.0338 - accuracy: 1.0000\n",
      "Epoch 21/30\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0312 - accuracy: 1.0000\n",
      "Epoch 22/30\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0289 - accuracy: 1.0000\n",
      "Epoch 23/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0269 - accuracy: 1.0000\n",
      "Epoch 24/30\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0251 - accuracy: 1.0000\n",
      "Epoch 25/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0235 - accuracy: 1.0000\n",
      "Epoch 26/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0220 - accuracy: 1.0000\n",
      "Epoch 27/30\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0207 - accuracy: 1.0000\n",
      "Epoch 28/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0195 - accuracy: 1.0000\n",
      "Epoch 29/30\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0185 - accuracy: 1.0000\n",
      "Epoch 30/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0175 - accuracy: 1.0000\n",
      "Epoch 1/30\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.9883 - accuracy: 0.5619\n",
      "Epoch 2/30\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.5929 - accuracy: 0.7257\n",
      "Epoch 3/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.3645 - accuracy: 0.8894\n",
      "Epoch 4/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.2383 - accuracy: 0.9469\n",
      "Epoch 5/30\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.1640 - accuracy: 0.9690\n",
      "Epoch 6/30\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.1198 - accuracy: 0.9823\n",
      "Epoch 7/30\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0952 - accuracy: 0.9912\n",
      "Epoch 8/30\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0766 - accuracy: 0.9912\n",
      "Epoch 9/30\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0614 - accuracy: 0.9956\n",
      "Epoch 10/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0515 - accuracy: 0.9956\n",
      "Epoch 11/30\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0443 - accuracy: 0.9956\n",
      "Epoch 12/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0380 - accuracy: 1.0000\n",
      "Epoch 13/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0335 - accuracy: 1.0000\n",
      "Epoch 14/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0298 - accuracy: 1.0000\n",
      "Epoch 15/30\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0270 - accuracy: 1.0000\n",
      "Epoch 16/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0245 - accuracy: 1.0000\n",
      "Epoch 17/30\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0221 - accuracy: 1.0000\n",
      "Epoch 18/30\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0204 - accuracy: 1.0000\n",
      "Epoch 19/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0189 - accuracy: 1.0000\n",
      "Epoch 20/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0175 - accuracy: 1.0000\n",
      "Epoch 21/30\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0162 - accuracy: 1.0000\n",
      "Epoch 22/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0152 - accuracy: 1.0000\n",
      "Epoch 23/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0142 - accuracy: 1.0000\n",
      "Epoch 24/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0133 - accuracy: 1.0000\n",
      "Epoch 25/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0126 - accuracy: 1.0000\n",
      "Epoch 26/30\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0118 - accuracy: 1.0000\n",
      "Epoch 27/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0112 - accuracy: 1.0000\n",
      "Epoch 28/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0106 - accuracy: 1.0000\n",
      "Epoch 29/30\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0101 - accuracy: 1.0000\n",
      "Epoch 30/30\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0096 - accuracy: 1.0000\n",
      "Epoch 1/40\n",
      "5/5 [==============================] - 1s 6ms/step - loss: 1.7459 - accuracy: 0.4800\n",
      "Epoch 2/40\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1.1525 - accuracy: 0.5644\n",
      "Epoch 3/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.7890 - accuracy: 0.7289\n",
      "Epoch 4/40\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.5204 - accuracy: 0.8222\n",
      "Epoch 5/40\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.3627 - accuracy: 0.8889\n",
      "Epoch 6/40\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.2646 - accuracy: 0.9244\n",
      "Epoch 7/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.2023 - accuracy: 0.9378\n",
      "Epoch 8/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1610 - accuracy: 0.9511\n",
      "Epoch 9/40\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.1331 - accuracy: 0.9644\n",
      "Epoch 10/40\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.1101 - accuracy: 0.9822\n",
      "Epoch 11/40\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0926 - accuracy: 0.9867\n",
      "Epoch 12/40\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0796 - accuracy: 0.9956\n",
      "Epoch 13/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0692 - accuracy: 0.9956\n",
      "Epoch 14/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0599 - accuracy: 0.9956\n",
      "Epoch 15/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0506 - accuracy: 0.9956\n",
      "Epoch 16/40\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0455 - accuracy: 0.9956\n",
      "Epoch 17/40\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0412 - accuracy: 1.0000\n",
      "Epoch 18/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0372 - accuracy: 1.0000\n",
      "Epoch 19/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0338 - accuracy: 1.0000\n",
      "Epoch 20/40\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0313 - accuracy: 1.0000\n",
      "Epoch 21/40\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0288 - accuracy: 1.0000\n",
      "Epoch 22/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0268 - accuracy: 1.0000\n",
      "Epoch 23/40\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0249 - accuracy: 1.0000\n",
      "Epoch 24/40\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0233 - accuracy: 1.0000\n",
      "Epoch 25/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0218 - accuracy: 1.0000\n",
      "Epoch 26/40\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0205 - accuracy: 1.0000\n",
      "Epoch 27/40\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0193 - accuracy: 1.0000\n",
      "Epoch 28/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0182 - accuracy: 1.0000\n",
      "Epoch 29/40\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0173 - accuracy: 1.0000\n",
      "Epoch 30/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0164 - accuracy: 1.0000\n",
      "Epoch 31/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0156 - accuracy: 1.0000\n",
      "Epoch 32/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0148 - accuracy: 1.0000\n",
      "Epoch 33/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0141 - accuracy: 1.0000\n",
      "Epoch 34/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0135 - accuracy: 1.0000\n",
      "Epoch 35/40\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0129 - accuracy: 1.0000\n",
      "Epoch 36/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0123 - accuracy: 1.0000\n",
      "Epoch 37/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0118 - accuracy: 1.0000\n",
      "Epoch 38/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0112 - accuracy: 1.0000\n",
      "Epoch 39/40\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0108 - accuracy: 1.0000\n",
      "Epoch 40/40\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0104 - accuracy: 1.0000\n",
      "Epoch 1/40\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1.8932 - accuracy: 0.2566\n",
      "Epoch 2/40\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1.2108 - accuracy: 0.4956\n",
      "Epoch 3/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.7964 - accuracy: 0.6858\n",
      "Epoch 4/40\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.5252 - accuracy: 0.8053\n",
      "Epoch 5/40\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.3634 - accuracy: 0.8805\n",
      "Epoch 6/40\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.2617 - accuracy: 0.9204\n",
      "Epoch 7/40\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.2030 - accuracy: 0.9425\n",
      "Epoch 8/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1576 - accuracy: 0.9779\n",
      "Epoch 9/40\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.1299 - accuracy: 0.9779\n",
      "Epoch 10/40\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.1089 - accuracy: 0.9823\n",
      "Epoch 11/40\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0936 - accuracy: 0.9867\n",
      "Epoch 12/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0814 - accuracy: 0.9912\n",
      "Epoch 13/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0714 - accuracy: 0.9912\n",
      "Epoch 14/40\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0631 - accuracy: 1.0000\n",
      "Epoch 15/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0564 - accuracy: 1.0000\n",
      "Epoch 16/40\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0508 - accuracy: 1.0000\n",
      "Epoch 17/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0463 - accuracy: 1.0000\n",
      "Epoch 18/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0423 - accuracy: 1.0000\n",
      "Epoch 19/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0389 - accuracy: 1.0000\n",
      "Epoch 20/40\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0359 - accuracy: 1.0000\n",
      "Epoch 21/40\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0332 - accuracy: 1.0000\n",
      "Epoch 22/40\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0310 - accuracy: 1.0000\n",
      "Epoch 23/40\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0289 - accuracy: 1.0000\n",
      "Epoch 24/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0269 - accuracy: 1.0000\n",
      "Epoch 25/40\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0252 - accuracy: 1.0000\n",
      "Epoch 26/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0237 - accuracy: 1.0000\n",
      "Epoch 27/40\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0223 - accuracy: 1.0000\n",
      "Epoch 28/40\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0211 - accuracy: 1.0000\n",
      "Epoch 29/40\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0199 - accuracy: 1.0000\n",
      "Epoch 30/40\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0188 - accuracy: 1.0000\n",
      "Epoch 31/40\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0179 - accuracy: 1.0000\n",
      "Epoch 32/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0170 - accuracy: 1.0000\n",
      "Epoch 33/40\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0161 - accuracy: 1.0000\n",
      "Epoch 34/40\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0154 - accuracy: 1.0000\n",
      "Epoch 35/40\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0147 - accuracy: 1.0000\n",
      "Epoch 36/40\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0140 - accuracy: 1.0000\n",
      "Epoch 37/40\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0134 - accuracy: 1.0000\n",
      "Epoch 38/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0129 - accuracy: 1.0000\n",
      "Epoch 39/40\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0123 - accuracy: 1.0000\n",
      "Epoch 40/40\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0118 - accuracy: 1.0000\n",
      "Epoch 1/40\n",
      "5/5 [==============================] - 1s 6ms/step - loss: 1.6342 - accuracy: 0.3850\n",
      "Epoch 2/40\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 1.0480 - accuracy: 0.5310\n",
      "Epoch 3/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.6703 - accuracy: 0.7080\n",
      "Epoch 4/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.4307 - accuracy: 0.8451\n",
      "Epoch 5/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.2884 - accuracy: 0.9204\n",
      "Epoch 6/40\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.2063 - accuracy: 0.9558\n",
      "Epoch 7/40\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.1535 - accuracy: 0.9779\n",
      "Epoch 8/40\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.1211 - accuracy: 0.9823\n",
      "Epoch 9/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0978 - accuracy: 0.9823\n",
      "Epoch 10/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0816 - accuracy: 0.9956\n",
      "Epoch 11/40\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0693 - accuracy: 0.9956\n",
      "Epoch 12/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0596 - accuracy: 1.0000\n",
      "Epoch 13/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0532 - accuracy: 1.0000\n",
      "Epoch 14/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0472 - accuracy: 1.0000\n",
      "Epoch 15/40\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0422 - accuracy: 1.0000\n",
      "Epoch 16/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0383 - accuracy: 1.0000\n",
      "Epoch 17/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0350 - accuracy: 1.0000\n",
      "Epoch 18/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0321 - accuracy: 1.0000\n",
      "Epoch 19/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0297 - accuracy: 1.0000\n",
      "Epoch 20/40\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0276 - accuracy: 1.0000\n",
      "Epoch 21/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0256 - accuracy: 1.0000\n",
      "Epoch 22/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0240 - accuracy: 1.0000\n",
      "Epoch 23/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0225 - accuracy: 1.0000\n",
      "Epoch 24/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0211 - accuracy: 1.0000\n",
      "Epoch 25/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0199 - accuracy: 1.0000\n",
      "Epoch 26/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0188 - accuracy: 1.0000\n",
      "Epoch 27/40\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0177 - accuracy: 1.0000\n",
      "Epoch 28/40\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0168 - accuracy: 1.0000\n",
      "Epoch 29/40\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0160 - accuracy: 1.0000\n",
      "Epoch 30/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0152 - accuracy: 1.0000\n",
      "Epoch 31/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0144 - accuracy: 1.0000\n",
      "Epoch 32/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0137 - accuracy: 1.0000\n",
      "Epoch 33/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0131 - accuracy: 1.0000\n",
      "Epoch 34/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0125 - accuracy: 1.0000\n",
      "Epoch 35/40\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0119 - accuracy: 1.0000\n",
      "Epoch 36/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0114 - accuracy: 1.0000\n",
      "Epoch 37/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0110 - accuracy: 1.0000\n",
      "Epoch 38/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0105 - accuracy: 1.0000\n",
      "Epoch 39/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0101 - accuracy: 1.0000\n",
      "Epoch 40/40\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0097 - accuracy: 1.0000\n",
      "Epoch 1/40\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 2.0731 - accuracy: 0.4425\n",
      "Epoch 2/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.4010 - accuracy: 0.5487\n",
      "Epoch 3/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.9497 - accuracy: 0.6770\n",
      "Epoch 4/40\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.6498 - accuracy: 0.7699\n",
      "Epoch 5/40\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.4366 - accuracy: 0.8540\n",
      "Epoch 6/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.3044 - accuracy: 0.9027\n",
      "Epoch 7/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.2205 - accuracy: 0.9558\n",
      "Epoch 8/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1622 - accuracy: 0.9779\n",
      "Epoch 9/40\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.1255 - accuracy: 0.9823\n",
      "Epoch 10/40\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.1024 - accuracy: 0.9867\n",
      "Epoch 11/40\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0855 - accuracy: 0.9867\n",
      "Epoch 12/40\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0730 - accuracy: 0.9867\n",
      "Epoch 13/40\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0630 - accuracy: 0.9912\n",
      "Epoch 14/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0558 - accuracy: 0.9912\n",
      "Epoch 15/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0492 - accuracy: 1.0000\n",
      "Epoch 16/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0443 - accuracy: 1.0000\n",
      "Epoch 17/40\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0401 - accuracy: 1.0000\n",
      "Epoch 18/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0364 - accuracy: 1.0000\n",
      "Epoch 19/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0336 - accuracy: 1.0000\n",
      "Epoch 20/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0309 - accuracy: 1.0000\n",
      "Epoch 21/40\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0288 - accuracy: 1.0000\n",
      "Epoch 22/40\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0269 - accuracy: 1.0000\n",
      "Epoch 23/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0251 - accuracy: 1.0000\n",
      "Epoch 24/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0236 - accuracy: 1.0000\n",
      "Epoch 25/40\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0222 - accuracy: 1.0000\n",
      "Epoch 26/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0209 - accuracy: 1.0000\n",
      "Epoch 27/40\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0197 - accuracy: 1.0000\n",
      "Epoch 28/40\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0187 - accuracy: 1.0000\n",
      "Epoch 29/40\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0178 - accuracy: 1.0000\n",
      "Epoch 30/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0169 - accuracy: 1.0000\n",
      "Epoch 31/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0161 - accuracy: 1.0000\n",
      "Epoch 32/40\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0153 - accuracy: 1.0000\n",
      "Epoch 33/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0146 - accuracy: 1.0000\n",
      "Epoch 34/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0140 - accuracy: 1.0000\n",
      "Epoch 35/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0134 - accuracy: 1.0000\n",
      "Epoch 36/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0128 - accuracy: 1.0000\n",
      "Epoch 37/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0123 - accuracy: 1.0000\n",
      "Epoch 38/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0118 - accuracy: 1.0000\n",
      "Epoch 39/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0114 - accuracy: 1.0000\n",
      "Epoch 40/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0109 - accuracy: 1.0000\n",
      "Epoch 1/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.8003 - accuracy: 0.2920\n",
      "Epoch 2/40\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1.1803 - accuracy: 0.4735\n",
      "Epoch 3/40\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.7746 - accuracy: 0.6770\n",
      "Epoch 4/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.5056 - accuracy: 0.8097\n",
      "Epoch 5/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.3543 - accuracy: 0.8938\n",
      "Epoch 6/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.2633 - accuracy: 0.9336\n",
      "Epoch 7/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.2013 - accuracy: 0.9602\n",
      "Epoch 8/40\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.1624 - accuracy: 0.9735\n",
      "Epoch 9/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1307 - accuracy: 0.9779\n",
      "Epoch 10/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1091 - accuracy: 0.9867\n",
      "Epoch 11/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0936 - accuracy: 0.9867\n",
      "Epoch 12/40\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0798 - accuracy: 0.9867\n",
      "Epoch 13/40\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0697 - accuracy: 0.9912\n",
      "Epoch 14/40\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0617 - accuracy: 0.9956\n",
      "Epoch 15/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0552 - accuracy: 0.9956\n",
      "Epoch 16/40\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0496 - accuracy: 1.0000\n",
      "Epoch 17/40\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0450 - accuracy: 1.0000\n",
      "Epoch 18/40\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0411 - accuracy: 1.0000\n",
      "Epoch 19/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0375 - accuracy: 1.0000\n",
      "Epoch 20/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0347 - accuracy: 1.0000\n",
      "Epoch 21/40\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0320 - accuracy: 1.0000\n",
      "Epoch 22/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0297 - accuracy: 1.0000\n",
      "Epoch 23/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0277 - accuracy: 1.0000\n",
      "Epoch 24/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0258 - accuracy: 1.0000\n",
      "Epoch 25/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0242 - accuracy: 1.0000\n",
      "Epoch 26/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0227 - accuracy: 1.0000\n",
      "Epoch 27/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0214 - accuracy: 1.0000\n",
      "Epoch 28/40\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0202 - accuracy: 1.0000\n",
      "Epoch 29/40\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0190 - accuracy: 1.0000\n",
      "Epoch 30/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0180 - accuracy: 1.0000\n",
      "Epoch 31/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0171 - accuracy: 1.0000\n",
      "Epoch 32/40\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0162 - accuracy: 1.0000\n",
      "Epoch 33/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0155 - accuracy: 1.0000\n",
      "Epoch 34/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0147 - accuracy: 1.0000\n",
      "Epoch 35/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0140 - accuracy: 1.0000\n",
      "Epoch 36/40\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0134 - accuracy: 1.0000\n",
      "Epoch 37/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0128 - accuracy: 1.0000\n",
      "Epoch 38/40\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0123 - accuracy: 1.0000\n",
      "Epoch 39/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0117 - accuracy: 1.0000\n",
      "Epoch 40/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0113 - accuracy: 1.0000\n",
      "Epoch 1/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.5800 - accuracy: 0.3717\n",
      "Epoch 2/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.0062 - accuracy: 0.5575\n",
      "Epoch 3/40\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6448 - accuracy: 0.7478\n",
      "Epoch 4/40\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.4328 - accuracy: 0.8584\n",
      "Epoch 5/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.3064 - accuracy: 0.9204\n",
      "Epoch 6/40\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.2269 - accuracy: 0.9381\n",
      "Epoch 7/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1724 - accuracy: 0.9602\n",
      "Epoch 8/40\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.1368 - accuracy: 0.9690\n",
      "Epoch 9/40\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.1100 - accuracy: 0.9823\n",
      "Epoch 10/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0909 - accuracy: 0.9912\n",
      "Epoch 11/40\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0759 - accuracy: 0.9912\n",
      "Epoch 12/40\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0651 - accuracy: 0.9956\n",
      "Epoch 13/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0575 - accuracy: 0.9956\n",
      "Epoch 14/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0503 - accuracy: 0.9956\n",
      "Epoch 15/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0445 - accuracy: 1.0000\n",
      "Epoch 16/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0400 - accuracy: 1.0000\n",
      "Epoch 17/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0365 - accuracy: 1.0000\n",
      "Epoch 18/40\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0332 - accuracy: 1.0000\n",
      "Epoch 19/40\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0303 - accuracy: 1.0000\n",
      "Epoch 20/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0281 - accuracy: 1.0000\n",
      "Epoch 21/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0261 - accuracy: 1.0000\n",
      "Epoch 22/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0241 - accuracy: 1.0000\n",
      "Epoch 23/40\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0226 - accuracy: 1.0000\n",
      "Epoch 24/40\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0211 - accuracy: 1.0000\n",
      "Epoch 25/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0197 - accuracy: 1.0000\n",
      "Epoch 26/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0185 - accuracy: 1.0000\n",
      "Epoch 27/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0174 - accuracy: 1.0000\n",
      "Epoch 28/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0165 - accuracy: 1.0000\n",
      "Epoch 29/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0156 - accuracy: 1.0000\n",
      "Epoch 30/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0147 - accuracy: 1.0000\n",
      "Epoch 31/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0140 - accuracy: 1.0000\n",
      "Epoch 32/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0133 - accuracy: 1.0000\n",
      "Epoch 33/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0127 - accuracy: 1.0000\n",
      "Epoch 34/40\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0121 - accuracy: 1.0000\n",
      "Epoch 35/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0115 - accuracy: 1.0000\n",
      "Epoch 36/40\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0110 - accuracy: 1.0000\n",
      "Epoch 37/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0106 - accuracy: 1.0000\n",
      "Epoch 38/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0101 - accuracy: 1.0000\n",
      "Epoch 39/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0097 - accuracy: 1.0000\n",
      "Epoch 40/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0093 - accuracy: 1.0000\n",
      "Epoch 1/40\n",
      "5/5 [==============================] - 1s 6ms/step - loss: 1.5305 - accuracy: 0.3894\n",
      "Epoch 2/40\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.9805 - accuracy: 0.6062\n",
      "Epoch 3/40\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6224 - accuracy: 0.7478\n",
      "Epoch 4/40\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.4170 - accuracy: 0.8496\n",
      "Epoch 5/40\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.2807 - accuracy: 0.9071\n",
      "Epoch 6/40\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.2002 - accuracy: 0.9602\n",
      "Epoch 7/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1521 - accuracy: 0.9646\n",
      "Epoch 8/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1194 - accuracy: 0.9823\n",
      "Epoch 9/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0956 - accuracy: 0.9867\n",
      "Epoch 10/40\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0786 - accuracy: 0.9867\n",
      "Epoch 11/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0662 - accuracy: 0.9956\n",
      "Epoch 12/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0572 - accuracy: 1.0000\n",
      "Epoch 13/40\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0503 - accuracy: 1.0000\n",
      "Epoch 14/40\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0443 - accuracy: 1.0000\n",
      "Epoch 15/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0399 - accuracy: 1.0000\n",
      "Epoch 16/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0361 - accuracy: 1.0000\n",
      "Epoch 17/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0330 - accuracy: 1.0000\n",
      "Epoch 18/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0302 - accuracy: 1.0000\n",
      "Epoch 19/40\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0279 - accuracy: 1.0000\n",
      "Epoch 20/40\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0257 - accuracy: 1.0000\n",
      "Epoch 21/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0240 - accuracy: 1.0000\n",
      "Epoch 22/40\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0223 - accuracy: 1.0000\n",
      "Epoch 23/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0210 - accuracy: 1.0000\n",
      "Epoch 24/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0196 - accuracy: 1.0000\n",
      "Epoch 25/40\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0185 - accuracy: 1.0000\n",
      "Epoch 26/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0174 - accuracy: 1.0000\n",
      "Epoch 27/40\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0165 - accuracy: 1.0000\n",
      "Epoch 28/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0156 - accuracy: 1.0000\n",
      "Epoch 29/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0148 - accuracy: 1.0000\n",
      "Epoch 30/40\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0141 - accuracy: 1.0000\n",
      "Epoch 31/40\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0126 - accuracy: 1.00 - 0s 6ms/step - loss: 0.0134 - accuracy: 1.0000\n",
      "Epoch 32/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0128 - accuracy: 1.0000\n",
      "Epoch 33/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0122 - accuracy: 1.0000\n",
      "Epoch 34/40\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0116 - accuracy: 1.0000\n",
      "Epoch 35/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0111 - accuracy: 1.0000\n",
      "Epoch 36/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0106 - accuracy: 1.0000\n",
      "Epoch 37/40\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0102 - accuracy: 1.0000\n",
      "Epoch 38/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0098 - accuracy: 1.0000\n",
      "Epoch 39/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0094 - accuracy: 1.0000\n",
      "Epoch 40/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0090 - accuracy: 1.0000\n",
      "Epoch 1/40\n",
      "5/5 [==============================] - 1s 6ms/step - loss: 1.8506 - accuracy: 0.3274\n",
      "Epoch 2/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.1959 - accuracy: 0.5221\n",
      "Epoch 3/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.7493 - accuracy: 0.7080\n",
      "Epoch 4/40\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.4737 - accuracy: 0.8451\n",
      "Epoch 5/40\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.3114 - accuracy: 0.9248\n",
      "Epoch 6/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.2175 - accuracy: 0.9779\n",
      "Epoch 7/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1665 - accuracy: 0.9867\n",
      "Epoch 8/40\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.1292 - accuracy: 0.9867\n",
      "Epoch 9/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1036 - accuracy: 0.9956\n",
      "Epoch 10/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0861 - accuracy: 0.9956\n",
      "Epoch 11/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0741 - accuracy: 0.9956\n",
      "Epoch 12/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0649 - accuracy: 0.9956\n",
      "Epoch 13/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0570 - accuracy: 1.0000\n",
      "Epoch 14/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0510 - accuracy: 1.0000\n",
      "Epoch 15/40\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0458 - accuracy: 1.0000\n",
      "Epoch 16/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0420 - accuracy: 1.0000\n",
      "Epoch 17/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0383 - accuracy: 1.0000\n",
      "Epoch 18/40\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0354 - accuracy: 1.0000\n",
      "Epoch 19/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0328 - accuracy: 1.0000\n",
      "Epoch 20/40\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0304 - accuracy: 1.0000\n",
      "Epoch 21/40\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0284 - accuracy: 1.0000\n",
      "Epoch 22/40\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0265 - accuracy: 1.0000\n",
      "Epoch 23/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0249 - accuracy: 1.0000\n",
      "Epoch 24/40\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 0.0234 - accuracy: 1.0000\n",
      "Epoch 25/40\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.0221 - accuracy: 1.0000\n",
      "Epoch 26/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0208 - accuracy: 1.0000\n",
      "Epoch 27/40\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0197 - accuracy: 1.0000\n",
      "Epoch 28/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0187 - accuracy: 1.0000\n",
      "Epoch 29/40\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0178 - accuracy: 1.0000\n",
      "Epoch 30/40\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0169 - accuracy: 1.0000\n",
      "Epoch 31/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0161 - accuracy: 1.0000\n",
      "Epoch 32/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0154 - accuracy: 1.0000\n",
      "Epoch 33/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0147 - accuracy: 1.0000\n",
      "Epoch 34/40\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0140 - accuracy: 1.0000\n",
      "Epoch 35/40\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0134 - accuracy: 1.0000\n",
      "Epoch 36/40\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0129 - accuracy: 1.0000\n",
      "Epoch 37/40\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0124 - accuracy: 1.0000\n",
      "Epoch 38/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0119 - accuracy: 1.0000\n",
      "Epoch 39/40\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0114 - accuracy: 1.0000\n",
      "Epoch 40/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0109 - accuracy: 1.0000\n",
      "Epoch 1/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.4621 - accuracy: 0.3363\n",
      "Epoch 2/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.9244 - accuracy: 0.5841\n",
      "Epoch 3/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.5880 - accuracy: 0.7699\n",
      "Epoch 4/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.3957 - accuracy: 0.8894\n",
      "Epoch 5/40\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.2795 - accuracy: 0.9204\n",
      "Epoch 6/40\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.2039 - accuracy: 0.9513\n",
      "Epoch 7/40\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.1574 - accuracy: 0.9646\n",
      "Epoch 8/40\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.1231 - accuracy: 0.9823\n",
      "Epoch 9/40\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0992 - accuracy: 0.9867\n",
      "Epoch 10/40\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0832 - accuracy: 0.9867\n",
      "Epoch 11/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0705 - accuracy: 0.9956\n",
      "Epoch 12/40\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0613 - accuracy: 0.9956\n",
      "Epoch 13/40\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0540 - accuracy: 1.0000\n",
      "Epoch 14/40\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0483 - accuracy: 1.0000\n",
      "Epoch 15/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0432 - accuracy: 1.0000\n",
      "Epoch 16/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0392 - accuracy: 1.0000\n",
      "Epoch 17/40\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0357 - accuracy: 1.0000\n",
      "Epoch 18/40\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0328 - accuracy: 1.0000\n",
      "Epoch 19/40\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0301 - accuracy: 1.0000\n",
      "Epoch 20/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0280 - accuracy: 1.0000\n",
      "Epoch 21/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0260 - accuracy: 1.0000\n",
      "Epoch 22/40\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0242 - accuracy: 1.0000\n",
      "Epoch 23/40\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0226 - accuracy: 1.0000\n",
      "Epoch 24/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0212 - accuracy: 1.0000\n",
      "Epoch 25/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0199 - accuracy: 1.0000\n",
      "Epoch 26/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0188 - accuracy: 1.0000\n",
      "Epoch 27/40\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0177 - accuracy: 1.0000\n",
      "Epoch 28/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0167 - accuracy: 1.0000\n",
      "Epoch 29/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0159 - accuracy: 1.0000\n",
      "Epoch 30/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0151 - accuracy: 1.0000\n",
      "Epoch 31/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0143 - accuracy: 1.0000\n",
      "Epoch 32/40\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0137 - accuracy: 1.0000\n",
      "Epoch 33/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0131 - accuracy: 1.0000\n",
      "Epoch 34/40\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0125 - accuracy: 1.0000\n",
      "Epoch 35/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0119 - accuracy: 1.0000\n",
      "Epoch 36/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0114 - accuracy: 1.0000\n",
      "Epoch 37/40\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0109 - accuracy: 1.0000\n",
      "Epoch 38/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0105 - accuracy: 1.0000\n",
      "Epoch 39/40\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0101 - accuracy: 1.0000\n",
      "Epoch 40/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0097 - accuracy: 1.0000\n",
      "Epoch 1/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.7682 - accuracy: 0.3142\n",
      "Epoch 2/40\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1.1441 - accuracy: 0.5265\n",
      "Epoch 3/40\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.7341 - accuracy: 0.7035\n",
      "Epoch 4/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.4747 - accuracy: 0.8496\n",
      "Epoch 5/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.3259 - accuracy: 0.9071\n",
      "Epoch 6/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.2377 - accuracy: 0.9469\n",
      "Epoch 7/40\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.1796 - accuracy: 0.9735\n",
      "Epoch 8/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1431 - accuracy: 0.9823\n",
      "Epoch 9/40\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.1185 - accuracy: 0.9823\n",
      "Epoch 10/40\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0989 - accuracy: 0.9867\n",
      "Epoch 11/40\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0851 - accuracy: 0.9912\n",
      "Epoch 12/40\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0734 - accuracy: 0.9912\n",
      "Epoch 13/40\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0641 - accuracy: 0.9912\n",
      "Epoch 14/40\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0563 - accuracy: 0.9912\n",
      "Epoch 15/40\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0505 - accuracy: 0.9912\n",
      "Epoch 16/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0451 - accuracy: 0.9956\n",
      "Epoch 17/40\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0409 - accuracy: 1.0000\n",
      "Epoch 18/40\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0368 - accuracy: 1.0000\n",
      "Epoch 19/40\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0335 - accuracy: 1.0000\n",
      "Epoch 20/40\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0309 - accuracy: 1.0000\n",
      "Epoch 21/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0287 - accuracy: 1.0000\n",
      "Epoch 22/40\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0267 - accuracy: 1.0000\n",
      "Epoch 23/40\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0248 - accuracy: 1.0000\n",
      "Epoch 24/40\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0232 - accuracy: 1.0000\n",
      "Epoch 25/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0218 - accuracy: 1.0000\n",
      "Epoch 26/40\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0205 - accuracy: 1.0000\n",
      "Epoch 27/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0193 - accuracy: 1.0000\n",
      "Epoch 28/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0183 - accuracy: 1.0000\n",
      "Epoch 29/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0173 - accuracy: 1.0000\n",
      "Epoch 30/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0164 - accuracy: 1.0000\n",
      "Epoch 31/40\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0156 - accuracy: 1.0000\n",
      "Epoch 32/40\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0149 - accuracy: 1.0000\n",
      "Epoch 33/40\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0142 - accuracy: 1.0000\n",
      "Epoch 34/40\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0135 - accuracy: 1.0000\n",
      "Epoch 35/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0129 - accuracy: 1.0000\n",
      "Epoch 36/40\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0123 - accuracy: 1.0000\n",
      "Epoch 37/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0118 - accuracy: 1.0000\n",
      "Epoch 38/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0113 - accuracy: 1.0000\n",
      "Epoch 39/40\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0108 - accuracy: 1.0000\n",
      "Epoch 40/40\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0104 - accuracy: 1.0000\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 3.0715 - accuracy: 0.2578\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 2.1524 - accuracy: 0.3022\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.4535 - accuracy: 0.4311\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.9271 - accuracy: 0.5689\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.5994 - accuracy: 0.7289\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.3955 - accuracy: 0.8444\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.2766 - accuracy: 0.9244\n",
      "Epoch 8/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 7ms/step - loss: 0.2091 - accuracy: 0.9467\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.1602 - accuracy: 0.9733\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.1304 - accuracy: 0.9822\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.1078 - accuracy: 0.9911\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0918 - accuracy: 0.9911\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0797 - accuracy: 0.9956\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0703 - accuracy: 1.0000\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0624 - accuracy: 1.0000\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0563 - accuracy: 1.0000\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0512 - accuracy: 1.0000\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0468 - accuracy: 1.0000\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0430 - accuracy: 1.0000\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0399 - accuracy: 1.0000\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0371 - accuracy: 1.0000\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0346 - accuracy: 1.0000\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0323 - accuracy: 1.0000\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0304 - accuracy: 1.0000\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0285 - accuracy: 1.0000\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0269 - accuracy: 1.0000\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0253 - accuracy: 1.0000\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0239 - accuracy: 1.0000\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0227 - accuracy: 1.0000\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0216 - accuracy: 1.0000\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0205 - accuracy: 1.0000\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0195 - accuracy: 1.0000\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0187 - accuracy: 1.0000\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0178 - accuracy: 1.0000\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0170 - accuracy: 1.0000\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0163 - accuracy: 1.0000\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0156 - accuracy: 1.0000\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0150 - accuracy: 1.0000\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0144 - accuracy: 1.0000\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0138 - accuracy: 1.0000\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0133 - accuracy: 1.0000\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0127 - accuracy: 1.0000\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0123 - accuracy: 1.0000\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0118 - accuracy: 1.0000\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0114 - accuracy: 1.0000\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0110 - accuracy: 1.0000\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0106 - accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0103 - accuracy: 1.0000\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0100 - accuracy: 1.0000\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0096 - accuracy: 1.0000\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 2.0172 - accuracy: 0.2301\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.2719 - accuracy: 0.3850\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.7921 - accuracy: 0.6150\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.4933 - accuracy: 0.8230\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.3282 - accuracy: 0.9292\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.2344 - accuracy: 0.9602\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1774 - accuracy: 0.9646\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.1408 - accuracy: 0.9735\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1154 - accuracy: 0.9823\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0967 - accuracy: 0.9912\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0827 - accuracy: 0.9956\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0716 - accuracy: 0.9956\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0630 - accuracy: 0.9956\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0559 - accuracy: 1.0000\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0502 - accuracy: 1.0000\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0457 - accuracy: 1.0000\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0413 - accuracy: 1.0000\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0381 - accuracy: 1.0000\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0349 - accuracy: 1.0000\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0324 - accuracy: 1.0000\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0301 - accuracy: 1.0000\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0282 - accuracy: 1.0000\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0264 - accuracy: 1.0000\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0247 - accuracy: 1.0000\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0232 - accuracy: 1.0000\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0220 - accuracy: 1.0000\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0207 - accuracy: 1.0000\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0196 - accuracy: 1.0000\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0186 - accuracy: 1.0000\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0177 - accuracy: 1.0000\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0168 - accuracy: 1.0000\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0160 - accuracy: 1.0000\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0152 - accuracy: 1.0000\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0146 - accuracy: 1.0000\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0139 - accuracy: 1.0000\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0133 - accuracy: 1.0000\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0128 - accuracy: 1.0000\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0123 - accuracy: 1.0000\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0118 - accuracy: 1.0000\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0113 - accuracy: 1.0000\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0109 - accuracy: 1.0000\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0105 - accuracy: 1.0000\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0121 - accuracy: 1.00 - 0s 5ms/step - loss: 0.0101 - accuracy: 1.0000\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0098 - accuracy: 1.0000\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0094 - accuracy: 1.0000\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0091 - accuracy: 1.0000\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0088 - accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0085 - accuracy: 1.0000\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0082 - accuracy: 1.0000\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0080 - accuracy: 1.0000\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 5ms/step - loss: 1.7050 - accuracy: 0.3894\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.0933 - accuracy: 0.5398\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.6808 - accuracy: 0.7389\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.4250 - accuracy: 0.8850\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.2892 - accuracy: 0.9381\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.2117 - accuracy: 0.9646\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.1618 - accuracy: 0.9690\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.1259 - accuracy: 0.9779\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.1043 - accuracy: 0.9823\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0885 - accuracy: 0.9823\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0752 - accuracy: 0.9867\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0661 - accuracy: 0.9956\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0588 - accuracy: 0.9956\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0517 - accuracy: 0.9956\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0472 - accuracy: 0.9956\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0423 - accuracy: 0.9956\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0388 - accuracy: 0.9956\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0355 - accuracy: 1.0000\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0326 - accuracy: 1.0000\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0303 - accuracy: 1.0000\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0281 - accuracy: 1.0000\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0261 - accuracy: 1.0000\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0245 - accuracy: 1.0000\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0229 - accuracy: 1.0000\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0215 - accuracy: 1.0000\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0203 - accuracy: 1.0000\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0191 - accuracy: 1.0000\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0181 - accuracy: 1.0000\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0171 - accuracy: 1.0000\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0162 - accuracy: 1.0000\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0154 - accuracy: 1.0000\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0147 - accuracy: 1.0000\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0140 - accuracy: 1.0000\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0133 - accuracy: 1.0000\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0128 - accuracy: 1.0000\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0122 - accuracy: 1.0000\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0117 - accuracy: 1.0000\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0112 - accuracy: 1.0000\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0107 - accuracy: 1.0000\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0104 - accuracy: 1.0000\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0099 - accuracy: 1.0000\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0096 - accuracy: 1.0000\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0092 - accuracy: 1.0000\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0089 - accuracy: 1.0000\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0086 - accuracy: 1.0000\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0082 - accuracy: 1.0000\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0080 - accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0077 - accuracy: 1.0000\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0074 - accuracy: 1.0000\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0072 - accuracy: 1.0000\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 3.1146 - accuracy: 0.1195\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 2.2767 - accuracy: 0.2212\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.6154 - accuracy: 0.3584\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.0965 - accuracy: 0.5664\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.7196 - accuracy: 0.6991\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.4778 - accuracy: 0.8407\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.3265 - accuracy: 0.9248\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.2314 - accuracy: 0.9558\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1763 - accuracy: 0.9735\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1398 - accuracy: 0.9823\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1154 - accuracy: 0.9912\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0978 - accuracy: 1.0000\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0851 - accuracy: 1.0000\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0747 - accuracy: 1.0000\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0668 - accuracy: 1.0000\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0603 - accuracy: 1.0000\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0547 - accuracy: 1.0000\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0501 - accuracy: 1.0000\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0461 - accuracy: 1.0000\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0427 - accuracy: 1.0000\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0397 - accuracy: 1.0000\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0369 - accuracy: 1.0000\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0345 - accuracy: 1.0000\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0324 - accuracy: 1.0000\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0305 - accuracy: 1.0000\n",
      "Epoch 26/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0287 - accuracy: 1.0000\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0270 - accuracy: 1.0000\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0256 - accuracy: 1.0000\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0242 - accuracy: 1.0000\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0230 - accuracy: 1.0000\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0218 - accuracy: 1.0000\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0208 - accuracy: 1.0000\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0198 - accuracy: 1.0000\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0188 - accuracy: 1.0000\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0180 - accuracy: 1.0000\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0173 - accuracy: 1.0000\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0165 - accuracy: 1.0000\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0159 - accuracy: 1.0000\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0152 - accuracy: 1.0000\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0146 - accuracy: 1.0000\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0141 - accuracy: 1.0000\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0135 - accuracy: 1.0000\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0130 - accuracy: 1.0000\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0126 - accuracy: 1.0000\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0121 - accuracy: 1.0000\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0117 - accuracy: 1.0000\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0113 - accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0109 - accuracy: 1.0000\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0106 - accuracy: 1.0000\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0102 - accuracy: 1.0000\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.7318 - accuracy: 0.3805\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.1161 - accuracy: 0.5265\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.7312 - accuracy: 0.7168\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.4986 - accuracy: 0.8319\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.3653 - accuracy: 0.8717\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.2708 - accuracy: 0.9248\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.2069 - accuracy: 0.9469\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.1632 - accuracy: 0.9602\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.1324 - accuracy: 0.9690\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1068 - accuracy: 0.9779\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0906 - accuracy: 0.9956\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0763 - accuracy: 0.9956\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0668 - accuracy: 0.9956\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0586 - accuracy: 1.0000\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0521 - accuracy: 1.0000\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0469 - accuracy: 1.0000\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0424 - accuracy: 1.0000\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0386 - accuracy: 1.0000\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0353 - accuracy: 1.0000\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0326 - accuracy: 1.0000\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0301 - accuracy: 1.0000\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0279 - accuracy: 1.0000\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0260 - accuracy: 1.0000\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0243 - accuracy: 1.0000\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0228 - accuracy: 1.0000\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0214 - accuracy: 1.0000\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0202 - accuracy: 1.0000\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0190 - accuracy: 1.0000\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0180 - accuracy: 1.0000\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0170 - accuracy: 1.0000\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0161 - accuracy: 1.0000\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0153 - accuracy: 1.0000\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0146 - accuracy: 1.0000\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0139 - accuracy: 1.0000\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0133 - accuracy: 1.0000\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0127 - accuracy: 1.0000\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0121 - accuracy: 1.0000\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0116 - accuracy: 1.0000\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0111 - accuracy: 1.0000\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0107 - accuracy: 1.0000\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0103 - accuracy: 1.0000\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0099 - accuracy: 1.0000\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0095 - accuracy: 1.0000\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0092 - accuracy: 1.0000\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0088 - accuracy: 1.0000\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0085 - accuracy: 1.0000\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0082 - accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0079 - accuracy: 1.0000\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0077 - accuracy: 1.0000\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0074 - accuracy: 1.0000\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1.8899 - accuracy: 0.3540\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1.1924 - accuracy: 0.5221\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.7247 - accuracy: 0.6903\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.4521 - accuracy: 0.8407\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.3056 - accuracy: 0.9248\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.2251 - accuracy: 0.9469\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.1749 - accuracy: 0.9513\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1412 - accuracy: 0.9602\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1159 - accuracy: 0.9779\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0984 - accuracy: 0.9867\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0841 - accuracy: 0.9956\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0724 - accuracy: 0.9956\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0642 - accuracy: 0.9956\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0570 - accuracy: 0.9956\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0517 - accuracy: 0.9956\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0465 - accuracy: 0.9956\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0423 - accuracy: 1.0000\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0387 - accuracy: 1.0000\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0354 - accuracy: 1.0000\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0327 - accuracy: 1.0000\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0304 - accuracy: 1.0000\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0282 - accuracy: 1.0000\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0264 - accuracy: 1.0000\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0248 - accuracy: 1.0000\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0232 - accuracy: 1.0000\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0219 - accuracy: 1.0000\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0207 - accuracy: 1.0000\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0195 - accuracy: 1.0000\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0185 - accuracy: 1.0000\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0176 - accuracy: 1.0000\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0167 - accuracy: 1.0000\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0159 - accuracy: 1.0000\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0152 - accuracy: 1.0000\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0144 - accuracy: 1.0000\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0138 - accuracy: 1.0000\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0132 - accuracy: 1.0000\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0127 - accuracy: 1.0000\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0122 - accuracy: 1.0000\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0117 - accuracy: 1.0000\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0112 - accuracy: 1.0000\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0108 - accuracy: 1.0000\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0104 - accuracy: 1.0000\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0100 - accuracy: 1.0000\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0096 - accuracy: 1.0000\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0093 - accuracy: 1.0000\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0090 - accuracy: 1.0000\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0086 - accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0083 - accuracy: 1.0000\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0081 - accuracy: 1.0000\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0078 - accuracy: 1.0000\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 6ms/step - loss: 1.8954 - accuracy: 0.4381\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1.3006 - accuracy: 0.5044\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.8421 - accuracy: 0.6327\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.5444 - accuracy: 0.7920\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.3651 - accuracy: 0.8761\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.2676 - accuracy: 0.9425\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.2052 - accuracy: 0.9513\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1615 - accuracy: 0.9602\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1296 - accuracy: 0.9690\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1059 - accuracy: 0.9779\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0891 - accuracy: 0.9912\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0756 - accuracy: 0.9956\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0661 - accuracy: 0.9956\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0576 - accuracy: 1.0000\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0516 - accuracy: 1.0000\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0461 - accuracy: 1.0000\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0418 - accuracy: 1.0000\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0379 - accuracy: 1.0000\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0347 - accuracy: 1.0000\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0321 - accuracy: 1.0000\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0296 - accuracy: 1.0000\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0273 - accuracy: 1.0000\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0255 - accuracy: 1.0000\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0238 - accuracy: 1.0000\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0223 - accuracy: 1.0000\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0209 - accuracy: 1.0000\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0197 - accuracy: 1.0000\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0186 - accuracy: 1.0000\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0176 - accuracy: 1.0000\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0166 - accuracy: 1.0000\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0158 - accuracy: 1.0000\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0150 - accuracy: 1.0000\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0143 - accuracy: 1.0000\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0136 - accuracy: 1.0000\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0130 - accuracy: 1.0000\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0124 - accuracy: 1.0000\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0119 - accuracy: 1.0000\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0114 - accuracy: 1.0000\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0109 - accuracy: 1.0000\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0104 - accuracy: 1.0000\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0100 - accuracy: 1.0000\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0097 - accuracy: 1.0000\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0093 - accuracy: 1.0000\n",
      "Epoch 44/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0089 - accuracy: 1.0000\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0086 - accuracy: 1.0000\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0083 - accuracy: 1.0000\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0080 - accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0077 - accuracy: 1.0000\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0075 - accuracy: 1.0000\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0071 - accuracy: 1.00 - 0s 6ms/step - loss: 0.0072 - accuracy: 1.0000\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.7892 - accuracy: 0.2212\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.1276 - accuracy: 0.4735\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.6995 - accuracy: 0.7345\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.4296 - accuracy: 0.8584\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.2840 - accuracy: 0.9248\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.2112 - accuracy: 0.9469\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1628 - accuracy: 0.9690\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1309 - accuracy: 0.9735\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1075 - accuracy: 0.9823\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0903 - accuracy: 0.9867\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0785 - accuracy: 0.9956\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0673 - accuracy: 0.9956\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0586 - accuracy: 1.0000\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0518 - accuracy: 1.0000\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0465 - accuracy: 1.0000\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0422 - accuracy: 1.0000\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0383 - accuracy: 1.0000\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0351 - accuracy: 1.0000\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0323 - accuracy: 1.0000\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0299 - accuracy: 1.0000\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0278 - accuracy: 1.0000\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0259 - accuracy: 1.0000\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0242 - accuracy: 1.0000\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0226 - accuracy: 1.0000\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0213 - accuracy: 1.0000\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0200 - accuracy: 1.0000\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0190 - accuracy: 1.0000\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0179 - accuracy: 1.0000\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0170 - accuracy: 1.0000\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0161 - accuracy: 1.0000\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0153 - accuracy: 1.0000\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0146 - accuracy: 1.0000\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0139 - accuracy: 1.0000\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0132 - accuracy: 1.0000\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0126 - accuracy: 1.0000\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0121 - accuracy: 1.0000\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0116 - accuracy: 1.0000\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0111 - accuracy: 1.0000\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0106 - accuracy: 1.0000\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0102 - accuracy: 1.0000\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0098 - accuracy: 1.0000\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0094 - accuracy: 1.0000\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0091 - accuracy: 1.0000\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0088 - accuracy: 1.0000\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0084 - accuracy: 1.0000\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0082 - accuracy: 1.0000\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0079 - accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0076 - accuracy: 1.0000\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0074 - accuracy: 1.0000\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0071 - accuracy: 1.0000\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 6ms/step - loss: 1.5831 - accuracy: 0.3938\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.9951 - accuracy: 0.5973\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.6331 - accuracy: 0.7699\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.4087 - accuracy: 0.8673\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.2727 - accuracy: 0.9336\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.1920 - accuracy: 0.9735\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.1401 - accuracy: 0.9912\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.1089 - accuracy: 0.9912\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0874 - accuracy: 0.9956\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0727 - accuracy: 1.0000\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0620 - accuracy: 1.0000\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0536 - accuracy: 1.0000\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0475 - accuracy: 1.0000\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0425 - accuracy: 1.0000\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0383 - accuracy: 1.0000\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0349 - accuracy: 1.0000\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0318 - accuracy: 1.0000\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0293 - accuracy: 1.0000\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0272 - accuracy: 1.0000\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0252 - accuracy: 1.0000\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0235 - accuracy: 1.0000\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0219 - accuracy: 1.0000\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0206 - accuracy: 1.0000\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0193 - accuracy: 1.0000\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0182 - accuracy: 1.0000\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0172 - accuracy: 1.0000\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0162 - accuracy: 1.0000\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0154 - accuracy: 1.0000\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0146 - accuracy: 1.0000\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0139 - accuracy: 1.0000\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0132 - accuracy: 1.0000\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0126 - accuracy: 1.0000\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0120 - accuracy: 1.0000\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0115 - accuracy: 1.0000\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0110 - accuracy: 1.0000\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0105 - accuracy: 1.0000\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0101 - accuracy: 1.0000\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0097 - accuracy: 1.0000\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0093 - accuracy: 1.0000\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0090 - accuracy: 1.0000\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0086 - accuracy: 1.0000\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0083 - accuracy: 1.0000\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0080 - accuracy: 1.0000\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0077 - accuracy: 1.0000\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0075 - accuracy: 1.0000\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0072 - accuracy: 1.0000\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0070 - accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0067 - accuracy: 1.0000\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0065 - accuracy: 1.0000\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0063 - accuracy: 1.0000\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 6ms/step - loss: 1.7051 - accuracy: 0.3319\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.1106 - accuracy: 0.5044\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.7079 - accuracy: 0.6903\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.4538 - accuracy: 0.8496\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.3029 - accuracy: 0.9159\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.2166 - accuracy: 0.9558\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.1654 - accuracy: 0.9690\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.1281 - accuracy: 0.9735\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1033 - accuracy: 0.9867\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0834 - accuracy: 1.0000\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0702 - accuracy: 1.0000\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0602 - accuracy: 1.0000\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0533 - accuracy: 1.0000\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0472 - accuracy: 1.0000\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0422 - accuracy: 1.0000\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0383 - accuracy: 1.0000\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0348 - accuracy: 1.0000\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0318 - accuracy: 1.0000\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0292 - accuracy: 1.0000\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0271 - accuracy: 1.0000\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0252 - accuracy: 1.0000\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0234 - accuracy: 1.0000\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0219 - accuracy: 1.0000\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0205 - accuracy: 1.0000\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0192 - accuracy: 1.0000\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - ETA: 0s - loss: 0.0170 - accuracy: 1.00 - 0s 8ms/step - loss: 0.0181 - accuracy: 1.0000\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0171 - accuracy: 1.0000\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0162 - accuracy: 1.0000\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0153 - accuracy: 1.0000\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0145 - accuracy: 1.0000\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0138 - accuracy: 1.0000\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0131 - accuracy: 1.0000\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0125 - accuracy: 1.0000\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0119 - accuracy: 1.0000\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0114 - accuracy: 1.0000\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0109 - accuracy: 1.0000\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0104 - accuracy: 1.0000\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0100 - accuracy: 1.0000\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0096 - accuracy: 1.0000\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0092 - accuracy: 1.0000\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0089 - accuracy: 1.0000\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0085 - accuracy: 1.0000\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0082 - accuracy: 1.0000\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0079 - accuracy: 1.0000\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0076 - accuracy: 1.0000\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0074 - accuracy: 1.0000\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0071 - accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0069 - accuracy: 1.0000\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0066 - accuracy: 1.0000\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0064 - accuracy: 1.0000\n",
      "Epoch 1/10\n",
      "3/3 [==============================] - 1s 6ms/step - loss: 2.4859 - accuracy: 0.1200\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 1.9569 - accuracy: 0.2178\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 1.5297 - accuracy: 0.3111\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 1.1869 - accuracy: 0.4711\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.9203 - accuracy: 0.6000\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.7208 - accuracy: 0.7511\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.5632 - accuracy: 0.8178\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.4500 - accuracy: 0.8800\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.3678 - accuracy: 0.9067\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3055 - accuracy: 0.9244\n",
      "Epoch 1/10\n",
      "3/3 [==============================] - 1s 7ms/step - loss: 2.2804 - accuracy: 0.1903\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 5ms/step - loss: 1.7884 - accuracy: 0.2522\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 1.3899 - accuracy: 0.3628\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 1.0571 - accuracy: 0.4912\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.8010 - accuracy: 0.6770\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.6037 - accuracy: 0.7876\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4637 - accuracy: 0.8628\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3655 - accuracy: 0.9071\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.2936 - accuracy: 0.9292\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.2400 - accuracy: 0.9469\n",
      "Epoch 1/10\n",
      "3/3 [==============================] - 1s 109ms/step - loss: 2.0846 - accuracy: 0.2080\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 1.6105 - accuracy: 0.2832\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 1.2357 - accuracy: 0.4558\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.9335 - accuracy: 0.5796\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.6983 - accuracy: 0.7257\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.5309 - accuracy: 0.8274\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.4102 - accuracy: 0.8850\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.3245 - accuracy: 0.9292\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.2647 - accuracy: 0.9425\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.2209 - accuracy: 0.9558\n",
      "Epoch 1/10\n",
      "3/3 [==============================] - 1s 7ms/step - loss: 2.2293 - accuracy: 0.2743\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 1.6918 - accuracy: 0.3407\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 1.2691 - accuracy: 0.4558\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.9186 - accuracy: 0.5619\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.6625 - accuracy: 0.7168\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.4694 - accuracy: 0.8186\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3465 - accuracy: 0.8805\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.2601 - accuracy: 0.9425\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.2041 - accuracy: 0.9469\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.1658 - accuracy: 0.9602\n",
      "Epoch 1/10\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 2.3294 - accuracy: 0.1549\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 1.8330 - accuracy: 0.2566\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 1.4448 - accuracy: 0.3584\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 1.1047 - accuracy: 0.4735\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.8427 - accuracy: 0.6460\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.6386 - accuracy: 0.7743\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.4979 - accuracy: 0.8319\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.3942 - accuracy: 0.8717\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.3139 - accuracy: 0.9027\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.2606 - accuracy: 0.9248\n",
      "Epoch 1/10\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 1.9025 - accuracy: 0.4823\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 1.5042 - accuracy: 0.5487\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 1.1869 - accuracy: 0.6327\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.9100 - accuracy: 0.6814\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.6906 - accuracy: 0.7257\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.4941 - accuracy: 0.8053\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.3655 - accuracy: 0.8717\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.2773 - accuracy: 0.9248\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.2149 - accuracy: 0.9602\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.1748 - accuracy: 0.9779\n",
      "Epoch 1/10\n",
      "3/3 [==============================] - 1s 6ms/step - loss: 1.7078 - accuracy: 0.3805\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 1.2894 - accuracy: 0.4912\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.9666 - accuracy: 0.6416\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.7325 - accuracy: 0.7301\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.5507 - accuracy: 0.7965\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4276 - accuracy: 0.8363\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3368 - accuracy: 0.8717\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.2733 - accuracy: 0.9159\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.2260 - accuracy: 0.9381\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.1896 - accuracy: 0.9425\n",
      "Epoch 1/10\n",
      "3/3 [==============================] - 1s 7ms/step - loss: 0.9533 - accuracy: 0.5531\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.7068 - accuracy: 0.6991\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.5368 - accuracy: 0.7965\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.4146 - accuracy: 0.8673\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.3267 - accuracy: 0.9115\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.2627 - accuracy: 0.9292\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.2146 - accuracy: 0.9558\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.1815 - accuracy: 0.9690\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.1546 - accuracy: 0.9867\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.1329 - accuracy: 0.9867\n",
      "Epoch 1/10\n",
      "3/3 [==============================] - 1s 9ms/step - loss: 1.5441 - accuracy: 0.3850\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 1.1546 - accuracy: 0.5265\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.8612 - accuracy: 0.6283\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.6509 - accuracy: 0.7257\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4916 - accuracy: 0.8097\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.3799 - accuracy: 0.8761\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.2968 - accuracy: 0.9159\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.2401 - accuracy: 0.9513\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.1987 - accuracy: 0.9602\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.1683 - accuracy: 0.9646\n",
      "Epoch 1/10\n",
      "3/3 [==============================] - 1s 6ms/step - loss: 1.8613 - accuracy: 0.3142\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 1.3648 - accuracy: 0.4292\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.9991 - accuracy: 0.5796\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.7171 - accuracy: 0.7080\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.5118 - accuracy: 0.7965\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3685 - accuracy: 0.8761\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.2751 - accuracy: 0.9204\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.2108 - accuracy: 0.9558\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.1668 - accuracy: 0.9735\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.1359 - accuracy: 0.9867\n",
      "Epoch 1/20\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 1.8160 - accuracy: 0.3556\n",
      "Epoch 2/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 1.3955 - accuracy: 0.4622\n",
      "Epoch 3/20\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 1.0646 - accuracy: 0.5644\n",
      "Epoch 4/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.8018 - accuracy: 0.6622\n",
      "Epoch 5/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.5973 - accuracy: 0.7644\n",
      "Epoch 6/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.4442 - accuracy: 0.8533\n",
      "Epoch 7/20\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.3354 - accuracy: 0.9067\n",
      "Epoch 8/20\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.2642 - accuracy: 0.9378\n",
      "Epoch 9/20\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.2129 - accuracy: 0.9511\n",
      "Epoch 10/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.1783 - accuracy: 0.9600\n",
      "Epoch 11/20\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.1517 - accuracy: 0.9644\n",
      "Epoch 12/20\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.1307 - accuracy: 0.9689\n",
      "Epoch 13/20\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1137 - accuracy: 0.9822\n",
      "Epoch 14/20\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.0993 - accuracy: 0.9822\n",
      "Epoch 15/20\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0880 - accuracy: 0.9867\n",
      "Epoch 16/20\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0786 - accuracy: 0.9867\n",
      "Epoch 17/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0703 - accuracy: 0.9956\n",
      "Epoch 18/20\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0634 - accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0581 - accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0534 - accuracy: 1.0000\n",
      "Epoch 1/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 2.2181 - accuracy: 0.2876\n",
      "Epoch 2/20\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 1.6890 - accuracy: 0.3496\n",
      "Epoch 3/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 1.2664 - accuracy: 0.4558\n",
      "Epoch 4/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.9375 - accuracy: 0.5929\n",
      "Epoch 5/20\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.6784 - accuracy: 0.7345\n",
      "Epoch 6/20\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.5015 - accuracy: 0.8319\n",
      "Epoch 7/20\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3846 - accuracy: 0.8584\n",
      "Epoch 8/20\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.3077 - accuracy: 0.9027\n",
      "Epoch 9/20\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.2534 - accuracy: 0.9204\n",
      "Epoch 10/20\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.2116 - accuracy: 0.9469\n",
      "Epoch 11/20\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.1808 - accuracy: 0.9513\n",
      "Epoch 12/20\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.1544 - accuracy: 0.9646\n",
      "Epoch 13/20\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.1321 - accuracy: 0.9779\n",
      "Epoch 14/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.1150 - accuracy: 0.9779\n",
      "Epoch 15/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.1006 - accuracy: 0.9823\n",
      "Epoch 16/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0896 - accuracy: 0.9912\n",
      "Epoch 17/20\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0804 - accuracy: 0.9956\n",
      "Epoch 18/20\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0736 - accuracy: 0.9956\n",
      "Epoch 19/20\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0674 - accuracy: 0.9956\n",
      "Epoch 20/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0621 - accuracy: 0.9956\n",
      "Epoch 1/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 1.5210 - accuracy: 0.4956\n",
      "Epoch 2/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 1.1990 - accuracy: 0.5619\n",
      "Epoch 3/20\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.9499 - accuracy: 0.6283\n",
      "Epoch 4/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.7549 - accuracy: 0.7124\n",
      "Epoch 5/20\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.5917 - accuracy: 0.7699\n",
      "Epoch 6/20\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.4582 - accuracy: 0.8274\n",
      "Epoch 7/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.3603 - accuracy: 0.8850\n",
      "Epoch 8/20\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.2891 - accuracy: 0.9336\n",
      "Epoch 9/20\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.2379 - accuracy: 0.9513\n",
      "Epoch 10/20\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.1970 - accuracy: 0.9646\n",
      "Epoch 11/20\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.1678 - accuracy: 0.9779\n",
      "Epoch 12/20\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.1436 - accuracy: 0.9823\n",
      "Epoch 13/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.1243 - accuracy: 0.9823\n",
      "Epoch 14/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.1080 - accuracy: 0.9912\n",
      "Epoch 15/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0951 - accuracy: 0.9912\n",
      "Epoch 16/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0845 - accuracy: 0.9912\n",
      "Epoch 17/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0756 - accuracy: 0.9912\n",
      "Epoch 18/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0684 - accuracy: 0.9912\n",
      "Epoch 19/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0625 - accuracy: 0.9956\n",
      "Epoch 20/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0571 - accuracy: 0.9956\n",
      "Epoch 1/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 1.7334 - accuracy: 0.3230\n",
      "Epoch 2/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 1.2848 - accuracy: 0.4115\n",
      "Epoch 3/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.9579 - accuracy: 0.5575\n",
      "Epoch 4/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.6997 - accuracy: 0.6549\n",
      "Epoch 5/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.5233 - accuracy: 0.7478\n",
      "Epoch 6/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.4004 - accuracy: 0.8274\n",
      "Epoch 7/20\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.3191 - accuracy: 0.9115\n",
      "Epoch 8/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.2584 - accuracy: 0.9292\n",
      "Epoch 9/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.2148 - accuracy: 0.9558\n",
      "Epoch 10/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.1792 - accuracy: 0.9735\n",
      "Epoch 11/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.1503 - accuracy: 0.9823\n",
      "Epoch 12/20\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.1287 - accuracy: 0.9912\n",
      "Epoch 13/20\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.1123 - accuracy: 0.9956\n",
      "Epoch 14/20\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0986 - accuracy: 1.0000\n",
      "Epoch 15/20\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0882 - accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0801 - accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0727 - accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0668 - accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0618 - accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0572 - accuracy: 1.0000\n",
      "Epoch 1/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 2.2263 - accuracy: 0.1858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 1.7684 - accuracy: 0.2655\n",
      "Epoch 3/20\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 1.4080 - accuracy: 0.3628\n",
      "Epoch 4/20\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 1.0968 - accuracy: 0.5044\n",
      "Epoch 5/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.8513 - accuracy: 0.6239\n",
      "Epoch 6/20\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.6550 - accuracy: 0.7168\n",
      "Epoch 7/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.5071 - accuracy: 0.8097\n",
      "Epoch 8/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.3976 - accuracy: 0.8938\n",
      "Epoch 9/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.3189 - accuracy: 0.9248\n",
      "Epoch 10/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.2587 - accuracy: 0.9469\n",
      "Epoch 11/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.2151 - accuracy: 0.9558\n",
      "Epoch 12/20\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.1819 - accuracy: 0.9779\n",
      "Epoch 13/20\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.1568 - accuracy: 0.9823\n",
      "Epoch 14/20\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1367 - accuracy: 0.9823\n",
      "Epoch 15/20\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.1214 - accuracy: 0.9823\n",
      "Epoch 16/20\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1089 - accuracy: 0.9867\n",
      "Epoch 17/20\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.0986 - accuracy: 0.9867\n",
      "Epoch 18/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0897 - accuracy: 0.9867\n",
      "Epoch 19/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0818 - accuracy: 0.9912\n",
      "Epoch 20/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0758 - accuracy: 0.9956\n",
      "Epoch 1/20\n",
      "3/3 [==============================] - 1s 9ms/step - loss: 0.9925 - accuracy: 0.5354\n",
      "Epoch 2/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.7152 - accuracy: 0.6858\n",
      "Epoch 3/20\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.5278 - accuracy: 0.8009\n",
      "Epoch 4/20\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.3916 - accuracy: 0.8628\n",
      "Epoch 5/20\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.2995 - accuracy: 0.9248\n",
      "Epoch 6/20\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.2366 - accuracy: 0.9602\n",
      "Epoch 7/20\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.1909 - accuracy: 0.9602\n",
      "Epoch 8/20\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.1566 - accuracy: 0.9735\n",
      "Epoch 9/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.1292 - accuracy: 0.9823\n",
      "Epoch 10/20\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.1097 - accuracy: 0.9867\n",
      "Epoch 11/20\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0934 - accuracy: 0.9867\n",
      "Epoch 12/20\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0917 - accuracy: 0.98 - 0s 7ms/step - loss: 0.0823 - accuracy: 0.9867\n",
      "Epoch 13/20\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.0726 - accuracy: 0.9912\n",
      "Epoch 14/20\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0644 - accuracy: 0.9956\n",
      "Epoch 15/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0581 - accuracy: 0.9956\n",
      "Epoch 16/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0523 - accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0475 - accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0432 - accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0396 - accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0362 - accuracy: 1.0000\n",
      "Epoch 1/20\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 1.7811 - accuracy: 0.2168\n",
      "Epoch 2/20\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 1.3174 - accuracy: 0.3805\n",
      "Epoch 3/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.9788 - accuracy: 0.5619\n",
      "Epoch 4/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.7336 - accuracy: 0.6991\n",
      "Epoch 5/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.5562 - accuracy: 0.7832\n",
      "Epoch 6/20\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.4287 - accuracy: 0.8540\n",
      "Epoch 7/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.3402 - accuracy: 0.8982\n",
      "Epoch 8/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.2778 - accuracy: 0.9292\n",
      "Epoch 9/20\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.2298 - accuracy: 0.9381\n",
      "Epoch 10/20\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.1925 - accuracy: 0.9469\n",
      "Epoch 11/20\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.1637 - accuracy: 0.9602\n",
      "Epoch 12/20\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.1414 - accuracy: 0.9690\n",
      "Epoch 13/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.1247 - accuracy: 0.9779\n",
      "Epoch 14/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.1101 - accuracy: 0.9823\n",
      "Epoch 15/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0985 - accuracy: 0.9823\n",
      "Epoch 16/20\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0881 - accuracy: 0.9823\n",
      "Epoch 17/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0796 - accuracy: 0.9823\n",
      "Epoch 18/20\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0716 - accuracy: 0.9823\n",
      "Epoch 19/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0660 - accuracy: 0.9912\n",
      "Epoch 20/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0600 - accuracy: 0.9956\n",
      "Epoch 1/20\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 1.7443 - accuracy: 0.4292\n",
      "Epoch 2/20\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 1.3415 - accuracy: 0.5265\n",
      "Epoch 3/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 1.0168 - accuracy: 0.5841\n",
      "Epoch 4/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.7614 - accuracy: 0.6726\n",
      "Epoch 5/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.5564 - accuracy: 0.7611\n",
      "Epoch 6/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.4202 - accuracy: 0.8319\n",
      "Epoch 7/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.3272 - accuracy: 0.8850\n",
      "Epoch 8/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.2654 - accuracy: 0.9159\n",
      "Epoch 9/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.2214 - accuracy: 0.9248\n",
      "Epoch 10/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.1893 - accuracy: 0.9558\n",
      "Epoch 11/20\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.1625 - accuracy: 0.9602\n",
      "Epoch 12/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.1398 - accuracy: 0.9690\n",
      "Epoch 13/20\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1203 - accuracy: 0.9690\n",
      "Epoch 14/20\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.1049 - accuracy: 0.9823\n",
      "Epoch 15/20\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0911 - accuracy: 0.9823\n",
      "Epoch 16/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0802 - accuracy: 0.9912\n",
      "Epoch 17/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0715 - accuracy: 0.9956\n",
      "Epoch 18/20\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0637 - accuracy: 0.9956\n",
      "Epoch 19/20\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0586 - accuracy: 0.9956\n",
      "Epoch 20/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0539 - accuracy: 0.9956\n",
      "Epoch 1/20\n",
      "3/3 [==============================] - 1s 6ms/step - loss: 1.4479 - accuracy: 0.4469\n",
      "Epoch 2/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 1.0984 - accuracy: 0.5575\n",
      "Epoch 3/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.8467 - accuracy: 0.6416\n",
      "Epoch 4/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.6435 - accuracy: 0.7478\n",
      "Epoch 5/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.4979 - accuracy: 0.8186\n",
      "Epoch 6/20\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.3898 - accuracy: 0.8894\n",
      "Epoch 7/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.3135 - accuracy: 0.9204\n",
      "Epoch 8/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.2570 - accuracy: 0.9381\n",
      "Epoch 9/20\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.2157 - accuracy: 0.9602\n",
      "Epoch 10/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.1813 - accuracy: 0.9735\n",
      "Epoch 11/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.1561 - accuracy: 0.9779\n",
      "Epoch 12/20\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.1347 - accuracy: 0.9823\n",
      "Epoch 13/20\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.1165 - accuracy: 0.9823\n",
      "Epoch 14/20\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.1035 - accuracy: 0.9912\n",
      "Epoch 15/20\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0912 - accuracy: 0.9912\n",
      "Epoch 16/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0813 - accuracy: 0.9912\n",
      "Epoch 17/20\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0726 - accuracy: 0.9912\n",
      "Epoch 18/20\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0659 - accuracy: 0.9912\n",
      "Epoch 19/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0594 - accuracy: 0.9912\n",
      "Epoch 20/20\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0531 - accuracy: 0.9912\n",
      "Epoch 1/20\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 1.9774 - accuracy: 0.2389\n",
      "Epoch 2/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 1.5254 - accuracy: 0.3717\n",
      "Epoch 3/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 1.1786 - accuracy: 0.4469\n",
      "Epoch 4/20\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.8924 - accuracy: 0.5796\n",
      "Epoch 5/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.6800 - accuracy: 0.7168\n",
      "Epoch 6/20\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.5189 - accuracy: 0.8186\n",
      "Epoch 7/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.4041 - accuracy: 0.8673\n",
      "Epoch 8/20\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.3217 - accuracy: 0.9115\n",
      "Epoch 9/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.2614 - accuracy: 0.9292\n",
      "Epoch 10/20\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.2170 - accuracy: 0.9558\n",
      "Epoch 11/20\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1815 - accuracy: 0.9735\n",
      "Epoch 12/20\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.1559 - accuracy: 0.9867\n",
      "Epoch 13/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.1342 - accuracy: 0.9867\n",
      "Epoch 14/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.1175 - accuracy: 0.9867\n",
      "Epoch 15/20\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.1038 - accuracy: 0.9912\n",
      "Epoch 16/20\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0918 - accuracy: 0.9912\n",
      "Epoch 17/20\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0828 - accuracy: 0.9912\n",
      "Epoch 18/20\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0748 - accuracy: 0.9912\n",
      "Epoch 19/20\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0687 - accuracy: 0.9956\n",
      "Epoch 20/20\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0628 - accuracy: 0.9956\n",
      "Epoch 1/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 2.7189 - accuracy: 0.1911\n",
      "Epoch 2/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 2.1896 - accuracy: 0.2756\n",
      "Epoch 3/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 1.7362 - accuracy: 0.3556\n",
      "Epoch 4/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 1.3501 - accuracy: 0.4489\n",
      "Epoch 5/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 1.0294 - accuracy: 0.5689\n",
      "Epoch 6/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.7730 - accuracy: 0.7022\n",
      "Epoch 7/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.5778 - accuracy: 0.8133\n",
      "Epoch 8/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.4441 - accuracy: 0.8400\n",
      "Epoch 9/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.3423 - accuracy: 0.8844\n",
      "Epoch 10/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.2695 - accuracy: 0.9289\n",
      "Epoch 11/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.2214 - accuracy: 0.9644\n",
      "Epoch 12/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.1855 - accuracy: 0.9689\n",
      "Epoch 13/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.1580 - accuracy: 0.9733\n",
      "Epoch 14/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.1381 - accuracy: 0.9733\n",
      "Epoch 15/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.1230 - accuracy: 0.9778\n",
      "Epoch 16/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.1102 - accuracy: 0.9911\n",
      "Epoch 17/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0995 - accuracy: 0.9911\n",
      "Epoch 18/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0909 - accuracy: 0.9911\n",
      "Epoch 19/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0833 - accuracy: 0.9911\n",
      "Epoch 20/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0766 - accuracy: 0.9956\n",
      "Epoch 21/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0709 - accuracy: 0.9956\n",
      "Epoch 22/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0658 - accuracy: 0.9956\n",
      "Epoch 23/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0614 - accuracy: 0.9956\n",
      "Epoch 24/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0574 - accuracy: 0.9956\n",
      "Epoch 25/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0537 - accuracy: 0.9956\n",
      "Epoch 26/30\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0507 - accuracy: 0.9956\n",
      "Epoch 27/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0477 - accuracy: 0.9956\n",
      "Epoch 28/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0452 - accuracy: 1.0000\n",
      "Epoch 29/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0428 - accuracy: 1.0000\n",
      "Epoch 30/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0407 - accuracy: 1.0000\n",
      "Epoch 1/30\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 1.2210 - accuracy: 0.5133\n",
      "Epoch 2/30\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.9087 - accuracy: 0.6106\n",
      "Epoch 3/30\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.6858 - accuracy: 0.7301\n",
      "Epoch 4/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.5231 - accuracy: 0.8274\n",
      "Epoch 5/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.4022 - accuracy: 0.8850\n",
      "Epoch 6/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.3148 - accuracy: 0.9159\n",
      "Epoch 7/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.2499 - accuracy: 0.9425\n",
      "Epoch 8/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.2050 - accuracy: 0.9558\n",
      "Epoch 9/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.1718 - accuracy: 0.9690\n",
      "Epoch 10/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.1465 - accuracy: 0.9735\n",
      "Epoch 11/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.1259 - accuracy: 0.9823\n",
      "Epoch 12/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.1101 - accuracy: 0.9823\n",
      "Epoch 13/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0971 - accuracy: 0.9867\n",
      "Epoch 14/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0867 - accuracy: 0.9867\n",
      "Epoch 15/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0770 - accuracy: 0.9912\n",
      "Epoch 16/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0683 - accuracy: 0.9912\n",
      "Epoch 17/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0616 - accuracy: 0.9912\n",
      "Epoch 18/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0560 - accuracy: 0.9956\n",
      "Epoch 19/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0507 - accuracy: 0.9956\n",
      "Epoch 20/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0466 - accuracy: 1.0000\n",
      "Epoch 21/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0430 - accuracy: 1.0000\n",
      "Epoch 22/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0401 - accuracy: 1.0000\n",
      "Epoch 23/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0373 - accuracy: 1.0000\n",
      "Epoch 24/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0349 - accuracy: 1.0000\n",
      "Epoch 25/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0329 - accuracy: 1.0000\n",
      "Epoch 26/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0308 - accuracy: 1.0000\n",
      "Epoch 27/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0290 - accuracy: 1.0000\n",
      "Epoch 28/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0275 - accuracy: 1.0000\n",
      "Epoch 29/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0258 - accuracy: 1.0000\n",
      "Epoch 30/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0246 - accuracy: 1.0000\n",
      "Epoch 1/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 2.1514 - accuracy: 0.2080\n",
      "Epoch 2/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 1.6701 - accuracy: 0.2788\n",
      "Epoch 3/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 1.2830 - accuracy: 0.3850\n",
      "Epoch 4/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.9772 - accuracy: 0.5752\n",
      "Epoch 5/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.7371 - accuracy: 0.6947\n",
      "Epoch 6/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.5580 - accuracy: 0.8097\n",
      "Epoch 7/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.4332 - accuracy: 0.8584\n",
      "Epoch 8/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.3401 - accuracy: 0.9159\n",
      "Epoch 9/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.2687 - accuracy: 0.9381\n",
      "Epoch 10/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.2210 - accuracy: 0.9469\n",
      "Epoch 11/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.1820 - accuracy: 0.9690\n",
      "Epoch 12/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.1537 - accuracy: 0.9779\n",
      "Epoch 13/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.1306 - accuracy: 0.9823\n",
      "Epoch 14/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.1124 - accuracy: 0.9823\n",
      "Epoch 15/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0991 - accuracy: 0.9912\n",
      "Epoch 16/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0881 - accuracy: 0.9912\n",
      "Epoch 17/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0787 - accuracy: 0.9956\n",
      "Epoch 18/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0712 - accuracy: 0.9956\n",
      "Epoch 19/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0653 - accuracy: 0.9956\n",
      "Epoch 20/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0598 - accuracy: 0.9956\n",
      "Epoch 21/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0551 - accuracy: 0.9956\n",
      "Epoch 22/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0508 - accuracy: 0.9956\n",
      "Epoch 23/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0474 - accuracy: 1.0000\n",
      "Epoch 24/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0441 - accuracy: 1.0000\n",
      "Epoch 25/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0414 - accuracy: 1.0000\n",
      "Epoch 26/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0389 - accuracy: 1.0000\n",
      "Epoch 27/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0367 - accuracy: 1.0000\n",
      "Epoch 28/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0349 - accuracy: 1.0000\n",
      "Epoch 29/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0330 - accuracy: 1.0000\n",
      "Epoch 30/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0315 - accuracy: 1.0000\n",
      "Epoch 1/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 2.8512 - accuracy: 0.2345\n",
      "Epoch 2/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 2.3382 - accuracy: 0.2699\n",
      "Epoch 3/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 1.8889 - accuracy: 0.3274\n",
      "Epoch 4/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 1.4818 - accuracy: 0.3938\n",
      "Epoch 5/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 1.1405 - accuracy: 0.5177\n",
      "Epoch 6/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.8562 - accuracy: 0.6372\n",
      "Epoch 7/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.6372 - accuracy: 0.7566\n",
      "Epoch 8/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.4768 - accuracy: 0.8451\n",
      "Epoch 9/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.3631 - accuracy: 0.9336\n",
      "Epoch 10/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.2852 - accuracy: 0.9558\n",
      "Epoch 11/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.2302 - accuracy: 0.9690\n",
      "Epoch 12/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.1876 - accuracy: 0.9779\n",
      "Epoch 13/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.1584 - accuracy: 0.9823\n",
      "Epoch 14/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.1351 - accuracy: 0.9823\n",
      "Epoch 15/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.1183 - accuracy: 0.9867\n",
      "Epoch 16/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.1037 - accuracy: 0.9867\n",
      "Epoch 17/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0914 - accuracy: 0.9867\n",
      "Epoch 18/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0825 - accuracy: 0.9912\n",
      "Epoch 19/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0738 - accuracy: 0.9956\n",
      "Epoch 20/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0678 - accuracy: 0.9956\n",
      "Epoch 21/30\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0620 - accuracy: 0.9956\n",
      "Epoch 22/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0574 - accuracy: 0.9956\n",
      "Epoch 23/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0534 - accuracy: 1.0000\n",
      "Epoch 24/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0500 - accuracy: 1.0000\n",
      "Epoch 25/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0469 - accuracy: 1.0000\n",
      "Epoch 26/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0442 - accuracy: 1.0000\n",
      "Epoch 27/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0418 - accuracy: 1.0000\n",
      "Epoch 28/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0396 - accuracy: 1.0000\n",
      "Epoch 29/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0375 - accuracy: 1.0000\n",
      "Epoch 30/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0357 - accuracy: 1.0000\n",
      "Epoch 1/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 2.6379 - accuracy: 0.3363\n",
      "Epoch 2/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 2.1823 - accuracy: 0.3673\n",
      "Epoch 3/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 1.7728 - accuracy: 0.4204\n",
      "Epoch 4/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 1.4162 - accuracy: 0.4779\n",
      "Epoch 5/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 1.0996 - accuracy: 0.5442\n",
      "Epoch 6/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.8428 - accuracy: 0.6150\n",
      "Epoch 7/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.6315 - accuracy: 0.7212\n",
      "Epoch 8/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.4708 - accuracy: 0.8363\n",
      "Epoch 9/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.3652 - accuracy: 0.8938\n",
      "Epoch 10/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.2855 - accuracy: 0.9292\n",
      "Epoch 11/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.2311 - accuracy: 0.9513\n",
      "Epoch 12/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.1899 - accuracy: 0.9646\n",
      "Epoch 13/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.1605 - accuracy: 0.9690\n",
      "Epoch 14/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.1371 - accuracy: 0.9823\n",
      "Epoch 15/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.1186 - accuracy: 0.9912\n",
      "Epoch 16/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.1041 - accuracy: 0.9912\n",
      "Epoch 17/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0919 - accuracy: 0.9956\n",
      "Epoch 18/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0822 - accuracy: 0.9956\n",
      "Epoch 19/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0737 - accuracy: 0.9956\n",
      "Epoch 20/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0673 - accuracy: 0.9956\n",
      "Epoch 21/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0617 - accuracy: 0.9956\n",
      "Epoch 22/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0567 - accuracy: 0.9956\n",
      "Epoch 23/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0529 - accuracy: 0.9956\n",
      "Epoch 24/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0488 - accuracy: 1.0000\n",
      "Epoch 25/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0457 - accuracy: 1.0000\n",
      "Epoch 26/30\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0427 - accuracy: 1.0000\n",
      "Epoch 27/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0403 - accuracy: 1.0000\n",
      "Epoch 28/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0380 - accuracy: 1.0000\n",
      "Epoch 29/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0359 - accuracy: 1.0000\n",
      "Epoch 30/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0341 - accuracy: 1.0000\n",
      "Epoch 1/30\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 1.4216 - accuracy: 0.5221\n",
      "Epoch 2/30\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 1.0522 - accuracy: 0.6150\n",
      "Epoch 3/30\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.7877 - accuracy: 0.6947\n",
      "Epoch 4/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.5781 - accuracy: 0.7920\n",
      "Epoch 5/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.4323 - accuracy: 0.8673\n",
      "Epoch 6/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.3240 - accuracy: 0.9027\n",
      "Epoch 7/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.2472 - accuracy: 0.9292\n",
      "Epoch 8/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.1958 - accuracy: 0.9469\n",
      "Epoch 9/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.1558 - accuracy: 0.9735\n",
      "Epoch 10/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.1268 - accuracy: 0.9823\n",
      "Epoch 11/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.1055 - accuracy: 0.9867\n",
      "Epoch 12/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0895 - accuracy: 0.9867\n",
      "Epoch 13/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0780 - accuracy: 0.9912\n",
      "Epoch 14/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0679 - accuracy: 1.0000\n",
      "Epoch 15/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0604 - accuracy: 1.0000\n",
      "Epoch 16/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0541 - accuracy: 1.0000\n",
      "Epoch 17/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0484 - accuracy: 1.0000\n",
      "Epoch 18/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0439 - accuracy: 1.0000\n",
      "Epoch 19/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0400 - accuracy: 1.0000\n",
      "Epoch 20/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0365 - accuracy: 1.0000\n",
      "Epoch 21/30\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0337 - accuracy: 1.0000\n",
      "Epoch 22/30\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.0314 - accuracy: 1.0000\n",
      "Epoch 23/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0291 - accuracy: 1.0000\n",
      "Epoch 24/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0272 - accuracy: 1.0000\n",
      "Epoch 25/30\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0256 - accuracy: 1.0000\n",
      "Epoch 26/30\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0242 - accuracy: 1.0000\n",
      "Epoch 27/30\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0229 - accuracy: 1.0000\n",
      "Epoch 28/30\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0218 - accuracy: 1.0000\n",
      "Epoch 29/30\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.0207 - accuracy: 1.0000\n",
      "Epoch 30/30\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0197 - accuracy: 1.0000\n",
      "Epoch 1/30\n",
      "3/3 [==============================] - 1s 6ms/step - loss: 1.9749 - accuracy: 0.3894\n",
      "Epoch 2/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 1.5612 - accuracy: 0.4469\n",
      "Epoch 3/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 1.2244 - accuracy: 0.5310\n",
      "Epoch 4/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.9468 - accuracy: 0.6195\n",
      "Epoch 5/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.7182 - accuracy: 0.6991\n",
      "Epoch 6/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.5487 - accuracy: 0.7832\n",
      "Epoch 7/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.4200 - accuracy: 0.8451\n",
      "Epoch 8/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.3275 - accuracy: 0.8894\n",
      "Epoch 9/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.2599 - accuracy: 0.9513\n",
      "Epoch 10/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.2143 - accuracy: 0.9558\n",
      "Epoch 11/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.1792 - accuracy: 0.9646\n",
      "Epoch 12/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.1526 - accuracy: 0.9690\n",
      "Epoch 13/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.1311 - accuracy: 0.9735\n",
      "Epoch 14/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.1146 - accuracy: 0.9823\n",
      "Epoch 15/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.1013 - accuracy: 0.9912\n",
      "Epoch 16/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0899 - accuracy: 0.9912\n",
      "Epoch 17/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0811 - accuracy: 0.9956\n",
      "Epoch 18/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0735 - accuracy: 0.9956\n",
      "Epoch 19/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0668 - accuracy: 0.9956\n",
      "Epoch 20/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0614 - accuracy: 0.9956\n",
      "Epoch 21/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0566 - accuracy: 0.9956\n",
      "Epoch 22/30\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0524 - accuracy: 0.9956\n",
      "Epoch 23/30\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0488 - accuracy: 0.9956\n",
      "Epoch 24/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0459 - accuracy: 0.9956\n",
      "Epoch 25/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0428 - accuracy: 1.0000\n",
      "Epoch 26/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0403 - accuracy: 1.0000\n",
      "Epoch 27/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0380 - accuracy: 1.0000\n",
      "Epoch 28/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0359 - accuracy: 1.0000\n",
      "Epoch 29/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0340 - accuracy: 1.0000\n",
      "Epoch 30/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0323 - accuracy: 1.0000\n",
      "Epoch 1/30\n",
      "3/3 [==============================] - 1s 10ms/step - loss: 3.7243 - accuracy: 0.1018\n",
      "Epoch 2/30\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 3.1243 - accuracy: 0.1372\n",
      "Epoch 3/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 2.5997 - accuracy: 0.1991\n",
      "Epoch 4/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 2.1135 - accuracy: 0.2655\n",
      "Epoch 5/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 1.6935 - accuracy: 0.3319\n",
      "Epoch 6/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 1.3233 - accuracy: 0.4027\n",
      "Epoch 7/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 1.0204 - accuracy: 0.5265\n",
      "Epoch 8/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 5ms/step - loss: 0.7794 - accuracy: 0.6903\n",
      "Epoch 9/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.5981 - accuracy: 0.7743\n",
      "Epoch 10/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.4606 - accuracy: 0.8496\n",
      "Epoch 11/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.3650 - accuracy: 0.9071\n",
      "Epoch 12/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.2955 - accuracy: 0.9425\n",
      "Epoch 13/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.2444 - accuracy: 0.9558\n",
      "Epoch 14/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.2076 - accuracy: 0.9646\n",
      "Epoch 15/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.1788 - accuracy: 0.9823\n",
      "Epoch 16/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.1568 - accuracy: 0.9823\n",
      "Epoch 17/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.1382 - accuracy: 0.9867\n",
      "Epoch 18/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.1234 - accuracy: 0.9867\n",
      "Epoch 19/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.1116 - accuracy: 0.9867\n",
      "Epoch 20/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.1006 - accuracy: 0.9867\n",
      "Epoch 21/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0920 - accuracy: 0.9912\n",
      "Epoch 22/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0846 - accuracy: 1.0000\n",
      "Epoch 23/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0780 - accuracy: 1.0000\n",
      "Epoch 24/30\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0726 - accuracy: 1.0000\n",
      "Epoch 25/30\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0678 - accuracy: 1.0000\n",
      "Epoch 26/30\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0636 - accuracy: 1.0000\n",
      "Epoch 27/30\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.0599 - accuracy: 1.0000\n",
      "Epoch 28/30\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0564 - accuracy: 1.0000\n",
      "Epoch 29/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0533 - accuracy: 1.0000\n",
      "Epoch 30/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0505 - accuracy: 1.0000\n",
      "Epoch 1/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 2.3274 - accuracy: 0.1726\n",
      "Epoch 2/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 1.8379 - accuracy: 0.2257\n",
      "Epoch 3/30\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 1.4461 - accuracy: 0.3230\n",
      "Epoch 4/30\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 1.1157 - accuracy: 0.5000\n",
      "Epoch 5/30\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.8614 - accuracy: 0.6106\n",
      "Epoch 6/30\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6637 - accuracy: 0.7301\n",
      "Epoch 7/30\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.5146 - accuracy: 0.8319\n",
      "Epoch 8/30\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.4056 - accuracy: 0.8673\n",
      "Epoch 9/30\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3249 - accuracy: 0.8938\n",
      "Epoch 10/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.2641 - accuracy: 0.9381\n",
      "Epoch 11/30\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.2201 - accuracy: 0.9469\n",
      "Epoch 12/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.1871 - accuracy: 0.9646\n",
      "Epoch 13/30\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.1611 - accuracy: 0.9735\n",
      "Epoch 14/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.1408 - accuracy: 0.9779\n",
      "Epoch 15/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.1245 - accuracy: 0.9779\n",
      "Epoch 16/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.1109 - accuracy: 0.9823\n",
      "Epoch 17/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0993 - accuracy: 0.9912\n",
      "Epoch 18/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0899 - accuracy: 0.9956\n",
      "Epoch 19/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0819 - accuracy: 0.9956\n",
      "Epoch 20/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0748 - accuracy: 0.9956\n",
      "Epoch 21/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0689 - accuracy: 0.9956\n",
      "Epoch 22/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0639 - accuracy: 0.9956\n",
      "Epoch 23/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0594 - accuracy: 0.9956\n",
      "Epoch 24/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0554 - accuracy: 0.9956\n",
      "Epoch 25/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0520 - accuracy: 0.9956\n",
      "Epoch 26/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0489 - accuracy: 0.9956\n",
      "Epoch 27/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0462 - accuracy: 0.9956\n",
      "Epoch 28/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0435 - accuracy: 1.0000\n",
      "Epoch 29/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0412 - accuracy: 1.0000\n",
      "Epoch 30/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0390 - accuracy: 1.0000\n",
      "Epoch 1/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 1.8705 - accuracy: 0.2965\n",
      "Epoch 2/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 1.4290 - accuracy: 0.4027\n",
      "Epoch 3/30\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 1.0791 - accuracy: 0.5575\n",
      "Epoch 4/30\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.8008 - accuracy: 0.6770\n",
      "Epoch 5/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.6025 - accuracy: 0.7920\n",
      "Epoch 6/30\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4598 - accuracy: 0.8628\n",
      "Epoch 7/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.3539 - accuracy: 0.8850\n",
      "Epoch 8/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.2795 - accuracy: 0.9027\n",
      "Epoch 9/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.2232 - accuracy: 0.9292\n",
      "Epoch 10/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.1805 - accuracy: 0.9558\n",
      "Epoch 11/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.1475 - accuracy: 0.9779\n",
      "Epoch 12/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.1233 - accuracy: 0.9867\n",
      "Epoch 13/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.1045 - accuracy: 0.9867\n",
      "Epoch 14/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0900 - accuracy: 0.9867\n",
      "Epoch 15/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0784 - accuracy: 0.9956\n",
      "Epoch 16/30\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0700 - accuracy: 0.9956\n",
      "Epoch 17/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0631 - accuracy: 0.9956\n",
      "Epoch 18/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0579 - accuracy: 0.9956\n",
      "Epoch 19/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0531 - accuracy: 1.0000\n",
      "Epoch 20/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0490 - accuracy: 1.0000\n",
      "Epoch 21/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0454 - accuracy: 1.0000\n",
      "Epoch 22/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0423 - accuracy: 1.0000\n",
      "Epoch 23/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0395 - accuracy: 1.0000\n",
      "Epoch 24/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0368 - accuracy: 1.0000\n",
      "Epoch 25/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0348 - accuracy: 1.0000\n",
      "Epoch 26/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0329 - accuracy: 1.0000\n",
      "Epoch 27/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0312 - accuracy: 1.0000\n",
      "Epoch 28/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0296 - accuracy: 1.0000\n",
      "Epoch 29/30\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0281 - accuracy: 1.0000\n",
      "Epoch 30/30\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0269 - accuracy: 1.0000\n",
      "Epoch 1/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 1.9972 - accuracy: 0.3778\n",
      "Epoch 2/40\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 1.5634 - accuracy: 0.4800\n",
      "Epoch 3/40\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 1.2225 - accuracy: 0.5956\n",
      "Epoch 4/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.9454 - accuracy: 0.6622\n",
      "Epoch 5/40\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.7250 - accuracy: 0.7422\n",
      "Epoch 6/40\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.5630 - accuracy: 0.8133\n",
      "Epoch 7/40\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4417 - accuracy: 0.8889\n",
      "Epoch 8/40\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.3436 - accuracy: 0.9111\n",
      "Epoch 9/40\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.2782 - accuracy: 0.9333\n",
      "Epoch 10/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.2318 - accuracy: 0.9333\n",
      "Epoch 11/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.1924 - accuracy: 0.9422\n",
      "Epoch 12/40\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.1620 - accuracy: 0.9556\n",
      "Epoch 13/40\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.1389 - accuracy: 0.9733\n",
      "Epoch 14/40\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1217 - accuracy: 0.9733\n",
      "Epoch 15/40\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.1064 - accuracy: 0.9867\n",
      "Epoch 16/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0934 - accuracy: 0.9911\n",
      "Epoch 17/40\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0835 - accuracy: 0.9911\n",
      "Epoch 18/40\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0753 - accuracy: 0.9911\n",
      "Epoch 19/40\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0684 - accuracy: 0.9911\n",
      "Epoch 20/40\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0621 - accuracy: 0.9911\n",
      "Epoch 21/40\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0572 - accuracy: 0.9911\n",
      "Epoch 22/40\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0528 - accuracy: 0.9911\n",
      "Epoch 23/40\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0511 - accuracy: 0.99 - 0s 7ms/step - loss: 0.0491 - accuracy: 0.9956\n",
      "Epoch 24/40\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0457 - accuracy: 1.0000\n",
      "Epoch 25/40\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0428 - accuracy: 1.0000\n",
      "Epoch 26/40\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0402 - accuracy: 1.0000\n",
      "Epoch 27/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0378 - accuracy: 1.0000\n",
      "Epoch 28/40\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0358 - accuracy: 1.0000\n",
      "Epoch 29/40\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.0337 - accuracy: 1.0000\n",
      "Epoch 30/40\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0320 - accuracy: 1.0000\n",
      "Epoch 31/40\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0305 - accuracy: 1.0000\n",
      "Epoch 32/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0289 - accuracy: 1.0000\n",
      "Epoch 33/40\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0276 - accuracy: 1.0000\n",
      "Epoch 34/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0264 - accuracy: 1.0000\n",
      "Epoch 35/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0252 - accuracy: 1.0000\n",
      "Epoch 36/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0242 - accuracy: 1.0000\n",
      "Epoch 37/40\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0232 - accuracy: 1.0000\n",
      "Epoch 38/40\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0223 - accuracy: 1.0000\n",
      "Epoch 39/40\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0215 - accuracy: 1.0000\n",
      "Epoch 40/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0207 - accuracy: 1.0000\n",
      "Epoch 1/40\n",
      "3/3 [==============================] - 1s 6ms/step - loss: 1.5543 - accuracy: 0.3761\n",
      "Epoch 2/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 1.1668 - accuracy: 0.4912\n",
      "Epoch 3/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.8687 - accuracy: 0.5885\n",
      "Epoch 4/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.6517 - accuracy: 0.7389\n",
      "Epoch 5/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.4929 - accuracy: 0.8274\n",
      "Epoch 6/40\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3787 - accuracy: 0.8628\n",
      "Epoch 7/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.2957 - accuracy: 0.9248\n",
      "Epoch 8/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.2383 - accuracy: 0.9425\n",
      "Epoch 9/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.1953 - accuracy: 0.9646\n",
      "Epoch 10/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.1641 - accuracy: 0.9646\n",
      "Epoch 11/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.1394 - accuracy: 0.9779\n",
      "Epoch 12/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.1201 - accuracy: 0.9779\n",
      "Epoch 13/40\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.1051 - accuracy: 0.9823\n",
      "Epoch 14/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0927 - accuracy: 0.9867\n",
      "Epoch 15/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0832 - accuracy: 0.9912\n",
      "Epoch 16/40\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0749 - accuracy: 0.9912\n",
      "Epoch 17/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0680 - accuracy: 0.9912\n",
      "Epoch 18/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0622 - accuracy: 0.9912\n",
      "Epoch 19/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0577 - accuracy: 0.9912\n",
      "Epoch 20/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0536 - accuracy: 0.9912\n",
      "Epoch 21/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0500 - accuracy: 0.9912\n",
      "Epoch 22/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0467 - accuracy: 0.9912\n",
      "Epoch 23/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0436 - accuracy: 0.9956\n",
      "Epoch 24/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0407 - accuracy: 0.9956\n",
      "Epoch 25/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0384 - accuracy: 0.9956\n",
      "Epoch 26/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0361 - accuracy: 1.0000\n",
      "Epoch 27/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0337 - accuracy: 1.0000\n",
      "Epoch 28/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0318 - accuracy: 1.0000\n",
      "Epoch 29/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0303 - accuracy: 1.0000\n",
      "Epoch 30/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0287 - accuracy: 1.0000\n",
      "Epoch 31/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0273 - accuracy: 1.0000\n",
      "Epoch 32/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0259 - accuracy: 1.0000\n",
      "Epoch 33/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0247 - accuracy: 1.0000\n",
      "Epoch 34/40\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.0236 - accuracy: 1.0000\n",
      "Epoch 35/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0225 - accuracy: 1.0000\n",
      "Epoch 36/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0216 - accuracy: 1.0000\n",
      "Epoch 37/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0207 - accuracy: 1.0000\n",
      "Epoch 38/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0199 - accuracy: 1.0000\n",
      "Epoch 39/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0191 - accuracy: 1.0000\n",
      "Epoch 40/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0184 - accuracy: 1.0000\n",
      "Epoch 1/40\n",
      "3/3 [==============================] - 1s 6ms/step - loss: 0.9628 - accuracy: 0.5708\n",
      "Epoch 2/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.6827 - accuracy: 0.7478\n",
      "Epoch 3/40\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.4913 - accuracy: 0.8363\n",
      "Epoch 4/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.3687 - accuracy: 0.8673\n",
      "Epoch 5/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.2836 - accuracy: 0.8938\n",
      "Epoch 6/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 7ms/step - loss: 0.2217 - accuracy: 0.9336\n",
      "Epoch 7/40\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.1773 - accuracy: 0.9513\n",
      "Epoch 8/40\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.1455 - accuracy: 0.9690\n",
      "Epoch 9/40\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.1220 - accuracy: 0.9735\n",
      "Epoch 10/40\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.1026 - accuracy: 0.9912\n",
      "Epoch 11/40\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0882 - accuracy: 0.9912\n",
      "Epoch 12/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0767 - accuracy: 0.9956\n",
      "Epoch 13/40\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0669 - accuracy: 0.9956\n",
      "Epoch 14/40\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0592 - accuracy: 0.9956\n",
      "Epoch 15/40\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0524 - accuracy: 0.9956\n",
      "Epoch 16/40\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0467 - accuracy: 0.9956\n",
      "Epoch 17/40\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0418 - accuracy: 1.0000\n",
      "Epoch 18/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0376 - accuracy: 1.0000\n",
      "Epoch 19/40\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0345 - accuracy: 1.0000\n",
      "Epoch 20/40\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0314 - accuracy: 1.0000\n",
      "Epoch 21/40\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0290 - accuracy: 1.0000\n",
      "Epoch 22/40\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0271 - accuracy: 1.0000\n",
      "Epoch 23/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0251 - accuracy: 1.0000\n",
      "Epoch 24/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0236 - accuracy: 1.0000\n",
      "Epoch 25/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0222 - accuracy: 1.0000\n",
      "Epoch 26/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0209 - accuracy: 1.0000\n",
      "Epoch 27/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0197 - accuracy: 1.0000\n",
      "Epoch 28/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0186 - accuracy: 1.0000\n",
      "Epoch 29/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0177 - accuracy: 1.0000\n",
      "Epoch 30/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0168 - accuracy: 1.0000\n",
      "Epoch 31/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0160 - accuracy: 1.0000\n",
      "Epoch 32/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0153 - accuracy: 1.0000\n",
      "Epoch 33/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0146 - accuracy: 1.0000\n",
      "Epoch 34/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0139 - accuracy: 1.0000\n",
      "Epoch 35/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0134 - accuracy: 1.0000\n",
      "Epoch 36/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0128 - accuracy: 1.0000\n",
      "Epoch 37/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0124 - accuracy: 1.0000\n",
      "Epoch 38/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0119 - accuracy: 1.0000\n",
      "Epoch 39/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0114 - accuracy: 1.0000\n",
      "Epoch 40/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0110 - accuracy: 1.0000\n",
      "Epoch 1/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 2.3574 - accuracy: 0.1681\n",
      "Epoch 2/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 1.8061 - accuracy: 0.2434\n",
      "Epoch 3/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 1.3844 - accuracy: 0.3673\n",
      "Epoch 4/40\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 1.0328 - accuracy: 0.5265\n",
      "Epoch 5/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.7741 - accuracy: 0.6726\n",
      "Epoch 6/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.5842 - accuracy: 0.7788\n",
      "Epoch 7/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.4432 - accuracy: 0.8673\n",
      "Epoch 8/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.3489 - accuracy: 0.9071\n",
      "Epoch 9/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.2784 - accuracy: 0.9292\n",
      "Epoch 10/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.2226 - accuracy: 0.9469\n",
      "Epoch 11/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.1807 - accuracy: 0.9646\n",
      "Epoch 12/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.1495 - accuracy: 0.9823\n",
      "Epoch 13/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.1252 - accuracy: 0.9823\n",
      "Epoch 14/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.1089 - accuracy: 0.9912\n",
      "Epoch 15/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0947 - accuracy: 0.9956\n",
      "Epoch 16/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0841 - accuracy: 0.9956\n",
      "Epoch 17/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0760 - accuracy: 1.0000\n",
      "Epoch 18/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0695 - accuracy: 1.0000\n",
      "Epoch 19/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0636 - accuracy: 1.0000\n",
      "Epoch 20/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0588 - accuracy: 1.0000\n",
      "Epoch 21/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0545 - accuracy: 1.0000\n",
      "Epoch 22/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0507 - accuracy: 1.0000\n",
      "Epoch 23/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0474 - accuracy: 1.0000\n",
      "Epoch 24/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0444 - accuracy: 1.0000\n",
      "Epoch 25/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0417 - accuracy: 1.0000\n",
      "Epoch 26/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0394 - accuracy: 1.0000\n",
      "Epoch 27/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0373 - accuracy: 1.0000\n",
      "Epoch 28/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0354 - accuracy: 1.0000\n",
      "Epoch 29/40\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0336 - accuracy: 1.0000\n",
      "Epoch 30/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0321 - accuracy: 1.0000\n",
      "Epoch 31/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0307 - accuracy: 1.0000\n",
      "Epoch 32/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0293 - accuracy: 1.0000\n",
      "Epoch 33/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0281 - accuracy: 1.0000\n",
      "Epoch 34/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0270 - accuracy: 1.0000\n",
      "Epoch 35/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0259 - accuracy: 1.0000\n",
      "Epoch 36/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0249 - accuracy: 1.0000\n",
      "Epoch 37/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0239 - accuracy: 1.0000\n",
      "Epoch 38/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0230 - accuracy: 1.0000\n",
      "Epoch 39/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0222 - accuracy: 1.0000\n",
      "Epoch 40/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0213 - accuracy: 1.0000\n",
      "Epoch 1/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 1.5099 - accuracy: 0.4735\n",
      "Epoch 2/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 1.1388 - accuracy: 0.5265\n",
      "Epoch 3/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.8425 - accuracy: 0.6283\n",
      "Epoch 4/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.6272 - accuracy: 0.7257\n",
      "Epoch 5/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.4677 - accuracy: 0.8274\n",
      "Epoch 6/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.3542 - accuracy: 0.8850\n",
      "Epoch 7/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.2821 - accuracy: 0.9381\n",
      "Epoch 8/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.2299 - accuracy: 0.9513\n",
      "Epoch 9/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.1931 - accuracy: 0.9646\n",
      "Epoch 10/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.1632 - accuracy: 0.9646\n",
      "Epoch 11/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.1395 - accuracy: 0.9646\n",
      "Epoch 12/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.1191 - accuracy: 0.9823\n",
      "Epoch 13/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.1033 - accuracy: 0.9823\n",
      "Epoch 14/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0891 - accuracy: 0.9823\n",
      "Epoch 15/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0788 - accuracy: 0.9867\n",
      "Epoch 16/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0698 - accuracy: 0.9912\n",
      "Epoch 17/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0628 - accuracy: 0.9912\n",
      "Epoch 18/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0571 - accuracy: 0.9956\n",
      "Epoch 19/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0521 - accuracy: 1.0000\n",
      "Epoch 20/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0482 - accuracy: 1.0000\n",
      "Epoch 21/40\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0501 - accuracy: 1.00 - 0s 7ms/step - loss: 0.0447 - accuracy: 1.0000\n",
      "Epoch 22/40\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.0413 - accuracy: 1.0000\n",
      "Epoch 23/40\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0386 - accuracy: 1.0000\n",
      "Epoch 24/40\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0361 - accuracy: 1.0000\n",
      "Epoch 25/40\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.0339 - accuracy: 1.0000\n",
      "Epoch 26/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0320 - accuracy: 1.0000\n",
      "Epoch 27/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0302 - accuracy: 1.0000\n",
      "Epoch 28/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0286 - accuracy: 1.0000\n",
      "Epoch 29/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0271 - accuracy: 1.0000\n",
      "Epoch 30/40\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0258 - accuracy: 1.0000\n",
      "Epoch 31/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0246 - accuracy: 1.0000\n",
      "Epoch 32/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0235 - accuracy: 1.0000\n",
      "Epoch 33/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0224 - accuracy: 1.0000\n",
      "Epoch 34/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0215 - accuracy: 1.0000\n",
      "Epoch 35/40\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0192 - accuracy: 1.00 - 0s 5ms/step - loss: 0.0206 - accuracy: 1.0000\n",
      "Epoch 36/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0198 - accuracy: 1.0000\n",
      "Epoch 37/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0190 - accuracy: 1.0000\n",
      "Epoch 38/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0183 - accuracy: 1.0000\n",
      "Epoch 39/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0176 - accuracy: 1.0000\n",
      "Epoch 40/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0169 - accuracy: 1.0000\n",
      "Epoch 1/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 1.2308 - accuracy: 0.5664\n",
      "Epoch 2/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.9265 - accuracy: 0.6637\n",
      "Epoch 3/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.7036 - accuracy: 0.7212\n",
      "Epoch 4/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.5261 - accuracy: 0.8053\n",
      "Epoch 5/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.4005 - accuracy: 0.8673\n",
      "Epoch 6/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.3071 - accuracy: 0.8938\n",
      "Epoch 7/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.2426 - accuracy: 0.9336\n",
      "Epoch 8/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.1968 - accuracy: 0.9513\n",
      "Epoch 9/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.1637 - accuracy: 0.9690\n",
      "Epoch 10/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.1378 - accuracy: 0.9779\n",
      "Epoch 11/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.1185 - accuracy: 0.9779\n",
      "Epoch 12/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.1020 - accuracy: 0.9779\n",
      "Epoch 13/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0887 - accuracy: 0.9779\n",
      "Epoch 14/40\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0762 - accuracy: 0.9823\n",
      "Epoch 15/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0670 - accuracy: 0.9823\n",
      "Epoch 16/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0591 - accuracy: 0.9867\n",
      "Epoch 17/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0526 - accuracy: 0.9956\n",
      "Epoch 18/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0479 - accuracy: 0.9956\n",
      "Epoch 19/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0437 - accuracy: 0.9956\n",
      "Epoch 20/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0399 - accuracy: 0.9956\n",
      "Epoch 21/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0370 - accuracy: 0.9956\n",
      "Epoch 22/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0345 - accuracy: 0.9956\n",
      "Epoch 23/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0323 - accuracy: 0.9956\n",
      "Epoch 24/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0303 - accuracy: 0.9956\n",
      "Epoch 25/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0286 - accuracy: 1.0000\n",
      "Epoch 26/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0270 - accuracy: 1.0000\n",
      "Epoch 27/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0255 - accuracy: 1.0000\n",
      "Epoch 28/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0242 - accuracy: 1.0000\n",
      "Epoch 29/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0231 - accuracy: 1.0000\n",
      "Epoch 30/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0219 - accuracy: 1.0000\n",
      "Epoch 31/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0209 - accuracy: 1.0000\n",
      "Epoch 32/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0200 - accuracy: 1.0000\n",
      "Epoch 33/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0191 - accuracy: 1.0000\n",
      "Epoch 34/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0182 - accuracy: 1.0000\n",
      "Epoch 35/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0173 - accuracy: 1.0000\n",
      "Epoch 36/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0165 - accuracy: 1.0000\n",
      "Epoch 37/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0158 - accuracy: 1.0000\n",
      "Epoch 38/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0151 - accuracy: 1.0000\n",
      "Epoch 39/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0145 - accuracy: 1.0000\n",
      "Epoch 40/40\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0139 - accuracy: 1.0000\n",
      "Epoch 1/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 1.8413 - accuracy: 0.4204\n",
      "Epoch 2/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 1.4246 - accuracy: 0.4558\n",
      "Epoch 3/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 1.1080 - accuracy: 0.5531\n",
      "Epoch 4/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.8602 - accuracy: 0.6195\n",
      "Epoch 5/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.6681 - accuracy: 0.7080\n",
      "Epoch 6/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.5276 - accuracy: 0.7566\n",
      "Epoch 7/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.4180 - accuracy: 0.8186\n",
      "Epoch 8/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.3383 - accuracy: 0.8805\n",
      "Epoch 9/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.2714 - accuracy: 0.9027\n",
      "Epoch 10/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.2216 - accuracy: 0.9381\n",
      "Epoch 11/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.1863 - accuracy: 0.9513\n",
      "Epoch 12/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.1541 - accuracy: 0.9735\n",
      "Epoch 13/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 5ms/step - loss: 0.1322 - accuracy: 0.9779\n",
      "Epoch 14/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.1144 - accuracy: 0.9823\n",
      "Epoch 15/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.1014 - accuracy: 0.9823\n",
      "Epoch 16/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0895 - accuracy: 0.9867\n",
      "Epoch 17/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0800 - accuracy: 0.9912\n",
      "Epoch 18/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0723 - accuracy: 0.9912\n",
      "Epoch 19/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0652 - accuracy: 0.9956\n",
      "Epoch 20/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0597 - accuracy: 1.0000\n",
      "Epoch 21/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0552 - accuracy: 1.0000\n",
      "Epoch 22/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0510 - accuracy: 1.0000\n",
      "Epoch 23/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0475 - accuracy: 1.0000\n",
      "Epoch 24/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0442 - accuracy: 1.0000\n",
      "Epoch 25/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0416 - accuracy: 1.0000\n",
      "Epoch 26/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0389 - accuracy: 1.0000\n",
      "Epoch 27/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0367 - accuracy: 1.0000\n",
      "Epoch 28/40\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0346 - accuracy: 1.0000\n",
      "Epoch 29/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0329 - accuracy: 1.0000\n",
      "Epoch 30/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0312 - accuracy: 1.0000\n",
      "Epoch 31/40\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0297 - accuracy: 1.0000\n",
      "Epoch 32/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0284 - accuracy: 1.0000\n",
      "Epoch 33/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0271 - accuracy: 1.0000\n",
      "Epoch 34/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0259 - accuracy: 1.0000\n",
      "Epoch 35/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0248 - accuracy: 1.0000\n",
      "Epoch 36/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0238 - accuracy: 1.0000\n",
      "Epoch 37/40\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0229 - accuracy: 1.0000\n",
      "Epoch 38/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0221 - accuracy: 1.0000\n",
      "Epoch 39/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0212 - accuracy: 1.0000\n",
      "Epoch 40/40\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0204 - accuracy: 1.0000\n",
      "Epoch 1/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 1.4796 - accuracy: 0.4336\n",
      "Epoch 2/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 1.1329 - accuracy: 0.5177\n",
      "Epoch 3/40\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.8701 - accuracy: 0.6062\n",
      "Epoch 4/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.6568 - accuracy: 0.7301\n",
      "Epoch 5/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.5027 - accuracy: 0.8142\n",
      "Epoch 6/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.3901 - accuracy: 0.8894\n",
      "Epoch 7/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.3108 - accuracy: 0.9115\n",
      "Epoch 8/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.2554 - accuracy: 0.9336\n",
      "Epoch 9/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.2149 - accuracy: 0.9469\n",
      "Epoch 10/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.1814 - accuracy: 0.9513\n",
      "Epoch 11/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.1551 - accuracy: 0.9602\n",
      "Epoch 12/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.1339 - accuracy: 0.9602\n",
      "Epoch 13/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.1163 - accuracy: 0.9779\n",
      "Epoch 14/40\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1184 - accuracy: 0.98 - 0s 6ms/step - loss: 0.1030 - accuracy: 0.9823\n",
      "Epoch 15/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0919 - accuracy: 0.9823\n",
      "Epoch 16/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0828 - accuracy: 0.9823\n",
      "Epoch 17/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0751 - accuracy: 0.9823\n",
      "Epoch 18/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0689 - accuracy: 0.9823\n",
      "Epoch 19/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0635 - accuracy: 0.9867\n",
      "Epoch 20/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0586 - accuracy: 0.9956\n",
      "Epoch 21/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0535 - accuracy: 0.9956\n",
      "Epoch 22/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0499 - accuracy: 0.9956\n",
      "Epoch 23/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0460 - accuracy: 0.9956\n",
      "Epoch 24/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0425 - accuracy: 0.9956\n",
      "Epoch 25/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0398 - accuracy: 1.0000\n",
      "Epoch 26/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0373 - accuracy: 1.0000\n",
      "Epoch 27/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0351 - accuracy: 1.0000\n",
      "Epoch 28/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0333 - accuracy: 1.0000\n",
      "Epoch 29/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0314 - accuracy: 1.0000\n",
      "Epoch 30/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0299 - accuracy: 1.0000\n",
      "Epoch 31/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0284 - accuracy: 1.0000\n",
      "Epoch 32/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0269 - accuracy: 1.0000\n",
      "Epoch 33/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0256 - accuracy: 1.0000\n",
      "Epoch 34/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0244 - accuracy: 1.0000\n",
      "Epoch 35/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0233 - accuracy: 1.0000\n",
      "Epoch 36/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0223 - accuracy: 1.0000\n",
      "Epoch 37/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0213 - accuracy: 1.0000\n",
      "Epoch 38/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0204 - accuracy: 1.0000\n",
      "Epoch 39/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0197 - accuracy: 1.0000\n",
      "Epoch 40/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0189 - accuracy: 1.0000\n",
      "Epoch 1/40\n",
      "3/3 [==============================] - 1s 7ms/step - loss: 1.7155 - accuracy: 0.5664\n",
      "Epoch 2/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 1.3193 - accuracy: 0.6460\n",
      "Epoch 3/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 1.0105 - accuracy: 0.7035\n",
      "Epoch 4/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.7549 - accuracy: 0.7522\n",
      "Epoch 5/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.5574 - accuracy: 0.8186\n",
      "Epoch 6/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.4017 - accuracy: 0.8805\n",
      "Epoch 7/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.3054 - accuracy: 0.9071\n",
      "Epoch 8/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.2393 - accuracy: 0.9381\n",
      "Epoch 9/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.1925 - accuracy: 0.9469\n",
      "Epoch 10/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.1606 - accuracy: 0.9690\n",
      "Epoch 11/40\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.1397 - accuracy: 0.9779\n",
      "Epoch 12/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.1196 - accuracy: 0.9823\n",
      "Epoch 13/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.1039 - accuracy: 0.9867\n",
      "Epoch 14/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0899 - accuracy: 0.9912\n",
      "Epoch 15/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0797 - accuracy: 0.9912\n",
      "Epoch 16/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0701 - accuracy: 0.9912\n",
      "Epoch 17/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0630 - accuracy: 0.9956\n",
      "Epoch 18/40\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0570 - accuracy: 0.9956\n",
      "Epoch 19/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0522 - accuracy: 0.9956\n",
      "Epoch 20/40\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0482 - accuracy: 0.9956\n",
      "Epoch 21/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0451 - accuracy: 0.9956\n",
      "Epoch 22/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0420 - accuracy: 0.9956\n",
      "Epoch 23/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0392 - accuracy: 0.9956\n",
      "Epoch 24/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0368 - accuracy: 0.9956\n",
      "Epoch 25/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0348 - accuracy: 0.9956\n",
      "Epoch 26/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0327 - accuracy: 0.9956\n",
      "Epoch 27/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0308 - accuracy: 0.9956\n",
      "Epoch 28/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0290 - accuracy: 0.9956\n",
      "Epoch 29/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0276 - accuracy: 0.9956\n",
      "Epoch 30/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0258 - accuracy: 0.9956\n",
      "Epoch 31/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0244 - accuracy: 1.0000\n",
      "Epoch 32/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0232 - accuracy: 1.0000\n",
      "Epoch 33/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0220 - accuracy: 1.0000\n",
      "Epoch 34/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0211 - accuracy: 1.0000\n",
      "Epoch 35/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0202 - accuracy: 1.0000\n",
      "Epoch 36/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0193 - accuracy: 1.0000\n",
      "Epoch 37/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0185 - accuracy: 1.0000\n",
      "Epoch 38/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0177 - accuracy: 1.0000\n",
      "Epoch 39/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0170 - accuracy: 1.0000\n",
      "Epoch 40/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0163 - accuracy: 1.0000\n",
      "Epoch 1/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 1.3663 - accuracy: 0.4779\n",
      "Epoch 2/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 1.0163 - accuracy: 0.5841\n",
      "Epoch 3/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.7509 - accuracy: 0.6593\n",
      "Epoch 4/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.5494 - accuracy: 0.7965\n",
      "Epoch 5/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.4026 - accuracy: 0.8584\n",
      "Epoch 6/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.3032 - accuracy: 0.9071\n",
      "Epoch 7/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.2368 - accuracy: 0.9204\n",
      "Epoch 8/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.1880 - accuracy: 0.9513\n",
      "Epoch 9/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.1536 - accuracy: 0.9646\n",
      "Epoch 10/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.1249 - accuracy: 0.9690\n",
      "Epoch 11/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.1037 - accuracy: 0.9867\n",
      "Epoch 12/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0872 - accuracy: 0.9912\n",
      "Epoch 13/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0745 - accuracy: 0.9956\n",
      "Epoch 14/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0643 - accuracy: 0.9956\n",
      "Epoch 15/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0566 - accuracy: 0.9956\n",
      "Epoch 16/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0510 - accuracy: 0.9956\n",
      "Epoch 17/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0459 - accuracy: 1.0000\n",
      "Epoch 18/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0419 - accuracy: 1.0000\n",
      "Epoch 19/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0385 - accuracy: 1.0000\n",
      "Epoch 20/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0358 - accuracy: 1.0000\n",
      "Epoch 21/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0332 - accuracy: 1.0000\n",
      "Epoch 22/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0311 - accuracy: 1.0000\n",
      "Epoch 23/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0291 - accuracy: 1.0000\n",
      "Epoch 24/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0275 - accuracy: 1.0000\n",
      "Epoch 25/40\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0257 - accuracy: 1.0000\n",
      "Epoch 26/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0244 - accuracy: 1.0000\n",
      "Epoch 27/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0231 - accuracy: 1.0000\n",
      "Epoch 28/40\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0219 - accuracy: 1.0000\n",
      "Epoch 29/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0207 - accuracy: 1.0000\n",
      "Epoch 30/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0198 - accuracy: 1.0000\n",
      "Epoch 31/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0188 - accuracy: 1.0000\n",
      "Epoch 32/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0180 - accuracy: 1.0000\n",
      "Epoch 33/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0173 - accuracy: 1.0000\n",
      "Epoch 34/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0166 - accuracy: 1.0000\n",
      "Epoch 35/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0159 - accuracy: 1.0000\n",
      "Epoch 36/40\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0153 - accuracy: 1.0000\n",
      "Epoch 37/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0147 - accuracy: 1.0000\n",
      "Epoch 38/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0142 - accuracy: 1.0000\n",
      "Epoch 39/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0137 - accuracy: 1.0000\n",
      "Epoch 40/40\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0132 - accuracy: 1.0000\n",
      "Epoch 1/50\n",
      "3/3 [==============================] - 1s 6ms/step - loss: 2.6434 - accuracy: 0.1511\n",
      "Epoch 2/50\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 2.1357 - accuracy: 0.2044\n",
      "Epoch 3/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 1.7155 - accuracy: 0.3333\n",
      "Epoch 4/50\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 1.3622 - accuracy: 0.4800\n",
      "Epoch 5/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 1.0663 - accuracy: 0.5778\n",
      "Epoch 6/50\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.8288 - accuracy: 0.6978\n",
      "Epoch 7/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.6366 - accuracy: 0.7556\n",
      "Epoch 8/50\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.4954 - accuracy: 0.8089\n",
      "Epoch 9/50\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3839 - accuracy: 0.8667\n",
      "Epoch 10/50\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.2985 - accuracy: 0.9378\n",
      "Epoch 11/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.2397 - accuracy: 0.9644\n",
      "Epoch 12/50\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.1982 - accuracy: 0.9778\n",
      "Epoch 13/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.1657 - accuracy: 0.9822\n",
      "Epoch 14/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.1428 - accuracy: 0.9867\n",
      "Epoch 15/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.1257 - accuracy: 0.9867\n",
      "Epoch 16/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.1112 - accuracy: 0.9867\n",
      "Epoch 17/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.1000 - accuracy: 0.9867\n",
      "Epoch 18/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0912 - accuracy: 0.9867\n",
      "Epoch 19/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0836 - accuracy: 0.9867\n",
      "Epoch 20/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0766 - accuracy: 0.9956\n",
      "Epoch 21/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0710 - accuracy: 1.0000\n",
      "Epoch 22/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0657 - accuracy: 1.0000\n",
      "Epoch 23/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0611 - accuracy: 1.0000\n",
      "Epoch 24/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0571 - accuracy: 1.0000\n",
      "Epoch 25/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0534 - accuracy: 1.0000\n",
      "Epoch 26/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0503 - accuracy: 1.0000\n",
      "Epoch 27/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0474 - accuracy: 1.0000\n",
      "Epoch 28/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0447 - accuracy: 1.0000\n",
      "Epoch 29/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0424 - accuracy: 1.0000\n",
      "Epoch 30/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0403 - accuracy: 1.0000\n",
      "Epoch 31/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0384 - accuracy: 1.0000\n",
      "Epoch 32/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0365 - accuracy: 1.0000\n",
      "Epoch 33/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0349 - accuracy: 1.0000\n",
      "Epoch 34/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0333 - accuracy: 1.0000\n",
      "Epoch 35/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0319 - accuracy: 1.0000\n",
      "Epoch 36/50\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0305 - accuracy: 1.0000\n",
      "Epoch 37/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0294 - accuracy: 1.0000\n",
      "Epoch 38/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0282 - accuracy: 1.0000\n",
      "Epoch 39/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0271 - accuracy: 1.0000\n",
      "Epoch 40/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0261 - accuracy: 1.0000\n",
      "Epoch 41/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0252 - accuracy: 1.0000\n",
      "Epoch 42/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0243 - accuracy: 1.0000\n",
      "Epoch 43/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0234 - accuracy: 1.0000\n",
      "Epoch 44/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0227 - accuracy: 1.0000\n",
      "Epoch 45/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0220 - accuracy: 1.0000\n",
      "Epoch 46/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0212 - accuracy: 1.0000\n",
      "Epoch 47/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0205 - accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0199 - accuracy: 1.0000\n",
      "Epoch 49/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0193 - accuracy: 1.0000\n",
      "Epoch 50/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0187 - accuracy: 1.0000\n",
      "Epoch 1/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 2.8968 - accuracy: 0.2655\n",
      "Epoch 2/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 2.3598 - accuracy: 0.3407\n",
      "Epoch 3/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 1.9224 - accuracy: 0.4071\n",
      "Epoch 4/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 1.5445 - accuracy: 0.4646\n",
      "Epoch 5/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 1.2394 - accuracy: 0.5708\n",
      "Epoch 6/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.9710 - accuracy: 0.6416\n",
      "Epoch 7/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.7672 - accuracy: 0.7080\n",
      "Epoch 8/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.5989 - accuracy: 0.7832\n",
      "Epoch 9/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.4736 - accuracy: 0.8319\n",
      "Epoch 10/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.3788 - accuracy: 0.8938\n",
      "Epoch 11/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.3046 - accuracy: 0.9071\n",
      "Epoch 12/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.2508 - accuracy: 0.9336\n",
      "Epoch 13/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.2089 - accuracy: 0.9602\n",
      "Epoch 14/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.1777 - accuracy: 0.9646\n",
      "Epoch 15/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.1517 - accuracy: 0.9690\n",
      "Epoch 16/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.1327 - accuracy: 0.9735\n",
      "Epoch 17/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.1158 - accuracy: 0.9823\n",
      "Epoch 18/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.1019 - accuracy: 0.9912\n",
      "Epoch 19/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0907 - accuracy: 0.9956\n",
      "Epoch 20/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0809 - accuracy: 0.9956\n",
      "Epoch 21/50\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0733 - accuracy: 1.0000\n",
      "Epoch 22/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0666 - accuracy: 1.0000\n",
      "Epoch 23/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0610 - accuracy: 1.0000\n",
      "Epoch 24/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0565 - accuracy: 1.0000\n",
      "Epoch 25/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0524 - accuracy: 1.0000\n",
      "Epoch 26/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0491 - accuracy: 1.0000\n",
      "Epoch 27/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0460 - accuracy: 1.0000\n",
      "Epoch 28/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0433 - accuracy: 1.0000\n",
      "Epoch 29/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0409 - accuracy: 1.0000\n",
      "Epoch 30/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0386 - accuracy: 1.0000\n",
      "Epoch 31/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0366 - accuracy: 1.0000\n",
      "Epoch 32/50\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0348 - accuracy: 1.0000\n",
      "Epoch 33/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0332 - accuracy: 1.0000\n",
      "Epoch 34/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0317 - accuracy: 1.0000\n",
      "Epoch 35/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0303 - accuracy: 1.0000\n",
      "Epoch 36/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0290 - accuracy: 1.0000\n",
      "Epoch 37/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0277 - accuracy: 1.0000\n",
      "Epoch 38/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0266 - accuracy: 1.0000\n",
      "Epoch 39/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0255 - accuracy: 1.0000\n",
      "Epoch 40/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0245 - accuracy: 1.0000\n",
      "Epoch 41/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0235 - accuracy: 1.0000\n",
      "Epoch 42/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0227 - accuracy: 1.0000\n",
      "Epoch 43/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0219 - accuracy: 1.0000\n",
      "Epoch 44/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0211 - accuracy: 1.0000\n",
      "Epoch 45/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0204 - accuracy: 1.0000\n",
      "Epoch 46/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0198 - accuracy: 1.0000\n",
      "Epoch 47/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0191 - accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0185 - accuracy: 1.0000\n",
      "Epoch 49/50\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0179 - accuracy: 1.0000\n",
      "Epoch 50/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0174 - accuracy: 1.0000\n",
      "Epoch 1/50\n",
      "3/3 [==============================] - 1s 6ms/step - loss: 2.0866 - accuracy: 0.3142\n",
      "Epoch 2/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 1.6314 - accuracy: 0.3628\n",
      "Epoch 3/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 1.2509 - accuracy: 0.4513\n",
      "Epoch 4/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.9464 - accuracy: 0.5885\n",
      "Epoch 5/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.6962 - accuracy: 0.7257\n",
      "Epoch 6/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.5171 - accuracy: 0.7965\n",
      "Epoch 7/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.3914 - accuracy: 0.8673\n",
      "Epoch 8/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.3002 - accuracy: 0.9071\n",
      "Epoch 9/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.2353 - accuracy: 0.9469\n",
      "Epoch 10/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.1901 - accuracy: 0.9558\n",
      "Epoch 11/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.1585 - accuracy: 0.9779\n",
      "Epoch 12/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.1345 - accuracy: 0.9867\n",
      "Epoch 13/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.1156 - accuracy: 0.9912\n",
      "Epoch 14/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.1012 - accuracy: 1.0000\n",
      "Epoch 15/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0898 - accuracy: 1.0000\n",
      "Epoch 16/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0806 - accuracy: 1.0000\n",
      "Epoch 17/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0727 - accuracy: 1.0000\n",
      "Epoch 18/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0661 - accuracy: 1.0000\n",
      "Epoch 19/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0606 - accuracy: 1.0000\n",
      "Epoch 20/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0558 - accuracy: 1.0000\n",
      "Epoch 21/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0516 - accuracy: 1.0000\n",
      "Epoch 22/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0478 - accuracy: 1.0000\n",
      "Epoch 23/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0448 - accuracy: 1.0000\n",
      "Epoch 24/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0417 - accuracy: 1.0000\n",
      "Epoch 25/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0391 - accuracy: 1.0000\n",
      "Epoch 26/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0368 - accuracy: 1.0000\n",
      "Epoch 27/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0348 - accuracy: 1.0000\n",
      "Epoch 28/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0330 - accuracy: 1.0000\n",
      "Epoch 29/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0314 - accuracy: 1.0000\n",
      "Epoch 30/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0298 - accuracy: 1.0000\n",
      "Epoch 31/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0284 - accuracy: 1.0000\n",
      "Epoch 32/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0271 - accuracy: 1.0000\n",
      "Epoch 33/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0259 - accuracy: 1.0000\n",
      "Epoch 34/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0247 - accuracy: 1.0000\n",
      "Epoch 35/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0237 - accuracy: 1.0000\n",
      "Epoch 36/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0227 - accuracy: 1.0000\n",
      "Epoch 37/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0218 - accuracy: 1.0000\n",
      "Epoch 38/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0210 - accuracy: 1.0000\n",
      "Epoch 39/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0202 - accuracy: 1.0000\n",
      "Epoch 40/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0195 - accuracy: 1.0000\n",
      "Epoch 41/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0188 - accuracy: 1.0000\n",
      "Epoch 42/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0182 - accuracy: 1.0000\n",
      "Epoch 43/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0175 - accuracy: 1.0000\n",
      "Epoch 44/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0169 - accuracy: 1.0000\n",
      "Epoch 45/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0164 - accuracy: 1.0000\n",
      "Epoch 46/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0158 - accuracy: 1.0000\n",
      "Epoch 47/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0153 - accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0148 - accuracy: 1.0000\n",
      "Epoch 49/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0144 - accuracy: 1.0000\n",
      "Epoch 50/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0139 - accuracy: 1.0000\n",
      "Epoch 1/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 2.1992 - accuracy: 0.3805\n",
      "Epoch 2/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 1.7644 - accuracy: 0.4469\n",
      "Epoch 3/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 1.4157 - accuracy: 0.5221\n",
      "Epoch 4/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 1.1081 - accuracy: 0.5973\n",
      "Epoch 5/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.8489 - accuracy: 0.6593\n",
      "Epoch 6/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.6418 - accuracy: 0.7522\n",
      "Epoch 7/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.4797 - accuracy: 0.8496\n",
      "Epoch 8/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.3649 - accuracy: 0.8894\n",
      "Epoch 9/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.2832 - accuracy: 0.9248\n",
      "Epoch 10/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.2304 - accuracy: 0.9513\n",
      "Epoch 11/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.1949 - accuracy: 0.9558\n",
      "Epoch 12/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.1679 - accuracy: 0.9646\n",
      "Epoch 13/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.1456 - accuracy: 0.9735\n",
      "Epoch 14/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.1279 - accuracy: 0.9735\n",
      "Epoch 15/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.1133 - accuracy: 0.9779\n",
      "Epoch 16/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.1002 - accuracy: 0.9779\n",
      "Epoch 17/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0895 - accuracy: 0.9867\n",
      "Epoch 18/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0803 - accuracy: 0.9912\n",
      "Epoch 19/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0728 - accuracy: 0.9912\n",
      "Epoch 20/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0664 - accuracy: 0.9956\n",
      "Epoch 21/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0610 - accuracy: 0.9956\n",
      "Epoch 22/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0564 - accuracy: 0.9956\n",
      "Epoch 23/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0521 - accuracy: 1.0000\n",
      "Epoch 24/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0486 - accuracy: 1.0000\n",
      "Epoch 25/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0456 - accuracy: 1.0000\n",
      "Epoch 26/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0427 - accuracy: 1.0000\n",
      "Epoch 27/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0402 - accuracy: 1.0000\n",
      "Epoch 28/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0379 - accuracy: 1.0000\n",
      "Epoch 29/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0359 - accuracy: 1.0000\n",
      "Epoch 30/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0340 - accuracy: 1.0000\n",
      "Epoch 31/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0323 - accuracy: 1.0000\n",
      "Epoch 32/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0307 - accuracy: 1.0000\n",
      "Epoch 33/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0293 - accuracy: 1.0000\n",
      "Epoch 34/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0279 - accuracy: 1.0000\n",
      "Epoch 35/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0266 - accuracy: 1.0000\n",
      "Epoch 36/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0255 - accuracy: 1.0000\n",
      "Epoch 37/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0245 - accuracy: 1.0000\n",
      "Epoch 38/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0234 - accuracy: 1.0000\n",
      "Epoch 39/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0225 - accuracy: 1.0000\n",
      "Epoch 40/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0216 - accuracy: 1.0000\n",
      "Epoch 41/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0207 - accuracy: 1.0000\n",
      "Epoch 42/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0199 - accuracy: 1.0000\n",
      "Epoch 43/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0192 - accuracy: 1.0000\n",
      "Epoch 44/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0185 - accuracy: 1.0000\n",
      "Epoch 45/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0178 - accuracy: 1.0000\n",
      "Epoch 46/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0172 - accuracy: 1.0000\n",
      "Epoch 47/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0166 - accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0161 - accuracy: 1.0000\n",
      "Epoch 49/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0155 - accuracy: 1.0000\n",
      "Epoch 50/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0150 - accuracy: 1.0000\n",
      "Epoch 1/50\n",
      "3/3 [==============================] - 1s 5ms/step - loss: 1.4628 - accuracy: 0.4071\n",
      "Epoch 2/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 1.1034 - accuracy: 0.5133\n",
      "Epoch 3/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.8298 - accuracy: 0.6637\n",
      "Epoch 4/50\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.6288 - accuracy: 0.7522\n",
      "Epoch 5/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.4731 - accuracy: 0.8451\n",
      "Epoch 6/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.3630 - accuracy: 0.9071\n",
      "Epoch 7/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.2807 - accuracy: 0.9381\n",
      "Epoch 8/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.2247 - accuracy: 0.9602\n",
      "Epoch 9/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.1795 - accuracy: 0.9735\n",
      "Epoch 10/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.1483 - accuracy: 0.9823\n",
      "Epoch 11/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.1261 - accuracy: 0.9912\n",
      "Epoch 12/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.1084 - accuracy: 0.9912\n",
      "Epoch 13/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0956 - accuracy: 0.9912\n",
      "Epoch 14/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0845 - accuracy: 0.9912\n",
      "Epoch 15/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0755 - accuracy: 0.9912\n",
      "Epoch 16/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0675 - accuracy: 0.9912\n",
      "Epoch 17/50\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0607 - accuracy: 1.0000\n",
      "Epoch 18/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0552 - accuracy: 1.0000\n",
      "Epoch 19/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0505 - accuracy: 1.0000\n",
      "Epoch 20/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0466 - accuracy: 1.0000\n",
      "Epoch 21/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0428 - accuracy: 1.0000\n",
      "Epoch 22/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0398 - accuracy: 1.0000\n",
      "Epoch 23/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0371 - accuracy: 1.0000\n",
      "Epoch 24/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0348 - accuracy: 1.0000\n",
      "Epoch 25/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0327 - accuracy: 1.0000\n",
      "Epoch 26/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0308 - accuracy: 1.0000\n",
      "Epoch 27/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0291 - accuracy: 1.0000\n",
      "Epoch 28/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0275 - accuracy: 1.0000\n",
      "Epoch 29/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0262 - accuracy: 1.0000\n",
      "Epoch 30/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0249 - accuracy: 1.0000\n",
      "Epoch 31/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0238 - accuracy: 1.0000\n",
      "Epoch 32/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0227 - accuracy: 1.0000\n",
      "Epoch 33/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0217 - accuracy: 1.0000\n",
      "Epoch 34/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0208 - accuracy: 1.0000\n",
      "Epoch 35/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0199 - accuracy: 1.0000\n",
      "Epoch 36/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0191 - accuracy: 1.0000\n",
      "Epoch 37/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0183 - accuracy: 1.0000\n",
      "Epoch 38/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0176 - accuracy: 1.0000\n",
      "Epoch 39/50\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0169 - accuracy: 1.0000\n",
      "Epoch 40/50\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0163 - accuracy: 1.0000\n",
      "Epoch 41/50\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0157 - accuracy: 1.0000\n",
      "Epoch 42/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0151 - accuracy: 1.0000\n",
      "Epoch 43/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0146 - accuracy: 1.0000\n",
      "Epoch 44/50\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0141 - accuracy: 1.0000\n",
      "Epoch 45/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0136 - accuracy: 1.0000\n",
      "Epoch 46/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0132 - accuracy: 1.0000\n",
      "Epoch 47/50\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0127 - accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0123 - accuracy: 1.0000\n",
      "Epoch 49/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0119 - accuracy: 1.0000\n",
      "Epoch 50/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0116 - accuracy: 1.0000\n",
      "Epoch 1/50\n",
      "3/3 [==============================] - 1s 7ms/step - loss: 2.1269 - accuracy: 0.3274\n",
      "Epoch 2/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 1.6636 - accuracy: 0.4204\n",
      "Epoch 3/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 1.3110 - accuracy: 0.5442\n",
      "Epoch 4/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 1.0051 - accuracy: 0.6018\n",
      "Epoch 5/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.7573 - accuracy: 0.6681\n",
      "Epoch 6/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.5671 - accuracy: 0.7522\n",
      "Epoch 7/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.4188 - accuracy: 0.8496\n",
      "Epoch 8/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.3257 - accuracy: 0.9115\n",
      "Epoch 9/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.2534 - accuracy: 0.9513\n",
      "Epoch 10/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.2083 - accuracy: 0.9646\n",
      "Epoch 11/50\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.1744 - accuracy: 0.9690\n",
      "Epoch 12/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.1499 - accuracy: 0.9735\n",
      "Epoch 13/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.1312 - accuracy: 0.9735\n",
      "Epoch 14/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.1149 - accuracy: 0.9735\n",
      "Epoch 15/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.1026 - accuracy: 0.9735\n",
      "Epoch 16/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0915 - accuracy: 0.9867\n",
      "Epoch 17/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0820 - accuracy: 0.9867\n",
      "Epoch 18/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0739 - accuracy: 0.9912\n",
      "Epoch 19/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0668 - accuracy: 0.9956\n",
      "Epoch 20/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0614 - accuracy: 0.9956\n",
      "Epoch 21/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0562 - accuracy: 1.0000\n",
      "Epoch 22/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0522 - accuracy: 1.0000\n",
      "Epoch 23/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0484 - accuracy: 1.0000\n",
      "Epoch 24/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0452 - accuracy: 1.0000\n",
      "Epoch 25/50\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.0384 - accuracy: 1.00 - 0s 6ms/step - loss: 0.0423 - accuracy: 1.0000\n",
      "Epoch 26/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0397 - accuracy: 1.0000\n",
      "Epoch 27/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0373 - accuracy: 1.0000\n",
      "Epoch 28/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0353 - accuracy: 1.0000\n",
      "Epoch 29/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0334 - accuracy: 1.0000\n",
      "Epoch 30/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0317 - accuracy: 1.0000\n",
      "Epoch 31/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0301 - accuracy: 1.0000\n",
      "Epoch 32/50\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0286 - accuracy: 1.0000\n",
      "Epoch 33/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0274 - accuracy: 1.0000\n",
      "Epoch 34/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0262 - accuracy: 1.0000\n",
      "Epoch 35/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0251 - accuracy: 1.0000\n",
      "Epoch 36/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0241 - accuracy: 1.0000\n",
      "Epoch 37/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0231 - accuracy: 1.0000\n",
      "Epoch 38/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0223 - accuracy: 1.0000\n",
      "Epoch 39/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0214 - accuracy: 1.0000\n",
      "Epoch 40/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0207 - accuracy: 1.0000\n",
      "Epoch 41/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0200 - accuracy: 1.0000\n",
      "Epoch 42/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0192 - accuracy: 1.0000\n",
      "Epoch 43/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0186 - accuracy: 1.0000\n",
      "Epoch 44/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0180 - accuracy: 1.0000\n",
      "Epoch 45/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0174 - accuracy: 1.0000\n",
      "Epoch 46/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0168 - accuracy: 1.0000\n",
      "Epoch 47/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0163 - accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0158 - accuracy: 1.0000\n",
      "Epoch 49/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0153 - accuracy: 1.0000\n",
      "Epoch 50/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0148 - accuracy: 1.0000\n",
      "Epoch 1/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 1.1539 - accuracy: 0.4735\n",
      "Epoch 2/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.8382 - accuracy: 0.6504\n",
      "Epoch 3/50\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.6144 - accuracy: 0.7257\n",
      "Epoch 4/50\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.4622 - accuracy: 0.8319\n",
      "Epoch 5/50\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.3581 - accuracy: 0.8982\n",
      "Epoch 6/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.2837 - accuracy: 0.9248\n",
      "Epoch 7/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.2257 - accuracy: 0.9425\n",
      "Epoch 8/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.1846 - accuracy: 0.9646\n",
      "Epoch 9/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.1522 - accuracy: 0.9690\n",
      "Epoch 10/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.1281 - accuracy: 0.9735\n",
      "Epoch 11/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.1098 - accuracy: 0.9735\n",
      "Epoch 12/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0958 - accuracy: 0.9779\n",
      "Epoch 13/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0833 - accuracy: 0.9779\n",
      "Epoch 14/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0745 - accuracy: 0.9867\n",
      "Epoch 15/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0663 - accuracy: 0.9956\n",
      "Epoch 16/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0598 - accuracy: 0.9956\n",
      "Epoch 17/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0541 - accuracy: 0.9956\n",
      "Epoch 18/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0496 - accuracy: 1.0000\n",
      "Epoch 19/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0456 - accuracy: 1.0000\n",
      "Epoch 20/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0419 - accuracy: 1.0000\n",
      "Epoch 21/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0391 - accuracy: 1.0000\n",
      "Epoch 22/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0363 - accuracy: 1.0000\n",
      "Epoch 23/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0339 - accuracy: 1.0000\n",
      "Epoch 24/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0318 - accuracy: 1.0000\n",
      "Epoch 25/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0298 - accuracy: 1.0000\n",
      "Epoch 26/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0280 - accuracy: 1.0000\n",
      "Epoch 27/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0264 - accuracy: 1.0000\n",
      "Epoch 28/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0249 - accuracy: 1.0000\n",
      "Epoch 29/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0237 - accuracy: 1.0000\n",
      "Epoch 30/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0226 - accuracy: 1.0000\n",
      "Epoch 31/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0214 - accuracy: 1.0000\n",
      "Epoch 32/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0205 - accuracy: 1.0000\n",
      "Epoch 33/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0195 - accuracy: 1.0000\n",
      "Epoch 34/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0187 - accuracy: 1.0000\n",
      "Epoch 35/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0178 - accuracy: 1.0000\n",
      "Epoch 36/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0171 - accuracy: 1.0000\n",
      "Epoch 37/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0164 - accuracy: 1.0000\n",
      "Epoch 38/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0157 - accuracy: 1.0000\n",
      "Epoch 39/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0151 - accuracy: 1.0000\n",
      "Epoch 40/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0146 - accuracy: 1.0000\n",
      "Epoch 41/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0140 - accuracy: 1.0000\n",
      "Epoch 42/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0135 - accuracy: 1.0000\n",
      "Epoch 43/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0131 - accuracy: 1.0000\n",
      "Epoch 44/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0126 - accuracy: 1.0000\n",
      "Epoch 45/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0122 - accuracy: 1.0000\n",
      "Epoch 46/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0118 - accuracy: 1.0000\n",
      "Epoch 47/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0114 - accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0110 - accuracy: 1.0000\n",
      "Epoch 49/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0107 - accuracy: 1.0000\n",
      "Epoch 50/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0104 - accuracy: 1.0000\n",
      "Epoch 1/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 1.1296 - accuracy: 0.5044\n",
      "Epoch 2/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.8445 - accuracy: 0.6283\n",
      "Epoch 3/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.6492 - accuracy: 0.7035\n",
      "Epoch 4/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.4984 - accuracy: 0.7743\n",
      "Epoch 5/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.3937 - accuracy: 0.8540\n",
      "Epoch 6/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.3118 - accuracy: 0.8982\n",
      "Epoch 7/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 5ms/step - loss: 0.2507 - accuracy: 0.9292\n",
      "Epoch 8/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.2068 - accuracy: 0.9469\n",
      "Epoch 9/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.1719 - accuracy: 0.9646\n",
      "Epoch 10/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.1457 - accuracy: 0.9690\n",
      "Epoch 11/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.1248 - accuracy: 0.9735\n",
      "Epoch 12/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.1095 - accuracy: 0.9779\n",
      "Epoch 13/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0966 - accuracy: 0.9823\n",
      "Epoch 14/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0855 - accuracy: 0.9867\n",
      "Epoch 15/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0764 - accuracy: 0.9912\n",
      "Epoch 16/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0686 - accuracy: 0.9956\n",
      "Epoch 17/50\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0620 - accuracy: 0.9956\n",
      "Epoch 18/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0567 - accuracy: 0.9956\n",
      "Epoch 19/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0519 - accuracy: 0.9956\n",
      "Epoch 20/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0477 - accuracy: 0.9956\n",
      "Epoch 21/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0441 - accuracy: 1.0000\n",
      "Epoch 22/50\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0409 - accuracy: 1.0000\n",
      "Epoch 23/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0381 - accuracy: 1.0000\n",
      "Epoch 24/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0355 - accuracy: 1.0000\n",
      "Epoch 25/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0334 - accuracy: 1.0000\n",
      "Epoch 26/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0314 - accuracy: 1.0000\n",
      "Epoch 27/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0296 - accuracy: 1.0000\n",
      "Epoch 28/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0280 - accuracy: 1.0000\n",
      "Epoch 29/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0265 - accuracy: 1.0000\n",
      "Epoch 30/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0252 - accuracy: 1.0000\n",
      "Epoch 31/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0240 - accuracy: 1.0000\n",
      "Epoch 32/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0228 - accuracy: 1.0000\n",
      "Epoch 33/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0218 - accuracy: 1.0000\n",
      "Epoch 34/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0208 - accuracy: 1.0000\n",
      "Epoch 35/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0199 - accuracy: 1.0000\n",
      "Epoch 36/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0190 - accuracy: 1.0000\n",
      "Epoch 37/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0182 - accuracy: 1.0000\n",
      "Epoch 38/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0175 - accuracy: 1.0000\n",
      "Epoch 39/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0168 - accuracy: 1.0000\n",
      "Epoch 40/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0161 - accuracy: 1.0000\n",
      "Epoch 41/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0155 - accuracy: 1.0000\n",
      "Epoch 42/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0149 - accuracy: 1.0000\n",
      "Epoch 43/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0144 - accuracy: 1.0000\n",
      "Epoch 44/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0139 - accuracy: 1.0000\n",
      "Epoch 45/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0134 - accuracy: 1.0000\n",
      "Epoch 46/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0130 - accuracy: 1.0000\n",
      "Epoch 47/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0125 - accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0121 - accuracy: 1.0000\n",
      "Epoch 49/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0117 - accuracy: 1.0000\n",
      "Epoch 50/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0114 - accuracy: 1.0000\n",
      "Epoch 1/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 1.5604 - accuracy: 0.3451\n",
      "Epoch 2/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 1.1405 - accuracy: 0.4823\n",
      "Epoch 3/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.8288 - accuracy: 0.6195\n",
      "Epoch 4/50\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.5998 - accuracy: 0.7434\n",
      "Epoch 5/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.4407 - accuracy: 0.8274\n",
      "Epoch 6/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.3386 - accuracy: 0.9159\n",
      "Epoch 7/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.2666 - accuracy: 0.9469\n",
      "Epoch 8/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.2126 - accuracy: 0.9558\n",
      "Epoch 9/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.1773 - accuracy: 0.9735\n",
      "Epoch 10/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.1495 - accuracy: 0.9823\n",
      "Epoch 11/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.1276 - accuracy: 0.9823\n",
      "Epoch 12/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.1118 - accuracy: 0.9867\n",
      "Epoch 13/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0988 - accuracy: 0.9867\n",
      "Epoch 14/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0878 - accuracy: 0.9867\n",
      "Epoch 15/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0788 - accuracy: 0.9867\n",
      "Epoch 16/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0711 - accuracy: 0.9912\n",
      "Epoch 17/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0650 - accuracy: 0.9912\n",
      "Epoch 18/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0590 - accuracy: 0.9956\n",
      "Epoch 19/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0546 - accuracy: 0.9956\n",
      "Epoch 20/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0503 - accuracy: 0.9956\n",
      "Epoch 21/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0470 - accuracy: 1.0000\n",
      "Epoch 22/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0438 - accuracy: 1.0000\n",
      "Epoch 23/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0411 - accuracy: 1.0000\n",
      "Epoch 24/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0385 - accuracy: 1.0000\n",
      "Epoch 25/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0363 - accuracy: 1.0000\n",
      "Epoch 26/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0342 - accuracy: 1.0000\n",
      "Epoch 27/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0323 - accuracy: 1.0000\n",
      "Epoch 28/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0306 - accuracy: 1.0000\n",
      "Epoch 29/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0291 - accuracy: 1.0000\n",
      "Epoch 30/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0277 - accuracy: 1.0000\n",
      "Epoch 31/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0265 - accuracy: 1.0000\n",
      "Epoch 32/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0253 - accuracy: 1.0000\n",
      "Epoch 33/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0241 - accuracy: 1.0000\n",
      "Epoch 34/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0231 - accuracy: 1.0000\n",
      "Epoch 35/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0222 - accuracy: 1.0000\n",
      "Epoch 36/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0213 - accuracy: 1.0000\n",
      "Epoch 37/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0205 - accuracy: 1.0000\n",
      "Epoch 38/50\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0197 - accuracy: 1.0000\n",
      "Epoch 39/50\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0189 - accuracy: 1.0000\n",
      "Epoch 40/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0183 - accuracy: 1.0000\n",
      "Epoch 41/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0176 - accuracy: 1.0000\n",
      "Epoch 42/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0170 - accuracy: 1.0000\n",
      "Epoch 43/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0164 - accuracy: 1.0000\n",
      "Epoch 44/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0159 - accuracy: 1.0000\n",
      "Epoch 45/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0153 - accuracy: 1.0000\n",
      "Epoch 46/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0148 - accuracy: 1.0000\n",
      "Epoch 47/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0143 - accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0139 - accuracy: 1.0000\n",
      "Epoch 49/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0135 - accuracy: 1.0000\n",
      "Epoch 50/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0131 - accuracy: 1.0000\n",
      "Epoch 1/50\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 2.2584 - accuracy: 0.2832\n",
      "Epoch 2/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 1.8465 - accuracy: 0.3628\n",
      "Epoch 3/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 1.5159 - accuracy: 0.4292\n",
      "Epoch 4/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 1.2350 - accuracy: 0.5000\n",
      "Epoch 5/50\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.9903 - accuracy: 0.5841\n",
      "Epoch 6/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.7998 - accuracy: 0.6549\n",
      "Epoch 7/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.6395 - accuracy: 0.7389\n",
      "Epoch 8/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.5101 - accuracy: 0.8009\n",
      "Epoch 9/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.4086 - accuracy: 0.8673\n",
      "Epoch 10/50\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.3355 - accuracy: 0.8982\n",
      "Epoch 11/50\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.2731 - accuracy: 0.9381\n",
      "Epoch 12/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.2280 - accuracy: 0.9602\n",
      "Epoch 13/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.1926 - accuracy: 0.9602\n",
      "Epoch 14/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.1666 - accuracy: 0.9735\n",
      "Epoch 15/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.1451 - accuracy: 0.9867\n",
      "Epoch 16/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.1289 - accuracy: 0.9867\n",
      "Epoch 17/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.1157 - accuracy: 0.9867\n",
      "Epoch 18/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.1036 - accuracy: 0.9867\n",
      "Epoch 19/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0946 - accuracy: 0.9867\n",
      "Epoch 20/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0855 - accuracy: 0.9912\n",
      "Epoch 21/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0784 - accuracy: 0.9912\n",
      "Epoch 22/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0722 - accuracy: 0.9956\n",
      "Epoch 23/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0669 - accuracy: 1.0000\n",
      "Epoch 24/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0622 - accuracy: 1.0000\n",
      "Epoch 25/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0578 - accuracy: 1.0000\n",
      "Epoch 26/50\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0543 - accuracy: 1.0000\n",
      "Epoch 27/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0509 - accuracy: 1.0000\n",
      "Epoch 28/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0479 - accuracy: 1.0000\n",
      "Epoch 29/50\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0451 - accuracy: 1.0000\n",
      "Epoch 30/50\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0426 - accuracy: 1.0000\n",
      "Epoch 31/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0405 - accuracy: 1.0000\n",
      "Epoch 32/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0383 - accuracy: 1.0000\n",
      "Epoch 33/50\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0366 - accuracy: 1.0000\n",
      "Epoch 34/50\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0349 - accuracy: 1.0000\n",
      "Epoch 35/50\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0332 - accuracy: 1.0000\n",
      "Epoch 36/50\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.0318 - accuracy: 1.0000\n",
      "Epoch 37/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0304 - accuracy: 1.0000\n",
      "Epoch 38/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0291 - accuracy: 1.0000\n",
      "Epoch 39/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0280 - accuracy: 1.0000\n",
      "Epoch 40/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0268 - accuracy: 1.0000\n",
      "Epoch 41/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0257 - accuracy: 1.0000\n",
      "Epoch 42/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0248 - accuracy: 1.0000\n",
      "Epoch 43/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0238 - accuracy: 1.0000\n",
      "Epoch 44/50\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0230 - accuracy: 1.0000\n",
      "Epoch 45/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0221 - accuracy: 1.0000\n",
      "Epoch 46/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0213 - accuracy: 1.0000\n",
      "Epoch 47/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0206 - accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0199 - accuracy: 1.0000\n",
      "Epoch 49/50\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0193 - accuracy: 1.0000\n",
      "Epoch 50/50\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0186 - accuracy: 1.0000\n",
      "Epoch 1/40\n",
      "9/9 [==============================] - 1s 5ms/step - loss: 1.4772 - accuracy: 0.4661\n",
      "Epoch 2/40\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 0.7313 - accuracy: 0.7052\n",
      "Epoch 3/40\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.3986 - accuracy: 0.8486\n",
      "Epoch 4/40\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.2401 - accuracy: 0.9203\n",
      "Epoch 5/40\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.1608 - accuracy: 0.9761\n",
      "Epoch 6/40\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.1178 - accuracy: 0.9801\n",
      "Epoch 7/40\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.0909 - accuracy: 0.9880\n",
      "Epoch 8/40\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.0731 - accuracy: 0.9960\n",
      "Epoch 9/40\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.0602 - accuracy: 0.9960\n",
      "Epoch 10/40\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.0506 - accuracy: 1.0000\n",
      "Epoch 11/40\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.0432 - accuracy: 1.0000\n",
      "Epoch 12/40\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.0379 - accuracy: 1.0000\n",
      "Epoch 13/40\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 0.0331 - accuracy: 1.0000\n",
      "Epoch 14/40\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 0.0296 - accuracy: 1.0000\n",
      "Epoch 15/40\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 0.0265 - accuracy: 1.0000\n",
      "Epoch 16/40\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.0240 - accuracy: 1.0000\n",
      "Epoch 17/40\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.0216 - accuracy: 1.0000\n",
      "Epoch 18/40\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.0198 - accuracy: 1.0000\n",
      "Epoch 19/40\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.0181 - accuracy: 1.0000\n",
      "Epoch 20/40\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.0167 - accuracy: 1.0000\n",
      "Epoch 21/40\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 0.0153 - accuracy: 1.0000\n",
      "Epoch 22/40\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.0142 - accuracy: 1.0000\n",
      "Epoch 23/40\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.0132 - accuracy: 1.0000\n",
      "Epoch 24/40\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.0123 - accuracy: 1.0000\n",
      "Epoch 25/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 5ms/step - loss: 0.0115 - accuracy: 1.0000\n",
      "Epoch 26/40\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.0107 - accuracy: 1.0000\n",
      "Epoch 27/40\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.0100 - accuracy: 1.0000\n",
      "Epoch 28/40\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.0095 - accuracy: 1.0000\n",
      "Epoch 29/40\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.0089 - accuracy: 1.0000\n",
      "Epoch 30/40\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 0.0084 - accuracy: 1.0000\n",
      "Epoch 31/40\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 0.0080 - accuracy: 1.0000\n",
      "Epoch 32/40\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 0.0076 - accuracy: 1.0000\n",
      "Epoch 33/40\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0072 - accuracy: 1.0000\n",
      "Epoch 34/40\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 0.0068 - accuracy: 1.0000\n",
      "Epoch 35/40\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 0.0065 - accuracy: 1.0000\n",
      "Epoch 36/40\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.0062 - accuracy: 1.0000\n",
      "Epoch 37/40\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0059 - accuracy: 1.0000\n",
      "Epoch 38/40\n",
      "9/9 [==============================] - 0s 57ms/step - loss: 0.0056 - accuracy: 1.0000\n",
      "Epoch 39/40\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 0.0054 - accuracy: 1.0000\n",
      "Epoch 40/40\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 0.0051 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10,\n",
       "             estimator=<keras.wrappers.scikit_learn.KerasClassifier object at 0x00000149ABE727F0>,\n",
       "             param_grid={'batch_size': [30, 50, 100],\n",
       "                         'epochs': [10, 20, 30, 40, 50]},\n",
       "             scoring='neg_log_loss')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define a range for the \"batch_size\" \n",
    "batch_size = [30 , 50 , 100 ]\n",
    "\n",
    "# define a range for the \"epochs\" \n",
    "epochs = [10, 20 , 30 , 40 , 50]\n",
    "\n",
    "# create a dictionary for grid parameter:\n",
    "param_grid = dict(batch_size = batch_size, epochs = epochs)\n",
    "print(param_grid,'\\n')\n",
    "\n",
    "# creat the grid, and define the metric for evaluating the model: \n",
    "grid = GridSearchCV(ann_model, param_grid, cv=10, scoring='neg_log_loss')\n",
    "\n",
    "# fit the grid (start the grid search):\n",
    "grid.fit(train_images_new, train_labels, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.1913588881226484\n",
      "{'batch_size': 30, 'epochs': 40}\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "(66, 3)\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.3349 - accuracy: 0.8636\n",
      "The accuracy is:  0.8636363744735718\n"
     ]
    }
   ],
   "source": [
    "# view the best results corresponding to the best structure of KerasClassifier:\n",
    "print(grid.best_score_)\n",
    "print(grid.best_params_)\n",
    "\n",
    "# Prediction:\n",
    "ann_pridict = grid.best_estimator_.model.predict(test_images_new, verbose=1)\n",
    "print (ann_pridict.shape)\n",
    "\n",
    "# Evaluation:\n",
    "ann_score = grid.best_estimator_.model.evaluate(test_images_new, test_labels, verbose=1)\n",
    "print('The accuracy is: ', ann_score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "9/9 [==============================] - 1s 29ms/step - loss: 1.7414 - accuracy: 0.3705 - val_loss: 1.4662 - val_accuracy: 0.4242\n",
      "Epoch 2/40\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.9422 - accuracy: 0.6056 - val_loss: 0.9758 - val_accuracy: 0.5758\n",
      "Epoch 3/40\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.5304 - accuracy: 0.8127 - val_loss: 0.7330 - val_accuracy: 0.6667\n",
      "Epoch 4/40\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.3306 - accuracy: 0.9004 - val_loss: 0.6408 - val_accuracy: 0.7424\n",
      "Epoch 5/40\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.2232 - accuracy: 0.9323 - val_loss: 0.5809 - val_accuracy: 0.8030\n",
      "Epoch 6/40\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.1613 - accuracy: 0.9681 - val_loss: 0.5242 - val_accuracy: 0.8182\n",
      "Epoch 7/40\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.1241 - accuracy: 0.9801 - val_loss: 0.4843 - val_accuracy: 0.8485\n",
      "Epoch 8/40\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.0989 - accuracy: 0.9841 - val_loss: 0.4512 - val_accuracy: 0.8485\n",
      "Epoch 9/40\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0805 - accuracy: 0.9841 - val_loss: 0.4357 - val_accuracy: 0.8485\n",
      "Epoch 10/40\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0674 - accuracy: 0.9920 - val_loss: 0.4122 - val_accuracy: 0.8636\n",
      "Epoch 11/40\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0566 - accuracy: 0.9960 - val_loss: 0.4004 - val_accuracy: 0.8636\n",
      "Epoch 12/40\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0492 - accuracy: 0.9960 - val_loss: 0.3911 - val_accuracy: 0.8636\n",
      "Epoch 13/40\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 0.0429 - accuracy: 1.0000 - val_loss: 0.3719 - val_accuracy: 0.8636\n",
      "Epoch 14/40\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0375 - accuracy: 1.0000 - val_loss: 0.3593 - val_accuracy: 0.8636\n",
      "Epoch 15/40\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0337 - accuracy: 1.0000 - val_loss: 0.3555 - val_accuracy: 0.8636\n",
      "Epoch 16/40\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 0.0300 - accuracy: 1.0000 - val_loss: 0.3500 - val_accuracy: 0.8636\n",
      "Epoch 17/40\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 0.0270 - accuracy: 1.0000 - val_loss: 0.3469 - val_accuracy: 0.8636\n",
      "Epoch 18/40\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0246 - accuracy: 1.0000 - val_loss: 0.3414 - val_accuracy: 0.8636\n",
      "Epoch 19/40\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0224 - accuracy: 1.0000 - val_loss: 0.3378 - val_accuracy: 0.8636\n",
      "Epoch 20/40\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0207 - accuracy: 1.0000 - val_loss: 0.3358 - val_accuracy: 0.8636\n",
      "Epoch 21/40\n",
      "9/9 [==============================] - 0s 16ms/step - loss: 0.0191 - accuracy: 1.0000 - val_loss: 0.3328 - val_accuracy: 0.8636\n",
      "Epoch 22/40\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.0177 - accuracy: 1.0000 - val_loss: 0.3310 - val_accuracy: 0.8636\n",
      "Epoch 23/40\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 0.0164 - accuracy: 1.0000 - val_loss: 0.3284 - val_accuracy: 0.8636\n",
      "Epoch 24/40\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.0153 - accuracy: 1.0000 - val_loss: 0.3255 - val_accuracy: 0.8636\n",
      "Epoch 25/40\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0143 - accuracy: 1.0000 - val_loss: 0.3197 - val_accuracy: 0.8636\n",
      "Epoch 26/40\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.0134 - accuracy: 1.0000 - val_loss: 0.3184 - val_accuracy: 0.8636\n",
      "Epoch 27/40\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0126 - accuracy: 1.0000 - val_loss: 0.3164 - val_accuracy: 0.8788\n",
      "Epoch 28/40\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0118 - accuracy: 1.0000 - val_loss: 0.3141 - val_accuracy: 0.8939\n",
      "Epoch 29/40\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 0.0111 - accuracy: 1.0000 - val_loss: 0.3134 - val_accuracy: 0.8788\n",
      "Epoch 30/40\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 0.0105 - accuracy: 1.0000 - val_loss: 0.3136 - val_accuracy: 0.8636\n",
      "Epoch 31/40\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0099 - accuracy: 1.0000 - val_loss: 0.3117 - val_accuracy: 0.8636\n",
      "Epoch 32/40\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0094 - accuracy: 1.0000 - val_loss: 0.3102 - val_accuracy: 0.8636\n",
      "Epoch 33/40\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0089 - accuracy: 1.0000 - val_loss: 0.3062 - val_accuracy: 0.8636\n",
      "Epoch 34/40\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0085 - accuracy: 1.0000 - val_loss: 0.3053 - val_accuracy: 0.8636\n",
      "Epoch 35/40\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 0.0080 - accuracy: 1.0000 - val_loss: 0.3042 - val_accuracy: 0.8636\n",
      "Epoch 36/40\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.0077 - accuracy: 1.0000 - val_loss: 0.3005 - val_accuracy: 0.8788\n",
      "Epoch 37/40\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 0.0073 - accuracy: 1.0000 - val_loss: 0.2981 - val_accuracy: 0.8788\n",
      "Epoch 38/40\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 0.0069 - accuracy: 1.0000 - val_loss: 0.2968 - val_accuracy: 0.8788\n",
      "Epoch 39/40\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0066 - accuracy: 1.0000 - val_loss: 0.2976 - val_accuracy: 0.8788\n",
      "Epoch 40/40\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0063 - accuracy: 1.0000 - val_loss: 0.2987 - val_accuracy: 0.8788\n"
     ]
    }
   ],
   "source": [
    "history2 = ann_model.fit(train_images_new, train_labels, batch_size=30, epochs=40, verbose=1, validation_data=(test_images_new, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_classes2 = ann_model.predict(test_images_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "accuracy = history2.history['accuracy']\n",
    "val_accuracy = history2.history['val_accuracy']\n",
    "loss = history2.history['loss']\n",
    "val_loss = history2.history['val_loss']\n",
    "epochs = range(len(accuracy))\n",
    "plt.plot(epochs, accuracy, 'r-', label='Training accuracy')\n",
    "plt.plot(epochs, val_accuracy, 'b', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'r-', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN using MLPClassifier\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hidden_layer_sizes': [(5,), (10,), (15,), (20,), (25,), (30,), (35,), (40,), (45,), (50,), (55,), (60,), (65,), (70,), (75,), (80,), (85,), (90,), (95,), (100,), (105,), (110,), (115,), (120,), (125,), (130,), (135,), (140,), (145,), (150,), (155,), (160,), (165,), (170,), (175,), (180,), (185,), (190,), (195,), (200,), (205,), (210,), (215,), (220,), (225,), (230,), (235,), (240,), (245,)]} \n",
      "\n",
      "Iteration 1, loss = 2.21031050\n",
      "Iteration 2, loss = 2.03151957\n",
      "Iteration 3, loss = 1.89750485\n",
      "Iteration 4, loss = 1.79104211\n",
      "Iteration 5, loss = 1.70058811\n",
      "Iteration 6, loss = 1.61901430\n",
      "Iteration 7, loss = 1.54156746\n",
      "Iteration 8, loss = 1.47062215\n",
      "Iteration 9, loss = 1.40726491\n",
      "Iteration 10, loss = 1.34887096\n",
      "Iteration 11, loss = 1.29808998\n",
      "Iteration 12, loss = 1.24988548\n",
      "Iteration 13, loss = 1.20554573\n",
      "Iteration 14, loss = 1.16441278\n",
      "Iteration 15, loss = 1.12714358\n",
      "Iteration 16, loss = 1.09213624\n",
      "Iteration 17, loss = 1.05929376\n",
      "Iteration 18, loss = 1.02951863\n",
      "Iteration 19, loss = 1.00235691\n",
      "Iteration 20, loss = 0.97803720\n",
      "Iteration 21, loss = 0.95568787\n",
      "Iteration 22, loss = 0.93429645\n",
      "Iteration 23, loss = 0.91483923\n",
      "Iteration 24, loss = 0.89691471\n",
      "Iteration 25, loss = 0.88022240\n",
      "Iteration 26, loss = 0.86395237\n",
      "Iteration 27, loss = 0.84863503\n",
      "Iteration 28, loss = 0.83547654\n",
      "Iteration 29, loss = 0.82376984\n",
      "Iteration 30, loss = 0.81453010\n",
      "Iteration 31, loss = 0.80324841\n",
      "Iteration 32, loss = 0.79224276\n",
      "Iteration 33, loss = 0.78278197\n",
      "Iteration 34, loss = 0.77365277\n",
      "Iteration 35, loss = 0.76496349\n",
      "Iteration 36, loss = 0.75667736\n",
      "Iteration 37, loss = 0.74855901\n",
      "Iteration 38, loss = 0.73861517\n",
      "Iteration 39, loss = 0.72976721\n",
      "Iteration 40, loss = 0.72314711\n",
      "Iteration 41, loss = 0.71882800\n",
      "Iteration 42, loss = 0.71321486\n",
      "Iteration 43, loss = 0.70878080\n",
      "Iteration 44, loss = 0.70041675\n",
      "Iteration 45, loss = 0.69294903\n",
      "Iteration 46, loss = 0.68839363\n",
      "Iteration 47, loss = 0.68230809\n",
      "Iteration 48, loss = 0.67755827\n",
      "Iteration 49, loss = 0.67374712\n",
      "Iteration 50, loss = 0.66757215\n",
      "Iteration 51, loss = 0.66104955\n",
      "Iteration 52, loss = 0.65533550\n",
      "Iteration 53, loss = 0.65159698\n",
      "Iteration 54, loss = 0.64797119\n",
      "Iteration 55, loss = 0.64380123\n",
      "Iteration 56, loss = 0.63895851\n",
      "Iteration 57, loss = 0.63321374\n",
      "Iteration 58, loss = 0.62725165\n",
      "Iteration 59, loss = 0.62252067\n",
      "Iteration 60, loss = 0.61842819\n",
      "Iteration 61, loss = 0.61393239\n",
      "Iteration 62, loss = 0.60801075\n",
      "Iteration 63, loss = 0.60145613\n",
      "Iteration 64, loss = 0.59734106\n",
      "Iteration 65, loss = 0.59617806\n",
      "Iteration 66, loss = 0.59366452\n",
      "Iteration 67, loss = 0.58928787\n",
      "Iteration 68, loss = 0.58425592\n",
      "Iteration 69, loss = 0.57891013\n",
      "Iteration 70, loss = 0.57473542\n",
      "Iteration 71, loss = 0.57113336\n",
      "Iteration 72, loss = 0.56826429\n",
      "Iteration 73, loss = 0.56495180\n",
      "Iteration 74, loss = 0.56048479\n",
      "Iteration 75, loss = 0.55762348\n",
      "Iteration 76, loss = 0.55380267\n",
      "Iteration 77, loss = 0.55134648\n",
      "Iteration 78, loss = 0.54927819\n",
      "Iteration 79, loss = 0.54394724\n",
      "Iteration 80, loss = 0.53726184\n",
      "Iteration 81, loss = 0.53448409\n",
      "Iteration 82, loss = 0.53494029\n",
      "Iteration 83, loss = 0.53520154\n",
      "Iteration 84, loss = 0.53340943\n",
      "Iteration 85, loss = 0.53037830\n",
      "Iteration 86, loss = 0.52720688\n",
      "Iteration 87, loss = 0.52490568\n",
      "Iteration 88, loss = 0.52256416\n",
      "Iteration 89, loss = 0.52112670\n",
      "Iteration 90, loss = 0.51742041\n",
      "Iteration 91, loss = 0.51467317\n",
      "Iteration 92, loss = 0.51438542\n",
      "Iteration 93, loss = 0.51415767\n",
      "Iteration 94, loss = 0.51468154\n",
      "Iteration 95, loss = 0.51268165\n",
      "Iteration 96, loss = 0.51244956\n",
      "Iteration 97, loss = 0.51304716\n",
      "Iteration 98, loss = 0.50782014\n",
      "Iteration 99, loss = 0.50700784\n",
      "Iteration 100, loss = 0.51152424\n",
      "Iteration 101, loss = 0.51467599\n",
      "Iteration 102, loss = 0.51072016\n",
      "Iteration 103, loss = 0.50503659\n",
      "Iteration 104, loss = 0.50441948\n",
      "Iteration 105, loss = 0.50555378\n",
      "Iteration 106, loss = 0.50598303\n",
      "Iteration 107, loss = 0.50504712\n",
      "Iteration 108, loss = 0.50401129\n",
      "Iteration 109, loss = 0.50267069\n",
      "Iteration 110, loss = 0.50073683\n",
      "Iteration 111, loss = 0.49950290\n",
      "Iteration 112, loss = 0.50119253\n",
      "Iteration 113, loss = 0.50128662\n",
      "Iteration 114, loss = 0.49853712\n",
      "Iteration 115, loss = 0.49751889\n",
      "Iteration 116, loss = 0.49785657\n",
      "Iteration 117, loss = 0.49762029\n",
      "Iteration 118, loss = 0.49722367\n",
      "Iteration 119, loss = 0.49671990\n",
      "Iteration 120, loss = 0.49607808\n",
      "Iteration 121, loss = 0.49543595\n",
      "Iteration 122, loss = 0.49366126\n",
      "Iteration 123, loss = 0.49208470\n",
      "Iteration 124, loss = 0.49246486\n",
      "Iteration 125, loss = 0.49485521\n",
      "Iteration 126, loss = 0.49433498\n",
      "Iteration 127, loss = 0.49463946\n",
      "Iteration 128, loss = 0.49569780\n",
      "Iteration 129, loss = 0.49626740\n",
      "Iteration 130, loss = 0.49554513\n",
      "Iteration 131, loss = 0.49271942\n",
      "Iteration 132, loss = 0.49143142\n",
      "Iteration 133, loss = 0.49171897\n",
      "Iteration 134, loss = 0.49071608\n",
      "Iteration 135, loss = 0.49081807\n",
      "Iteration 136, loss = 0.49252693\n",
      "Iteration 137, loss = 0.49430184\n",
      "Iteration 138, loss = 0.49336596\n",
      "Iteration 139, loss = 0.49215838\n",
      "Iteration 140, loss = 0.49203437\n",
      "Iteration 141, loss = 0.49357722\n",
      "Iteration 142, loss = 0.49536937\n",
      "Iteration 143, loss = 0.49317911\n",
      "Iteration 144, loss = 0.49097115\n",
      "Iteration 145, loss = 0.48865371\n",
      "Iteration 146, loss = 0.48784474\n",
      "Iteration 147, loss = 0.48790258\n",
      "Iteration 148, loss = 0.48912817\n",
      "Iteration 149, loss = 0.48855667\n",
      "Iteration 150, loss = 0.48882098\n",
      "Iteration 151, loss = 0.49086493\n",
      "Iteration 152, loss = 0.49252264\n",
      "Iteration 153, loss = 0.49200791\n",
      "Iteration 154, loss = 0.49123158\n",
      "Iteration 155, loss = 0.48910804\n",
      "Iteration 156, loss = 0.48958298\n",
      "Iteration 157, loss = 0.48943658\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.20530368\n",
      "Iteration 2, loss = 2.02936631\n",
      "Iteration 3, loss = 1.89089647\n",
      "Iteration 4, loss = 1.78138226\n",
      "Iteration 5, loss = 1.68957305\n",
      "Iteration 6, loss = 1.60874238\n",
      "Iteration 7, loss = 1.53492374\n",
      "Iteration 8, loss = 1.46667341\n",
      "Iteration 9, loss = 1.40298292\n",
      "Iteration 10, loss = 1.34381379\n",
      "Iteration 11, loss = 1.29003486\n",
      "Iteration 12, loss = 1.24029836\n",
      "Iteration 13, loss = 1.19522510\n",
      "Iteration 14, loss = 1.15341015\n",
      "Iteration 15, loss = 1.11427354\n",
      "Iteration 16, loss = 1.07890750\n",
      "Iteration 17, loss = 1.04637163\n",
      "Iteration 18, loss = 1.01692967\n",
      "Iteration 19, loss = 0.99059774\n",
      "Iteration 20, loss = 0.96560030\n",
      "Iteration 21, loss = 0.94357343\n",
      "Iteration 22, loss = 0.92222838\n",
      "Iteration 23, loss = 0.90334841\n",
      "Iteration 24, loss = 0.88554481\n",
      "Iteration 25, loss = 0.86918729\n",
      "Iteration 26, loss = 0.85433790\n",
      "Iteration 27, loss = 0.83887187\n",
      "Iteration 28, loss = 0.82460385\n",
      "Iteration 29, loss = 0.81155010\n",
      "Iteration 30, loss = 0.80055642\n",
      "Iteration 31, loss = 0.79040258\n",
      "Iteration 32, loss = 0.78020190\n",
      "Iteration 33, loss = 0.77014803\n",
      "Iteration 34, loss = 0.76030836\n",
      "Iteration 35, loss = 0.75102575\n",
      "Iteration 36, loss = 0.74405743\n",
      "Iteration 37, loss = 0.73594215\n",
      "Iteration 38, loss = 0.72830395\n",
      "Iteration 39, loss = 0.72179787\n",
      "Iteration 40, loss = 0.71595127\n",
      "Iteration 41, loss = 0.70944150\n",
      "Iteration 42, loss = 0.70211573\n",
      "Iteration 43, loss = 0.69496361\n",
      "Iteration 44, loss = 0.69161731\n",
      "Iteration 45, loss = 0.68734733\n",
      "Iteration 46, loss = 0.68229998\n",
      "Iteration 47, loss = 0.67739107\n",
      "Iteration 48, loss = 0.67290557\n",
      "Iteration 49, loss = 0.66911125\n",
      "Iteration 50, loss = 0.66544025\n",
      "Iteration 51, loss = 0.66138674\n",
      "Iteration 52, loss = 0.65731257\n",
      "Iteration 53, loss = 0.65345573\n",
      "Iteration 54, loss = 0.65176043\n",
      "Iteration 55, loss = 0.64786534\n",
      "Iteration 56, loss = 0.64364172\n",
      "Iteration 57, loss = 0.64030418\n",
      "Iteration 58, loss = 0.63721755\n",
      "Iteration 59, loss = 0.63330766\n",
      "Iteration 60, loss = 0.63029233\n",
      "Iteration 61, loss = 0.62520616\n",
      "Iteration 62, loss = 0.62084063\n",
      "Iteration 63, loss = 0.61948255\n",
      "Iteration 64, loss = 0.61806238\n",
      "Iteration 65, loss = 0.61336228\n",
      "Iteration 66, loss = 0.60819333\n",
      "Iteration 67, loss = 0.60464825\n",
      "Iteration 68, loss = 0.60063114\n",
      "Iteration 69, loss = 0.59587380\n",
      "Iteration 70, loss = 0.59322066\n",
      "Iteration 71, loss = 0.59326982\n",
      "Iteration 72, loss = 0.59144438\n",
      "Iteration 73, loss = 0.58805639\n",
      "Iteration 74, loss = 0.58334601\n",
      "Iteration 75, loss = 0.57768870\n",
      "Iteration 76, loss = 0.57495583\n",
      "Iteration 77, loss = 0.57382700\n",
      "Iteration 78, loss = 0.57352298\n",
      "Iteration 79, loss = 0.56886400\n",
      "Iteration 80, loss = 0.56334685\n",
      "Iteration 81, loss = 0.56044054\n",
      "Iteration 82, loss = 0.56133471\n",
      "Iteration 83, loss = 0.55979813\n",
      "Iteration 84, loss = 0.55600836\n",
      "Iteration 85, loss = 0.55172687\n",
      "Iteration 86, loss = 0.54853269\n",
      "Iteration 87, loss = 0.54568186\n",
      "Iteration 88, loss = 0.54402743\n",
      "Iteration 89, loss = 0.54102755\n",
      "Iteration 90, loss = 0.53868088\n",
      "Iteration 91, loss = 0.53838983\n",
      "Iteration 92, loss = 0.53590690\n",
      "Iteration 93, loss = 0.53429381\n",
      "Iteration 94, loss = 0.53207032\n",
      "Iteration 95, loss = 0.52961727\n",
      "Iteration 96, loss = 0.52807421\n",
      "Iteration 97, loss = 0.52618487\n",
      "Iteration 98, loss = 0.52298345\n",
      "Iteration 99, loss = 0.52123273\n",
      "Iteration 100, loss = 0.51918420\n",
      "Iteration 101, loss = 0.51732322\n",
      "Iteration 102, loss = 0.51457624\n",
      "Iteration 103, loss = 0.51277038\n",
      "Iteration 104, loss = 0.51044223\n",
      "Iteration 105, loss = 0.50750198\n",
      "Iteration 106, loss = 0.50378374\n",
      "Iteration 107, loss = 0.50326511\n",
      "Iteration 108, loss = 0.50748469\n",
      "Iteration 109, loss = 0.50889749\n",
      "Iteration 110, loss = 0.50578228\n",
      "Iteration 111, loss = 0.50304420\n",
      "Iteration 112, loss = 0.50026579\n",
      "Iteration 113, loss = 0.49763713\n",
      "Iteration 114, loss = 0.49559461\n",
      "Iteration 115, loss = 0.49559357\n",
      "Iteration 116, loss = 0.49413099\n",
      "Iteration 117, loss = 0.49252359\n",
      "Iteration 118, loss = 0.49107383\n",
      "Iteration 119, loss = 0.49012322\n",
      "Iteration 120, loss = 0.49206178\n",
      "Iteration 121, loss = 0.49221259\n",
      "Iteration 122, loss = 0.49293720\n",
      "Iteration 123, loss = 0.49201132\n",
      "Iteration 124, loss = 0.49383031\n",
      "Iteration 125, loss = 0.49396522\n",
      "Iteration 126, loss = 0.49124140\n",
      "Iteration 127, loss = 0.48896336\n",
      "Iteration 128, loss = 0.48923108\n",
      "Iteration 129, loss = 0.48870221\n",
      "Iteration 130, loss = 0.48580195\n",
      "Iteration 131, loss = 0.48255203\n",
      "Iteration 132, loss = 0.48056520\n",
      "Iteration 133, loss = 0.48190169\n",
      "Iteration 134, loss = 0.48383999\n",
      "Iteration 135, loss = 0.48535935\n",
      "Iteration 136, loss = 0.48564794\n",
      "Iteration 137, loss = 0.48483818\n",
      "Iteration 138, loss = 0.48395672\n",
      "Iteration 139, loss = 0.48330066\n",
      "Iteration 140, loss = 0.48354121\n",
      "Iteration 141, loss = 0.48285772\n",
      "Iteration 142, loss = 0.48342886\n",
      "Iteration 143, loss = 0.48401473\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.21218719\n",
      "Iteration 2, loss = 2.03950093\n",
      "Iteration 3, loss = 1.90600350\n",
      "Iteration 4, loss = 1.79918881\n",
      "Iteration 5, loss = 1.70895026\n",
      "Iteration 6, loss = 1.62955268\n",
      "Iteration 7, loss = 1.55604786\n",
      "Iteration 8, loss = 1.48846633\n",
      "Iteration 9, loss = 1.42538359\n",
      "Iteration 10, loss = 1.36659952\n",
      "Iteration 11, loss = 1.31348278\n",
      "Iteration 12, loss = 1.26372572\n",
      "Iteration 13, loss = 1.21826199\n",
      "Iteration 14, loss = 1.17498237\n",
      "Iteration 15, loss = 1.13461597\n",
      "Iteration 16, loss = 1.09771830\n",
      "Iteration 17, loss = 1.06394662\n",
      "Iteration 18, loss = 1.03337198\n",
      "Iteration 19, loss = 1.00517156\n",
      "Iteration 20, loss = 0.97910271\n",
      "Iteration 21, loss = 0.95552559\n",
      "Iteration 22, loss = 0.93338420\n",
      "Iteration 23, loss = 0.91384269\n",
      "Iteration 24, loss = 0.89641356\n",
      "Iteration 25, loss = 0.88088175\n",
      "Iteration 26, loss = 0.86364666\n",
      "Iteration 27, loss = 0.84666122\n",
      "Iteration 28, loss = 0.83368434\n",
      "Iteration 29, loss = 0.82131423\n",
      "Iteration 30, loss = 0.80993469\n",
      "Iteration 31, loss = 0.79768284\n",
      "Iteration 32, loss = 0.78510078\n",
      "Iteration 33, loss = 0.77316143\n",
      "Iteration 34, loss = 0.76240019\n",
      "Iteration 35, loss = 0.75280406\n",
      "Iteration 36, loss = 0.74526304\n",
      "Iteration 37, loss = 0.73770865\n",
      "Iteration 38, loss = 0.72861758\n",
      "Iteration 39, loss = 0.71996651\n",
      "Iteration 40, loss = 0.71213189\n",
      "Iteration 41, loss = 0.70266185\n",
      "Iteration 42, loss = 0.69351344\n",
      "Iteration 43, loss = 0.68449578\n",
      "Iteration 44, loss = 0.67758705\n",
      "Iteration 45, loss = 0.66958054\n",
      "Iteration 46, loss = 0.66273807\n",
      "Iteration 47, loss = 0.65575283\n",
      "Iteration 48, loss = 0.64894507\n",
      "Iteration 49, loss = 0.64223853\n",
      "Iteration 50, loss = 0.63499740\n",
      "Iteration 51, loss = 0.62786056\n",
      "Iteration 52, loss = 0.62167636\n",
      "Iteration 53, loss = 0.61763593\n",
      "Iteration 54, loss = 0.61796775\n",
      "Iteration 55, loss = 0.61223673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 56, loss = 0.60411007\n",
      "Iteration 57, loss = 0.59820224\n",
      "Iteration 58, loss = 0.59453104\n",
      "Iteration 59, loss = 0.58998833\n",
      "Iteration 60, loss = 0.58702916\n",
      "Iteration 61, loss = 0.58301508\n",
      "Iteration 62, loss = 0.57928819\n",
      "Iteration 63, loss = 0.57758390\n",
      "Iteration 64, loss = 0.57566333\n",
      "Iteration 65, loss = 0.56965094\n",
      "Iteration 66, loss = 0.56435775\n",
      "Iteration 67, loss = 0.56309253\n",
      "Iteration 68, loss = 0.55997553\n",
      "Iteration 69, loss = 0.55434413\n",
      "Iteration 70, loss = 0.55055179\n",
      "Iteration 71, loss = 0.54952296\n",
      "Iteration 72, loss = 0.54817932\n",
      "Iteration 73, loss = 0.54574468\n",
      "Iteration 74, loss = 0.54478849\n",
      "Iteration 75, loss = 0.54029254\n",
      "Iteration 76, loss = 0.53663589\n",
      "Iteration 77, loss = 0.53552754\n",
      "Iteration 78, loss = 0.53493831\n",
      "Iteration 79, loss = 0.53202425\n",
      "Iteration 80, loss = 0.53026864\n",
      "Iteration 81, loss = 0.53014386\n",
      "Iteration 82, loss = 0.52885286\n",
      "Iteration 83, loss = 0.52701599\n",
      "Iteration 84, loss = 0.52484209\n",
      "Iteration 85, loss = 0.52334872\n",
      "Iteration 86, loss = 0.52276104\n",
      "Iteration 87, loss = 0.52169870\n",
      "Iteration 88, loss = 0.52097306\n",
      "Iteration 89, loss = 0.51992922\n",
      "Iteration 90, loss = 0.51944351\n",
      "Iteration 91, loss = 0.52040685\n",
      "Iteration 92, loss = 0.51992174\n",
      "Iteration 93, loss = 0.52041821\n",
      "Iteration 94, loss = 0.52055068\n",
      "Iteration 95, loss = 0.52095664\n",
      "Iteration 96, loss = 0.52113319\n",
      "Iteration 97, loss = 0.51952177\n",
      "Iteration 98, loss = 0.51766657\n",
      "Iteration 99, loss = 0.51868444\n",
      "Iteration 100, loss = 0.51931486\n",
      "Iteration 101, loss = 0.51872989\n",
      "Iteration 102, loss = 0.51708650\n",
      "Iteration 103, loss = 0.51513305\n",
      "Iteration 104, loss = 0.51355649\n",
      "Iteration 105, loss = 0.51234765\n",
      "Iteration 106, loss = 0.51136010\n",
      "Iteration 107, loss = 0.51068939\n",
      "Iteration 108, loss = 0.51200076\n",
      "Iteration 109, loss = 0.51364492\n",
      "Iteration 110, loss = 0.51306447\n",
      "Iteration 111, loss = 0.51219255\n",
      "Iteration 112, loss = 0.51155960\n",
      "Iteration 113, loss = 0.51015365\n",
      "Iteration 114, loss = 0.50792538\n",
      "Iteration 115, loss = 0.50732284\n",
      "Iteration 116, loss = 0.50645413\n",
      "Iteration 117, loss = 0.50580610\n",
      "Iteration 118, loss = 0.50542601\n",
      "Iteration 119, loss = 0.50598519\n",
      "Iteration 120, loss = 0.50793884\n",
      "Iteration 121, loss = 0.50720759\n",
      "Iteration 122, loss = 0.50702194\n",
      "Iteration 123, loss = 0.50676952\n",
      "Iteration 124, loss = 0.51011125\n",
      "Iteration 125, loss = 0.51209750\n",
      "Iteration 126, loss = 0.50938546\n",
      "Iteration 127, loss = 0.50714351\n",
      "Iteration 128, loss = 0.50821929\n",
      "Iteration 129, loss = 0.50852271\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.18831267\n",
      "Iteration 2, loss = 2.01604212\n",
      "Iteration 3, loss = 1.88116773\n",
      "Iteration 4, loss = 1.77243616\n",
      "Iteration 5, loss = 1.67914704\n",
      "Iteration 6, loss = 1.59833031\n",
      "Iteration 7, loss = 1.52428774\n",
      "Iteration 8, loss = 1.45752179\n",
      "Iteration 9, loss = 1.39581601\n",
      "Iteration 10, loss = 1.33882214\n",
      "Iteration 11, loss = 1.28726317\n",
      "Iteration 12, loss = 1.23990802\n",
      "Iteration 13, loss = 1.19698533\n",
      "Iteration 14, loss = 1.15594368\n",
      "Iteration 15, loss = 1.11814062\n",
      "Iteration 16, loss = 1.08313212\n",
      "Iteration 17, loss = 1.05083360\n",
      "Iteration 18, loss = 1.02140389\n",
      "Iteration 19, loss = 0.99451192\n",
      "Iteration 20, loss = 0.96925605\n",
      "Iteration 21, loss = 0.94563989\n",
      "Iteration 22, loss = 0.92357046\n",
      "Iteration 23, loss = 0.90380989\n",
      "Iteration 24, loss = 0.88626550\n",
      "Iteration 25, loss = 0.87093507\n",
      "Iteration 26, loss = 0.85382231\n",
      "Iteration 27, loss = 0.83580014\n",
      "Iteration 28, loss = 0.82234052\n",
      "Iteration 29, loss = 0.80960996\n",
      "Iteration 30, loss = 0.79669094\n",
      "Iteration 31, loss = 0.78411828\n",
      "Iteration 32, loss = 0.77165218\n",
      "Iteration 33, loss = 0.75978063\n",
      "Iteration 34, loss = 0.74795230\n",
      "Iteration 35, loss = 0.73667400\n",
      "Iteration 36, loss = 0.72717760\n",
      "Iteration 37, loss = 0.71721377\n",
      "Iteration 38, loss = 0.70732255\n",
      "Iteration 39, loss = 0.69815424\n",
      "Iteration 40, loss = 0.68942741\n",
      "Iteration 41, loss = 0.67967043\n",
      "Iteration 42, loss = 0.66967953\n",
      "Iteration 43, loss = 0.65969037\n",
      "Iteration 44, loss = 0.65167999\n",
      "Iteration 45, loss = 0.64292882\n",
      "Iteration 46, loss = 0.63556930\n",
      "Iteration 47, loss = 0.62789836\n",
      "Iteration 48, loss = 0.62093321\n",
      "Iteration 49, loss = 0.61482518\n",
      "Iteration 50, loss = 0.60868987\n",
      "Iteration 51, loss = 0.60101102\n",
      "Iteration 52, loss = 0.59416123\n",
      "Iteration 53, loss = 0.58898948\n",
      "Iteration 54, loss = 0.58724065\n",
      "Iteration 55, loss = 0.58236129\n",
      "Iteration 56, loss = 0.57607134\n",
      "Iteration 57, loss = 0.57100083\n",
      "Iteration 58, loss = 0.56733756\n",
      "Iteration 59, loss = 0.56239969\n",
      "Iteration 60, loss = 0.55788626\n",
      "Iteration 61, loss = 0.55375789\n",
      "Iteration 62, loss = 0.55006066\n",
      "Iteration 63, loss = 0.54841989\n",
      "Iteration 64, loss = 0.54643561\n",
      "Iteration 65, loss = 0.54054232\n",
      "Iteration 66, loss = 0.53626352\n",
      "Iteration 67, loss = 0.53605181\n",
      "Iteration 68, loss = 0.53319304\n",
      "Iteration 69, loss = 0.52783853\n",
      "Iteration 70, loss = 0.52395487\n",
      "Iteration 71, loss = 0.52267219\n",
      "Iteration 72, loss = 0.52284161\n",
      "Iteration 73, loss = 0.51993827\n",
      "Iteration 74, loss = 0.51725842\n",
      "Iteration 75, loss = 0.51121175\n",
      "Iteration 76, loss = 0.50814265\n",
      "Iteration 77, loss = 0.50781753\n",
      "Iteration 78, loss = 0.50749431\n",
      "Iteration 79, loss = 0.50486595\n",
      "Iteration 80, loss = 0.50307240\n",
      "Iteration 81, loss = 0.50281930\n",
      "Iteration 82, loss = 0.50083154\n",
      "Iteration 83, loss = 0.49872071\n",
      "Iteration 84, loss = 0.49695795\n",
      "Iteration 85, loss = 0.49634170\n",
      "Iteration 86, loss = 0.49637973\n",
      "Iteration 87, loss = 0.49473271\n",
      "Iteration 88, loss = 0.49317582\n",
      "Iteration 89, loss = 0.49204307\n",
      "Iteration 90, loss = 0.49175897\n",
      "Iteration 91, loss = 0.49187661\n",
      "Iteration 92, loss = 0.49101736\n",
      "Iteration 93, loss = 0.49175601\n",
      "Iteration 94, loss = 0.49211453\n",
      "Iteration 95, loss = 0.49258158\n",
      "Iteration 96, loss = 0.49221151\n",
      "Iteration 97, loss = 0.49043860\n",
      "Iteration 98, loss = 0.48931351\n",
      "Iteration 99, loss = 0.49061044\n",
      "Iteration 100, loss = 0.49050752\n",
      "Iteration 101, loss = 0.48939556\n",
      "Iteration 102, loss = 0.48828268\n",
      "Iteration 103, loss = 0.48680686\n",
      "Iteration 104, loss = 0.48594952\n",
      "Iteration 105, loss = 0.48463390\n",
      "Iteration 106, loss = 0.48335701\n",
      "Iteration 107, loss = 0.48200278\n",
      "Iteration 108, loss = 0.48377019\n",
      "Iteration 109, loss = 0.48563854\n",
      "Iteration 110, loss = 0.48684610\n",
      "Iteration 111, loss = 0.48609205\n",
      "Iteration 112, loss = 0.48439915\n",
      "Iteration 113, loss = 0.48176292\n",
      "Iteration 114, loss = 0.47918680\n",
      "Iteration 115, loss = 0.47857641\n",
      "Iteration 116, loss = 0.47756078\n",
      "Iteration 117, loss = 0.47729897\n",
      "Iteration 118, loss = 0.47787810\n",
      "Iteration 119, loss = 0.47845880\n",
      "Iteration 120, loss = 0.48020441\n",
      "Iteration 121, loss = 0.48022309\n",
      "Iteration 122, loss = 0.47992422\n",
      "Iteration 123, loss = 0.48063709\n",
      "Iteration 124, loss = 0.48554605\n",
      "Iteration 125, loss = 0.48846358\n",
      "Iteration 126, loss = 0.48371320\n",
      "Iteration 127, loss = 0.47921586\n",
      "Iteration 128, loss = 0.47922649\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.19774346\n",
      "Iteration 2, loss = 2.02247715\n",
      "Iteration 3, loss = 1.88533026\n",
      "Iteration 4, loss = 1.77739401\n",
      "Iteration 5, loss = 1.68652366\n",
      "Iteration 6, loss = 1.60832513\n",
      "Iteration 7, loss = 1.53604699\n",
      "Iteration 8, loss = 1.46951432\n",
      "Iteration 9, loss = 1.40769192\n",
      "Iteration 10, loss = 1.35022198\n",
      "Iteration 11, loss = 1.29772564\n",
      "Iteration 12, loss = 1.24863752\n",
      "Iteration 13, loss = 1.20428690\n",
      "Iteration 14, loss = 1.16221222\n",
      "Iteration 15, loss = 1.12371576\n",
      "Iteration 16, loss = 1.08801321\n",
      "Iteration 17, loss = 1.05521833\n",
      "Iteration 18, loss = 1.02529662\n",
      "Iteration 19, loss = 0.99748494\n",
      "Iteration 20, loss = 0.97250819\n",
      "Iteration 21, loss = 0.94949218\n",
      "Iteration 22, loss = 0.92725433\n",
      "Iteration 23, loss = 0.90670279\n",
      "Iteration 24, loss = 0.88740731\n",
      "Iteration 25, loss = 0.87105669\n",
      "Iteration 26, loss = 0.85431192\n",
      "Iteration 27, loss = 0.83886855\n",
      "Iteration 28, loss = 0.82606053\n",
      "Iteration 29, loss = 0.81239242\n",
      "Iteration 30, loss = 0.79822804\n",
      "Iteration 31, loss = 0.78510656\n",
      "Iteration 32, loss = 0.77249410\n",
      "Iteration 33, loss = 0.76057690\n",
      "Iteration 34, loss = 0.74855091\n",
      "Iteration 35, loss = 0.73714340\n",
      "Iteration 36, loss = 0.72594386\n",
      "Iteration 37, loss = 0.71688221\n",
      "Iteration 38, loss = 0.70778481\n",
      "Iteration 39, loss = 0.69904841\n",
      "Iteration 40, loss = 0.69042810\n",
      "Iteration 41, loss = 0.68112717\n",
      "Iteration 42, loss = 0.67104922\n",
      "Iteration 43, loss = 0.66182095\n",
      "Iteration 44, loss = 0.65469565\n",
      "Iteration 45, loss = 0.64613305\n",
      "Iteration 46, loss = 0.63954702\n",
      "Iteration 47, loss = 0.63326834\n",
      "Iteration 48, loss = 0.62713488\n",
      "Iteration 49, loss = 0.62112531\n",
      "Iteration 50, loss = 0.61533747\n",
      "Iteration 51, loss = 0.60832298\n",
      "Iteration 52, loss = 0.60264254\n",
      "Iteration 53, loss = 0.59813121\n",
      "Iteration 54, loss = 0.59635580\n",
      "Iteration 55, loss = 0.59273000\n",
      "Iteration 56, loss = 0.58832445\n",
      "Iteration 57, loss = 0.58258070\n",
      "Iteration 58, loss = 0.57942500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 59, loss = 0.57536706\n",
      "Iteration 60, loss = 0.57023475\n",
      "Iteration 61, loss = 0.56579496\n",
      "Iteration 62, loss = 0.56424813\n",
      "Iteration 63, loss = 0.56474826\n",
      "Iteration 64, loss = 0.56337788\n",
      "Iteration 65, loss = 0.55664613\n",
      "Iteration 66, loss = 0.54970121\n",
      "Iteration 67, loss = 0.54693144\n",
      "Iteration 68, loss = 0.54463095\n",
      "Iteration 69, loss = 0.54107623\n",
      "Iteration 70, loss = 0.53913913\n",
      "Iteration 71, loss = 0.53841067\n",
      "Iteration 72, loss = 0.53805687\n",
      "Iteration 73, loss = 0.53648727\n",
      "Iteration 74, loss = 0.53534242\n",
      "Iteration 75, loss = 0.52984505\n",
      "Iteration 76, loss = 0.52665811\n",
      "Iteration 77, loss = 0.52639365\n",
      "Iteration 78, loss = 0.52678608\n",
      "Iteration 79, loss = 0.52342042\n",
      "Iteration 80, loss = 0.52020699\n",
      "Iteration 81, loss = 0.51944386\n",
      "Iteration 82, loss = 0.51871597\n",
      "Iteration 83, loss = 0.51792341\n",
      "Iteration 84, loss = 0.51700509\n",
      "Iteration 85, loss = 0.51551630\n",
      "Iteration 86, loss = 0.51414688\n",
      "Iteration 87, loss = 0.51223725\n",
      "Iteration 88, loss = 0.51088949\n",
      "Iteration 89, loss = 0.51064407\n",
      "Iteration 90, loss = 0.51048816\n",
      "Iteration 91, loss = 0.51081704\n",
      "Iteration 92, loss = 0.50806553\n",
      "Iteration 93, loss = 0.50815263\n",
      "Iteration 94, loss = 0.50866891\n",
      "Iteration 95, loss = 0.50927180\n",
      "Iteration 96, loss = 0.50854377\n",
      "Iteration 97, loss = 0.50760547\n",
      "Iteration 98, loss = 0.50729724\n",
      "Iteration 99, loss = 0.50707900\n",
      "Iteration 100, loss = 0.50559973\n",
      "Iteration 101, loss = 0.50281462\n",
      "Iteration 102, loss = 0.50105779\n",
      "Iteration 103, loss = 0.50059876\n",
      "Iteration 104, loss = 0.50084712\n",
      "Iteration 105, loss = 0.49919164\n",
      "Iteration 106, loss = 0.49775539\n",
      "Iteration 107, loss = 0.49539270\n",
      "Iteration 108, loss = 0.49676955\n",
      "Iteration 109, loss = 0.49774852\n",
      "Iteration 110, loss = 0.49671106\n",
      "Iteration 111, loss = 0.49749595\n",
      "Iteration 112, loss = 0.49950722\n",
      "Iteration 113, loss = 0.49456820\n",
      "Iteration 114, loss = 0.48961528\n",
      "Iteration 115, loss = 0.48804765\n",
      "Iteration 116, loss = 0.48819924\n",
      "Iteration 117, loss = 0.48962442\n",
      "Iteration 118, loss = 0.49177984\n",
      "Iteration 119, loss = 0.49285986\n",
      "Iteration 120, loss = 0.49351195\n",
      "Iteration 121, loss = 0.49233314\n",
      "Iteration 122, loss = 0.48926410\n",
      "Iteration 123, loss = 0.48807454\n",
      "Iteration 124, loss = 0.49044671\n",
      "Iteration 125, loss = 0.49361305\n",
      "Iteration 126, loss = 0.49013506\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.20370455\n",
      "Iteration 2, loss = 2.02510813\n",
      "Iteration 3, loss = 1.88723626\n",
      "Iteration 4, loss = 1.77926396\n",
      "Iteration 5, loss = 1.68579799\n",
      "Iteration 6, loss = 1.60562308\n",
      "Iteration 7, loss = 1.53235261\n",
      "Iteration 8, loss = 1.46413247\n",
      "Iteration 9, loss = 1.40075528\n",
      "Iteration 10, loss = 1.34036007\n",
      "Iteration 11, loss = 1.28591206\n",
      "Iteration 12, loss = 1.23552695\n",
      "Iteration 13, loss = 1.19094515\n",
      "Iteration 14, loss = 1.14931931\n",
      "Iteration 15, loss = 1.11142015\n",
      "Iteration 16, loss = 1.07645190\n",
      "Iteration 17, loss = 1.04329949\n",
      "Iteration 18, loss = 1.01332053\n",
      "Iteration 19, loss = 0.98623664\n",
      "Iteration 20, loss = 0.96194902\n",
      "Iteration 21, loss = 0.93941304\n",
      "Iteration 22, loss = 0.91850046\n",
      "Iteration 23, loss = 0.89986770\n",
      "Iteration 24, loss = 0.88219255\n",
      "Iteration 25, loss = 0.86770405\n",
      "Iteration 26, loss = 0.84999158\n",
      "Iteration 27, loss = 0.83344960\n",
      "Iteration 28, loss = 0.82078637\n",
      "Iteration 29, loss = 0.80888218\n",
      "Iteration 30, loss = 0.79672214\n",
      "Iteration 31, loss = 0.78478995\n",
      "Iteration 32, loss = 0.77321260\n",
      "Iteration 33, loss = 0.76257336\n",
      "Iteration 34, loss = 0.75348332\n",
      "Iteration 35, loss = 0.74505913\n",
      "Iteration 36, loss = 0.73646866\n",
      "Iteration 37, loss = 0.72943278\n",
      "Iteration 38, loss = 0.72267200\n",
      "Iteration 39, loss = 0.71509818\n",
      "Iteration 40, loss = 0.70702670\n",
      "Iteration 41, loss = 0.69757779\n",
      "Iteration 42, loss = 0.68851349\n",
      "Iteration 43, loss = 0.68003810\n",
      "Iteration 44, loss = 0.67296910\n",
      "Iteration 45, loss = 0.66482636\n",
      "Iteration 46, loss = 0.65728261\n",
      "Iteration 47, loss = 0.64928391\n",
      "Iteration 48, loss = 0.64174334\n",
      "Iteration 49, loss = 0.63477904\n",
      "Iteration 50, loss = 0.62885942\n",
      "Iteration 51, loss = 0.62182565\n",
      "Iteration 52, loss = 0.61562211\n",
      "Iteration 53, loss = 0.61023332\n",
      "Iteration 54, loss = 0.60641980\n",
      "Iteration 55, loss = 0.60191580\n",
      "Iteration 56, loss = 0.59756697\n",
      "Iteration 57, loss = 0.59174199\n",
      "Iteration 58, loss = 0.58662781\n",
      "Iteration 59, loss = 0.58178756\n",
      "Iteration 60, loss = 0.57650367\n",
      "Iteration 61, loss = 0.57070311\n",
      "Iteration 62, loss = 0.56780550\n",
      "Iteration 63, loss = 0.56560280\n",
      "Iteration 64, loss = 0.56190262\n",
      "Iteration 65, loss = 0.55707369\n",
      "Iteration 66, loss = 0.55110366\n",
      "Iteration 67, loss = 0.54846830\n",
      "Iteration 68, loss = 0.54595275\n",
      "Iteration 69, loss = 0.54421405\n",
      "Iteration 70, loss = 0.54387478\n",
      "Iteration 71, loss = 0.54286946\n",
      "Iteration 72, loss = 0.54113361\n",
      "Iteration 73, loss = 0.53837310\n",
      "Iteration 74, loss = 0.53511367\n",
      "Iteration 75, loss = 0.53034831\n",
      "Iteration 76, loss = 0.52682905\n",
      "Iteration 77, loss = 0.52443352\n",
      "Iteration 78, loss = 0.52280072\n",
      "Iteration 79, loss = 0.51997456\n",
      "Iteration 80, loss = 0.51839719\n",
      "Iteration 81, loss = 0.51973064\n",
      "Iteration 82, loss = 0.51953448\n",
      "Iteration 83, loss = 0.51598922\n",
      "Iteration 84, loss = 0.51307570\n",
      "Iteration 85, loss = 0.51171827\n",
      "Iteration 86, loss = 0.51045102\n",
      "Iteration 87, loss = 0.50811614\n",
      "Iteration 88, loss = 0.50691512\n",
      "Iteration 89, loss = 0.50673674\n",
      "Iteration 90, loss = 0.50683011\n",
      "Iteration 91, loss = 0.50485835\n",
      "Iteration 92, loss = 0.50389059\n",
      "Iteration 93, loss = 0.50305392\n",
      "Iteration 94, loss = 0.50294252\n",
      "Iteration 95, loss = 0.50313086\n",
      "Iteration 96, loss = 0.50196886\n",
      "Iteration 97, loss = 0.49931093\n",
      "Iteration 98, loss = 0.49785836\n",
      "Iteration 99, loss = 0.49831008\n",
      "Iteration 100, loss = 0.49820615\n",
      "Iteration 101, loss = 0.49724334\n",
      "Iteration 102, loss = 0.49634620\n",
      "Iteration 103, loss = 0.49379707\n",
      "Iteration 104, loss = 0.49198813\n",
      "Iteration 105, loss = 0.49111498\n",
      "Iteration 106, loss = 0.48986510\n",
      "Iteration 107, loss = 0.48993085\n",
      "Iteration 108, loss = 0.49035756\n",
      "Iteration 109, loss = 0.48832916\n",
      "Iteration 110, loss = 0.48702017\n",
      "Iteration 111, loss = 0.48809078\n",
      "Iteration 112, loss = 0.49366414\n",
      "Iteration 113, loss = 0.48942805\n",
      "Iteration 114, loss = 0.48477543\n",
      "Iteration 115, loss = 0.48349025\n",
      "Iteration 116, loss = 0.48493976\n",
      "Iteration 117, loss = 0.48784732\n",
      "Iteration 118, loss = 0.49149475\n",
      "Iteration 119, loss = 0.49156093\n",
      "Iteration 120, loss = 0.49023221\n",
      "Iteration 121, loss = 0.48646295\n",
      "Iteration 122, loss = 0.48351330\n",
      "Iteration 123, loss = 0.48230904\n",
      "Iteration 124, loss = 0.48140163\n",
      "Iteration 125, loss = 0.48240172\n",
      "Iteration 126, loss = 0.48286681\n",
      "Iteration 127, loss = 0.48176779\n",
      "Iteration 128, loss = 0.48059590\n",
      "Iteration 129, loss = 0.47886055\n",
      "Iteration 130, loss = 0.47855350\n",
      "Iteration 131, loss = 0.47800634\n",
      "Iteration 132, loss = 0.47854371\n",
      "Iteration 133, loss = 0.47915685\n",
      "Iteration 134, loss = 0.48009245\n",
      "Iteration 135, loss = 0.48146655\n",
      "Iteration 136, loss = 0.48328107\n",
      "Iteration 137, loss = 0.48117272\n",
      "Iteration 138, loss = 0.47897272\n",
      "Iteration 139, loss = 0.47740803\n",
      "Iteration 140, loss = 0.47646798\n",
      "Iteration 141, loss = 0.47734405\n",
      "Iteration 142, loss = 0.48077940\n",
      "Iteration 143, loss = 0.48169158\n",
      "Iteration 144, loss = 0.48018311\n",
      "Iteration 145, loss = 0.47895597\n",
      "Iteration 146, loss = 0.47852988\n",
      "Iteration 147, loss = 0.47867002\n",
      "Iteration 148, loss = 0.47809565\n",
      "Iteration 149, loss = 0.47785827\n",
      "Iteration 150, loss = 0.47740837\n",
      "Iteration 151, loss = 0.47669344\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.20353446\n",
      "Iteration 2, loss = 2.02623034\n",
      "Iteration 3, loss = 1.88982747\n",
      "Iteration 4, loss = 1.78357517\n",
      "Iteration 5, loss = 1.69325766\n",
      "Iteration 6, loss = 1.61516730\n",
      "Iteration 7, loss = 1.54446211\n",
      "Iteration 8, loss = 1.47895928\n",
      "Iteration 9, loss = 1.41830757\n",
      "Iteration 10, loss = 1.36072081\n",
      "Iteration 11, loss = 1.30825537\n",
      "Iteration 12, loss = 1.25892093\n",
      "Iteration 13, loss = 1.21439193\n",
      "Iteration 14, loss = 1.17310151\n",
      "Iteration 15, loss = 1.13507706\n",
      "Iteration 16, loss = 1.09961733\n",
      "Iteration 17, loss = 1.06538313\n",
      "Iteration 18, loss = 1.03424744\n",
      "Iteration 19, loss = 1.00583662\n",
      "Iteration 20, loss = 0.98055003\n",
      "Iteration 21, loss = 0.95612453\n",
      "Iteration 22, loss = 0.93357386\n",
      "Iteration 23, loss = 0.91290751\n",
      "Iteration 24, loss = 0.89375935\n",
      "Iteration 25, loss = 0.87869683\n",
      "Iteration 26, loss = 0.86039762\n",
      "Iteration 27, loss = 0.84235797\n",
      "Iteration 28, loss = 0.82680334\n",
      "Iteration 29, loss = 0.81144187\n",
      "Iteration 30, loss = 0.79567599\n",
      "Iteration 31, loss = 0.78084748\n",
      "Iteration 32, loss = 0.76656595\n",
      "Iteration 33, loss = 0.75339864\n",
      "Iteration 34, loss = 0.74044098\n",
      "Iteration 35, loss = 0.72899825\n",
      "Iteration 36, loss = 0.71751527\n",
      "Iteration 37, loss = 0.70751996\n",
      "Iteration 38, loss = 0.69894768\n",
      "Iteration 39, loss = 0.69119043\n",
      "Iteration 40, loss = 0.68336958\n",
      "Iteration 41, loss = 0.67386890\n",
      "Iteration 42, loss = 0.66360214\n",
      "Iteration 43, loss = 0.65362843\n",
      "Iteration 44, loss = 0.64502081\n",
      "Iteration 45, loss = 0.63637245\n",
      "Iteration 46, loss = 0.62973585\n",
      "Iteration 47, loss = 0.62391228\n",
      "Iteration 48, loss = 0.61900558\n",
      "Iteration 49, loss = 0.61397136\n",
      "Iteration 50, loss = 0.60923071\n",
      "Iteration 51, loss = 0.60166811\n",
      "Iteration 52, loss = 0.59564250\n",
      "Iteration 53, loss = 0.59166570\n",
      "Iteration 54, loss = 0.58936496\n",
      "Iteration 55, loss = 0.58541319\n",
      "Iteration 56, loss = 0.58119143\n",
      "Iteration 57, loss = 0.57705467\n",
      "Iteration 58, loss = 0.57471452\n",
      "Iteration 59, loss = 0.57301267\n",
      "Iteration 60, loss = 0.56866699\n",
      "Iteration 61, loss = 0.56261669\n",
      "Iteration 62, loss = 0.55914799\n",
      "Iteration 63, loss = 0.55832106\n",
      "Iteration 64, loss = 0.55443119\n",
      "Iteration 65, loss = 0.54962780\n",
      "Iteration 66, loss = 0.54522291\n",
      "Iteration 67, loss = 0.54456027\n",
      "Iteration 68, loss = 0.54389403\n",
      "Iteration 69, loss = 0.54543550\n",
      "Iteration 70, loss = 0.54237901\n",
      "Iteration 71, loss = 0.53840387\n",
      "Iteration 72, loss = 0.53676486\n",
      "Iteration 73, loss = 0.53418163\n",
      "Iteration 74, loss = 0.53217816\n",
      "Iteration 75, loss = 0.52869614\n",
      "Iteration 76, loss = 0.52598258\n",
      "Iteration 77, loss = 0.52421082\n",
      "Iteration 78, loss = 0.52392772\n",
      "Iteration 79, loss = 0.52256247\n",
      "Iteration 80, loss = 0.52191363\n",
      "Iteration 81, loss = 0.52249899\n",
      "Iteration 82, loss = 0.52003048\n",
      "Iteration 83, loss = 0.51813193\n",
      "Iteration 84, loss = 0.51718776\n",
      "Iteration 85, loss = 0.51661649\n",
      "Iteration 86, loss = 0.51667877\n",
      "Iteration 87, loss = 0.51560635\n",
      "Iteration 88, loss = 0.51461531\n",
      "Iteration 89, loss = 0.51289582\n",
      "Iteration 90, loss = 0.51129085\n",
      "Iteration 91, loss = 0.50841839\n",
      "Iteration 92, loss = 0.50737251\n",
      "Iteration 93, loss = 0.50827551\n",
      "Iteration 94, loss = 0.51043500\n",
      "Iteration 95, loss = 0.51247778\n",
      "Iteration 96, loss = 0.51150300\n",
      "Iteration 97, loss = 0.50824859\n",
      "Iteration 98, loss = 0.50545379\n",
      "Iteration 99, loss = 0.50494600\n",
      "Iteration 100, loss = 0.50395438\n",
      "Iteration 101, loss = 0.50266875\n",
      "Iteration 102, loss = 0.50208206\n",
      "Iteration 103, loss = 0.50202794\n",
      "Iteration 104, loss = 0.50225572\n",
      "Iteration 105, loss = 0.50247520\n",
      "Iteration 106, loss = 0.50051592\n",
      "Iteration 107, loss = 0.49918945\n",
      "Iteration 108, loss = 0.49887044\n",
      "Iteration 109, loss = 0.49840390\n",
      "Iteration 110, loss = 0.49594624\n",
      "Iteration 111, loss = 0.49442719\n",
      "Iteration 112, loss = 0.49791970\n",
      "Iteration 113, loss = 0.49868518\n",
      "Iteration 114, loss = 0.49581970\n",
      "Iteration 115, loss = 0.49195616\n",
      "Iteration 116, loss = 0.49215608\n",
      "Iteration 117, loss = 0.49411915\n",
      "Iteration 118, loss = 0.49612522\n",
      "Iteration 119, loss = 0.49442052\n",
      "Iteration 120, loss = 0.49414049\n",
      "Iteration 121, loss = 0.49326378\n",
      "Iteration 122, loss = 0.49305536\n",
      "Iteration 123, loss = 0.49381547\n",
      "Iteration 124, loss = 0.49298182\n",
      "Iteration 125, loss = 0.49292214\n",
      "Iteration 126, loss = 0.49265548\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.19494908\n",
      "Iteration 2, loss = 2.01981377\n",
      "Iteration 3, loss = 1.88419815\n",
      "Iteration 4, loss = 1.77825683\n",
      "Iteration 5, loss = 1.68791061\n",
      "Iteration 6, loss = 1.60973664\n",
      "Iteration 7, loss = 1.53909687\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8, loss = 1.47392475\n",
      "Iteration 9, loss = 1.41346894\n",
      "Iteration 10, loss = 1.35620528\n",
      "Iteration 11, loss = 1.30392613\n",
      "Iteration 12, loss = 1.25443632\n",
      "Iteration 13, loss = 1.21017281\n",
      "Iteration 14, loss = 1.16957765\n",
      "Iteration 15, loss = 1.13138174\n",
      "Iteration 16, loss = 1.09588274\n",
      "Iteration 17, loss = 1.06225679\n",
      "Iteration 18, loss = 1.03147114\n",
      "Iteration 19, loss = 1.00340319\n",
      "Iteration 20, loss = 0.97843045\n",
      "Iteration 21, loss = 0.95486278\n",
      "Iteration 22, loss = 0.93367309\n",
      "Iteration 23, loss = 0.91499038\n",
      "Iteration 24, loss = 0.89732613\n",
      "Iteration 25, loss = 0.88339657\n",
      "Iteration 26, loss = 0.86785016\n",
      "Iteration 27, loss = 0.85267173\n",
      "Iteration 28, loss = 0.84033081\n",
      "Iteration 29, loss = 0.82863015\n",
      "Iteration 30, loss = 0.81568987\n",
      "Iteration 31, loss = 0.80403996\n",
      "Iteration 32, loss = 0.79244813\n",
      "Iteration 33, loss = 0.78152289\n",
      "Iteration 34, loss = 0.77099628\n",
      "Iteration 35, loss = 0.76177084\n",
      "Iteration 36, loss = 0.75335692\n",
      "Iteration 37, loss = 0.74619241\n",
      "Iteration 38, loss = 0.73998423\n",
      "Iteration 39, loss = 0.73323096\n",
      "Iteration 40, loss = 0.72570749\n",
      "Iteration 41, loss = 0.71751039\n",
      "Iteration 42, loss = 0.70808545\n",
      "Iteration 43, loss = 0.69913234\n",
      "Iteration 44, loss = 0.69130440\n",
      "Iteration 45, loss = 0.68209802\n",
      "Iteration 46, loss = 0.67452294\n",
      "Iteration 47, loss = 0.66734627\n",
      "Iteration 48, loss = 0.66115504\n",
      "Iteration 49, loss = 0.65532922\n",
      "Iteration 50, loss = 0.64958832\n",
      "Iteration 51, loss = 0.64195060\n",
      "Iteration 52, loss = 0.63469080\n",
      "Iteration 53, loss = 0.62902207\n",
      "Iteration 54, loss = 0.62490709\n",
      "Iteration 55, loss = 0.61890413\n",
      "Iteration 56, loss = 0.61330548\n",
      "Iteration 57, loss = 0.60506728\n",
      "Iteration 58, loss = 0.60051612\n",
      "Iteration 59, loss = 0.59879769\n",
      "Iteration 60, loss = 0.59626886\n",
      "Iteration 61, loss = 0.59014562\n",
      "Iteration 62, loss = 0.58504731\n",
      "Iteration 63, loss = 0.58159744\n",
      "Iteration 64, loss = 0.57778866\n",
      "Iteration 65, loss = 0.57295920\n",
      "Iteration 66, loss = 0.56832827\n",
      "Iteration 67, loss = 0.56574182\n",
      "Iteration 68, loss = 0.56401930\n",
      "Iteration 69, loss = 0.56197270\n",
      "Iteration 70, loss = 0.55842333\n",
      "Iteration 71, loss = 0.55421962\n",
      "Iteration 72, loss = 0.55085096\n",
      "Iteration 73, loss = 0.54744992\n",
      "Iteration 74, loss = 0.54595980\n",
      "Iteration 75, loss = 0.54311445\n",
      "Iteration 76, loss = 0.54023119\n",
      "Iteration 77, loss = 0.53678426\n",
      "Iteration 78, loss = 0.53437486\n",
      "Iteration 79, loss = 0.53137857\n",
      "Iteration 80, loss = 0.52976384\n",
      "Iteration 81, loss = 0.52806214\n",
      "Iteration 82, loss = 0.52585321\n",
      "Iteration 83, loss = 0.52394793\n",
      "Iteration 84, loss = 0.52303039\n",
      "Iteration 85, loss = 0.52266145\n",
      "Iteration 86, loss = 0.52147670\n",
      "Iteration 87, loss = 0.51936337\n",
      "Iteration 88, loss = 0.51801781\n",
      "Iteration 89, loss = 0.51665945\n",
      "Iteration 90, loss = 0.51635995\n",
      "Iteration 91, loss = 0.51446543\n",
      "Iteration 92, loss = 0.51259299\n",
      "Iteration 93, loss = 0.51130065\n",
      "Iteration 94, loss = 0.51059157\n",
      "Iteration 95, loss = 0.51046510\n",
      "Iteration 96, loss = 0.50958581\n",
      "Iteration 97, loss = 0.50780541\n",
      "Iteration 98, loss = 0.50557425\n",
      "Iteration 99, loss = 0.50548103\n",
      "Iteration 100, loss = 0.50535722\n",
      "Iteration 101, loss = 0.50722127\n",
      "Iteration 102, loss = 0.50372839\n",
      "Iteration 103, loss = 0.49898727\n",
      "Iteration 104, loss = 0.49860629\n",
      "Iteration 105, loss = 0.49999348\n",
      "Iteration 106, loss = 0.50019844\n",
      "Iteration 107, loss = 0.49957228\n",
      "Iteration 108, loss = 0.49815163\n",
      "Iteration 109, loss = 0.49599960\n",
      "Iteration 110, loss = 0.49282193\n",
      "Iteration 111, loss = 0.49015636\n",
      "Iteration 112, loss = 0.49174483\n",
      "Iteration 113, loss = 0.49176964\n",
      "Iteration 114, loss = 0.49231004\n",
      "Iteration 115, loss = 0.49175086\n",
      "Iteration 116, loss = 0.49342468\n",
      "Iteration 117, loss = 0.49542226\n",
      "Iteration 118, loss = 0.49518146\n",
      "Iteration 119, loss = 0.49295305\n",
      "Iteration 120, loss = 0.49223497\n",
      "Iteration 121, loss = 0.49023448\n",
      "Iteration 122, loss = 0.48946281\n",
      "Iteration 123, loss = 0.49174881\n",
      "Iteration 124, loss = 0.49114447\n",
      "Iteration 125, loss = 0.48839421\n",
      "Iteration 126, loss = 0.48733647\n",
      "Iteration 127, loss = 0.48656890\n",
      "Iteration 128, loss = 0.48551968\n",
      "Iteration 129, loss = 0.48480925\n",
      "Iteration 130, loss = 0.48438336\n",
      "Iteration 131, loss = 0.48395111\n",
      "Iteration 132, loss = 0.48385481\n",
      "Iteration 133, loss = 0.48355599\n",
      "Iteration 134, loss = 0.48330159\n",
      "Iteration 135, loss = 0.48284769\n",
      "Iteration 136, loss = 0.48528735\n",
      "Iteration 137, loss = 0.48456687\n",
      "Iteration 138, loss = 0.48274809\n",
      "Iteration 139, loss = 0.48027141\n",
      "Iteration 140, loss = 0.47774099\n",
      "Iteration 141, loss = 0.47651129\n",
      "Iteration 142, loss = 0.47857644\n",
      "Iteration 143, loss = 0.47990195\n",
      "Iteration 144, loss = 0.48122828\n",
      "Iteration 145, loss = 0.48335810\n",
      "Iteration 146, loss = 0.48469466\n",
      "Iteration 147, loss = 0.48410562\n",
      "Iteration 148, loss = 0.48273798\n",
      "Iteration 149, loss = 0.48214154\n",
      "Iteration 150, loss = 0.48115152\n",
      "Iteration 151, loss = 0.47894629\n",
      "Iteration 152, loss = 0.47730240\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.19694184\n",
      "Iteration 2, loss = 2.01811660\n",
      "Iteration 3, loss = 1.87827453\n",
      "Iteration 4, loss = 1.76885691\n",
      "Iteration 5, loss = 1.67836039\n",
      "Iteration 6, loss = 1.60011530\n",
      "Iteration 7, loss = 1.52841109\n",
      "Iteration 8, loss = 1.46159569\n",
      "Iteration 9, loss = 1.40000013\n",
      "Iteration 10, loss = 1.34234252\n",
      "Iteration 11, loss = 1.28986130\n",
      "Iteration 12, loss = 1.24035261\n",
      "Iteration 13, loss = 1.19562897\n",
      "Iteration 14, loss = 1.15453850\n",
      "Iteration 15, loss = 1.11566018\n",
      "Iteration 16, loss = 1.07985753\n",
      "Iteration 17, loss = 1.04638138\n",
      "Iteration 18, loss = 1.01581103\n",
      "Iteration 19, loss = 0.98803817\n",
      "Iteration 20, loss = 0.96318063\n",
      "Iteration 21, loss = 0.93911313\n",
      "Iteration 22, loss = 0.91739687\n",
      "Iteration 23, loss = 0.89801011\n",
      "Iteration 24, loss = 0.88043440\n",
      "Iteration 25, loss = 0.86552050\n",
      "Iteration 26, loss = 0.84928305\n",
      "Iteration 27, loss = 0.83312423\n",
      "Iteration 28, loss = 0.81979313\n",
      "Iteration 29, loss = 0.80686722\n",
      "Iteration 30, loss = 0.79257227\n",
      "Iteration 31, loss = 0.77963027\n",
      "Iteration 32, loss = 0.76792304\n",
      "Iteration 33, loss = 0.75685096\n",
      "Iteration 34, loss = 0.74563385\n",
      "Iteration 35, loss = 0.73554238\n",
      "Iteration 36, loss = 0.72570746\n",
      "Iteration 37, loss = 0.71701370\n",
      "Iteration 38, loss = 0.71037737\n",
      "Iteration 39, loss = 0.70392232\n",
      "Iteration 40, loss = 0.69678548\n",
      "Iteration 41, loss = 0.68692678\n",
      "Iteration 42, loss = 0.67666215\n",
      "Iteration 43, loss = 0.66681706\n",
      "Iteration 44, loss = 0.65885032\n",
      "Iteration 45, loss = 0.65111608\n",
      "Iteration 46, loss = 0.64503583\n",
      "Iteration 47, loss = 0.63829922\n",
      "Iteration 48, loss = 0.63124516\n",
      "Iteration 49, loss = 0.62401312\n",
      "Iteration 50, loss = 0.61792329\n",
      "Iteration 51, loss = 0.61085550\n",
      "Iteration 52, loss = 0.60458405\n",
      "Iteration 53, loss = 0.59916453\n",
      "Iteration 54, loss = 0.59557886\n",
      "Iteration 55, loss = 0.59067079\n",
      "Iteration 56, loss = 0.58646066\n",
      "Iteration 57, loss = 0.58082492\n",
      "Iteration 58, loss = 0.57650217\n",
      "Iteration 59, loss = 0.57322705\n",
      "Iteration 60, loss = 0.56984861\n",
      "Iteration 61, loss = 0.56405403\n",
      "Iteration 62, loss = 0.55937508\n",
      "Iteration 63, loss = 0.55648995\n",
      "Iteration 64, loss = 0.55308965\n",
      "Iteration 65, loss = 0.54785268\n",
      "Iteration 66, loss = 0.54199916\n",
      "Iteration 67, loss = 0.53836915\n",
      "Iteration 68, loss = 0.53686138\n",
      "Iteration 69, loss = 0.53695190\n",
      "Iteration 70, loss = 0.53485338\n",
      "Iteration 71, loss = 0.53057871\n",
      "Iteration 72, loss = 0.52667886\n",
      "Iteration 73, loss = 0.52237187\n",
      "Iteration 74, loss = 0.52025040\n",
      "Iteration 75, loss = 0.51848653\n",
      "Iteration 76, loss = 0.51696804\n",
      "Iteration 77, loss = 0.51518232\n",
      "Iteration 78, loss = 0.51420529\n",
      "Iteration 79, loss = 0.51193504\n",
      "Iteration 80, loss = 0.51021685\n",
      "Iteration 81, loss = 0.50919624\n",
      "Iteration 82, loss = 0.50651645\n",
      "Iteration 83, loss = 0.50538230\n",
      "Iteration 84, loss = 0.50459050\n",
      "Iteration 85, loss = 0.50379977\n",
      "Iteration 86, loss = 0.50171610\n",
      "Iteration 87, loss = 0.50005769\n",
      "Iteration 88, loss = 0.50049308\n",
      "Iteration 89, loss = 0.50023869\n",
      "Iteration 90, loss = 0.50028654\n",
      "Iteration 91, loss = 0.49934341\n",
      "Iteration 92, loss = 0.49803574\n",
      "Iteration 93, loss = 0.49776176\n",
      "Iteration 94, loss = 0.49759562\n",
      "Iteration 95, loss = 0.49766260\n",
      "Iteration 96, loss = 0.49595335\n",
      "Iteration 97, loss = 0.49473117\n",
      "Iteration 98, loss = 0.49415224\n",
      "Iteration 99, loss = 0.49428724\n",
      "Iteration 100, loss = 0.49303687\n",
      "Iteration 101, loss = 0.49460284\n",
      "Iteration 102, loss = 0.49123592\n",
      "Iteration 103, loss = 0.48995861\n",
      "Iteration 104, loss = 0.49139108\n",
      "Iteration 105, loss = 0.49323373\n",
      "Iteration 106, loss = 0.49342115\n",
      "Iteration 107, loss = 0.49260142\n",
      "Iteration 108, loss = 0.49119066\n",
      "Iteration 109, loss = 0.48959301\n",
      "Iteration 110, loss = 0.48808916\n",
      "Iteration 111, loss = 0.48638248\n",
      "Iteration 112, loss = 0.48865589\n",
      "Iteration 113, loss = 0.48999788\n",
      "Iteration 114, loss = 0.49014456\n",
      "Iteration 115, loss = 0.48860427\n",
      "Iteration 116, loss = 0.49058449\n",
      "Iteration 117, loss = 0.49222803\n",
      "Iteration 118, loss = 0.49176353\n",
      "Iteration 119, loss = 0.48818962\n",
      "Iteration 120, loss = 0.48685290\n",
      "Iteration 121, loss = 0.48606458\n",
      "Iteration 122, loss = 0.48774427\n",
      "Iteration 123, loss = 0.49080119\n",
      "Iteration 124, loss = 0.49209425\n",
      "Iteration 125, loss = 0.48774342\n",
      "Iteration 126, loss = 0.48503034\n",
      "Iteration 127, loss = 0.48529081\n",
      "Iteration 128, loss = 0.48609892\n",
      "Iteration 129, loss = 0.48536019\n",
      "Iteration 130, loss = 0.48460499\n",
      "Iteration 131, loss = 0.48518699\n",
      "Iteration 132, loss = 0.48560495\n",
      "Iteration 133, loss = 0.48465300\n",
      "Iteration 134, loss = 0.48208838\n",
      "Iteration 135, loss = 0.47959937\n",
      "Iteration 136, loss = 0.48164174\n",
      "Iteration 137, loss = 0.48246915\n",
      "Iteration 138, loss = 0.48421729\n",
      "Iteration 139, loss = 0.48395362\n",
      "Iteration 140, loss = 0.48204070\n",
      "Iteration 141, loss = 0.48119022\n",
      "Iteration 142, loss = 0.48361714\n",
      "Iteration 143, loss = 0.48374176\n",
      "Iteration 144, loss = 0.48517184\n",
      "Iteration 145, loss = 0.48830172\n",
      "Iteration 146, loss = 0.48996167\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.20721781\n",
      "Iteration 2, loss = 2.02671300\n",
      "Iteration 3, loss = 1.88646320\n",
      "Iteration 4, loss = 1.77588182\n",
      "Iteration 5, loss = 1.68525458\n",
      "Iteration 6, loss = 1.60805446\n",
      "Iteration 7, loss = 1.53875034\n",
      "Iteration 8, loss = 1.47387593\n",
      "Iteration 9, loss = 1.41308652\n",
      "Iteration 10, loss = 1.35649885\n",
      "Iteration 11, loss = 1.30462125\n",
      "Iteration 12, loss = 1.25564396\n",
      "Iteration 13, loss = 1.21090306\n",
      "Iteration 14, loss = 1.16925932\n",
      "Iteration 15, loss = 1.12986014\n",
      "Iteration 16, loss = 1.09363330\n",
      "Iteration 17, loss = 1.06056007\n",
      "Iteration 18, loss = 1.02992947\n",
      "Iteration 19, loss = 1.00124842\n",
      "Iteration 20, loss = 0.97517622\n",
      "Iteration 21, loss = 0.94994548\n",
      "Iteration 22, loss = 0.92765913\n",
      "Iteration 23, loss = 0.90777116\n",
      "Iteration 24, loss = 0.88990148\n",
      "Iteration 25, loss = 0.87438032\n",
      "Iteration 26, loss = 0.85809558\n",
      "Iteration 27, loss = 0.84246729\n",
      "Iteration 28, loss = 0.82903741\n",
      "Iteration 29, loss = 0.81551775\n",
      "Iteration 30, loss = 0.80038181\n",
      "Iteration 31, loss = 0.78589779\n",
      "Iteration 32, loss = 0.77276956\n",
      "Iteration 33, loss = 0.76096012\n",
      "Iteration 34, loss = 0.74966168\n",
      "Iteration 35, loss = 0.73920101\n",
      "Iteration 36, loss = 0.72870274\n",
      "Iteration 37, loss = 0.71906170\n",
      "Iteration 38, loss = 0.71080656\n",
      "Iteration 39, loss = 0.70203296\n",
      "Iteration 40, loss = 0.69342221\n",
      "Iteration 41, loss = 0.68378890\n",
      "Iteration 42, loss = 0.67330971\n",
      "Iteration 43, loss = 0.66407497\n",
      "Iteration 44, loss = 0.65617567\n",
      "Iteration 45, loss = 0.64656129\n",
      "Iteration 46, loss = 0.63797711\n",
      "Iteration 47, loss = 0.62943705\n",
      "Iteration 48, loss = 0.62245498\n",
      "Iteration 49, loss = 0.61661288\n",
      "Iteration 50, loss = 0.61154456\n",
      "Iteration 51, loss = 0.60529033\n",
      "Iteration 52, loss = 0.59979032\n",
      "Iteration 53, loss = 0.59528788\n",
      "Iteration 54, loss = 0.59217847\n",
      "Iteration 55, loss = 0.58683984\n",
      "Iteration 56, loss = 0.58229074\n",
      "Iteration 57, loss = 0.57723745\n",
      "Iteration 58, loss = 0.57355944\n",
      "Iteration 59, loss = 0.57253837\n",
      "Iteration 60, loss = 0.57011542\n",
      "Iteration 61, loss = 0.56490683\n",
      "Iteration 62, loss = 0.56046391\n",
      "Iteration 63, loss = 0.55810195\n",
      "Iteration 64, loss = 0.55530983\n",
      "Iteration 65, loss = 0.55181440\n",
      "Iteration 66, loss = 0.54932725\n",
      "Iteration 67, loss = 0.54753289\n",
      "Iteration 68, loss = 0.54644534\n",
      "Iteration 69, loss = 0.54420835\n",
      "Iteration 70, loss = 0.54059598\n",
      "Iteration 71, loss = 0.53661871\n",
      "Iteration 72, loss = 0.53563357\n",
      "Iteration 73, loss = 0.53322549\n",
      "Iteration 74, loss = 0.53123779\n",
      "Iteration 75, loss = 0.52842701\n",
      "Iteration 76, loss = 0.52517565\n",
      "Iteration 77, loss = 0.52202994\n",
      "Iteration 78, loss = 0.52109287\n",
      "Iteration 79, loss = 0.51828652\n",
      "Iteration 80, loss = 0.51664576\n",
      "Iteration 81, loss = 0.51607990\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 82, loss = 0.51322810\n",
      "Iteration 83, loss = 0.51060308\n",
      "Iteration 84, loss = 0.51010535\n",
      "Iteration 85, loss = 0.51015995\n",
      "Iteration 86, loss = 0.50773635\n",
      "Iteration 87, loss = 0.50560712\n",
      "Iteration 88, loss = 0.50547144\n",
      "Iteration 89, loss = 0.50372660\n",
      "Iteration 90, loss = 0.50307258\n",
      "Iteration 91, loss = 0.50264452\n",
      "Iteration 92, loss = 0.50240626\n",
      "Iteration 93, loss = 0.50206015\n",
      "Iteration 94, loss = 0.50100718\n",
      "Iteration 95, loss = 0.50016445\n",
      "Iteration 96, loss = 0.49813425\n",
      "Iteration 97, loss = 0.49552146\n",
      "Iteration 98, loss = 0.49348426\n",
      "Iteration 99, loss = 0.49342046\n",
      "Iteration 100, loss = 0.49321369\n",
      "Iteration 101, loss = 0.49442526\n",
      "Iteration 102, loss = 0.48835175\n",
      "Iteration 103, loss = 0.48525959\n",
      "Iteration 104, loss = 0.48651145\n",
      "Iteration 105, loss = 0.48807181\n",
      "Iteration 106, loss = 0.48977559\n",
      "Iteration 107, loss = 0.48969523\n",
      "Iteration 108, loss = 0.48834626\n",
      "Iteration 109, loss = 0.48537761\n",
      "Iteration 110, loss = 0.48129466\n",
      "Iteration 111, loss = 0.47844587\n",
      "Iteration 112, loss = 0.47930615\n",
      "Iteration 113, loss = 0.47998913\n",
      "Iteration 114, loss = 0.48037317\n",
      "Iteration 115, loss = 0.47908607\n",
      "Iteration 116, loss = 0.47999015\n",
      "Iteration 117, loss = 0.48037180\n",
      "Iteration 118, loss = 0.47887607\n",
      "Iteration 119, loss = 0.47696481\n",
      "Iteration 120, loss = 0.47724958\n",
      "Iteration 121, loss = 0.47837483\n",
      "Iteration 122, loss = 0.47950668\n",
      "Iteration 123, loss = 0.48281395\n",
      "Iteration 124, loss = 0.48150365\n",
      "Iteration 125, loss = 0.47638077\n",
      "Iteration 126, loss = 0.47617729\n",
      "Iteration 127, loss = 0.47843519\n",
      "Iteration 128, loss = 0.47711802\n",
      "Iteration 129, loss = 0.47328671\n",
      "Iteration 130, loss = 0.47066159\n",
      "Iteration 131, loss = 0.47069024\n",
      "Iteration 132, loss = 0.47210229\n",
      "Iteration 133, loss = 0.47212317\n",
      "Iteration 134, loss = 0.47140856\n",
      "Iteration 135, loss = 0.46939496\n",
      "Iteration 136, loss = 0.46784798\n",
      "Iteration 137, loss = 0.46736966\n",
      "Iteration 138, loss = 0.47001190\n",
      "Iteration 139, loss = 0.47181915\n",
      "Iteration 140, loss = 0.47210661\n",
      "Iteration 141, loss = 0.47105592\n",
      "Iteration 142, loss = 0.47089909\n",
      "Iteration 143, loss = 0.46868627\n",
      "Iteration 144, loss = 0.46944380\n",
      "Iteration 145, loss = 0.47277551\n",
      "Iteration 146, loss = 0.47612950\n",
      "Iteration 147, loss = 0.47639452\n",
      "Iteration 148, loss = 0.47481499\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.01047048\n",
      "Iteration 2, loss = 1.78286129\n",
      "Iteration 3, loss = 1.61176605\n",
      "Iteration 4, loss = 1.46212150\n",
      "Iteration 5, loss = 1.33275840\n",
      "Iteration 6, loss = 1.21924995\n",
      "Iteration 7, loss = 1.12362261\n",
      "Iteration 8, loss = 1.04009155\n",
      "Iteration 9, loss = 0.96585649\n",
      "Iteration 10, loss = 0.89976476\n",
      "Iteration 11, loss = 0.84005599\n",
      "Iteration 12, loss = 0.78852793\n",
      "Iteration 13, loss = 0.74419662\n",
      "Iteration 14, loss = 0.70271226\n",
      "Iteration 15, loss = 0.66570398\n",
      "Iteration 16, loss = 0.63349648\n",
      "Iteration 17, loss = 0.60586424\n",
      "Iteration 18, loss = 0.58188601\n",
      "Iteration 19, loss = 0.56033192\n",
      "Iteration 20, loss = 0.54013765\n",
      "Iteration 21, loss = 0.52293294\n",
      "Iteration 22, loss = 0.50932982\n",
      "Iteration 23, loss = 0.49725996\n",
      "Iteration 24, loss = 0.48519517\n",
      "Iteration 25, loss = 0.47687248\n",
      "Iteration 26, loss = 0.46984068\n",
      "Iteration 27, loss = 0.46136459\n",
      "Iteration 28, loss = 0.45284610\n",
      "Iteration 29, loss = 0.44645496\n",
      "Iteration 30, loss = 0.44315123\n",
      "Iteration 31, loss = 0.43936872\n",
      "Iteration 32, loss = 0.43421242\n",
      "Iteration 33, loss = 0.42782477\n",
      "Iteration 34, loss = 0.42283303\n",
      "Iteration 35, loss = 0.41892250\n",
      "Iteration 36, loss = 0.41405373\n",
      "Iteration 37, loss = 0.41015390\n",
      "Iteration 38, loss = 0.40874654\n",
      "Iteration 39, loss = 0.40839702\n",
      "Iteration 40, loss = 0.40749157\n",
      "Iteration 41, loss = 0.40785636\n",
      "Iteration 42, loss = 0.40507170\n",
      "Iteration 43, loss = 0.40294364\n",
      "Iteration 44, loss = 0.40274782\n",
      "Iteration 45, loss = 0.40057424\n",
      "Iteration 46, loss = 0.39813471\n",
      "Iteration 47, loss = 0.39835562\n",
      "Iteration 48, loss = 0.40073535\n",
      "Iteration 49, loss = 0.40014903\n",
      "Iteration 50, loss = 0.39566245\n",
      "Iteration 51, loss = 0.39382155\n",
      "Iteration 52, loss = 0.39724333\n",
      "Iteration 53, loss = 0.39888438\n",
      "Iteration 54, loss = 0.39181125\n",
      "Iteration 55, loss = 0.38806121\n",
      "Iteration 56, loss = 0.39044336\n",
      "Iteration 57, loss = 0.39574676\n",
      "Iteration 58, loss = 0.39976471\n",
      "Iteration 59, loss = 0.39958513\n",
      "Iteration 60, loss = 0.39838933\n",
      "Iteration 61, loss = 0.39806836\n",
      "Iteration 62, loss = 0.39453636\n",
      "Iteration 63, loss = 0.38918052\n",
      "Iteration 64, loss = 0.38296218\n",
      "Iteration 65, loss = 0.38916190\n",
      "Iteration 66, loss = 0.39680630\n",
      "Iteration 67, loss = 0.40011733\n",
      "Iteration 68, loss = 0.39679367\n",
      "Iteration 69, loss = 0.39327561\n",
      "Iteration 70, loss = 0.39168048\n",
      "Iteration 71, loss = 0.38854971\n",
      "Iteration 72, loss = 0.38736640\n",
      "Iteration 73, loss = 0.38972262\n",
      "Iteration 74, loss = 0.39398775\n",
      "Iteration 75, loss = 0.39151933\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.01796370\n",
      "Iteration 2, loss = 1.77797918\n",
      "Iteration 3, loss = 1.60727628\n",
      "Iteration 4, loss = 1.46324458\n",
      "Iteration 5, loss = 1.33454301\n",
      "Iteration 6, loss = 1.22262521\n",
      "Iteration 7, loss = 1.12445053\n",
      "Iteration 8, loss = 1.04023756\n",
      "Iteration 9, loss = 0.96338892\n",
      "Iteration 10, loss = 0.89597433\n",
      "Iteration 11, loss = 0.83729426\n",
      "Iteration 12, loss = 0.78576190\n",
      "Iteration 13, loss = 0.74226638\n",
      "Iteration 14, loss = 0.70503629\n",
      "Iteration 15, loss = 0.66807596\n",
      "Iteration 16, loss = 0.63535915\n",
      "Iteration 17, loss = 0.60904825\n",
      "Iteration 18, loss = 0.58659607\n",
      "Iteration 19, loss = 0.56503110\n",
      "Iteration 20, loss = 0.54544492\n",
      "Iteration 21, loss = 0.52955700\n",
      "Iteration 22, loss = 0.51661970\n",
      "Iteration 23, loss = 0.50734854\n",
      "Iteration 24, loss = 0.49721811\n",
      "Iteration 25, loss = 0.48203912\n",
      "Iteration 26, loss = 0.46917089\n",
      "Iteration 27, loss = 0.46218254\n",
      "Iteration 28, loss = 0.45538297\n",
      "Iteration 29, loss = 0.44821672\n",
      "Iteration 30, loss = 0.44279300\n",
      "Iteration 31, loss = 0.43789786\n",
      "Iteration 32, loss = 0.43407154\n",
      "Iteration 33, loss = 0.42760185\n",
      "Iteration 34, loss = 0.42562044\n",
      "Iteration 35, loss = 0.42469039\n",
      "Iteration 36, loss = 0.42247364\n",
      "Iteration 37, loss = 0.41889587\n",
      "Iteration 38, loss = 0.41465374\n",
      "Iteration 39, loss = 0.41190934\n",
      "Iteration 40, loss = 0.40780037\n",
      "Iteration 41, loss = 0.40531765\n",
      "Iteration 42, loss = 0.40409397\n",
      "Iteration 43, loss = 0.40588021\n",
      "Iteration 44, loss = 0.40427436\n",
      "Iteration 45, loss = 0.40121261\n",
      "Iteration 46, loss = 0.39832643\n",
      "Iteration 47, loss = 0.39657010\n",
      "Iteration 48, loss = 0.39152665\n",
      "Iteration 49, loss = 0.39068792\n",
      "Iteration 50, loss = 0.39526099\n",
      "Iteration 51, loss = 0.39409731\n",
      "Iteration 52, loss = 0.39472783\n",
      "Iteration 53, loss = 0.39496233\n",
      "Iteration 54, loss = 0.39678119\n",
      "Iteration 55, loss = 0.39969743\n",
      "Iteration 56, loss = 0.40039509\n",
      "Iteration 57, loss = 0.39634815\n",
      "Iteration 58, loss = 0.39320140\n",
      "Iteration 59, loss = 0.39172344\n",
      "Iteration 60, loss = 0.38952197\n",
      "Iteration 61, loss = 0.38698193\n",
      "Iteration 62, loss = 0.38564024\n",
      "Iteration 63, loss = 0.38954616\n",
      "Iteration 64, loss = 0.39343978\n",
      "Iteration 65, loss = 0.39450080\n",
      "Iteration 66, loss = 0.39191222\n",
      "Iteration 67, loss = 0.38892316\n",
      "Iteration 68, loss = 0.38992130\n",
      "Iteration 69, loss = 0.39386182\n",
      "Iteration 70, loss = 0.39688365\n",
      "Iteration 71, loss = 0.39686424\n",
      "Iteration 72, loss = 0.39389463\n",
      "Iteration 73, loss = 0.39007472\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.01491775\n",
      "Iteration 2, loss = 1.77817778\n",
      "Iteration 3, loss = 1.60887213\n",
      "Iteration 4, loss = 1.46572323\n",
      "Iteration 5, loss = 1.33732110\n",
      "Iteration 6, loss = 1.22617006\n",
      "Iteration 7, loss = 1.12764456\n",
      "Iteration 8, loss = 1.04292184\n",
      "Iteration 9, loss = 0.96658736\n",
      "Iteration 10, loss = 0.90028875\n",
      "Iteration 11, loss = 0.84202477\n",
      "Iteration 12, loss = 0.79076044\n",
      "Iteration 13, loss = 0.74602612\n",
      "Iteration 14, loss = 0.70792054\n",
      "Iteration 15, loss = 0.67183898\n",
      "Iteration 16, loss = 0.63888427\n",
      "Iteration 17, loss = 0.61203908\n",
      "Iteration 18, loss = 0.58994539\n",
      "Iteration 19, loss = 0.56884451\n",
      "Iteration 20, loss = 0.54986022\n",
      "Iteration 21, loss = 0.53555595\n",
      "Iteration 22, loss = 0.51898187\n",
      "Iteration 23, loss = 0.50261352\n",
      "Iteration 24, loss = 0.49093786\n",
      "Iteration 25, loss = 0.47922623\n",
      "Iteration 26, loss = 0.46619520\n",
      "Iteration 27, loss = 0.45736905\n",
      "Iteration 28, loss = 0.44794980\n",
      "Iteration 29, loss = 0.44094657\n",
      "Iteration 30, loss = 0.43660224\n",
      "Iteration 31, loss = 0.43276739\n",
      "Iteration 32, loss = 0.42971129\n",
      "Iteration 33, loss = 0.42512206\n",
      "Iteration 34, loss = 0.42304844\n",
      "Iteration 35, loss = 0.42039997\n",
      "Iteration 36, loss = 0.41667676\n",
      "Iteration 37, loss = 0.41379338\n",
      "Iteration 38, loss = 0.41283163\n",
      "Iteration 39, loss = 0.41158900\n",
      "Iteration 40, loss = 0.40558798\n",
      "Iteration 41, loss = 0.39955130\n",
      "Iteration 42, loss = 0.39721989\n",
      "Iteration 43, loss = 0.39749852\n",
      "Iteration 44, loss = 0.39559840\n",
      "Iteration 45, loss = 0.39291640\n",
      "Iteration 46, loss = 0.39172988\n",
      "Iteration 47, loss = 0.39212131\n",
      "Iteration 48, loss = 0.38685997\n",
      "Iteration 49, loss = 0.38483689\n",
      "Iteration 50, loss = 0.38826983\n",
      "Iteration 51, loss = 0.38875980\n",
      "Iteration 52, loss = 0.39004930\n",
      "Iteration 53, loss = 0.38849162\n",
      "Iteration 54, loss = 0.38723761\n",
      "Iteration 55, loss = 0.38733058\n",
      "Iteration 56, loss = 0.38775025\n",
      "Iteration 57, loss = 0.38678515\n",
      "Iteration 58, loss = 0.38587035\n",
      "Iteration 59, loss = 0.38414440\n",
      "Iteration 60, loss = 0.38187097\n",
      "Iteration 61, loss = 0.38037439\n",
      "Iteration 62, loss = 0.37856711\n",
      "Iteration 63, loss = 0.38172445\n",
      "Iteration 64, loss = 0.38400490\n",
      "Iteration 65, loss = 0.38595867\n",
      "Iteration 66, loss = 0.38614963\n",
      "Iteration 67, loss = 0.38541650\n",
      "Iteration 68, loss = 0.38574016\n",
      "Iteration 69, loss = 0.38703003\n",
      "Iteration 70, loss = 0.38707024\n",
      "Iteration 71, loss = 0.38581158\n",
      "Iteration 72, loss = 0.38412341\n",
      "Iteration 73, loss = 0.38328338\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.02611172\n",
      "Iteration 2, loss = 1.77774512\n",
      "Iteration 3, loss = 1.59706285\n",
      "Iteration 4, loss = 1.44655287\n",
      "Iteration 5, loss = 1.31673444\n",
      "Iteration 6, loss = 1.20932960\n",
      "Iteration 7, loss = 1.11551570\n",
      "Iteration 8, loss = 1.03642773\n",
      "Iteration 9, loss = 0.96604302\n",
      "Iteration 10, loss = 0.90566937\n",
      "Iteration 11, loss = 0.85201360\n",
      "Iteration 12, loss = 0.80443144\n",
      "Iteration 13, loss = 0.76311087\n",
      "Iteration 14, loss = 0.72735282\n",
      "Iteration 15, loss = 0.69261247\n",
      "Iteration 16, loss = 0.66033491\n",
      "Iteration 17, loss = 0.63369268\n",
      "Iteration 18, loss = 0.61168562\n",
      "Iteration 19, loss = 0.59134286\n",
      "Iteration 20, loss = 0.57294044\n",
      "Iteration 21, loss = 0.55682239\n",
      "Iteration 22, loss = 0.53870522\n",
      "Iteration 23, loss = 0.52014530\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 24, loss = 0.50759918\n",
      "Iteration 25, loss = 0.49679323\n",
      "Iteration 26, loss = 0.48424462\n",
      "Iteration 27, loss = 0.47397572\n",
      "Iteration 28, loss = 0.46418572\n",
      "Iteration 29, loss = 0.45736368\n",
      "Iteration 30, loss = 0.45312788\n",
      "Iteration 31, loss = 0.44729196\n",
      "Iteration 32, loss = 0.44171351\n",
      "Iteration 33, loss = 0.43422219\n",
      "Iteration 34, loss = 0.43148126\n",
      "Iteration 35, loss = 0.42752410\n",
      "Iteration 36, loss = 0.42349007\n",
      "Iteration 37, loss = 0.42324634\n",
      "Iteration 38, loss = 0.42072419\n",
      "Iteration 39, loss = 0.41598374\n",
      "Iteration 40, loss = 0.41022013\n",
      "Iteration 41, loss = 0.40744453\n",
      "Iteration 42, loss = 0.40395090\n",
      "Iteration 43, loss = 0.40346660\n",
      "Iteration 44, loss = 0.40387322\n",
      "Iteration 45, loss = 0.40260914\n",
      "Iteration 46, loss = 0.40150755\n",
      "Iteration 47, loss = 0.40201006\n",
      "Iteration 48, loss = 0.39748570\n",
      "Iteration 49, loss = 0.39537409\n",
      "Iteration 50, loss = 0.39710131\n",
      "Iteration 51, loss = 0.39496662\n",
      "Iteration 52, loss = 0.39387497\n",
      "Iteration 53, loss = 0.39187926\n",
      "Iteration 54, loss = 0.39195200\n",
      "Iteration 55, loss = 0.39255761\n",
      "Iteration 56, loss = 0.39339308\n",
      "Iteration 57, loss = 0.39240603\n",
      "Iteration 58, loss = 0.39063620\n",
      "Iteration 59, loss = 0.38725519\n",
      "Iteration 60, loss = 0.38506624\n",
      "Iteration 61, loss = 0.38444858\n",
      "Iteration 62, loss = 0.38333403\n",
      "Iteration 63, loss = 0.38402926\n",
      "Iteration 64, loss = 0.38639580\n",
      "Iteration 65, loss = 0.38900946\n",
      "Iteration 66, loss = 0.38752027\n",
      "Iteration 67, loss = 0.38592420\n",
      "Iteration 68, loss = 0.38833052\n",
      "Iteration 69, loss = 0.39206509\n",
      "Iteration 70, loss = 0.39380356\n",
      "Iteration 71, loss = 0.39037297\n",
      "Iteration 72, loss = 0.38477165\n",
      "Iteration 73, loss = 0.38183630\n",
      "Iteration 74, loss = 0.38297460\n",
      "Iteration 75, loss = 0.38582935\n",
      "Iteration 76, loss = 0.38625170\n",
      "Iteration 77, loss = 0.38684630\n",
      "Iteration 78, loss = 0.37916751\n",
      "Iteration 79, loss = 0.38197445\n",
      "Iteration 80, loss = 0.38577574\n",
      "Iteration 81, loss = 0.38114255\n",
      "Iteration 82, loss = 0.37851094\n",
      "Iteration 83, loss = 0.38065778\n",
      "Iteration 84, loss = 0.37984874\n",
      "Iteration 85, loss = 0.37972199\n",
      "Iteration 86, loss = 0.38068318\n",
      "Iteration 87, loss = 0.37971998\n",
      "Iteration 88, loss = 0.37596888\n",
      "Iteration 89, loss = 0.37380676\n",
      "Iteration 90, loss = 0.37273732\n",
      "Iteration 91, loss = 0.37552880\n",
      "Iteration 92, loss = 0.37836674\n",
      "Iteration 93, loss = 0.37525470\n",
      "Iteration 94, loss = 0.37453361\n",
      "Iteration 95, loss = 0.37683751\n",
      "Iteration 96, loss = 0.37767104\n",
      "Iteration 97, loss = 0.37399586\n",
      "Iteration 98, loss = 0.37372400\n",
      "Iteration 99, loss = 0.37442777\n",
      "Iteration 100, loss = 0.37696592\n",
      "Iteration 101, loss = 0.37913352\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.01853435\n",
      "Iteration 2, loss = 1.77980446\n",
      "Iteration 3, loss = 1.60487340\n",
      "Iteration 4, loss = 1.46084223\n",
      "Iteration 5, loss = 1.33662248\n",
      "Iteration 6, loss = 1.23117232\n",
      "Iteration 7, loss = 1.13570543\n",
      "Iteration 8, loss = 1.05299077\n",
      "Iteration 9, loss = 0.97905396\n",
      "Iteration 10, loss = 0.91605010\n",
      "Iteration 11, loss = 0.86022247\n",
      "Iteration 12, loss = 0.80812673\n",
      "Iteration 13, loss = 0.76259628\n",
      "Iteration 14, loss = 0.72327002\n",
      "Iteration 15, loss = 0.68759137\n",
      "Iteration 16, loss = 0.65532513\n",
      "Iteration 17, loss = 0.62681863\n",
      "Iteration 18, loss = 0.60264510\n",
      "Iteration 19, loss = 0.58222876\n",
      "Iteration 20, loss = 0.56446987\n",
      "Iteration 21, loss = 0.54930443\n",
      "Iteration 22, loss = 0.53226066\n",
      "Iteration 23, loss = 0.51515831\n",
      "Iteration 24, loss = 0.50311496\n",
      "Iteration 25, loss = 0.49304961\n",
      "Iteration 26, loss = 0.48185525\n",
      "Iteration 27, loss = 0.47225337\n",
      "Iteration 28, loss = 0.46268411\n",
      "Iteration 29, loss = 0.45601798\n",
      "Iteration 30, loss = 0.45093086\n",
      "Iteration 31, loss = 0.44573878\n",
      "Iteration 32, loss = 0.44090465\n",
      "Iteration 33, loss = 0.43512128\n",
      "Iteration 34, loss = 0.43185647\n",
      "Iteration 35, loss = 0.42838040\n",
      "Iteration 36, loss = 0.42301625\n",
      "Iteration 37, loss = 0.42129070\n",
      "Iteration 38, loss = 0.41863414\n",
      "Iteration 39, loss = 0.41562287\n",
      "Iteration 40, loss = 0.41114852\n",
      "Iteration 41, loss = 0.40824551\n",
      "Iteration 42, loss = 0.40732043\n",
      "Iteration 43, loss = 0.40826822\n",
      "Iteration 44, loss = 0.40816611\n",
      "Iteration 45, loss = 0.40533826\n",
      "Iteration 46, loss = 0.40296301\n",
      "Iteration 47, loss = 0.40282185\n",
      "Iteration 48, loss = 0.39758354\n",
      "Iteration 49, loss = 0.39779223\n",
      "Iteration 50, loss = 0.40077257\n",
      "Iteration 51, loss = 0.39917426\n",
      "Iteration 52, loss = 0.39727197\n",
      "Iteration 53, loss = 0.39345406\n",
      "Iteration 54, loss = 0.39193446\n",
      "Iteration 55, loss = 0.39107341\n",
      "Iteration 56, loss = 0.39115652\n",
      "Iteration 57, loss = 0.39091993\n",
      "Iteration 58, loss = 0.39059097\n",
      "Iteration 59, loss = 0.38669146\n",
      "Iteration 60, loss = 0.38399570\n",
      "Iteration 61, loss = 0.38310367\n",
      "Iteration 62, loss = 0.38255694\n",
      "Iteration 63, loss = 0.38555883\n",
      "Iteration 64, loss = 0.38808745\n",
      "Iteration 65, loss = 0.38867728\n",
      "Iteration 66, loss = 0.38647307\n",
      "Iteration 67, loss = 0.38605964\n",
      "Iteration 68, loss = 0.38890016\n",
      "Iteration 69, loss = 0.39221914\n",
      "Iteration 70, loss = 0.39411560\n",
      "Iteration 71, loss = 0.39330834\n",
      "Iteration 72, loss = 0.39006371\n",
      "Iteration 73, loss = 0.38646911\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.01943444\n",
      "Iteration 2, loss = 1.77939930\n",
      "Iteration 3, loss = 1.60193479\n",
      "Iteration 4, loss = 1.45359039\n",
      "Iteration 5, loss = 1.32471918\n",
      "Iteration 6, loss = 1.21676844\n",
      "Iteration 7, loss = 1.12046066\n",
      "Iteration 8, loss = 1.03782523\n",
      "Iteration 9, loss = 0.96422295\n",
      "Iteration 10, loss = 0.90080642\n",
      "Iteration 11, loss = 0.84494531\n",
      "Iteration 12, loss = 0.79291933\n",
      "Iteration 13, loss = 0.74766314\n",
      "Iteration 14, loss = 0.70854331\n",
      "Iteration 15, loss = 0.67321354\n",
      "Iteration 16, loss = 0.64174306\n",
      "Iteration 17, loss = 0.61457513\n",
      "Iteration 18, loss = 0.59182946\n",
      "Iteration 19, loss = 0.57245283\n",
      "Iteration 20, loss = 0.55561914\n",
      "Iteration 21, loss = 0.54102897\n",
      "Iteration 22, loss = 0.52405377\n",
      "Iteration 23, loss = 0.50803354\n",
      "Iteration 24, loss = 0.49699793\n",
      "Iteration 25, loss = 0.48739482\n",
      "Iteration 26, loss = 0.47677430\n",
      "Iteration 27, loss = 0.46528682\n",
      "Iteration 28, loss = 0.45477750\n",
      "Iteration 29, loss = 0.44932675\n",
      "Iteration 30, loss = 0.44670553\n",
      "Iteration 31, loss = 0.44230156\n",
      "Iteration 32, loss = 0.43664031\n",
      "Iteration 33, loss = 0.43043543\n",
      "Iteration 34, loss = 0.42481907\n",
      "Iteration 35, loss = 0.42121966\n",
      "Iteration 36, loss = 0.41796977\n",
      "Iteration 37, loss = 0.41780540\n",
      "Iteration 38, loss = 0.41558710\n",
      "Iteration 39, loss = 0.41282102\n",
      "Iteration 40, loss = 0.40909622\n",
      "Iteration 41, loss = 0.40560061\n",
      "Iteration 42, loss = 0.40537618\n",
      "Iteration 43, loss = 0.40806783\n",
      "Iteration 44, loss = 0.40729988\n",
      "Iteration 45, loss = 0.40355567\n",
      "Iteration 46, loss = 0.40144448\n",
      "Iteration 47, loss = 0.40076256\n",
      "Iteration 48, loss = 0.39979687\n",
      "Iteration 49, loss = 0.39902885\n",
      "Iteration 50, loss = 0.39757924\n",
      "Iteration 51, loss = 0.39607942\n",
      "Iteration 52, loss = 0.39513820\n",
      "Iteration 53, loss = 0.39380579\n",
      "Iteration 54, loss = 0.39343714\n",
      "Iteration 55, loss = 0.39142374\n",
      "Iteration 56, loss = 0.38776154\n",
      "Iteration 57, loss = 0.38427709\n",
      "Iteration 58, loss = 0.38378695\n",
      "Iteration 59, loss = 0.38305771\n",
      "Iteration 60, loss = 0.38277905\n",
      "Iteration 61, loss = 0.38026335\n",
      "Iteration 62, loss = 0.37861165\n",
      "Iteration 63, loss = 0.38019655\n",
      "Iteration 64, loss = 0.38103033\n",
      "Iteration 65, loss = 0.38204094\n",
      "Iteration 66, loss = 0.38344835\n",
      "Iteration 67, loss = 0.38462242\n",
      "Iteration 68, loss = 0.38493565\n",
      "Iteration 69, loss = 0.38333711\n",
      "Iteration 70, loss = 0.38102083\n",
      "Iteration 71, loss = 0.38074438\n",
      "Iteration 72, loss = 0.38059693\n",
      "Iteration 73, loss = 0.38065646\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.01289210\n",
      "Iteration 2, loss = 1.77709522\n",
      "Iteration 3, loss = 1.60309379\n",
      "Iteration 4, loss = 1.45884532\n",
      "Iteration 5, loss = 1.33413306\n",
      "Iteration 6, loss = 1.22633896\n",
      "Iteration 7, loss = 1.12938543\n",
      "Iteration 8, loss = 1.04486988\n",
      "Iteration 9, loss = 0.97027148\n",
      "Iteration 10, loss = 0.90678636\n",
      "Iteration 11, loss = 0.84893270\n",
      "Iteration 12, loss = 0.79565515\n",
      "Iteration 13, loss = 0.74727861\n",
      "Iteration 14, loss = 0.70661827\n",
      "Iteration 15, loss = 0.67252069\n",
      "Iteration 16, loss = 0.64153129\n",
      "Iteration 17, loss = 0.61419468\n",
      "Iteration 18, loss = 0.59058790\n",
      "Iteration 19, loss = 0.57132869\n",
      "Iteration 20, loss = 0.55462563\n",
      "Iteration 21, loss = 0.54080235\n",
      "Iteration 22, loss = 0.52481630\n",
      "Iteration 23, loss = 0.50850003\n",
      "Iteration 24, loss = 0.49684637\n",
      "Iteration 25, loss = 0.48785924\n",
      "Iteration 26, loss = 0.47873923\n",
      "Iteration 27, loss = 0.46743207\n",
      "Iteration 28, loss = 0.45641999\n",
      "Iteration 29, loss = 0.44921296\n",
      "Iteration 30, loss = 0.44555826\n",
      "Iteration 31, loss = 0.44089322\n",
      "Iteration 32, loss = 0.43594585\n",
      "Iteration 33, loss = 0.43008137\n",
      "Iteration 34, loss = 0.42385373\n",
      "Iteration 35, loss = 0.41820197\n",
      "Iteration 36, loss = 0.41435689\n",
      "Iteration 37, loss = 0.41467007\n",
      "Iteration 38, loss = 0.41333909\n",
      "Iteration 39, loss = 0.41157321\n",
      "Iteration 40, loss = 0.40856725\n",
      "Iteration 41, loss = 0.40237392\n",
      "Iteration 42, loss = 0.40203022\n",
      "Iteration 43, loss = 0.40630724\n",
      "Iteration 44, loss = 0.40466722\n",
      "Iteration 45, loss = 0.40013384\n",
      "Iteration 46, loss = 0.39805298\n",
      "Iteration 47, loss = 0.39697831\n",
      "Iteration 48, loss = 0.39584525\n",
      "Iteration 49, loss = 0.39463838\n",
      "Iteration 50, loss = 0.39282851\n",
      "Iteration 51, loss = 0.39229931\n",
      "Iteration 52, loss = 0.39213233\n",
      "Iteration 53, loss = 0.39049113\n",
      "Iteration 54, loss = 0.39063433\n",
      "Iteration 55, loss = 0.38797643\n",
      "Iteration 56, loss = 0.38315165\n",
      "Iteration 57, loss = 0.37929939\n",
      "Iteration 58, loss = 0.37904455\n",
      "Iteration 59, loss = 0.37923802\n",
      "Iteration 60, loss = 0.37909789\n",
      "Iteration 61, loss = 0.37544604\n",
      "Iteration 62, loss = 0.37218592\n",
      "Iteration 63, loss = 0.37287698\n",
      "Iteration 64, loss = 0.37227141\n",
      "Iteration 65, loss = 0.37434758\n",
      "Iteration 66, loss = 0.37734508\n",
      "Iteration 67, loss = 0.37858998\n",
      "Iteration 68, loss = 0.37805968\n",
      "Iteration 69, loss = 0.37438501\n",
      "Iteration 70, loss = 0.37205913\n",
      "Iteration 71, loss = 0.37445078\n",
      "Iteration 72, loss = 0.37856783\n",
      "Iteration 73, loss = 0.38030141\n",
      "Iteration 74, loss = 0.37955091\n",
      "Iteration 75, loss = 0.37923147\n",
      "Iteration 76, loss = 0.37731761\n",
      "Iteration 77, loss = 0.37597434\n",
      "Iteration 78, loss = 0.37214389\n",
      "Iteration 79, loss = 0.37237688\n",
      "Iteration 80, loss = 0.37083058\n",
      "Iteration 81, loss = 0.37060327\n",
      "Iteration 82, loss = 0.37133143\n",
      "Iteration 83, loss = 0.36888007\n",
      "Iteration 84, loss = 0.36780708\n",
      "Iteration 85, loss = 0.36984149\n",
      "Iteration 86, loss = 0.37053877\n",
      "Iteration 87, loss = 0.37227768\n",
      "Iteration 88, loss = 0.37204679\n",
      "Iteration 89, loss = 0.37309076\n",
      "Iteration 90, loss = 0.37070669\n",
      "Iteration 91, loss = 0.37132727\n",
      "Iteration 92, loss = 0.37621450\n",
      "Iteration 93, loss = 0.37671991\n",
      "Iteration 94, loss = 0.37405538\n",
      "Iteration 95, loss = 0.37369763\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.02035959\n",
      "Iteration 2, loss = 1.78571277\n",
      "Iteration 3, loss = 1.61541358\n",
      "Iteration 4, loss = 1.47987656\n",
      "Iteration 5, loss = 1.35813052\n",
      "Iteration 6, loss = 1.25303049\n",
      "Iteration 7, loss = 1.15866279\n",
      "Iteration 8, loss = 1.07692344\n",
      "Iteration 9, loss = 1.00430089\n",
      "Iteration 10, loss = 0.94106390\n",
      "Iteration 11, loss = 0.88336009\n",
      "Iteration 12, loss = 0.82960463\n",
      "Iteration 13, loss = 0.78002216\n",
      "Iteration 14, loss = 0.73822722\n",
      "Iteration 15, loss = 0.70317742\n",
      "Iteration 16, loss = 0.67053669\n",
      "Iteration 17, loss = 0.64116723\n",
      "Iteration 18, loss = 0.61611672\n",
      "Iteration 19, loss = 0.59512009\n",
      "Iteration 20, loss = 0.57676629\n",
      "Iteration 21, loss = 0.56137743\n",
      "Iteration 22, loss = 0.54508286\n",
      "Iteration 23, loss = 0.52918127\n",
      "Iteration 24, loss = 0.51746623\n",
      "Iteration 25, loss = 0.50793968\n",
      "Iteration 26, loss = 0.49871535\n",
      "Iteration 27, loss = 0.48768125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 28, loss = 0.47670408\n",
      "Iteration 29, loss = 0.46995590\n",
      "Iteration 30, loss = 0.46599973\n",
      "Iteration 31, loss = 0.46056850\n",
      "Iteration 32, loss = 0.45628888\n",
      "Iteration 33, loss = 0.45106186\n",
      "Iteration 34, loss = 0.44545297\n",
      "Iteration 35, loss = 0.44073161\n",
      "Iteration 36, loss = 0.43534720\n",
      "Iteration 37, loss = 0.43528823\n",
      "Iteration 38, loss = 0.43327272\n",
      "Iteration 39, loss = 0.43041518\n",
      "Iteration 40, loss = 0.42677405\n",
      "Iteration 41, loss = 0.42416385\n",
      "Iteration 42, loss = 0.42312040\n",
      "Iteration 43, loss = 0.42512862\n",
      "Iteration 44, loss = 0.42514762\n",
      "Iteration 45, loss = 0.42283890\n",
      "Iteration 46, loss = 0.42218695\n",
      "Iteration 47, loss = 0.42273352\n",
      "Iteration 48, loss = 0.42108353\n",
      "Iteration 49, loss = 0.41855185\n",
      "Iteration 50, loss = 0.41466088\n",
      "Iteration 51, loss = 0.41146167\n",
      "Iteration 52, loss = 0.40918063\n",
      "Iteration 53, loss = 0.40822306\n",
      "Iteration 54, loss = 0.41079038\n",
      "Iteration 55, loss = 0.40886862\n",
      "Iteration 56, loss = 0.40313032\n",
      "Iteration 57, loss = 0.39831108\n",
      "Iteration 58, loss = 0.39754900\n",
      "Iteration 59, loss = 0.39960859\n",
      "Iteration 60, loss = 0.40084897\n",
      "Iteration 61, loss = 0.39647122\n",
      "Iteration 62, loss = 0.39275450\n",
      "Iteration 63, loss = 0.39366217\n",
      "Iteration 64, loss = 0.39423201\n",
      "Iteration 65, loss = 0.39089079\n",
      "Iteration 66, loss = 0.39158495\n",
      "Iteration 67, loss = 0.39438816\n",
      "Iteration 68, loss = 0.40018754\n",
      "Iteration 69, loss = 0.39763484\n",
      "Iteration 70, loss = 0.39631099\n",
      "Iteration 71, loss = 0.39847564\n",
      "Iteration 72, loss = 0.40042156\n",
      "Iteration 73, loss = 0.39946376\n",
      "Iteration 74, loss = 0.39751584\n",
      "Iteration 75, loss = 0.39646139\n",
      "Iteration 76, loss = 0.39409980\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.01739670\n",
      "Iteration 2, loss = 1.77992930\n",
      "Iteration 3, loss = 1.60309445\n",
      "Iteration 4, loss = 1.45859362\n",
      "Iteration 5, loss = 1.33085749\n",
      "Iteration 6, loss = 1.22069810\n",
      "Iteration 7, loss = 1.12323905\n",
      "Iteration 8, loss = 1.03812002\n",
      "Iteration 9, loss = 0.96380854\n",
      "Iteration 10, loss = 0.89987976\n",
      "Iteration 11, loss = 0.84292197\n",
      "Iteration 12, loss = 0.79135912\n",
      "Iteration 13, loss = 0.74308020\n",
      "Iteration 14, loss = 0.70303990\n",
      "Iteration 15, loss = 0.67050013\n",
      "Iteration 16, loss = 0.64128289\n",
      "Iteration 17, loss = 0.61516227\n",
      "Iteration 18, loss = 0.59164573\n",
      "Iteration 19, loss = 0.57164651\n",
      "Iteration 20, loss = 0.55437232\n",
      "Iteration 21, loss = 0.54058511\n",
      "Iteration 22, loss = 0.52558846\n",
      "Iteration 23, loss = 0.51127280\n",
      "Iteration 24, loss = 0.50098593\n",
      "Iteration 25, loss = 0.49186896\n",
      "Iteration 26, loss = 0.48234639\n",
      "Iteration 27, loss = 0.47164468\n",
      "Iteration 28, loss = 0.46109423\n",
      "Iteration 29, loss = 0.45491424\n",
      "Iteration 30, loss = 0.45271674\n",
      "Iteration 31, loss = 0.44797424\n",
      "Iteration 32, loss = 0.44282769\n",
      "Iteration 33, loss = 0.43717764\n",
      "Iteration 34, loss = 0.43199794\n",
      "Iteration 35, loss = 0.42805669\n",
      "Iteration 36, loss = 0.42403271\n",
      "Iteration 37, loss = 0.42492163\n",
      "Iteration 38, loss = 0.42404568\n",
      "Iteration 39, loss = 0.42090305\n",
      "Iteration 40, loss = 0.41624734\n",
      "Iteration 41, loss = 0.41185793\n",
      "Iteration 42, loss = 0.41349493\n",
      "Iteration 43, loss = 0.41810232\n",
      "Iteration 44, loss = 0.41753437\n",
      "Iteration 45, loss = 0.41373076\n",
      "Iteration 46, loss = 0.41203179\n",
      "Iteration 47, loss = 0.41237499\n",
      "Iteration 48, loss = 0.41217282\n",
      "Iteration 49, loss = 0.41129412\n",
      "Iteration 50, loss = 0.40821543\n",
      "Iteration 51, loss = 0.40560179\n",
      "Iteration 52, loss = 0.40287966\n",
      "Iteration 53, loss = 0.40050205\n",
      "Iteration 54, loss = 0.40097492\n",
      "Iteration 55, loss = 0.40052164\n",
      "Iteration 56, loss = 0.39527776\n",
      "Iteration 57, loss = 0.38998552\n",
      "Iteration 58, loss = 0.38842219\n",
      "Iteration 59, loss = 0.39041548\n",
      "Iteration 60, loss = 0.39214679\n",
      "Iteration 61, loss = 0.38903377\n",
      "Iteration 62, loss = 0.38586972\n",
      "Iteration 63, loss = 0.38615932\n",
      "Iteration 64, loss = 0.38602751\n",
      "Iteration 65, loss = 0.38267682\n",
      "Iteration 66, loss = 0.38348437\n",
      "Iteration 67, loss = 0.38484344\n",
      "Iteration 68, loss = 0.38754518\n",
      "Iteration 69, loss = 0.38586735\n",
      "Iteration 70, loss = 0.38656293\n",
      "Iteration 71, loss = 0.38970298\n",
      "Iteration 72, loss = 0.39178251\n",
      "Iteration 73, loss = 0.39042219\n",
      "Iteration 74, loss = 0.38854849\n",
      "Iteration 75, loss = 0.38785320\n",
      "Iteration 76, loss = 0.38602001\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.00569159\n",
      "Iteration 2, loss = 1.77124136\n",
      "Iteration 3, loss = 1.59676354\n",
      "Iteration 4, loss = 1.45404748\n",
      "Iteration 5, loss = 1.32753208\n",
      "Iteration 6, loss = 1.21909992\n",
      "Iteration 7, loss = 1.12251765\n",
      "Iteration 8, loss = 1.03719067\n",
      "Iteration 9, loss = 0.96169646\n",
      "Iteration 10, loss = 0.89620494\n",
      "Iteration 11, loss = 0.83729976\n",
      "Iteration 12, loss = 0.78533764\n",
      "Iteration 13, loss = 0.73781149\n",
      "Iteration 14, loss = 0.69783120\n",
      "Iteration 15, loss = 0.66458074\n",
      "Iteration 16, loss = 0.63439658\n",
      "Iteration 17, loss = 0.60712274\n",
      "Iteration 18, loss = 0.58294763\n",
      "Iteration 19, loss = 0.56213566\n",
      "Iteration 20, loss = 0.54431661\n",
      "Iteration 21, loss = 0.53027679\n",
      "Iteration 22, loss = 0.51534640\n",
      "Iteration 23, loss = 0.50093408\n",
      "Iteration 24, loss = 0.48973444\n",
      "Iteration 25, loss = 0.48097000\n",
      "Iteration 26, loss = 0.47346681\n",
      "Iteration 27, loss = 0.46455964\n",
      "Iteration 28, loss = 0.45431697\n",
      "Iteration 29, loss = 0.44761924\n",
      "Iteration 30, loss = 0.44493598\n",
      "Iteration 31, loss = 0.44254455\n",
      "Iteration 32, loss = 0.43899507\n",
      "Iteration 33, loss = 0.43348169\n",
      "Iteration 34, loss = 0.42591129\n",
      "Iteration 35, loss = 0.41977793\n",
      "Iteration 36, loss = 0.41599947\n",
      "Iteration 37, loss = 0.41925832\n",
      "Iteration 38, loss = 0.42140176\n",
      "Iteration 39, loss = 0.41937088\n",
      "Iteration 40, loss = 0.41215728\n",
      "Iteration 41, loss = 0.40539789\n",
      "Iteration 42, loss = 0.40698701\n",
      "Iteration 43, loss = 0.41350749\n",
      "Iteration 44, loss = 0.41477384\n",
      "Iteration 45, loss = 0.41102575\n",
      "Iteration 46, loss = 0.40783399\n",
      "Iteration 47, loss = 0.40585516\n",
      "Iteration 48, loss = 0.40483599\n",
      "Iteration 49, loss = 0.40509470\n",
      "Iteration 50, loss = 0.40224824\n",
      "Iteration 51, loss = 0.39843451\n",
      "Iteration 52, loss = 0.39451483\n",
      "Iteration 53, loss = 0.39209618\n",
      "Iteration 54, loss = 0.39407937\n",
      "Iteration 55, loss = 0.39449877\n",
      "Iteration 56, loss = 0.39082149\n",
      "Iteration 57, loss = 0.38658916\n",
      "Iteration 58, loss = 0.38497050\n",
      "Iteration 59, loss = 0.38479117\n",
      "Iteration 60, loss = 0.38777389\n",
      "Iteration 61, loss = 0.38521910\n",
      "Iteration 62, loss = 0.38331699\n",
      "Iteration 63, loss = 0.38347663\n",
      "Iteration 64, loss = 0.38417220\n",
      "Iteration 65, loss = 0.37862425\n",
      "Iteration 66, loss = 0.37762205\n",
      "Iteration 67, loss = 0.37746224\n",
      "Iteration 68, loss = 0.38171694\n",
      "Iteration 69, loss = 0.38231276\n",
      "Iteration 70, loss = 0.38302460\n",
      "Iteration 71, loss = 0.38423505\n",
      "Iteration 72, loss = 0.38381539\n",
      "Iteration 73, loss = 0.38215683\n",
      "Iteration 74, loss = 0.38172723\n",
      "Iteration 75, loss = 0.38306838\n",
      "Iteration 76, loss = 0.38245456\n",
      "Iteration 77, loss = 0.38069038\n",
      "Iteration 78, loss = 0.37814314\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.12918566\n",
      "Iteration 2, loss = 1.79400828\n",
      "Iteration 3, loss = 1.58205941\n",
      "Iteration 4, loss = 1.42707475\n",
      "Iteration 5, loss = 1.29604642\n",
      "Iteration 6, loss = 1.17849567\n",
      "Iteration 7, loss = 1.07363212\n",
      "Iteration 8, loss = 0.98267675\n",
      "Iteration 9, loss = 0.90163001\n",
      "Iteration 10, loss = 0.83310754\n",
      "Iteration 11, loss = 0.77411351\n",
      "Iteration 12, loss = 0.72211780\n",
      "Iteration 13, loss = 0.67606151\n",
      "Iteration 14, loss = 0.63675120\n",
      "Iteration 15, loss = 0.60223072\n",
      "Iteration 16, loss = 0.57066005\n",
      "Iteration 17, loss = 0.54341547\n",
      "Iteration 18, loss = 0.51758545\n",
      "Iteration 19, loss = 0.49597399\n",
      "Iteration 20, loss = 0.47859632\n",
      "Iteration 21, loss = 0.46376617\n",
      "Iteration 22, loss = 0.45088200\n",
      "Iteration 23, loss = 0.43685426\n",
      "Iteration 24, loss = 0.42408952\n",
      "Iteration 25, loss = 0.41304581\n",
      "Iteration 26, loss = 0.40616310\n",
      "Iteration 27, loss = 0.40030135\n",
      "Iteration 28, loss = 0.39530944\n",
      "Iteration 29, loss = 0.39147639\n",
      "Iteration 30, loss = 0.38445650\n",
      "Iteration 31, loss = 0.37969730\n",
      "Iteration 32, loss = 0.37645732\n",
      "Iteration 33, loss = 0.37514664\n",
      "Iteration 34, loss = 0.37289659\n",
      "Iteration 35, loss = 0.37063745\n",
      "Iteration 36, loss = 0.37017326\n",
      "Iteration 37, loss = 0.37073321\n",
      "Iteration 38, loss = 0.36512967\n",
      "Iteration 39, loss = 0.36151912\n",
      "Iteration 40, loss = 0.36100751\n",
      "Iteration 41, loss = 0.36142758\n",
      "Iteration 42, loss = 0.36975928\n",
      "Iteration 43, loss = 0.37074237\n",
      "Iteration 44, loss = 0.36601441\n",
      "Iteration 45, loss = 0.35805914\n",
      "Iteration 46, loss = 0.35717955\n",
      "Iteration 47, loss = 0.36387137\n",
      "Iteration 48, loss = 0.36635383\n",
      "Iteration 49, loss = 0.36615811\n",
      "Iteration 50, loss = 0.35856879\n",
      "Iteration 51, loss = 0.35384239\n",
      "Iteration 52, loss = 0.35643504\n",
      "Iteration 53, loss = 0.35877381\n",
      "Iteration 54, loss = 0.36310709\n",
      "Iteration 55, loss = 0.36595349\n",
      "Iteration 56, loss = 0.36460993\n",
      "Iteration 57, loss = 0.35861421\n",
      "Iteration 58, loss = 0.35336908\n",
      "Iteration 59, loss = 0.35142608\n",
      "Iteration 60, loss = 0.35102628\n",
      "Iteration 61, loss = 0.35377599\n",
      "Iteration 62, loss = 0.35314145\n",
      "Iteration 63, loss = 0.35225743\n",
      "Iteration 64, loss = 0.35284761\n",
      "Iteration 65, loss = 0.34905240\n",
      "Iteration 66, loss = 0.34676793\n",
      "Iteration 67, loss = 0.34694603\n",
      "Iteration 68, loss = 0.34509613\n",
      "Iteration 69, loss = 0.34333904\n",
      "Iteration 70, loss = 0.34158276\n",
      "Iteration 71, loss = 0.34290830\n",
      "Iteration 72, loss = 0.34630078\n",
      "Iteration 73, loss = 0.34478894\n",
      "Iteration 74, loss = 0.34539611\n",
      "Iteration 75, loss = 0.34598840\n",
      "Iteration 76, loss = 0.34606854\n",
      "Iteration 77, loss = 0.34527221\n",
      "Iteration 78, loss = 0.34248637\n",
      "Iteration 79, loss = 0.34135973\n",
      "Iteration 80, loss = 0.34132485\n",
      "Iteration 81, loss = 0.34275980\n",
      "Iteration 82, loss = 0.34303004\n",
      "Iteration 83, loss = 0.34307099\n",
      "Iteration 84, loss = 0.34142599\n",
      "Iteration 85, loss = 0.33527210\n",
      "Iteration 86, loss = 0.33296477\n",
      "Iteration 87, loss = 0.33667576\n",
      "Iteration 88, loss = 0.34301554\n",
      "Iteration 89, loss = 0.34731275\n",
      "Iteration 90, loss = 0.34958648\n",
      "Iteration 91, loss = 0.34861530\n",
      "Iteration 92, loss = 0.34429564\n",
      "Iteration 93, loss = 0.34732290\n",
      "Iteration 94, loss = 0.34945024\n",
      "Iteration 95, loss = 0.34645907\n",
      "Iteration 96, loss = 0.34361475\n",
      "Iteration 97, loss = 0.34166426\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.12457787\n",
      "Iteration 2, loss = 1.78872944\n",
      "Iteration 3, loss = 1.58155468\n",
      "Iteration 4, loss = 1.43314286\n",
      "Iteration 5, loss = 1.30198431\n",
      "Iteration 6, loss = 1.18426073\n",
      "Iteration 7, loss = 1.07621274\n",
      "Iteration 8, loss = 0.97875877\n",
      "Iteration 9, loss = 0.89556196\n",
      "Iteration 10, loss = 0.82674396\n",
      "Iteration 11, loss = 0.77016173\n",
      "Iteration 12, loss = 0.71971635\n",
      "Iteration 13, loss = 0.67794533\n",
      "Iteration 14, loss = 0.63835665\n",
      "Iteration 15, loss = 0.60634994\n",
      "Iteration 16, loss = 0.57608359\n",
      "Iteration 17, loss = 0.55015460\n",
      "Iteration 18, loss = 0.52486187\n",
      "Iteration 19, loss = 0.50432869\n",
      "Iteration 20, loss = 0.48785689\n",
      "Iteration 21, loss = 0.47247315\n",
      "Iteration 22, loss = 0.45800365\n",
      "Iteration 23, loss = 0.44556148\n",
      "Iteration 24, loss = 0.43306819\n",
      "Iteration 25, loss = 0.42155694\n",
      "Iteration 26, loss = 0.41524378\n",
      "Iteration 27, loss = 0.40934595\n",
      "Iteration 28, loss = 0.40549715\n",
      "Iteration 29, loss = 0.39987395\n",
      "Iteration 30, loss = 0.39440610\n",
      "Iteration 31, loss = 0.39020981\n",
      "Iteration 32, loss = 0.38289269\n",
      "Iteration 33, loss = 0.37731046\n",
      "Iteration 34, loss = 0.37478450\n",
      "Iteration 35, loss = 0.36710942\n",
      "Iteration 36, loss = 0.36612003\n",
      "Iteration 37, loss = 0.36328075\n",
      "Iteration 38, loss = 0.36288513\n",
      "Iteration 39, loss = 0.36206610\n",
      "Iteration 40, loss = 0.36045082\n",
      "Iteration 41, loss = 0.35462788\n",
      "Iteration 42, loss = 0.35028097\n",
      "Iteration 43, loss = 0.34892560\n",
      "Iteration 44, loss = 0.35159362\n",
      "Iteration 45, loss = 0.35595728\n",
      "Iteration 46, loss = 0.35678837\n",
      "Iteration 47, loss = 0.35483574\n",
      "Iteration 48, loss = 0.35288113\n",
      "Iteration 49, loss = 0.34994497\n",
      "Iteration 50, loss = 0.34711525\n",
      "Iteration 51, loss = 0.34388158\n",
      "Iteration 52, loss = 0.34229917\n",
      "Iteration 53, loss = 0.34231619\n",
      "Iteration 54, loss = 0.34005764\n",
      "Iteration 55, loss = 0.33769550\n",
      "Iteration 56, loss = 0.33843205\n",
      "Iteration 57, loss = 0.34296355\n",
      "Iteration 58, loss = 0.34063086\n",
      "Iteration 59, loss = 0.33841658\n",
      "Iteration 60, loss = 0.33911836\n",
      "Iteration 61, loss = 0.34002447\n",
      "Iteration 62, loss = 0.34023595\n",
      "Iteration 63, loss = 0.33927577\n",
      "Iteration 64, loss = 0.33816035\n",
      "Iteration 65, loss = 0.33879610\n",
      "Iteration 66, loss = 0.34180164\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.12721914\n",
      "Iteration 2, loss = 1.79301591\n",
      "Iteration 3, loss = 1.58792489\n",
      "Iteration 4, loss = 1.44043460\n",
      "Iteration 5, loss = 1.30991371\n",
      "Iteration 6, loss = 1.18911769\n",
      "Iteration 7, loss = 1.07883803\n",
      "Iteration 8, loss = 0.97837130\n",
      "Iteration 9, loss = 0.89314607\n",
      "Iteration 10, loss = 0.82100468\n",
      "Iteration 11, loss = 0.76175450\n",
      "Iteration 12, loss = 0.70865068\n",
      "Iteration 13, loss = 0.66380909\n",
      "Iteration 14, loss = 0.62181349\n",
      "Iteration 15, loss = 0.58919382\n",
      "Iteration 16, loss = 0.55880515\n",
      "Iteration 17, loss = 0.53249399\n",
      "Iteration 18, loss = 0.50720653\n",
      "Iteration 19, loss = 0.48750659\n",
      "Iteration 20, loss = 0.47187630\n",
      "Iteration 21, loss = 0.45745176\n",
      "Iteration 22, loss = 0.44522397\n",
      "Iteration 23, loss = 0.43500542\n",
      "Iteration 24, loss = 0.42164217\n",
      "Iteration 25, loss = 0.40984128\n",
      "Iteration 26, loss = 0.40372474\n",
      "Iteration 27, loss = 0.39877134\n",
      "Iteration 28, loss = 0.39452297\n",
      "Iteration 29, loss = 0.38945044\n",
      "Iteration 30, loss = 0.38546713\n",
      "Iteration 31, loss = 0.38312044\n",
      "Iteration 32, loss = 0.37756841\n",
      "Iteration 33, loss = 0.37345649\n",
      "Iteration 34, loss = 0.37137825\n",
      "Iteration 35, loss = 0.36451130\n",
      "Iteration 36, loss = 0.36298441\n",
      "Iteration 37, loss = 0.35935918\n",
      "Iteration 38, loss = 0.35653025\n",
      "Iteration 39, loss = 0.35316437\n",
      "Iteration 40, loss = 0.35342223\n",
      "Iteration 41, loss = 0.35085074\n",
      "Iteration 42, loss = 0.34764614\n",
      "Iteration 43, loss = 0.34914928\n",
      "Iteration 44, loss = 0.35279825\n",
      "Iteration 45, loss = 0.35546326\n",
      "Iteration 46, loss = 0.35567016\n",
      "Iteration 47, loss = 0.35386507\n",
      "Iteration 48, loss = 0.35238551\n",
      "Iteration 49, loss = 0.35088268\n",
      "Iteration 50, loss = 0.34735030\n",
      "Iteration 51, loss = 0.34524532\n",
      "Iteration 52, loss = 0.34386005\n",
      "Iteration 53, loss = 0.34315845\n",
      "Iteration 54, loss = 0.34117111\n",
      "Iteration 55, loss = 0.33889354\n",
      "Iteration 56, loss = 0.33884312\n",
      "Iteration 57, loss = 0.34395993\n",
      "Iteration 58, loss = 0.34292675\n",
      "Iteration 59, loss = 0.33976535\n",
      "Iteration 60, loss = 0.34000837\n",
      "Iteration 61, loss = 0.34075198\n",
      "Iteration 62, loss = 0.34147773\n",
      "Iteration 63, loss = 0.34007961\n",
      "Iteration 64, loss = 0.33768389\n",
      "Iteration 65, loss = 0.33772177\n",
      "Iteration 66, loss = 0.34097587\n",
      "Iteration 67, loss = 0.34687393\n",
      "Iteration 68, loss = 0.34795127\n",
      "Iteration 69, loss = 0.34773422\n",
      "Iteration 70, loss = 0.34609766\n",
      "Iteration 71, loss = 0.34382742\n",
      "Iteration 72, loss = 0.34240452\n",
      "Iteration 73, loss = 0.34235314\n",
      "Iteration 74, loss = 0.34080043\n",
      "Iteration 75, loss = 0.33938608\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.13891581\n",
      "Iteration 2, loss = 1.79603054\n",
      "Iteration 3, loss = 1.58034166\n",
      "Iteration 4, loss = 1.42536833\n",
      "Iteration 5, loss = 1.29079047\n",
      "Iteration 6, loss = 1.17033207\n",
      "Iteration 7, loss = 1.06312950\n",
      "Iteration 8, loss = 0.96714722\n",
      "Iteration 9, loss = 0.88485126\n",
      "Iteration 10, loss = 0.81498287\n",
      "Iteration 11, loss = 0.75511603\n",
      "Iteration 12, loss = 0.70387644\n",
      "Iteration 13, loss = 0.66060859\n",
      "Iteration 14, loss = 0.62089214\n",
      "Iteration 15, loss = 0.58868911\n",
      "Iteration 16, loss = 0.55857137\n",
      "Iteration 17, loss = 0.53309398\n",
      "Iteration 18, loss = 0.50784641\n",
      "Iteration 19, loss = 0.48742828\n",
      "Iteration 20, loss = 0.47098903\n",
      "Iteration 21, loss = 0.45773987\n",
      "Iteration 22, loss = 0.44674177\n",
      "Iteration 23, loss = 0.43842923\n",
      "Iteration 24, loss = 0.42555708\n",
      "Iteration 25, loss = 0.41013597\n",
      "Iteration 26, loss = 0.40037384\n",
      "Iteration 27, loss = 0.39518999\n",
      "Iteration 28, loss = 0.39241947\n",
      "Iteration 29, loss = 0.38749715\n",
      "Iteration 30, loss = 0.38256077\n",
      "Iteration 31, loss = 0.37770878\n",
      "Iteration 32, loss = 0.37173241\n",
      "Iteration 33, loss = 0.36743023\n",
      "Iteration 34, loss = 0.36468993\n",
      "Iteration 35, loss = 0.35847185\n",
      "Iteration 36, loss = 0.35642927\n",
      "Iteration 37, loss = 0.35415915\n",
      "Iteration 38, loss = 0.35386429\n",
      "Iteration 39, loss = 0.35242732\n",
      "Iteration 40, loss = 0.35134876\n",
      "Iteration 41, loss = 0.34745551\n",
      "Iteration 42, loss = 0.34408336\n",
      "Iteration 43, loss = 0.34248879\n",
      "Iteration 44, loss = 0.34338559\n",
      "Iteration 45, loss = 0.34476081\n",
      "Iteration 46, loss = 0.34645606\n",
      "Iteration 47, loss = 0.34630287\n",
      "Iteration 48, loss = 0.34313170\n",
      "Iteration 49, loss = 0.34036694\n",
      "Iteration 50, loss = 0.33767474\n",
      "Iteration 51, loss = 0.33633500\n",
      "Iteration 52, loss = 0.33632063\n",
      "Iteration 53, loss = 0.33745442\n",
      "Iteration 54, loss = 0.33787714\n",
      "Iteration 55, loss = 0.33569612\n",
      "Iteration 56, loss = 0.33457603\n",
      "Iteration 57, loss = 0.33608947\n",
      "Iteration 58, loss = 0.33243477\n",
      "Iteration 59, loss = 0.33121804\n",
      "Iteration 60, loss = 0.33213750\n",
      "Iteration 61, loss = 0.33268130\n",
      "Iteration 62, loss = 0.33204637\n",
      "Iteration 63, loss = 0.33031223\n",
      "Iteration 64, loss = 0.32841811\n",
      "Iteration 65, loss = 0.32996068\n",
      "Iteration 66, loss = 0.33465733\n",
      "Iteration 67, loss = 0.33951200\n",
      "Iteration 68, loss = 0.34043119\n",
      "Iteration 69, loss = 0.33812651\n",
      "Iteration 70, loss = 0.33400414\n",
      "Iteration 71, loss = 0.33035165\n",
      "Iteration 72, loss = 0.32880125\n",
      "Iteration 73, loss = 0.32785241\n",
      "Iteration 74, loss = 0.32601248\n",
      "Iteration 75, loss = 0.32512000\n",
      "Iteration 76, loss = 0.32204687\n",
      "Iteration 77, loss = 0.32232085\n",
      "Iteration 78, loss = 0.32617845\n",
      "Iteration 79, loss = 0.32945770\n",
      "Iteration 80, loss = 0.33041641\n",
      "Iteration 81, loss = 0.32754867\n",
      "Iteration 82, loss = 0.32657734\n",
      "Iteration 83, loss = 0.32482919\n",
      "Iteration 84, loss = 0.32573321\n",
      "Iteration 85, loss = 0.32463686\n",
      "Iteration 86, loss = 0.33014271\n",
      "Iteration 87, loss = 0.33325149\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.13272222\n",
      "Iteration 2, loss = 1.79852685\n",
      "Iteration 3, loss = 1.58966380\n",
      "Iteration 4, loss = 1.43759354\n",
      "Iteration 5, loss = 1.30683614\n",
      "Iteration 6, loss = 1.18902065\n",
      "Iteration 7, loss = 1.08267701\n",
      "Iteration 8, loss = 0.98613345\n",
      "Iteration 9, loss = 0.90165405\n",
      "Iteration 10, loss = 0.82919221\n",
      "Iteration 11, loss = 0.76840334\n",
      "Iteration 12, loss = 0.71597098\n",
      "Iteration 13, loss = 0.67132707\n",
      "Iteration 14, loss = 0.62996246\n",
      "Iteration 15, loss = 0.59667621\n",
      "Iteration 16, loss = 0.56509984\n",
      "Iteration 17, loss = 0.53855228\n",
      "Iteration 18, loss = 0.51155830\n",
      "Iteration 19, loss = 0.49007886\n",
      "Iteration 20, loss = 0.47325556\n",
      "Iteration 21, loss = 0.45934519\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 22, loss = 0.44776211\n",
      "Iteration 23, loss = 0.43883454\n",
      "Iteration 24, loss = 0.42525966\n",
      "Iteration 25, loss = 0.40968587\n",
      "Iteration 26, loss = 0.40141906\n",
      "Iteration 27, loss = 0.39828115\n",
      "Iteration 28, loss = 0.39589345\n",
      "Iteration 29, loss = 0.39116161\n",
      "Iteration 30, loss = 0.38455782\n",
      "Iteration 31, loss = 0.37902689\n",
      "Iteration 32, loss = 0.37376979\n",
      "Iteration 33, loss = 0.37109225\n",
      "Iteration 34, loss = 0.37171444\n",
      "Iteration 35, loss = 0.36703286\n",
      "Iteration 36, loss = 0.36568305\n",
      "Iteration 37, loss = 0.36312543\n",
      "Iteration 38, loss = 0.36112462\n",
      "Iteration 39, loss = 0.35697610\n",
      "Iteration 40, loss = 0.35519235\n",
      "Iteration 41, loss = 0.35363472\n",
      "Iteration 42, loss = 0.35000430\n",
      "Iteration 43, loss = 0.34901417\n",
      "Iteration 44, loss = 0.35190165\n",
      "Iteration 45, loss = 0.35493211\n",
      "Iteration 46, loss = 0.35732278\n",
      "Iteration 47, loss = 0.35556727\n",
      "Iteration 48, loss = 0.35290849\n",
      "Iteration 49, loss = 0.34863040\n",
      "Iteration 50, loss = 0.34433495\n",
      "Iteration 51, loss = 0.34109393\n",
      "Iteration 52, loss = 0.33936417\n",
      "Iteration 53, loss = 0.33880726\n",
      "Iteration 54, loss = 0.33834136\n",
      "Iteration 55, loss = 0.33831544\n",
      "Iteration 56, loss = 0.33827680\n",
      "Iteration 57, loss = 0.33886416\n",
      "Iteration 58, loss = 0.33659185\n",
      "Iteration 59, loss = 0.33583958\n",
      "Iteration 60, loss = 0.33593713\n",
      "Iteration 61, loss = 0.33583695\n",
      "Iteration 62, loss = 0.33631010\n",
      "Iteration 63, loss = 0.33494585\n",
      "Iteration 64, loss = 0.33272020\n",
      "Iteration 65, loss = 0.33289201\n",
      "Iteration 66, loss = 0.33581594\n",
      "Iteration 67, loss = 0.33940521\n",
      "Iteration 68, loss = 0.34179802\n",
      "Iteration 69, loss = 0.34139964\n",
      "Iteration 70, loss = 0.33830595\n",
      "Iteration 71, loss = 0.33581816\n",
      "Iteration 72, loss = 0.33613711\n",
      "Iteration 73, loss = 0.33506169\n",
      "Iteration 74, loss = 0.33256840\n",
      "Iteration 75, loss = 0.33182898\n",
      "Iteration 76, loss = 0.33012984\n",
      "Iteration 77, loss = 0.33055587\n",
      "Iteration 78, loss = 0.33374362\n",
      "Iteration 79, loss = 0.33692919\n",
      "Iteration 80, loss = 0.33728289\n",
      "Iteration 81, loss = 0.33518463\n",
      "Iteration 82, loss = 0.33376172\n",
      "Iteration 83, loss = 0.33120525\n",
      "Iteration 84, loss = 0.33629867\n",
      "Iteration 85, loss = 0.33374171\n",
      "Iteration 86, loss = 0.33477228\n",
      "Iteration 87, loss = 0.33526434\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.12295718\n",
      "Iteration 2, loss = 1.78812189\n",
      "Iteration 3, loss = 1.57791998\n",
      "Iteration 4, loss = 1.42213634\n",
      "Iteration 5, loss = 1.28818705\n",
      "Iteration 6, loss = 1.16988330\n",
      "Iteration 7, loss = 1.06550226\n",
      "Iteration 8, loss = 0.97281854\n",
      "Iteration 9, loss = 0.89234466\n",
      "Iteration 10, loss = 0.82369691\n",
      "Iteration 11, loss = 0.76600679\n",
      "Iteration 12, loss = 0.71579314\n",
      "Iteration 13, loss = 0.67216464\n",
      "Iteration 14, loss = 0.63208564\n",
      "Iteration 15, loss = 0.59711779\n",
      "Iteration 16, loss = 0.56604933\n",
      "Iteration 17, loss = 0.54024258\n",
      "Iteration 18, loss = 0.51373965\n",
      "Iteration 19, loss = 0.49265729\n",
      "Iteration 20, loss = 0.47492478\n",
      "Iteration 21, loss = 0.46051744\n",
      "Iteration 22, loss = 0.44827230\n",
      "Iteration 23, loss = 0.43756703\n",
      "Iteration 24, loss = 0.42351591\n",
      "Iteration 25, loss = 0.41064932\n",
      "Iteration 26, loss = 0.40174557\n",
      "Iteration 27, loss = 0.39320296\n",
      "Iteration 28, loss = 0.38813950\n",
      "Iteration 29, loss = 0.38392891\n",
      "Iteration 30, loss = 0.37871833\n",
      "Iteration 31, loss = 0.37535798\n",
      "Iteration 32, loss = 0.37151804\n",
      "Iteration 33, loss = 0.36749788\n",
      "Iteration 34, loss = 0.36349394\n",
      "Iteration 35, loss = 0.35717247\n",
      "Iteration 36, loss = 0.35594925\n",
      "Iteration 37, loss = 0.35371430\n",
      "Iteration 38, loss = 0.35367906\n",
      "Iteration 39, loss = 0.35061015\n",
      "Iteration 40, loss = 0.34743556\n",
      "Iteration 41, loss = 0.34569773\n",
      "Iteration 42, loss = 0.34234163\n",
      "Iteration 43, loss = 0.33945068\n",
      "Iteration 44, loss = 0.34143112\n",
      "Iteration 45, loss = 0.34439281\n",
      "Iteration 46, loss = 0.34808747\n",
      "Iteration 47, loss = 0.34923568\n",
      "Iteration 48, loss = 0.34809553\n",
      "Iteration 49, loss = 0.34507416\n",
      "Iteration 50, loss = 0.34323703\n",
      "Iteration 51, loss = 0.33789867\n",
      "Iteration 52, loss = 0.33228869\n",
      "Iteration 53, loss = 0.33357737\n",
      "Iteration 54, loss = 0.33321260\n",
      "Iteration 55, loss = 0.33724365\n",
      "Iteration 56, loss = 0.33839324\n",
      "Iteration 57, loss = 0.33737679\n",
      "Iteration 58, loss = 0.33367828\n",
      "Iteration 59, loss = 0.33159853\n",
      "Iteration 60, loss = 0.33085443\n",
      "Iteration 61, loss = 0.33089547\n",
      "Iteration 62, loss = 0.33419680\n",
      "Iteration 63, loss = 0.33587479\n",
      "Iteration 64, loss = 0.33432650\n",
      "Iteration 65, loss = 0.33245398\n",
      "Iteration 66, loss = 0.33141918\n",
      "Iteration 67, loss = 0.33034951\n",
      "Iteration 68, loss = 0.33197056\n",
      "Iteration 69, loss = 0.33081567\n",
      "Iteration 70, loss = 0.32888184\n",
      "Iteration 71, loss = 0.32902358\n",
      "Iteration 72, loss = 0.33087611\n",
      "Iteration 73, loss = 0.33032649\n",
      "Iteration 74, loss = 0.32741495\n",
      "Iteration 75, loss = 0.32485536\n",
      "Iteration 76, loss = 0.32189541\n",
      "Iteration 77, loss = 0.32150955\n",
      "Iteration 78, loss = 0.32466675\n",
      "Iteration 79, loss = 0.32947774\n",
      "Iteration 80, loss = 0.33137813\n",
      "Iteration 81, loss = 0.33039537\n",
      "Iteration 82, loss = 0.32950304\n",
      "Iteration 83, loss = 0.32669605\n",
      "Iteration 84, loss = 0.32586128\n",
      "Iteration 85, loss = 0.32485169\n",
      "Iteration 86, loss = 0.32243633\n",
      "Iteration 87, loss = 0.32353413\n",
      "Iteration 88, loss = 0.32365311\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.13298601\n",
      "Iteration 2, loss = 1.79454967\n",
      "Iteration 3, loss = 1.58470973\n",
      "Iteration 4, loss = 1.43216632\n",
      "Iteration 5, loss = 1.30469595\n",
      "Iteration 6, loss = 1.19008291\n",
      "Iteration 7, loss = 1.08735973\n",
      "Iteration 8, loss = 0.99430365\n",
      "Iteration 9, loss = 0.91284213\n",
      "Iteration 10, loss = 0.84320282\n",
      "Iteration 11, loss = 0.78423114\n",
      "Iteration 12, loss = 0.73216863\n",
      "Iteration 13, loss = 0.68626594\n",
      "Iteration 14, loss = 0.64341073\n",
      "Iteration 15, loss = 0.60540465\n",
      "Iteration 16, loss = 0.57205608\n",
      "Iteration 17, loss = 0.54503973\n",
      "Iteration 18, loss = 0.51742440\n",
      "Iteration 19, loss = 0.49473465\n",
      "Iteration 20, loss = 0.47460343\n",
      "Iteration 21, loss = 0.45827562\n",
      "Iteration 22, loss = 0.44546681\n",
      "Iteration 23, loss = 0.43585290\n",
      "Iteration 24, loss = 0.42254244\n",
      "Iteration 25, loss = 0.40921863\n",
      "Iteration 26, loss = 0.39963362\n",
      "Iteration 27, loss = 0.39209392\n",
      "Iteration 28, loss = 0.38424272\n",
      "Iteration 29, loss = 0.38090924\n",
      "Iteration 30, loss = 0.37846547\n",
      "Iteration 31, loss = 0.37541863\n",
      "Iteration 32, loss = 0.37044217\n",
      "Iteration 33, loss = 0.36671072\n",
      "Iteration 34, loss = 0.36481242\n",
      "Iteration 35, loss = 0.36175577\n",
      "Iteration 36, loss = 0.36162867\n",
      "Iteration 37, loss = 0.35865768\n",
      "Iteration 38, loss = 0.35624111\n",
      "Iteration 39, loss = 0.35334976\n",
      "Iteration 40, loss = 0.35226732\n",
      "Iteration 41, loss = 0.35149199\n",
      "Iteration 42, loss = 0.34854791\n",
      "Iteration 43, loss = 0.34854167\n",
      "Iteration 44, loss = 0.34905708\n",
      "Iteration 45, loss = 0.35120778\n",
      "Iteration 46, loss = 0.35160306\n",
      "Iteration 47, loss = 0.35069710\n",
      "Iteration 48, loss = 0.35176098\n",
      "Iteration 49, loss = 0.35292024\n",
      "Iteration 50, loss = 0.35327965\n",
      "Iteration 51, loss = 0.34881738\n",
      "Iteration 52, loss = 0.34152644\n",
      "Iteration 53, loss = 0.33962174\n",
      "Iteration 54, loss = 0.33809049\n",
      "Iteration 55, loss = 0.34184860\n",
      "Iteration 56, loss = 0.34470229\n",
      "Iteration 57, loss = 0.34284209\n",
      "Iteration 58, loss = 0.33766482\n",
      "Iteration 59, loss = 0.34051929\n",
      "Iteration 60, loss = 0.34457629\n",
      "Iteration 61, loss = 0.34629253\n",
      "Iteration 62, loss = 0.34679328\n",
      "Iteration 63, loss = 0.34443149\n",
      "Iteration 64, loss = 0.33940278\n",
      "Iteration 65, loss = 0.33770567\n",
      "Iteration 66, loss = 0.33928604\n",
      "Iteration 67, loss = 0.33858164\n",
      "Iteration 68, loss = 0.33867170\n",
      "Iteration 69, loss = 0.33654633\n",
      "Iteration 70, loss = 0.33460285\n",
      "Iteration 71, loss = 0.33631611\n",
      "Iteration 72, loss = 0.34048473\n",
      "Iteration 73, loss = 0.34272195\n",
      "Iteration 74, loss = 0.34003824\n",
      "Iteration 75, loss = 0.33660592\n",
      "Iteration 76, loss = 0.33183289\n",
      "Iteration 77, loss = 0.33014504\n",
      "Iteration 78, loss = 0.33228948\n",
      "Iteration 79, loss = 0.33525055\n",
      "Iteration 80, loss = 0.33643595\n",
      "Iteration 81, loss = 0.33435264\n",
      "Iteration 82, loss = 0.33238040\n",
      "Iteration 83, loss = 0.32941981\n",
      "Iteration 84, loss = 0.33012983\n",
      "Iteration 85, loss = 0.33094071\n",
      "Iteration 86, loss = 0.33064644\n",
      "Iteration 87, loss = 0.33157569\n",
      "Iteration 88, loss = 0.33069811\n",
      "Iteration 89, loss = 0.32860164\n",
      "Iteration 90, loss = 0.32804241\n",
      "Iteration 91, loss = 0.32898899\n",
      "Iteration 92, loss = 0.33028164\n",
      "Iteration 93, loss = 0.33007912\n",
      "Iteration 94, loss = 0.32795365\n",
      "Iteration 95, loss = 0.32692627\n",
      "Iteration 96, loss = 0.32634993\n",
      "Iteration 97, loss = 0.32692138\n",
      "Iteration 98, loss = 0.32799545\n",
      "Iteration 99, loss = 0.33143452\n",
      "Iteration 100, loss = 0.33100868\n",
      "Iteration 101, loss = 0.33030350\n",
      "Iteration 102, loss = 0.32868830\n",
      "Iteration 103, loss = 0.33087546\n",
      "Iteration 104, loss = 0.33503612\n",
      "Iteration 105, loss = 0.33850699\n",
      "Iteration 106, loss = 0.33540128\n",
      "Iteration 107, loss = 0.33030877\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.13732389\n",
      "Iteration 2, loss = 1.80058848\n",
      "Iteration 3, loss = 1.58936978\n",
      "Iteration 4, loss = 1.44014250\n",
      "Iteration 5, loss = 1.31541797\n",
      "Iteration 6, loss = 1.20191058\n",
      "Iteration 7, loss = 1.09986363\n",
      "Iteration 8, loss = 1.00603946\n",
      "Iteration 9, loss = 0.92386547\n",
      "Iteration 10, loss = 0.85343604\n",
      "Iteration 11, loss = 0.79393991\n",
      "Iteration 12, loss = 0.74219005\n",
      "Iteration 13, loss = 0.69692817\n",
      "Iteration 14, loss = 0.65407545\n",
      "Iteration 15, loss = 0.61581511\n",
      "Iteration 16, loss = 0.58310778\n",
      "Iteration 17, loss = 0.55773796\n",
      "Iteration 18, loss = 0.53212453\n",
      "Iteration 19, loss = 0.51027725\n",
      "Iteration 20, loss = 0.49016114\n",
      "Iteration 21, loss = 0.47251687\n",
      "Iteration 22, loss = 0.45845765\n",
      "Iteration 23, loss = 0.44829914\n",
      "Iteration 24, loss = 0.43585945\n",
      "Iteration 25, loss = 0.42351821\n",
      "Iteration 26, loss = 0.41550995\n",
      "Iteration 27, loss = 0.40839521\n",
      "Iteration 28, loss = 0.39949428\n",
      "Iteration 29, loss = 0.39542468\n",
      "Iteration 30, loss = 0.39512055\n",
      "Iteration 31, loss = 0.39447074\n",
      "Iteration 32, loss = 0.38710140\n",
      "Iteration 33, loss = 0.37949923\n",
      "Iteration 34, loss = 0.37459344\n",
      "Iteration 35, loss = 0.36879151\n",
      "Iteration 36, loss = 0.36965712\n",
      "Iteration 37, loss = 0.36811799\n",
      "Iteration 38, loss = 0.36720634\n",
      "Iteration 39, loss = 0.36643364\n",
      "Iteration 40, loss = 0.36545129\n",
      "Iteration 41, loss = 0.36352701\n",
      "Iteration 42, loss = 0.35709644\n",
      "Iteration 43, loss = 0.35312214\n",
      "Iteration 44, loss = 0.35325049\n",
      "Iteration 45, loss = 0.35757559\n",
      "Iteration 46, loss = 0.36160156\n",
      "Iteration 47, loss = 0.36345331\n",
      "Iteration 48, loss = 0.36241015\n",
      "Iteration 49, loss = 0.36133236\n",
      "Iteration 50, loss = 0.36024815\n",
      "Iteration 51, loss = 0.35806942\n",
      "Iteration 52, loss = 0.35380860\n",
      "Iteration 53, loss = 0.35528299\n",
      "Iteration 54, loss = 0.35107431\n",
      "Iteration 55, loss = 0.34932483\n",
      "Iteration 56, loss = 0.34952218\n",
      "Iteration 57, loss = 0.34638758\n",
      "Iteration 58, loss = 0.34502877\n",
      "Iteration 59, loss = 0.34911089\n",
      "Iteration 60, loss = 0.35403260\n",
      "Iteration 61, loss = 0.35229608\n",
      "Iteration 62, loss = 0.35534449\n",
      "Iteration 63, loss = 0.35622700\n",
      "Iteration 64, loss = 0.35058002\n",
      "Iteration 65, loss = 0.35139491\n",
      "Iteration 66, loss = 0.35711272\n",
      "Iteration 67, loss = 0.35535593\n",
      "Iteration 68, loss = 0.35022012\n",
      "Iteration 69, loss = 0.34447296\n",
      "Iteration 70, loss = 0.33986963\n",
      "Iteration 71, loss = 0.33917792\n",
      "Iteration 72, loss = 0.34297985\n",
      "Iteration 73, loss = 0.34749816\n",
      "Iteration 74, loss = 0.34920069\n",
      "Iteration 75, loss = 0.34907362\n",
      "Iteration 76, loss = 0.34281343\n",
      "Iteration 77, loss = 0.34495749\n",
      "Iteration 78, loss = 0.35175310\n",
      "Iteration 79, loss = 0.35639147\n",
      "Iteration 80, loss = 0.35326195\n",
      "Iteration 81, loss = 0.34532895\n",
      "Iteration 82, loss = 0.34251407\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.13063617\n",
      "Iteration 2, loss = 1.79270384\n",
      "Iteration 3, loss = 1.57923181\n",
      "Iteration 4, loss = 1.42606630\n",
      "Iteration 5, loss = 1.29926341\n",
      "Iteration 6, loss = 1.18500977\n",
      "Iteration 7, loss = 1.08364029\n",
      "Iteration 8, loss = 0.99236226\n",
      "Iteration 9, loss = 0.91378561\n",
      "Iteration 10, loss = 0.84766023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11, loss = 0.79324295\n",
      "Iteration 12, loss = 0.74514284\n",
      "Iteration 13, loss = 0.70178848\n",
      "Iteration 14, loss = 0.66023581\n",
      "Iteration 15, loss = 0.62246469\n",
      "Iteration 16, loss = 0.58913981\n",
      "Iteration 17, loss = 0.56259034\n",
      "Iteration 18, loss = 0.53550210\n",
      "Iteration 19, loss = 0.51350750\n",
      "Iteration 20, loss = 0.49354505\n",
      "Iteration 21, loss = 0.47657937\n",
      "Iteration 22, loss = 0.46243667\n",
      "Iteration 23, loss = 0.45094468\n",
      "Iteration 24, loss = 0.43722791\n",
      "Iteration 25, loss = 0.42463953\n",
      "Iteration 26, loss = 0.41668886\n",
      "Iteration 27, loss = 0.40929338\n",
      "Iteration 28, loss = 0.40228790\n",
      "Iteration 29, loss = 0.39582371\n",
      "Iteration 30, loss = 0.39309603\n",
      "Iteration 31, loss = 0.39103096\n",
      "Iteration 32, loss = 0.38411783\n",
      "Iteration 33, loss = 0.37777907\n",
      "Iteration 34, loss = 0.37398305\n",
      "Iteration 35, loss = 0.37025408\n",
      "Iteration 36, loss = 0.37065716\n",
      "Iteration 37, loss = 0.36754610\n",
      "Iteration 38, loss = 0.36603041\n",
      "Iteration 39, loss = 0.36352299\n",
      "Iteration 40, loss = 0.36206603\n",
      "Iteration 41, loss = 0.36017267\n",
      "Iteration 42, loss = 0.35498812\n",
      "Iteration 43, loss = 0.34906607\n",
      "Iteration 44, loss = 0.34800794\n",
      "Iteration 45, loss = 0.35239196\n",
      "Iteration 46, loss = 0.35592107\n",
      "Iteration 47, loss = 0.35877129\n",
      "Iteration 48, loss = 0.36000834\n",
      "Iteration 49, loss = 0.35791596\n",
      "Iteration 50, loss = 0.35612247\n",
      "Iteration 51, loss = 0.35517766\n",
      "Iteration 52, loss = 0.35141942\n",
      "Iteration 53, loss = 0.35116422\n",
      "Iteration 54, loss = 0.34969216\n",
      "Iteration 55, loss = 0.35168552\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.13725508\n",
      "Iteration 2, loss = 1.80141789\n",
      "Iteration 3, loss = 1.58830415\n",
      "Iteration 4, loss = 1.43491048\n",
      "Iteration 5, loss = 1.30826906\n",
      "Iteration 6, loss = 1.19469611\n",
      "Iteration 7, loss = 1.09259707\n",
      "Iteration 8, loss = 1.00024803\n",
      "Iteration 9, loss = 0.91920930\n",
      "Iteration 10, loss = 0.85058584\n",
      "Iteration 11, loss = 0.79425178\n",
      "Iteration 12, loss = 0.74385226\n",
      "Iteration 13, loss = 0.69906172\n",
      "Iteration 14, loss = 0.65610676\n",
      "Iteration 15, loss = 0.61701120\n",
      "Iteration 16, loss = 0.58232685\n",
      "Iteration 17, loss = 0.55445395\n",
      "Iteration 18, loss = 0.52637553\n",
      "Iteration 19, loss = 0.50229806\n",
      "Iteration 20, loss = 0.47995642\n",
      "Iteration 21, loss = 0.46157887\n",
      "Iteration 22, loss = 0.44703787\n",
      "Iteration 23, loss = 0.43495280\n",
      "Iteration 24, loss = 0.42157139\n",
      "Iteration 25, loss = 0.40948665\n",
      "Iteration 26, loss = 0.40186282\n",
      "Iteration 27, loss = 0.39412197\n",
      "Iteration 28, loss = 0.38677731\n",
      "Iteration 29, loss = 0.38056505\n",
      "Iteration 30, loss = 0.37883420\n",
      "Iteration 31, loss = 0.37892776\n",
      "Iteration 32, loss = 0.37489379\n",
      "Iteration 33, loss = 0.36960894\n",
      "Iteration 34, loss = 0.36529794\n",
      "Iteration 35, loss = 0.36115588\n",
      "Iteration 36, loss = 0.36089881\n",
      "Iteration 37, loss = 0.35940541\n",
      "Iteration 38, loss = 0.36130094\n",
      "Iteration 39, loss = 0.35629048\n",
      "Iteration 40, loss = 0.35386199\n",
      "Iteration 41, loss = 0.35254186\n",
      "Iteration 42, loss = 0.34851062\n",
      "Iteration 43, loss = 0.34366528\n",
      "Iteration 44, loss = 0.34324728\n",
      "Iteration 45, loss = 0.34710393\n",
      "Iteration 46, loss = 0.35007292\n",
      "Iteration 47, loss = 0.35147329\n",
      "Iteration 48, loss = 0.35018404\n",
      "Iteration 49, loss = 0.34752341\n",
      "Iteration 50, loss = 0.34846356\n",
      "Iteration 51, loss = 0.34890471\n",
      "Iteration 52, loss = 0.34484855\n",
      "Iteration 53, loss = 0.34231856\n",
      "Iteration 54, loss = 0.34196430\n",
      "Iteration 55, loss = 0.34317050\n",
      "Iteration 56, loss = 0.34423844\n",
      "Iteration 57, loss = 0.33921037\n",
      "Iteration 58, loss = 0.33459852\n",
      "Iteration 59, loss = 0.33547528\n",
      "Iteration 60, loss = 0.33774960\n",
      "Iteration 61, loss = 0.33797555\n",
      "Iteration 62, loss = 0.34390079\n",
      "Iteration 63, loss = 0.34646559\n",
      "Iteration 64, loss = 0.34213582\n",
      "Iteration 65, loss = 0.34103850\n",
      "Iteration 66, loss = 0.34076903\n",
      "Iteration 67, loss = 0.33805203\n",
      "Iteration 68, loss = 0.33555462\n",
      "Iteration 69, loss = 0.33194205\n",
      "Iteration 70, loss = 0.32814829\n",
      "Iteration 71, loss = 0.32816394\n",
      "Iteration 72, loss = 0.33091673\n",
      "Iteration 73, loss = 0.33710934\n",
      "Iteration 74, loss = 0.34084936\n",
      "Iteration 75, loss = 0.34154804\n",
      "Iteration 76, loss = 0.33689770\n",
      "Iteration 77, loss = 0.33725181\n",
      "Iteration 78, loss = 0.34084296\n",
      "Iteration 79, loss = 0.34143277\n",
      "Iteration 80, loss = 0.33548526\n",
      "Iteration 81, loss = 0.33111219\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.02409856\n",
      "Iteration 2, loss = 1.69832000\n",
      "Iteration 3, loss = 1.49345390\n",
      "Iteration 4, loss = 1.32561054\n",
      "Iteration 5, loss = 1.18009159\n",
      "Iteration 6, loss = 1.04396684\n",
      "Iteration 7, loss = 0.92822163\n",
      "Iteration 8, loss = 0.83527398\n",
      "Iteration 9, loss = 0.76074987\n",
      "Iteration 10, loss = 0.70051889\n",
      "Iteration 11, loss = 0.64939009\n",
      "Iteration 12, loss = 0.60524086\n",
      "Iteration 13, loss = 0.56445326\n",
      "Iteration 14, loss = 0.52770737\n",
      "Iteration 15, loss = 0.49556246\n",
      "Iteration 16, loss = 0.47106641\n",
      "Iteration 17, loss = 0.45289683\n",
      "Iteration 18, loss = 0.43626186\n",
      "Iteration 19, loss = 0.42450462\n",
      "Iteration 20, loss = 0.41322623\n",
      "Iteration 21, loss = 0.39901311\n",
      "Iteration 22, loss = 0.38492570\n",
      "Iteration 23, loss = 0.37682100\n",
      "Iteration 24, loss = 0.36948482\n",
      "Iteration 25, loss = 0.36067663\n",
      "Iteration 26, loss = 0.35706484\n",
      "Iteration 27, loss = 0.35692217\n",
      "Iteration 28, loss = 0.35780009\n",
      "Iteration 29, loss = 0.35847786\n",
      "Iteration 30, loss = 0.35187393\n",
      "Iteration 31, loss = 0.34811804\n",
      "Iteration 32, loss = 0.34016136\n",
      "Iteration 33, loss = 0.34144899\n",
      "Iteration 34, loss = 0.34600726\n",
      "Iteration 35, loss = 0.34252538\n",
      "Iteration 36, loss = 0.34390996\n",
      "Iteration 37, loss = 0.33533807\n",
      "Iteration 38, loss = 0.32600726\n",
      "Iteration 39, loss = 0.32376498\n",
      "Iteration 40, loss = 0.32470014\n",
      "Iteration 41, loss = 0.32592298\n",
      "Iteration 42, loss = 0.32467356\n",
      "Iteration 43, loss = 0.32311232\n",
      "Iteration 44, loss = 0.32351809\n",
      "Iteration 45, loss = 0.32710116\n",
      "Iteration 46, loss = 0.33340188\n",
      "Iteration 47, loss = 0.33355609\n",
      "Iteration 48, loss = 0.33148602\n",
      "Iteration 49, loss = 0.32373426\n",
      "Iteration 50, loss = 0.32198669\n",
      "Iteration 51, loss = 0.32586334\n",
      "Iteration 52, loss = 0.33224700\n",
      "Iteration 53, loss = 0.32618879\n",
      "Iteration 54, loss = 0.32132412\n",
      "Iteration 55, loss = 0.31827319\n",
      "Iteration 56, loss = 0.31984989\n",
      "Iteration 57, loss = 0.32355765\n",
      "Iteration 58, loss = 0.32568018\n",
      "Iteration 59, loss = 0.32498329\n",
      "Iteration 60, loss = 0.31869991\n",
      "Iteration 61, loss = 0.31759614\n",
      "Iteration 62, loss = 0.32250347\n",
      "Iteration 63, loss = 0.32013552\n",
      "Iteration 64, loss = 0.31531908\n",
      "Iteration 65, loss = 0.31133569\n",
      "Iteration 66, loss = 0.31137963\n",
      "Iteration 67, loss = 0.31382925\n",
      "Iteration 68, loss = 0.31949794\n",
      "Iteration 69, loss = 0.31528366\n",
      "Iteration 70, loss = 0.31413815\n",
      "Iteration 71, loss = 0.31935955\n",
      "Iteration 72, loss = 0.32478298\n",
      "Iteration 73, loss = 0.32386501\n",
      "Iteration 74, loss = 0.31836243\n",
      "Iteration 75, loss = 0.31264158\n",
      "Iteration 76, loss = 0.31145487\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.02959530\n",
      "Iteration 2, loss = 1.68577923\n",
      "Iteration 3, loss = 1.44741881\n",
      "Iteration 4, loss = 1.26027305\n",
      "Iteration 5, loss = 1.09573521\n",
      "Iteration 6, loss = 0.96182212\n",
      "Iteration 7, loss = 0.85312247\n",
      "Iteration 8, loss = 0.76903521\n",
      "Iteration 9, loss = 0.70229601\n",
      "Iteration 10, loss = 0.64745661\n",
      "Iteration 11, loss = 0.60240995\n",
      "Iteration 12, loss = 0.56266344\n",
      "Iteration 13, loss = 0.52779732\n",
      "Iteration 14, loss = 0.49922100\n",
      "Iteration 15, loss = 0.47499118\n",
      "Iteration 16, loss = 0.45472213\n",
      "Iteration 17, loss = 0.43426317\n",
      "Iteration 18, loss = 0.41510715\n",
      "Iteration 19, loss = 0.39944089\n",
      "Iteration 20, loss = 0.39057709\n",
      "Iteration 21, loss = 0.37923291\n",
      "Iteration 22, loss = 0.37227428\n",
      "Iteration 23, loss = 0.36694642\n",
      "Iteration 24, loss = 0.35955071\n",
      "Iteration 25, loss = 0.35409705\n",
      "Iteration 26, loss = 0.34970288\n",
      "Iteration 27, loss = 0.34489962\n",
      "Iteration 28, loss = 0.34111727\n",
      "Iteration 29, loss = 0.33874003\n",
      "Iteration 30, loss = 0.33775695\n",
      "Iteration 31, loss = 0.33635238\n",
      "Iteration 32, loss = 0.33805497\n",
      "Iteration 33, loss = 0.33670783\n",
      "Iteration 34, loss = 0.32966351\n",
      "Iteration 35, loss = 0.33087097\n",
      "Iteration 36, loss = 0.33470633\n",
      "Iteration 37, loss = 0.33494938\n",
      "Iteration 38, loss = 0.33450646\n",
      "Iteration 39, loss = 0.33028013\n",
      "Iteration 40, loss = 0.32774760\n",
      "Iteration 41, loss = 0.32751794\n",
      "Iteration 42, loss = 0.32809422\n",
      "Iteration 43, loss = 0.33063418\n",
      "Iteration 44, loss = 0.33175926\n",
      "Iteration 45, loss = 0.32500698\n",
      "Iteration 46, loss = 0.32105324\n",
      "Iteration 47, loss = 0.31910154\n",
      "Iteration 48, loss = 0.31910848\n",
      "Iteration 49, loss = 0.32714482\n",
      "Iteration 50, loss = 0.33583966\n",
      "Iteration 51, loss = 0.33039606\n",
      "Iteration 52, loss = 0.32086371\n",
      "Iteration 53, loss = 0.31964777\n",
      "Iteration 54, loss = 0.32750217\n",
      "Iteration 55, loss = 0.32372518\n",
      "Iteration 56, loss = 0.31518808\n",
      "Iteration 57, loss = 0.31245451\n",
      "Iteration 58, loss = 0.31133410\n",
      "Iteration 59, loss = 0.31171796\n",
      "Iteration 60, loss = 0.31290508\n",
      "Iteration 61, loss = 0.31274048\n",
      "Iteration 62, loss = 0.31297735\n",
      "Iteration 63, loss = 0.31226780\n",
      "Iteration 64, loss = 0.31278459\n",
      "Iteration 65, loss = 0.31390379\n",
      "Iteration 66, loss = 0.31492588\n",
      "Iteration 67, loss = 0.31372634\n",
      "Iteration 68, loss = 0.31507285\n",
      "Iteration 69, loss = 0.31981617\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.01365531\n",
      "Iteration 2, loss = 1.68357377\n",
      "Iteration 3, loss = 1.46131377\n",
      "Iteration 4, loss = 1.28463818\n",
      "Iteration 5, loss = 1.12295277\n",
      "Iteration 6, loss = 0.98675083\n",
      "Iteration 7, loss = 0.87500603\n",
      "Iteration 8, loss = 0.78647212\n",
      "Iteration 9, loss = 0.71473391\n",
      "Iteration 10, loss = 0.65722816\n",
      "Iteration 11, loss = 0.61000475\n",
      "Iteration 12, loss = 0.56929531\n",
      "Iteration 13, loss = 0.53374853\n",
      "Iteration 14, loss = 0.50468511\n",
      "Iteration 15, loss = 0.47991747\n",
      "Iteration 16, loss = 0.45904909\n",
      "Iteration 17, loss = 0.43797227\n",
      "Iteration 18, loss = 0.41886287\n",
      "Iteration 19, loss = 0.40362478\n",
      "Iteration 20, loss = 0.39534423\n",
      "Iteration 21, loss = 0.38288447\n",
      "Iteration 22, loss = 0.37547912\n",
      "Iteration 23, loss = 0.37008251\n",
      "Iteration 24, loss = 0.36256175\n",
      "Iteration 25, loss = 0.35632517\n",
      "Iteration 26, loss = 0.35097642\n",
      "Iteration 27, loss = 0.34561907\n",
      "Iteration 28, loss = 0.34175461\n",
      "Iteration 29, loss = 0.33871135\n",
      "Iteration 30, loss = 0.33974035\n",
      "Iteration 31, loss = 0.33945541\n",
      "Iteration 32, loss = 0.33994549\n",
      "Iteration 33, loss = 0.33648565\n",
      "Iteration 34, loss = 0.32731753\n",
      "Iteration 35, loss = 0.32741939\n",
      "Iteration 36, loss = 0.33134212\n",
      "Iteration 37, loss = 0.33433294\n",
      "Iteration 38, loss = 0.33676551\n",
      "Iteration 39, loss = 0.33392089\n",
      "Iteration 40, loss = 0.32996267\n",
      "Iteration 41, loss = 0.32925430\n",
      "Iteration 42, loss = 0.33070732\n",
      "Iteration 43, loss = 0.33261121\n",
      "Iteration 44, loss = 0.33397579\n",
      "Iteration 45, loss = 0.32690216\n",
      "Iteration 46, loss = 0.32362291\n",
      "Iteration 47, loss = 0.32265299\n",
      "Iteration 48, loss = 0.32349064\n",
      "Iteration 49, loss = 0.33140569\n",
      "Iteration 50, loss = 0.33930738\n",
      "Iteration 51, loss = 0.33182111\n",
      "Iteration 52, loss = 0.32168591\n",
      "Iteration 53, loss = 0.32149425\n",
      "Iteration 54, loss = 0.33144766\n",
      "Iteration 55, loss = 0.33154169\n",
      "Iteration 56, loss = 0.32552189\n",
      "Iteration 57, loss = 0.31811715\n",
      "Iteration 58, loss = 0.31274170\n",
      "Iteration 59, loss = 0.31390105\n",
      "Iteration 60, loss = 0.31630753\n",
      "Iteration 61, loss = 0.31655441\n",
      "Iteration 62, loss = 0.31782343\n",
      "Iteration 63, loss = 0.31959269\n",
      "Iteration 64, loss = 0.32074730\n",
      "Iteration 65, loss = 0.32031323\n",
      "Iteration 66, loss = 0.31926005\n",
      "Iteration 67, loss = 0.31746115\n",
      "Iteration 68, loss = 0.31901603\n",
      "Iteration 69, loss = 0.32767818\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.04164888\n",
      "Iteration 2, loss = 1.69736243\n",
      "Iteration 3, loss = 1.46799471\n",
      "Iteration 4, loss = 1.29440040\n",
      "Iteration 5, loss = 1.13377595\n",
      "Iteration 6, loss = 1.00032644\n",
      "Iteration 7, loss = 0.88889006\n",
      "Iteration 8, loss = 0.80114137\n",
      "Iteration 9, loss = 0.73103509\n",
      "Iteration 10, loss = 0.67601965\n",
      "Iteration 11, loss = 0.63136253\n",
      "Iteration 12, loss = 0.59295733\n",
      "Iteration 13, loss = 0.55996659\n",
      "Iteration 14, loss = 0.53186579\n",
      "Iteration 15, loss = 0.50746881\n",
      "Iteration 16, loss = 0.48410287\n",
      "Iteration 17, loss = 0.46234057\n",
      "Iteration 18, loss = 0.44255016\n",
      "Iteration 19, loss = 0.42467258\n",
      "Iteration 20, loss = 0.41155946\n",
      "Iteration 21, loss = 0.39617664\n",
      "Iteration 22, loss = 0.38710833\n",
      "Iteration 23, loss = 0.38002573\n",
      "Iteration 24, loss = 0.37028115\n",
      "Iteration 25, loss = 0.36143419\n",
      "Iteration 26, loss = 0.35704558\n",
      "Iteration 27, loss = 0.35105655\n",
      "Iteration 28, loss = 0.34587866\n",
      "Iteration 29, loss = 0.34068279\n",
      "Iteration 30, loss = 0.34067188\n",
      "Iteration 31, loss = 0.33915402\n",
      "Iteration 32, loss = 0.33911981\n",
      "Iteration 33, loss = 0.33693015\n",
      "Iteration 34, loss = 0.33122904\n",
      "Iteration 35, loss = 0.33105833\n",
      "Iteration 36, loss = 0.33435256\n",
      "Iteration 37, loss = 0.33516758\n",
      "Iteration 38, loss = 0.33381061\n",
      "Iteration 39, loss = 0.33134783\n",
      "Iteration 40, loss = 0.32885672\n",
      "Iteration 41, loss = 0.32646004\n",
      "Iteration 42, loss = 0.32602355\n",
      "Iteration 43, loss = 0.32738684\n",
      "Iteration 44, loss = 0.32982475\n",
      "Iteration 45, loss = 0.32806158\n",
      "Iteration 46, loss = 0.32280192\n",
      "Iteration 47, loss = 0.31711152\n",
      "Iteration 48, loss = 0.31280630\n",
      "Iteration 49, loss = 0.31594709\n",
      "Iteration 50, loss = 0.32130821\n",
      "Iteration 51, loss = 0.31844402\n",
      "Iteration 52, loss = 0.31433914\n",
      "Iteration 53, loss = 0.31095619\n",
      "Iteration 54, loss = 0.31447813\n",
      "Iteration 55, loss = 0.31441229\n",
      "Iteration 56, loss = 0.31390067\n",
      "Iteration 57, loss = 0.31251956\n",
      "Iteration 58, loss = 0.30978902\n",
      "Iteration 59, loss = 0.30980948\n",
      "Iteration 60, loss = 0.31194405\n",
      "Iteration 61, loss = 0.31205264\n",
      "Iteration 62, loss = 0.31366364\n",
      "Iteration 63, loss = 0.31541501\n",
      "Iteration 64, loss = 0.31738049\n",
      "Iteration 65, loss = 0.31705466\n",
      "Iteration 66, loss = 0.31537005\n",
      "Iteration 67, loss = 0.31149047\n",
      "Iteration 68, loss = 0.31177791\n",
      "Iteration 69, loss = 0.31805167\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.03378898\n",
      "Iteration 2, loss = 1.69503757\n",
      "Iteration 3, loss = 1.47398048\n",
      "Iteration 4, loss = 1.30523337\n",
      "Iteration 5, loss = 1.14820442\n",
      "Iteration 6, loss = 1.01376872\n",
      "Iteration 7, loss = 0.90230694\n",
      "Iteration 8, loss = 0.81538767\n",
      "Iteration 9, loss = 0.74387146\n",
      "Iteration 10, loss = 0.68640148\n",
      "Iteration 11, loss = 0.63903292\n",
      "Iteration 12, loss = 0.59837168\n",
      "Iteration 13, loss = 0.56176542\n",
      "Iteration 14, loss = 0.53000780\n",
      "Iteration 15, loss = 0.50161705\n",
      "Iteration 16, loss = 0.47629029\n",
      "Iteration 17, loss = 0.45326450\n",
      "Iteration 18, loss = 0.43387284\n",
      "Iteration 19, loss = 0.41705301\n",
      "Iteration 20, loss = 0.40589360\n",
      "Iteration 21, loss = 0.39346975\n",
      "Iteration 22, loss = 0.38447733\n",
      "Iteration 23, loss = 0.37668991\n",
      "Iteration 24, loss = 0.36799794\n",
      "Iteration 25, loss = 0.36110074\n",
      "Iteration 26, loss = 0.35730032\n",
      "Iteration 27, loss = 0.35354469\n",
      "Iteration 28, loss = 0.35026527\n",
      "Iteration 29, loss = 0.34578519\n",
      "Iteration 30, loss = 0.34420289\n",
      "Iteration 31, loss = 0.33944225\n",
      "Iteration 32, loss = 0.33827059\n",
      "Iteration 33, loss = 0.33785280\n",
      "Iteration 34, loss = 0.33166524\n",
      "Iteration 35, loss = 0.32532504\n",
      "Iteration 36, loss = 0.32534179\n",
      "Iteration 37, loss = 0.32708121\n",
      "Iteration 38, loss = 0.32960550\n",
      "Iteration 39, loss = 0.33126118\n",
      "Iteration 40, loss = 0.33070888\n",
      "Iteration 41, loss = 0.32666538\n",
      "Iteration 42, loss = 0.32352146\n",
      "Iteration 43, loss = 0.32045208\n",
      "Iteration 44, loss = 0.31959714\n",
      "Iteration 45, loss = 0.31987222\n",
      "Iteration 46, loss = 0.31777535\n",
      "Iteration 47, loss = 0.31774855\n",
      "Iteration 48, loss = 0.31770311\n",
      "Iteration 49, loss = 0.31960425\n",
      "Iteration 50, loss = 0.32045972\n",
      "Iteration 51, loss = 0.31502586\n",
      "Iteration 52, loss = 0.30990669\n",
      "Iteration 53, loss = 0.31119464\n",
      "Iteration 54, loss = 0.31920292\n",
      "Iteration 55, loss = 0.31654182\n",
      "Iteration 56, loss = 0.31273241\n",
      "Iteration 57, loss = 0.31037037\n",
      "Iteration 58, loss = 0.30770526\n",
      "Iteration 59, loss = 0.31054793\n",
      "Iteration 60, loss = 0.31360194\n",
      "Iteration 61, loss = 0.31279093\n",
      "Iteration 62, loss = 0.31069954\n",
      "Iteration 63, loss = 0.30984078\n",
      "Iteration 64, loss = 0.31160207\n",
      "Iteration 65, loss = 0.31310964\n",
      "Iteration 66, loss = 0.31497460\n",
      "Iteration 67, loss = 0.31446887\n",
      "Iteration 68, loss = 0.31631377\n",
      "Iteration 69, loss = 0.32127300\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.03090008\n",
      "Iteration 2, loss = 1.69102767\n",
      "Iteration 3, loss = 1.46738465\n",
      "Iteration 4, loss = 1.29284342\n",
      "Iteration 5, loss = 1.13835624\n",
      "Iteration 6, loss = 1.00652722\n",
      "Iteration 7, loss = 0.89424348\n",
      "Iteration 8, loss = 0.80589513\n",
      "Iteration 9, loss = 0.73676119\n",
      "Iteration 10, loss = 0.68216277\n",
      "Iteration 11, loss = 0.63526176\n",
      "Iteration 12, loss = 0.59302839\n",
      "Iteration 13, loss = 0.55478968\n",
      "Iteration 14, loss = 0.52191298\n",
      "Iteration 15, loss = 0.49361322\n",
      "Iteration 16, loss = 0.46886117\n",
      "Iteration 17, loss = 0.44704537\n",
      "Iteration 18, loss = 0.42874813\n",
      "Iteration 19, loss = 0.41230009\n",
      "Iteration 20, loss = 0.39917591\n",
      "Iteration 21, loss = 0.38706344\n",
      "Iteration 22, loss = 0.37835885\n",
      "Iteration 23, loss = 0.36946866\n",
      "Iteration 24, loss = 0.36172194\n",
      "Iteration 25, loss = 0.35572687\n",
      "Iteration 26, loss = 0.35210428\n",
      "Iteration 27, loss = 0.35031504\n",
      "Iteration 28, loss = 0.34606226\n",
      "Iteration 29, loss = 0.33864752\n",
      "Iteration 30, loss = 0.33507376\n",
      "Iteration 31, loss = 0.33127946\n",
      "Iteration 32, loss = 0.32807809\n",
      "Iteration 33, loss = 0.32888976\n",
      "Iteration 34, loss = 0.32410010\n",
      "Iteration 35, loss = 0.32208978\n",
      "Iteration 36, loss = 0.32679163\n",
      "Iteration 37, loss = 0.32940799\n",
      "Iteration 38, loss = 0.32813192\n",
      "Iteration 39, loss = 0.32558630\n",
      "Iteration 40, loss = 0.32356123\n",
      "Iteration 41, loss = 0.31922011\n",
      "Iteration 42, loss = 0.31562544\n",
      "Iteration 43, loss = 0.31315877\n",
      "Iteration 44, loss = 0.31493377\n",
      "Iteration 45, loss = 0.31328805\n",
      "Iteration 46, loss = 0.31375949\n",
      "Iteration 47, loss = 0.31502848\n",
      "Iteration 48, loss = 0.31341734\n",
      "Iteration 49, loss = 0.31373941\n",
      "Iteration 50, loss = 0.31347897\n",
      "Iteration 51, loss = 0.31259954\n",
      "Iteration 52, loss = 0.31069077\n",
      "Iteration 53, loss = 0.31414738\n",
      "Iteration 54, loss = 0.31730428\n",
      "Iteration 55, loss = 0.31042803\n",
      "Iteration 56, loss = 0.30548641\n",
      "Iteration 57, loss = 0.30591095\n",
      "Iteration 58, loss = 0.30688265\n",
      "Iteration 59, loss = 0.30681723\n",
      "Iteration 60, loss = 0.30448181\n",
      "Iteration 61, loss = 0.30232326\n",
      "Iteration 62, loss = 0.30216565\n",
      "Iteration 63, loss = 0.30352435\n",
      "Iteration 64, loss = 0.30654054\n",
      "Iteration 65, loss = 0.30848224\n",
      "Iteration 66, loss = 0.31264004\n",
      "Iteration 67, loss = 0.31535455\n",
      "Iteration 68, loss = 0.31687288\n",
      "Iteration 69, loss = 0.31495097\n",
      "Iteration 70, loss = 0.31138208\n",
      "Iteration 71, loss = 0.31050254\n",
      "Iteration 72, loss = 0.30673327\n",
      "Iteration 73, loss = 0.30395030\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.02503213\n",
      "Iteration 2, loss = 1.68291369\n",
      "Iteration 3, loss = 1.45250577\n",
      "Iteration 4, loss = 1.27324264\n",
      "Iteration 5, loss = 1.11381869\n",
      "Iteration 6, loss = 0.97663658\n",
      "Iteration 7, loss = 0.86136656\n",
      "Iteration 8, loss = 0.77007388\n",
      "Iteration 9, loss = 0.69841887\n",
      "Iteration 10, loss = 0.64190755\n",
      "Iteration 11, loss = 0.59610454\n",
      "Iteration 12, loss = 0.55498447\n",
      "Iteration 13, loss = 0.51817578\n",
      "Iteration 14, loss = 0.48541637\n",
      "Iteration 15, loss = 0.45842810\n",
      "Iteration 16, loss = 0.43496257\n",
      "Iteration 17, loss = 0.41414849\n",
      "Iteration 18, loss = 0.39912701\n",
      "Iteration 19, loss = 0.38763648\n",
      "Iteration 20, loss = 0.37877416\n",
      "Iteration 21, loss = 0.37075980\n",
      "Iteration 22, loss = 0.36662347\n",
      "Iteration 23, loss = 0.36170654\n",
      "Iteration 24, loss = 0.35660645\n",
      "Iteration 25, loss = 0.34954692\n",
      "Iteration 26, loss = 0.34545198\n",
      "Iteration 27, loss = 0.34332332\n",
      "Iteration 28, loss = 0.34051662\n",
      "Iteration 29, loss = 0.33462408\n",
      "Iteration 30, loss = 0.33101939\n",
      "Iteration 31, loss = 0.32772589\n",
      "Iteration 32, loss = 0.32515524\n",
      "Iteration 33, loss = 0.32621679\n",
      "Iteration 34, loss = 0.32575854\n",
      "Iteration 35, loss = 0.31984085\n",
      "Iteration 36, loss = 0.31982619\n",
      "Iteration 37, loss = 0.32125362\n",
      "Iteration 38, loss = 0.32151491\n",
      "Iteration 39, loss = 0.32107279\n",
      "Iteration 40, loss = 0.31903382\n",
      "Iteration 41, loss = 0.31759458\n",
      "Iteration 42, loss = 0.31760057\n",
      "Iteration 43, loss = 0.31665879\n",
      "Iteration 44, loss = 0.31677617\n",
      "Iteration 45, loss = 0.31587149\n",
      "Iteration 46, loss = 0.31327709\n",
      "Iteration 47, loss = 0.31329468\n",
      "Iteration 48, loss = 0.31533700\n",
      "Iteration 49, loss = 0.31629119\n",
      "Iteration 50, loss = 0.31513341\n",
      "Iteration 51, loss = 0.31056389\n",
      "Iteration 52, loss = 0.30724738\n",
      "Iteration 53, loss = 0.31169474\n",
      "Iteration 54, loss = 0.32065989\n",
      "Iteration 55, loss = 0.31860118\n",
      "Iteration 56, loss = 0.31494431\n",
      "Iteration 57, loss = 0.31361178\n",
      "Iteration 58, loss = 0.30860188\n",
      "Iteration 59, loss = 0.30737181\n",
      "Iteration 60, loss = 0.30652903\n",
      "Iteration 61, loss = 0.30790804\n",
      "Iteration 62, loss = 0.30522029\n",
      "Iteration 63, loss = 0.30302380\n",
      "Iteration 64, loss = 0.30470092\n",
      "Iteration 65, loss = 0.30804859\n",
      "Iteration 66, loss = 0.31092227\n",
      "Iteration 67, loss = 0.31062420\n",
      "Iteration 68, loss = 0.31292571\n",
      "Iteration 69, loss = 0.31750417\n",
      "Iteration 70, loss = 0.31631644\n",
      "Iteration 71, loss = 0.31463419\n",
      "Iteration 72, loss = 0.30953094\n",
      "Iteration 73, loss = 0.30711000\n",
      "Iteration 74, loss = 0.31296337\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.02822427\n",
      "Iteration 2, loss = 1.68992850\n",
      "Iteration 3, loss = 1.46176670\n",
      "Iteration 4, loss = 1.28215754\n",
      "Iteration 5, loss = 1.11973932\n",
      "Iteration 6, loss = 0.98098389\n",
      "Iteration 7, loss = 0.86773360\n",
      "Iteration 8, loss = 0.77875539\n",
      "Iteration 9, loss = 0.70929703\n",
      "Iteration 10, loss = 0.65313636\n",
      "Iteration 11, loss = 0.60663611\n",
      "Iteration 12, loss = 0.56499187\n",
      "Iteration 13, loss = 0.52697572\n",
      "Iteration 14, loss = 0.49430858\n",
      "Iteration 15, loss = 0.46866076\n",
      "Iteration 16, loss = 0.44450719\n",
      "Iteration 17, loss = 0.42402698\n",
      "Iteration 18, loss = 0.41064298\n",
      "Iteration 19, loss = 0.40045224\n",
      "Iteration 20, loss = 0.39118916\n",
      "Iteration 21, loss = 0.38364339\n",
      "Iteration 22, loss = 0.37824299\n",
      "Iteration 23, loss = 0.37241301\n",
      "Iteration 24, loss = 0.36790707\n",
      "Iteration 25, loss = 0.36068299\n",
      "Iteration 26, loss = 0.35618206\n",
      "Iteration 27, loss = 0.35552897\n",
      "Iteration 28, loss = 0.35310862\n",
      "Iteration 29, loss = 0.34855752\n",
      "Iteration 30, loss = 0.34347981\n",
      "Iteration 31, loss = 0.33820056\n",
      "Iteration 32, loss = 0.33289860\n",
      "Iteration 33, loss = 0.33348916\n",
      "Iteration 34, loss = 0.33253355\n",
      "Iteration 35, loss = 0.32742988\n",
      "Iteration 36, loss = 0.32933038\n",
      "Iteration 37, loss = 0.33302630\n",
      "Iteration 38, loss = 0.33510434\n",
      "Iteration 39, loss = 0.33537731\n",
      "Iteration 40, loss = 0.33230444\n",
      "Iteration 41, loss = 0.32891749\n",
      "Iteration 42, loss = 0.32895997\n",
      "Iteration 43, loss = 0.32646650\n",
      "Iteration 44, loss = 0.32927807\n",
      "Iteration 45, loss = 0.33133059\n",
      "Iteration 46, loss = 0.32600073\n",
      "Iteration 47, loss = 0.32277511\n",
      "Iteration 48, loss = 0.32693702\n",
      "Iteration 49, loss = 0.32594387\n",
      "Iteration 50, loss = 0.32476478\n",
      "Iteration 51, loss = 0.32373831\n",
      "Iteration 52, loss = 0.31884619\n",
      "Iteration 53, loss = 0.31831190\n",
      "Iteration 54, loss = 0.32406751\n",
      "Iteration 55, loss = 0.32616460\n",
      "Iteration 56, loss = 0.32386054\n",
      "Iteration 57, loss = 0.32307316\n",
      "Iteration 58, loss = 0.31949262\n",
      "Iteration 59, loss = 0.31809163\n",
      "Iteration 60, loss = 0.31624537\n",
      "Iteration 61, loss = 0.31660859\n",
      "Iteration 62, loss = 0.31460721\n",
      "Iteration 63, loss = 0.31380500\n",
      "Iteration 64, loss = 0.31471348\n",
      "Iteration 65, loss = 0.31632325\n",
      "Iteration 66, loss = 0.31912675\n",
      "Iteration 67, loss = 0.31984723\n",
      "Iteration 68, loss = 0.32302848\n",
      "Iteration 69, loss = 0.32781726\n",
      "Iteration 70, loss = 0.32834374\n",
      "Iteration 71, loss = 0.32616310\n",
      "Iteration 72, loss = 0.31883012\n",
      "Iteration 73, loss = 0.31408349\n",
      "Iteration 74, loss = 0.32015573\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.03939328\n",
      "Iteration 2, loss = 1.69686966\n",
      "Iteration 3, loss = 1.47092978\n",
      "Iteration 4, loss = 1.29333987\n",
      "Iteration 5, loss = 1.13500356\n",
      "Iteration 6, loss = 0.99746408\n",
      "Iteration 7, loss = 0.88280402\n",
      "Iteration 8, loss = 0.79120439\n",
      "Iteration 9, loss = 0.71754690\n",
      "Iteration 10, loss = 0.65753489\n",
      "Iteration 11, loss = 0.60984555\n",
      "Iteration 12, loss = 0.56879537\n",
      "Iteration 13, loss = 0.53274749\n",
      "Iteration 14, loss = 0.50153967\n",
      "Iteration 15, loss = 0.47586645\n",
      "Iteration 16, loss = 0.45085919\n",
      "Iteration 17, loss = 0.42932933\n",
      "Iteration 18, loss = 0.41362209\n",
      "Iteration 19, loss = 0.40113537\n",
      "Iteration 20, loss = 0.38984068\n",
      "Iteration 21, loss = 0.38256357\n",
      "Iteration 22, loss = 0.37954619\n",
      "Iteration 23, loss = 0.37573387\n",
      "Iteration 24, loss = 0.37248451\n",
      "Iteration 25, loss = 0.36246968\n",
      "Iteration 26, loss = 0.35323070\n",
      "Iteration 27, loss = 0.34999712\n",
      "Iteration 28, loss = 0.34760251\n",
      "Iteration 29, loss = 0.34374649\n",
      "Iteration 30, loss = 0.33931853\n",
      "Iteration 31, loss = 0.33582915\n",
      "Iteration 32, loss = 0.33303978\n",
      "Iteration 33, loss = 0.33227314\n",
      "Iteration 34, loss = 0.33001421\n",
      "Iteration 35, loss = 0.32328790\n",
      "Iteration 36, loss = 0.32520418\n",
      "Iteration 37, loss = 0.33214884\n",
      "Iteration 38, loss = 0.33587513\n",
      "Iteration 39, loss = 0.33345876\n",
      "Iteration 40, loss = 0.32736911\n",
      "Iteration 41, loss = 0.32154672\n",
      "Iteration 42, loss = 0.32018793\n",
      "Iteration 43, loss = 0.31747252\n",
      "Iteration 44, loss = 0.31944458\n",
      "Iteration 45, loss = 0.32233377\n",
      "Iteration 46, loss = 0.32019980\n",
      "Iteration 47, loss = 0.31654843\n",
      "Iteration 48, loss = 0.31811007\n",
      "Iteration 49, loss = 0.31712756\n",
      "Iteration 50, loss = 0.31543288\n",
      "Iteration 51, loss = 0.31431522\n",
      "Iteration 52, loss = 0.31104259\n",
      "Iteration 53, loss = 0.31043782\n",
      "Iteration 54, loss = 0.31475668\n",
      "Iteration 55, loss = 0.31786726\n",
      "Iteration 56, loss = 0.31675462\n",
      "Iteration 57, loss = 0.31691867\n",
      "Iteration 58, loss = 0.31322844\n",
      "Iteration 59, loss = 0.31163736\n",
      "Iteration 60, loss = 0.30952084\n",
      "Iteration 61, loss = 0.31009186\n",
      "Iteration 62, loss = 0.30681758\n",
      "Iteration 63, loss = 0.30549579\n",
      "Iteration 64, loss = 0.30618302\n",
      "Iteration 65, loss = 0.30756838\n",
      "Iteration 66, loss = 0.31116073\n",
      "Iteration 67, loss = 0.31250516\n",
      "Iteration 68, loss = 0.31740636\n",
      "Iteration 69, loss = 0.32390395\n",
      "Iteration 70, loss = 0.32392159\n",
      "Iteration 71, loss = 0.32079800\n",
      "Iteration 72, loss = 0.31257035\n",
      "Iteration 73, loss = 0.30823335\n",
      "Iteration 74, loss = 0.31466696\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.02469733\n",
      "Iteration 2, loss = 1.69010873\n",
      "Iteration 3, loss = 1.47237615\n",
      "Iteration 4, loss = 1.29810656\n",
      "Iteration 5, loss = 1.14032024\n",
      "Iteration 6, loss = 1.00255246\n",
      "Iteration 7, loss = 0.88622153\n",
      "Iteration 8, loss = 0.79233825\n",
      "Iteration 9, loss = 0.71880886\n",
      "Iteration 10, loss = 0.65921571\n",
      "Iteration 11, loss = 0.61149383\n",
      "Iteration 12, loss = 0.56783649\n",
      "Iteration 13, loss = 0.52728928\n",
      "Iteration 14, loss = 0.49291288\n",
      "Iteration 15, loss = 0.46751731\n",
      "Iteration 16, loss = 0.44244647\n",
      "Iteration 17, loss = 0.42105183\n",
      "Iteration 18, loss = 0.40548296\n",
      "Iteration 19, loss = 0.39386933\n",
      "Iteration 20, loss = 0.38307401\n",
      "Iteration 21, loss = 0.37412142\n",
      "Iteration 22, loss = 0.36768409\n",
      "Iteration 23, loss = 0.36219144\n",
      "Iteration 24, loss = 0.35872609\n",
      "Iteration 25, loss = 0.35302672\n",
      "Iteration 26, loss = 0.34721925\n",
      "Iteration 27, loss = 0.34420872\n",
      "Iteration 28, loss = 0.33965096\n",
      "Iteration 29, loss = 0.33496790\n",
      "Iteration 30, loss = 0.32979217\n",
      "Iteration 31, loss = 0.32589282\n",
      "Iteration 32, loss = 0.32153229\n",
      "Iteration 33, loss = 0.31969336\n",
      "Iteration 34, loss = 0.31828005\n",
      "Iteration 35, loss = 0.31426833\n",
      "Iteration 36, loss = 0.31747441\n",
      "Iteration 37, loss = 0.32229164\n",
      "Iteration 38, loss = 0.32411527\n",
      "Iteration 39, loss = 0.31979526\n",
      "Iteration 40, loss = 0.31314597\n",
      "Iteration 41, loss = 0.30958678\n",
      "Iteration 42, loss = 0.31138046\n",
      "Iteration 43, loss = 0.31071488\n",
      "Iteration 44, loss = 0.31401547\n",
      "Iteration 45, loss = 0.31705443\n",
      "Iteration 46, loss = 0.30986846\n",
      "Iteration 47, loss = 0.30457352\n",
      "Iteration 48, loss = 0.30855171\n",
      "Iteration 49, loss = 0.31053977\n",
      "Iteration 50, loss = 0.31013778\n",
      "Iteration 51, loss = 0.30734885\n",
      "Iteration 52, loss = 0.30567460\n",
      "Iteration 53, loss = 0.30421086\n",
      "Iteration 54, loss = 0.30772556\n",
      "Iteration 55, loss = 0.30983290\n",
      "Iteration 56, loss = 0.31034284\n",
      "Iteration 57, loss = 0.31138830\n",
      "Iteration 58, loss = 0.30902731\n",
      "Iteration 59, loss = 0.30743440\n",
      "Iteration 60, loss = 0.30462005\n",
      "Iteration 61, loss = 0.30537861\n",
      "Iteration 62, loss = 0.30457024\n",
      "Iteration 63, loss = 0.30494181\n",
      "Iteration 64, loss = 0.30261225\n",
      "Iteration 65, loss = 0.30127841\n",
      "Iteration 66, loss = 0.30374102\n",
      "Iteration 67, loss = 0.30638224\n",
      "Iteration 68, loss = 0.30951371\n",
      "Iteration 69, loss = 0.31040016\n",
      "Iteration 70, loss = 0.30768399\n",
      "Iteration 71, loss = 0.30456743\n",
      "Iteration 72, loss = 0.30091274\n",
      "Iteration 73, loss = 0.30096602\n",
      "Iteration 74, loss = 0.30645296\n",
      "Iteration 75, loss = 0.31210323\n",
      "Iteration 76, loss = 0.31364618\n",
      "Iteration 77, loss = 0.31108992\n",
      "Iteration 78, loss = 0.30464563\n",
      "Iteration 79, loss = 0.29983486\n",
      "Iteration 80, loss = 0.30104706\n",
      "Iteration 81, loss = 0.30231047\n",
      "Iteration 82, loss = 0.29694507\n",
      "Iteration 83, loss = 0.29422923\n",
      "Iteration 84, loss = 0.29094732\n",
      "Iteration 85, loss = 0.29255816\n",
      "Iteration 86, loss = 0.29929946\n",
      "Iteration 87, loss = 0.30400327\n",
      "Iteration 88, loss = 0.30241868\n",
      "Iteration 89, loss = 0.30768451\n",
      "Iteration 90, loss = 0.31036763\n",
      "Iteration 91, loss = 0.30883563\n",
      "Iteration 92, loss = 0.30509408\n",
      "Iteration 93, loss = 0.30299952\n",
      "Iteration 94, loss = 0.30787005\n",
      "Iteration 95, loss = 0.31317784\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.91473728\n",
      "Iteration 2, loss = 1.58473980\n",
      "Iteration 3, loss = 1.34924622\n",
      "Iteration 4, loss = 1.15475680\n",
      "Iteration 5, loss = 0.98188737\n",
      "Iteration 6, loss = 0.84106362\n",
      "Iteration 7, loss = 0.73725913\n",
      "Iteration 8, loss = 0.66817825\n",
      "Iteration 9, loss = 0.61278409\n",
      "Iteration 10, loss = 0.56223378\n",
      "Iteration 11, loss = 0.51774934\n",
      "Iteration 12, loss = 0.48064808\n",
      "Iteration 13, loss = 0.45205749\n",
      "Iteration 14, loss = 0.42877169\n",
      "Iteration 15, loss = 0.40949254\n",
      "Iteration 16, loss = 0.39539270\n",
      "Iteration 17, loss = 0.38153384\n",
      "Iteration 18, loss = 0.37463435\n",
      "Iteration 19, loss = 0.36917197\n",
      "Iteration 20, loss = 0.36310714\n",
      "Iteration 21, loss = 0.35135556\n",
      "Iteration 22, loss = 0.34524558\n",
      "Iteration 23, loss = 0.34563793\n",
      "Iteration 24, loss = 0.33837243\n",
      "Iteration 25, loss = 0.33319079\n",
      "Iteration 26, loss = 0.33640771\n",
      "Iteration 27, loss = 0.32831973\n",
      "Iteration 28, loss = 0.31873422\n",
      "Iteration 29, loss = 0.31785161\n",
      "Iteration 30, loss = 0.32213642\n",
      "Iteration 31, loss = 0.32585313\n",
      "Iteration 32, loss = 0.32477815\n",
      "Iteration 33, loss = 0.32727383\n",
      "Iteration 34, loss = 0.32899540\n",
      "Iteration 35, loss = 0.32313452\n",
      "Iteration 36, loss = 0.31803618\n",
      "Iteration 37, loss = 0.31846422\n",
      "Iteration 38, loss = 0.31667221\n",
      "Iteration 39, loss = 0.31611707\n",
      "Iteration 40, loss = 0.31870868\n",
      "Iteration 41, loss = 0.31699720\n",
      "Iteration 42, loss = 0.31224748\n",
      "Iteration 43, loss = 0.30633438\n",
      "Iteration 44, loss = 0.31537497\n",
      "Iteration 45, loss = 0.31442319\n",
      "Iteration 46, loss = 0.30888658\n",
      "Iteration 47, loss = 0.30411276\n",
      "Iteration 48, loss = 0.30887730\n",
      "Iteration 49, loss = 0.31342526\n",
      "Iteration 50, loss = 0.31421872\n",
      "Iteration 51, loss = 0.31255751\n",
      "Iteration 52, loss = 0.30966700\n",
      "Iteration 53, loss = 0.30613807\n",
      "Iteration 54, loss = 0.30530671\n",
      "Iteration 55, loss = 0.30492036\n",
      "Iteration 56, loss = 0.30084453\n",
      "Iteration 57, loss = 0.30293776\n",
      "Iteration 58, loss = 0.30515723\n",
      "Iteration 59, loss = 0.31149769\n",
      "Iteration 60, loss = 0.31081360\n",
      "Iteration 61, loss = 0.30945231\n",
      "Iteration 62, loss = 0.31153427\n",
      "Iteration 63, loss = 0.31188882\n",
      "Iteration 64, loss = 0.30763374\n",
      "Iteration 65, loss = 0.30687741\n",
      "Iteration 66, loss = 0.30783010\n",
      "Iteration 67, loss = 0.30986542\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.89658519\n",
      "Iteration 2, loss = 1.55901591\n",
      "Iteration 3, loss = 1.30750770\n",
      "Iteration 4, loss = 1.10177442\n",
      "Iteration 5, loss = 0.93240408\n",
      "Iteration 6, loss = 0.79969742\n",
      "Iteration 7, loss = 0.70106976\n",
      "Iteration 8, loss = 0.63022228\n",
      "Iteration 9, loss = 0.57736869\n",
      "Iteration 10, loss = 0.53924956\n",
      "Iteration 11, loss = 0.50540086\n",
      "Iteration 12, loss = 0.47159010\n",
      "Iteration 13, loss = 0.43671731\n",
      "Iteration 14, loss = 0.40940965\n",
      "Iteration 15, loss = 0.39106435\n",
      "Iteration 16, loss = 0.37723531\n",
      "Iteration 17, loss = 0.36761653\n",
      "Iteration 18, loss = 0.36049535\n",
      "Iteration 19, loss = 0.35193165\n",
      "Iteration 20, loss = 0.34424872\n",
      "Iteration 21, loss = 0.33687180\n",
      "Iteration 22, loss = 0.33856683\n",
      "Iteration 23, loss = 0.33159217\n",
      "Iteration 24, loss = 0.31899935\n",
      "Iteration 25, loss = 0.31682779\n",
      "Iteration 26, loss = 0.31566034\n",
      "Iteration 27, loss = 0.31495370\n",
      "Iteration 28, loss = 0.31659072\n",
      "Iteration 29, loss = 0.31263928\n",
      "Iteration 30, loss = 0.31030388\n",
      "Iteration 31, loss = 0.31156563\n",
      "Iteration 32, loss = 0.31270302\n",
      "Iteration 33, loss = 0.31191138\n",
      "Iteration 34, loss = 0.31192512\n",
      "Iteration 35, loss = 0.31533353\n",
      "Iteration 36, loss = 0.32114434\n",
      "Iteration 37, loss = 0.31301530\n",
      "Iteration 38, loss = 0.31106253\n",
      "Iteration 39, loss = 0.32325915\n",
      "Iteration 40, loss = 0.32737740\n",
      "Iteration 41, loss = 0.31636111\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.91413098\n",
      "Iteration 2, loss = 1.57802897\n",
      "Iteration 3, loss = 1.32685620\n",
      "Iteration 4, loss = 1.11866268\n",
      "Iteration 5, loss = 0.95067481\n",
      "Iteration 6, loss = 0.82117273\n",
      "Iteration 7, loss = 0.72060090\n",
      "Iteration 8, loss = 0.64450290\n",
      "Iteration 9, loss = 0.58495667\n",
      "Iteration 10, loss = 0.54379512\n",
      "Iteration 11, loss = 0.50874138\n",
      "Iteration 12, loss = 0.47667287\n",
      "Iteration 13, loss = 0.44284909\n",
      "Iteration 14, loss = 0.41663957\n",
      "Iteration 15, loss = 0.39558989\n",
      "Iteration 16, loss = 0.37955214\n",
      "Iteration 17, loss = 0.36815197\n",
      "Iteration 18, loss = 0.35767890\n",
      "Iteration 19, loss = 0.34633199\n",
      "Iteration 20, loss = 0.34091327\n",
      "Iteration 21, loss = 0.33806379\n",
      "Iteration 22, loss = 0.34240882\n",
      "Iteration 23, loss = 0.33145347\n",
      "Iteration 24, loss = 0.31765867\n",
      "Iteration 25, loss = 0.31809024\n",
      "Iteration 26, loss = 0.31655521\n",
      "Iteration 27, loss = 0.31384046\n",
      "Iteration 28, loss = 0.31660790\n",
      "Iteration 29, loss = 0.31243815\n",
      "Iteration 30, loss = 0.30933913\n",
      "Iteration 31, loss = 0.31072537\n",
      "Iteration 32, loss = 0.31527460\n",
      "Iteration 33, loss = 0.31642763\n",
      "Iteration 34, loss = 0.30742740\n",
      "Iteration 35, loss = 0.30582206\n",
      "Iteration 36, loss = 0.32018528\n",
      "Iteration 37, loss = 0.31549501\n",
      "Iteration 38, loss = 0.30733820\n",
      "Iteration 39, loss = 0.31738091\n",
      "Iteration 40, loss = 0.32602990\n",
      "Iteration 41, loss = 0.31554181\n",
      "Iteration 42, loss = 0.30742535\n",
      "Iteration 43, loss = 0.30533014\n",
      "Iteration 44, loss = 0.30635076\n",
      "Iteration 45, loss = 0.30786844\n",
      "Iteration 46, loss = 0.31124207\n",
      "Iteration 47, loss = 0.31487294\n",
      "Iteration 48, loss = 0.31094354\n",
      "Iteration 49, loss = 0.30618430\n",
      "Iteration 50, loss = 0.30453391\n",
      "Iteration 51, loss = 0.30356467\n",
      "Iteration 52, loss = 0.30325323\n",
      "Iteration 53, loss = 0.30336554\n",
      "Iteration 54, loss = 0.30869698\n",
      "Iteration 55, loss = 0.31106119\n",
      "Iteration 56, loss = 0.30676132\n",
      "Iteration 57, loss = 0.30268175\n",
      "Iteration 58, loss = 0.30101802\n",
      "Iteration 59, loss = 0.30224517\n",
      "Iteration 60, loss = 0.30179380\n",
      "Iteration 61, loss = 0.30318007\n",
      "Iteration 62, loss = 0.30242364\n",
      "Iteration 63, loss = 0.29963130\n",
      "Iteration 64, loss = 0.30084436\n",
      "Iteration 65, loss = 0.30435022\n",
      "Iteration 66, loss = 0.30143094\n",
      "Iteration 67, loss = 0.30212332\n",
      "Iteration 68, loss = 0.30302266\n",
      "Iteration 69, loss = 0.30732119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 70, loss = 0.31088955\n",
      "Iteration 71, loss = 0.30627907\n",
      "Iteration 72, loss = 0.30391470\n",
      "Iteration 73, loss = 0.30547155\n",
      "Iteration 74, loss = 0.30908672\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.89148731\n",
      "Iteration 2, loss = 1.54585969\n",
      "Iteration 3, loss = 1.28669990\n",
      "Iteration 4, loss = 1.07663393\n",
      "Iteration 5, loss = 0.91006574\n",
      "Iteration 6, loss = 0.78684085\n",
      "Iteration 7, loss = 0.69462477\n",
      "Iteration 8, loss = 0.62778409\n",
      "Iteration 9, loss = 0.57712322\n",
      "Iteration 10, loss = 0.53690061\n",
      "Iteration 11, loss = 0.50036015\n",
      "Iteration 12, loss = 0.46883501\n",
      "Iteration 13, loss = 0.43785967\n",
      "Iteration 14, loss = 0.41304264\n",
      "Iteration 15, loss = 0.39344372\n",
      "Iteration 16, loss = 0.37811182\n",
      "Iteration 17, loss = 0.36702148\n",
      "Iteration 18, loss = 0.35545469\n",
      "Iteration 19, loss = 0.34288448\n",
      "Iteration 20, loss = 0.33483964\n",
      "Iteration 21, loss = 0.32933661\n",
      "Iteration 22, loss = 0.32958987\n",
      "Iteration 23, loss = 0.32035937\n",
      "Iteration 24, loss = 0.30986818\n",
      "Iteration 25, loss = 0.31147011\n",
      "Iteration 26, loss = 0.30993914\n",
      "Iteration 27, loss = 0.31018739\n",
      "Iteration 28, loss = 0.31632620\n",
      "Iteration 29, loss = 0.31493836\n",
      "Iteration 30, loss = 0.31010095\n",
      "Iteration 31, loss = 0.30731098\n",
      "Iteration 32, loss = 0.31219271\n",
      "Iteration 33, loss = 0.31260625\n",
      "Iteration 34, loss = 0.30147089\n",
      "Iteration 35, loss = 0.29827748\n",
      "Iteration 36, loss = 0.31130445\n",
      "Iteration 37, loss = 0.30704476\n",
      "Iteration 38, loss = 0.30347313\n",
      "Iteration 39, loss = 0.31425189\n",
      "Iteration 40, loss = 0.32253965\n",
      "Iteration 41, loss = 0.31403862\n",
      "Iteration 42, loss = 0.30455075\n",
      "Iteration 43, loss = 0.29998437\n",
      "Iteration 44, loss = 0.30217029\n",
      "Iteration 45, loss = 0.30282399\n",
      "Iteration 46, loss = 0.30561627\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.90765327\n",
      "Iteration 2, loss = 1.56773306\n",
      "Iteration 3, loss = 1.31106998\n",
      "Iteration 4, loss = 1.10073845\n",
      "Iteration 5, loss = 0.93336135\n",
      "Iteration 6, loss = 0.80521662\n",
      "Iteration 7, loss = 0.70630953\n",
      "Iteration 8, loss = 0.63068483\n",
      "Iteration 9, loss = 0.57216748\n",
      "Iteration 10, loss = 0.52466878\n",
      "Iteration 11, loss = 0.48309407\n",
      "Iteration 12, loss = 0.44823559\n",
      "Iteration 13, loss = 0.41720262\n",
      "Iteration 14, loss = 0.39499466\n",
      "Iteration 15, loss = 0.37912684\n",
      "Iteration 16, loss = 0.36653053\n",
      "Iteration 17, loss = 0.35606333\n",
      "Iteration 18, loss = 0.34771366\n",
      "Iteration 19, loss = 0.33669386\n",
      "Iteration 20, loss = 0.32798764\n",
      "Iteration 21, loss = 0.32891329\n",
      "Iteration 22, loss = 0.33418237\n",
      "Iteration 23, loss = 0.32446307\n",
      "Iteration 24, loss = 0.31118514\n",
      "Iteration 25, loss = 0.31280891\n",
      "Iteration 26, loss = 0.30903985\n",
      "Iteration 27, loss = 0.30885808\n",
      "Iteration 28, loss = 0.31679144\n",
      "Iteration 29, loss = 0.31496069\n",
      "Iteration 30, loss = 0.31253515\n",
      "Iteration 31, loss = 0.31498516\n",
      "Iteration 32, loss = 0.32140612\n",
      "Iteration 33, loss = 0.31991038\n",
      "Iteration 34, loss = 0.30817333\n",
      "Iteration 35, loss = 0.30334716\n",
      "Iteration 36, loss = 0.31320699\n",
      "Iteration 37, loss = 0.31194633\n",
      "Iteration 38, loss = 0.30794317\n",
      "Iteration 39, loss = 0.31047862\n",
      "Iteration 40, loss = 0.31283800\n",
      "Iteration 41, loss = 0.31061379\n",
      "Iteration 42, loss = 0.30954093\n",
      "Iteration 43, loss = 0.30721710\n",
      "Iteration 44, loss = 0.30565836\n",
      "Iteration 45, loss = 0.30539161\n",
      "Iteration 46, loss = 0.30735924\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.90370782\n",
      "Iteration 2, loss = 1.56843020\n",
      "Iteration 3, loss = 1.31416184\n",
      "Iteration 4, loss = 1.10493804\n",
      "Iteration 5, loss = 0.93605376\n",
      "Iteration 6, loss = 0.80596812\n",
      "Iteration 7, loss = 0.70658917\n",
      "Iteration 8, loss = 0.63361633\n",
      "Iteration 9, loss = 0.57526412\n",
      "Iteration 10, loss = 0.52457682\n",
      "Iteration 11, loss = 0.48203499\n",
      "Iteration 12, loss = 0.44624779\n",
      "Iteration 13, loss = 0.41542224\n",
      "Iteration 14, loss = 0.39314868\n",
      "Iteration 15, loss = 0.37798037\n",
      "Iteration 16, loss = 0.36642478\n",
      "Iteration 17, loss = 0.35404541\n",
      "Iteration 18, loss = 0.34549212\n",
      "Iteration 19, loss = 0.33383305\n",
      "Iteration 20, loss = 0.32432178\n",
      "Iteration 21, loss = 0.32398324\n",
      "Iteration 22, loss = 0.32744050\n",
      "Iteration 23, loss = 0.31889937\n",
      "Iteration 24, loss = 0.30870666\n",
      "Iteration 25, loss = 0.30612894\n",
      "Iteration 26, loss = 0.30777648\n",
      "Iteration 27, loss = 0.31292456\n",
      "Iteration 28, loss = 0.31497839\n",
      "Iteration 29, loss = 0.31308141\n",
      "Iteration 30, loss = 0.30597990\n",
      "Iteration 31, loss = 0.31018224\n",
      "Iteration 32, loss = 0.32097908\n",
      "Iteration 33, loss = 0.32000767\n",
      "Iteration 34, loss = 0.30963430\n",
      "Iteration 35, loss = 0.30117755\n",
      "Iteration 36, loss = 0.30442635\n",
      "Iteration 37, loss = 0.30999132\n",
      "Iteration 38, loss = 0.31060325\n",
      "Iteration 39, loss = 0.30642919\n",
      "Iteration 40, loss = 0.30110391\n",
      "Iteration 41, loss = 0.30534657\n",
      "Iteration 42, loss = 0.30526382\n",
      "Iteration 43, loss = 0.30154065\n",
      "Iteration 44, loss = 0.29994273\n",
      "Iteration 45, loss = 0.29791695\n",
      "Iteration 46, loss = 0.30034160\n",
      "Iteration 47, loss = 0.30196855\n",
      "Iteration 48, loss = 0.29873558\n",
      "Iteration 49, loss = 0.29707692\n",
      "Iteration 50, loss = 0.29466751\n",
      "Iteration 51, loss = 0.29370509\n",
      "Iteration 52, loss = 0.29060574\n",
      "Iteration 53, loss = 0.29491999\n",
      "Iteration 54, loss = 0.30300074\n",
      "Iteration 55, loss = 0.29897789\n",
      "Iteration 56, loss = 0.29471465\n",
      "Iteration 57, loss = 0.29654004\n",
      "Iteration 58, loss = 0.30174275\n",
      "Iteration 59, loss = 0.31026741\n",
      "Iteration 60, loss = 0.30857329\n",
      "Iteration 61, loss = 0.29899723\n",
      "Iteration 62, loss = 0.29110445\n",
      "Iteration 63, loss = 0.28750144\n",
      "Iteration 64, loss = 0.28812363\n",
      "Iteration 65, loss = 0.29335452\n",
      "Iteration 66, loss = 0.29943334\n",
      "Iteration 67, loss = 0.30263459\n",
      "Iteration 68, loss = 0.29981268\n",
      "Iteration 69, loss = 0.29636595\n",
      "Iteration 70, loss = 0.30156090\n",
      "Iteration 71, loss = 0.30589467\n",
      "Iteration 72, loss = 0.30896500\n",
      "Iteration 73, loss = 0.30829950\n",
      "Iteration 74, loss = 0.31141985\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.90792399\n",
      "Iteration 2, loss = 1.57866098\n",
      "Iteration 3, loss = 1.32519987\n",
      "Iteration 4, loss = 1.11351993\n",
      "Iteration 5, loss = 0.94354022\n",
      "Iteration 6, loss = 0.81302776\n",
      "Iteration 7, loss = 0.71402079\n",
      "Iteration 8, loss = 0.64181046\n",
      "Iteration 9, loss = 0.58176877\n",
      "Iteration 10, loss = 0.52878290\n",
      "Iteration 11, loss = 0.48483059\n",
      "Iteration 12, loss = 0.44784909\n",
      "Iteration 13, loss = 0.41737069\n",
      "Iteration 14, loss = 0.39471947\n",
      "Iteration 15, loss = 0.37889321\n",
      "Iteration 16, loss = 0.36643948\n",
      "Iteration 17, loss = 0.35417812\n",
      "Iteration 18, loss = 0.34719910\n",
      "Iteration 19, loss = 0.33961706\n",
      "Iteration 20, loss = 0.33119162\n",
      "Iteration 21, loss = 0.32674964\n",
      "Iteration 22, loss = 0.32593743\n",
      "Iteration 23, loss = 0.32003204\n",
      "Iteration 24, loss = 0.31188079\n",
      "Iteration 25, loss = 0.30674144\n",
      "Iteration 26, loss = 0.30672881\n",
      "Iteration 27, loss = 0.31145528\n",
      "Iteration 28, loss = 0.31608254\n",
      "Iteration 29, loss = 0.31443674\n",
      "Iteration 30, loss = 0.30764461\n",
      "Iteration 31, loss = 0.31317402\n",
      "Iteration 32, loss = 0.32473255\n",
      "Iteration 33, loss = 0.31934922\n",
      "Iteration 34, loss = 0.30928356\n",
      "Iteration 35, loss = 0.30109909\n",
      "Iteration 36, loss = 0.30109043\n",
      "Iteration 37, loss = 0.30583070\n",
      "Iteration 38, loss = 0.30829356\n",
      "Iteration 39, loss = 0.30653549\n",
      "Iteration 40, loss = 0.30132452\n",
      "Iteration 41, loss = 0.30533442\n",
      "Iteration 42, loss = 0.30494871\n",
      "Iteration 43, loss = 0.29974630\n",
      "Iteration 44, loss = 0.29790475\n",
      "Iteration 45, loss = 0.30046938\n",
      "Iteration 46, loss = 0.30100412\n",
      "Iteration 47, loss = 0.30691063\n",
      "Iteration 48, loss = 0.30547217\n",
      "Iteration 49, loss = 0.30280687\n",
      "Iteration 50, loss = 0.30363105\n",
      "Iteration 51, loss = 0.30715359\n",
      "Iteration 52, loss = 0.29824555\n",
      "Iteration 53, loss = 0.29701697\n",
      "Iteration 54, loss = 0.30690760\n",
      "Iteration 55, loss = 0.30856050\n",
      "Iteration 56, loss = 0.30684618\n",
      "Iteration 57, loss = 0.30662829\n",
      "Iteration 58, loss = 0.30525577\n",
      "Iteration 59, loss = 0.30813356\n",
      "Iteration 60, loss = 0.31011690\n",
      "Iteration 61, loss = 0.30412118\n",
      "Iteration 62, loss = 0.29502284\n",
      "Iteration 63, loss = 0.28949003\n",
      "Iteration 64, loss = 0.29001532\n",
      "Iteration 65, loss = 0.29609980\n",
      "Iteration 66, loss = 0.30365219\n",
      "Iteration 67, loss = 0.30753747\n",
      "Iteration 68, loss = 0.30752745\n",
      "Iteration 69, loss = 0.30252124\n",
      "Iteration 70, loss = 0.30024415\n",
      "Iteration 71, loss = 0.30228769\n",
      "Iteration 72, loss = 0.30737847\n",
      "Iteration 73, loss = 0.31068651\n",
      "Iteration 74, loss = 0.31434771\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.91432176\n",
      "Iteration 2, loss = 1.58929965\n",
      "Iteration 3, loss = 1.33986272\n",
      "Iteration 4, loss = 1.12834289\n",
      "Iteration 5, loss = 0.95468697\n",
      "Iteration 6, loss = 0.81935606\n",
      "Iteration 7, loss = 0.71815477\n",
      "Iteration 8, loss = 0.64661740\n",
      "Iteration 9, loss = 0.58778317\n",
      "Iteration 10, loss = 0.53371509\n",
      "Iteration 11, loss = 0.48830448\n",
      "Iteration 12, loss = 0.45018989\n",
      "Iteration 13, loss = 0.41903451\n",
      "Iteration 14, loss = 0.39680379\n",
      "Iteration 15, loss = 0.38001820\n",
      "Iteration 16, loss = 0.36743909\n",
      "Iteration 17, loss = 0.35623175\n",
      "Iteration 18, loss = 0.35200250\n",
      "Iteration 19, loss = 0.34611307\n",
      "Iteration 20, loss = 0.33914876\n",
      "Iteration 21, loss = 0.33725270\n",
      "Iteration 22, loss = 0.33473129\n",
      "Iteration 23, loss = 0.32318688\n",
      "Iteration 24, loss = 0.31588663\n",
      "Iteration 25, loss = 0.31628631\n",
      "Iteration 26, loss = 0.31950761\n",
      "Iteration 27, loss = 0.32308805\n",
      "Iteration 28, loss = 0.32506867\n",
      "Iteration 29, loss = 0.32241948\n",
      "Iteration 30, loss = 0.31434548\n",
      "Iteration 31, loss = 0.32219565\n",
      "Iteration 32, loss = 0.33382603\n",
      "Iteration 33, loss = 0.32615843\n",
      "Iteration 34, loss = 0.31509674\n",
      "Iteration 35, loss = 0.30937166\n",
      "Iteration 36, loss = 0.31286625\n",
      "Iteration 37, loss = 0.31833696\n",
      "Iteration 38, loss = 0.31820391\n",
      "Iteration 39, loss = 0.31297766\n",
      "Iteration 40, loss = 0.30734396\n",
      "Iteration 41, loss = 0.31161375\n",
      "Iteration 42, loss = 0.31188253\n",
      "Iteration 43, loss = 0.30653515\n",
      "Iteration 44, loss = 0.30690369\n",
      "Iteration 45, loss = 0.31094097\n",
      "Iteration 46, loss = 0.31145455\n",
      "Iteration 47, loss = 0.31684454\n",
      "Iteration 48, loss = 0.31417557\n",
      "Iteration 49, loss = 0.31031153\n",
      "Iteration 50, loss = 0.30923943\n",
      "Iteration 51, loss = 0.31000387\n",
      "Iteration 52, loss = 0.30928907\n",
      "Iteration 53, loss = 0.31460743\n",
      "Iteration 54, loss = 0.32329856\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.90326882\n",
      "Iteration 2, loss = 1.56772086\n",
      "Iteration 3, loss = 1.31155173\n",
      "Iteration 4, loss = 1.09277520\n",
      "Iteration 5, loss = 0.91586771\n",
      "Iteration 6, loss = 0.78204049\n",
      "Iteration 7, loss = 0.68623349\n",
      "Iteration 8, loss = 0.61960349\n",
      "Iteration 9, loss = 0.56588601\n",
      "Iteration 10, loss = 0.51659706\n",
      "Iteration 11, loss = 0.47350191\n",
      "Iteration 12, loss = 0.43913107\n",
      "Iteration 13, loss = 0.41106889\n",
      "Iteration 14, loss = 0.38975144\n",
      "Iteration 15, loss = 0.37212285\n",
      "Iteration 16, loss = 0.35920092\n",
      "Iteration 17, loss = 0.34926330\n",
      "Iteration 18, loss = 0.34658998\n",
      "Iteration 19, loss = 0.34311378\n",
      "Iteration 20, loss = 0.33646129\n",
      "Iteration 21, loss = 0.33202774\n",
      "Iteration 22, loss = 0.32578351\n",
      "Iteration 23, loss = 0.31967479\n",
      "Iteration 24, loss = 0.31743675\n",
      "Iteration 25, loss = 0.31767518\n",
      "Iteration 26, loss = 0.31736102\n",
      "Iteration 27, loss = 0.31615483\n",
      "Iteration 28, loss = 0.31504727\n",
      "Iteration 29, loss = 0.31834607\n",
      "Iteration 30, loss = 0.31520246\n",
      "Iteration 31, loss = 0.32179076\n",
      "Iteration 32, loss = 0.33056159\n",
      "Iteration 33, loss = 0.32494223\n",
      "Iteration 34, loss = 0.31553593\n",
      "Iteration 35, loss = 0.31139931\n",
      "Iteration 36, loss = 0.31382039\n",
      "Iteration 37, loss = 0.31834641\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 38, loss = 0.31668536\n",
      "Iteration 39, loss = 0.31091627\n",
      "Iteration 40, loss = 0.30662991\n",
      "Iteration 41, loss = 0.31239221\n",
      "Iteration 42, loss = 0.31271429\n",
      "Iteration 43, loss = 0.30499439\n",
      "Iteration 44, loss = 0.30394505\n",
      "Iteration 45, loss = 0.30858453\n",
      "Iteration 46, loss = 0.30671815\n",
      "Iteration 47, loss = 0.31169904\n",
      "Iteration 48, loss = 0.31203288\n",
      "Iteration 49, loss = 0.31119997\n",
      "Iteration 50, loss = 0.31135863\n",
      "Iteration 51, loss = 0.31337657\n",
      "Iteration 52, loss = 0.30951509\n",
      "Iteration 53, loss = 0.31186619\n",
      "Iteration 54, loss = 0.32060164\n",
      "Iteration 55, loss = 0.31647772\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.90952184\n",
      "Iteration 2, loss = 1.57808408\n",
      "Iteration 3, loss = 1.31990624\n",
      "Iteration 4, loss = 1.10095079\n",
      "Iteration 5, loss = 0.92446399\n",
      "Iteration 6, loss = 0.78788568\n",
      "Iteration 7, loss = 0.68895926\n",
      "Iteration 8, loss = 0.61825748\n",
      "Iteration 9, loss = 0.56131283\n",
      "Iteration 10, loss = 0.50980847\n",
      "Iteration 11, loss = 0.46390326\n",
      "Iteration 12, loss = 0.42874794\n",
      "Iteration 13, loss = 0.40000815\n",
      "Iteration 14, loss = 0.37819174\n",
      "Iteration 15, loss = 0.36015097\n",
      "Iteration 16, loss = 0.34730617\n",
      "Iteration 17, loss = 0.33589624\n",
      "Iteration 18, loss = 0.33368487\n",
      "Iteration 19, loss = 0.33392141\n",
      "Iteration 20, loss = 0.32664396\n",
      "Iteration 21, loss = 0.32033977\n",
      "Iteration 22, loss = 0.31297182\n",
      "Iteration 23, loss = 0.30985061\n",
      "Iteration 24, loss = 0.30952642\n",
      "Iteration 25, loss = 0.30884374\n",
      "Iteration 26, loss = 0.30726612\n",
      "Iteration 27, loss = 0.30534448\n",
      "Iteration 28, loss = 0.30238398\n",
      "Iteration 29, loss = 0.31393153\n",
      "Iteration 30, loss = 0.30948431\n",
      "Iteration 31, loss = 0.30446032\n",
      "Iteration 32, loss = 0.30668807\n",
      "Iteration 33, loss = 0.30991111\n",
      "Iteration 34, loss = 0.30873092\n",
      "Iteration 35, loss = 0.30284111\n",
      "Iteration 36, loss = 0.30065392\n",
      "Iteration 37, loss = 0.30207430\n",
      "Iteration 38, loss = 0.30262678\n",
      "Iteration 39, loss = 0.30053704\n",
      "Iteration 40, loss = 0.29748001\n",
      "Iteration 41, loss = 0.29924749\n",
      "Iteration 42, loss = 0.29808122\n",
      "Iteration 43, loss = 0.29801535\n",
      "Iteration 44, loss = 0.29924561\n",
      "Iteration 45, loss = 0.29872915\n",
      "Iteration 46, loss = 0.29682352\n",
      "Iteration 47, loss = 0.30426824\n",
      "Iteration 48, loss = 0.30016428\n",
      "Iteration 49, loss = 0.29547148\n",
      "Iteration 50, loss = 0.29839246\n",
      "Iteration 51, loss = 0.30287242\n",
      "Iteration 52, loss = 0.30125704\n",
      "Iteration 53, loss = 0.30445282\n",
      "Iteration 54, loss = 0.30747190\n",
      "Iteration 55, loss = 0.30238194\n",
      "Iteration 56, loss = 0.29904061\n",
      "Iteration 57, loss = 0.30087148\n",
      "Iteration 58, loss = 0.30135102\n",
      "Iteration 59, loss = 0.30288050\n",
      "Iteration 60, loss = 0.30195194\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.29978876\n",
      "Iteration 2, loss = 1.66776028\n",
      "Iteration 3, loss = 1.32961026\n",
      "Iteration 4, loss = 1.11587654\n",
      "Iteration 5, loss = 0.96121450\n",
      "Iteration 6, loss = 0.83939237\n",
      "Iteration 7, loss = 0.74063138\n",
      "Iteration 8, loss = 0.65976687\n",
      "Iteration 9, loss = 0.59850831\n",
      "Iteration 10, loss = 0.54972864\n",
      "Iteration 11, loss = 0.51000483\n",
      "Iteration 12, loss = 0.47748353\n",
      "Iteration 13, loss = 0.44728669\n",
      "Iteration 14, loss = 0.42486238\n",
      "Iteration 15, loss = 0.40780656\n",
      "Iteration 16, loss = 0.38993266\n",
      "Iteration 17, loss = 0.37290105\n",
      "Iteration 18, loss = 0.35901467\n",
      "Iteration 19, loss = 0.34872889\n",
      "Iteration 20, loss = 0.34323071\n",
      "Iteration 21, loss = 0.33935086\n",
      "Iteration 22, loss = 0.33194471\n",
      "Iteration 23, loss = 0.32775893\n",
      "Iteration 24, loss = 0.32151624\n",
      "Iteration 25, loss = 0.32241846\n",
      "Iteration 26, loss = 0.32755612\n",
      "Iteration 27, loss = 0.32362626\n",
      "Iteration 28, loss = 0.32515707\n",
      "Iteration 29, loss = 0.31537598\n",
      "Iteration 30, loss = 0.31807817\n",
      "Iteration 31, loss = 0.32366174\n",
      "Iteration 32, loss = 0.32771277\n",
      "Iteration 33, loss = 0.32771864\n",
      "Iteration 34, loss = 0.31953337\n",
      "Iteration 35, loss = 0.30954629\n",
      "Iteration 36, loss = 0.31692687\n",
      "Iteration 37, loss = 0.33327625\n",
      "Iteration 38, loss = 0.33345131\n",
      "Iteration 39, loss = 0.32169054\n",
      "Iteration 40, loss = 0.31084500\n",
      "Iteration 41, loss = 0.30517715\n",
      "Iteration 42, loss = 0.31471124\n",
      "Iteration 43, loss = 0.32649184\n",
      "Iteration 44, loss = 0.33338986\n",
      "Iteration 45, loss = 0.32365178\n",
      "Iteration 46, loss = 0.31368993\n",
      "Iteration 47, loss = 0.32022232\n",
      "Iteration 48, loss = 0.33010277\n",
      "Iteration 49, loss = 0.32908518\n",
      "Iteration 50, loss = 0.31541137\n",
      "Iteration 51, loss = 0.30198424\n",
      "Iteration 52, loss = 0.29445144\n",
      "Iteration 53, loss = 0.29886747\n",
      "Iteration 54, loss = 0.31126539\n",
      "Iteration 55, loss = 0.31702815\n",
      "Iteration 56, loss = 0.31663810\n",
      "Iteration 57, loss = 0.31771780\n",
      "Iteration 58, loss = 0.31656535\n",
      "Iteration 59, loss = 0.31009055\n",
      "Iteration 60, loss = 0.30938762\n",
      "Iteration 61, loss = 0.31191577\n",
      "Iteration 62, loss = 0.30845486\n",
      "Iteration 63, loss = 0.29962031\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.30041522\n",
      "Iteration 2, loss = 1.65314313\n",
      "Iteration 3, loss = 1.29612833\n",
      "Iteration 4, loss = 1.09096108\n",
      "Iteration 5, loss = 0.94735777\n",
      "Iteration 6, loss = 0.83301787\n",
      "Iteration 7, loss = 0.73695941\n",
      "Iteration 8, loss = 0.66129259\n",
      "Iteration 9, loss = 0.60712796\n",
      "Iteration 10, loss = 0.56156739\n",
      "Iteration 11, loss = 0.52495448\n",
      "Iteration 12, loss = 0.49416054\n",
      "Iteration 13, loss = 0.46992720\n",
      "Iteration 14, loss = 0.44631576\n",
      "Iteration 15, loss = 0.42327917\n",
      "Iteration 16, loss = 0.40238142\n",
      "Iteration 17, loss = 0.38362011\n",
      "Iteration 18, loss = 0.37104103\n",
      "Iteration 19, loss = 0.35561329\n",
      "Iteration 20, loss = 0.34661429\n",
      "Iteration 21, loss = 0.34117260\n",
      "Iteration 22, loss = 0.33585616\n",
      "Iteration 23, loss = 0.33015353\n",
      "Iteration 24, loss = 0.32917966\n",
      "Iteration 25, loss = 0.32149302\n",
      "Iteration 26, loss = 0.31775342\n",
      "Iteration 27, loss = 0.32146309\n",
      "Iteration 28, loss = 0.31386459\n",
      "Iteration 29, loss = 0.30976582\n",
      "Iteration 30, loss = 0.31073059\n",
      "Iteration 31, loss = 0.31212118\n",
      "Iteration 32, loss = 0.31417927\n",
      "Iteration 33, loss = 0.31210263\n",
      "Iteration 34, loss = 0.30609373\n",
      "Iteration 35, loss = 0.30117036\n",
      "Iteration 36, loss = 0.30099485\n",
      "Iteration 37, loss = 0.30436090\n",
      "Iteration 38, loss = 0.30516586\n",
      "Iteration 39, loss = 0.30469722\n",
      "Iteration 40, loss = 0.30441078\n",
      "Iteration 41, loss = 0.30127460\n",
      "Iteration 42, loss = 0.30839141\n",
      "Iteration 43, loss = 0.31258152\n",
      "Iteration 44, loss = 0.30758561\n",
      "Iteration 45, loss = 0.29892374\n",
      "Iteration 46, loss = 0.29457254\n",
      "Iteration 47, loss = 0.30114703\n",
      "Iteration 48, loss = 0.31321437\n",
      "Iteration 49, loss = 0.32052345\n",
      "Iteration 50, loss = 0.30958718\n",
      "Iteration 51, loss = 0.30226351\n",
      "Iteration 52, loss = 0.30763890\n",
      "Iteration 53, loss = 0.30784798\n",
      "Iteration 54, loss = 0.30620084\n",
      "Iteration 55, loss = 0.30818884\n",
      "Iteration 56, loss = 0.30285111\n",
      "Iteration 57, loss = 0.28916862\n",
      "Iteration 58, loss = 0.29056945\n",
      "Iteration 59, loss = 0.30294916\n",
      "Iteration 60, loss = 0.30961524\n",
      "Iteration 61, loss = 0.30828917\n",
      "Iteration 62, loss = 0.30188290\n",
      "Iteration 63, loss = 0.29023310\n",
      "Iteration 64, loss = 0.28651473\n",
      "Iteration 65, loss = 0.29284310\n",
      "Iteration 66, loss = 0.29517341\n",
      "Iteration 67, loss = 0.29507001\n",
      "Iteration 68, loss = 0.29461040\n",
      "Iteration 69, loss = 0.29331902\n",
      "Iteration 70, loss = 0.29219065\n",
      "Iteration 71, loss = 0.29314892\n",
      "Iteration 72, loss = 0.30170739\n",
      "Iteration 73, loss = 0.29151620\n",
      "Iteration 74, loss = 0.28848941\n",
      "Iteration 75, loss = 0.29447216\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.30343606\n",
      "Iteration 2, loss = 1.66317384\n",
      "Iteration 3, loss = 1.30451320\n",
      "Iteration 4, loss = 1.09551885\n",
      "Iteration 5, loss = 0.94922232\n",
      "Iteration 6, loss = 0.83157322\n",
      "Iteration 7, loss = 0.73293453\n",
      "Iteration 8, loss = 0.65550290\n",
      "Iteration 9, loss = 0.59974073\n",
      "Iteration 10, loss = 0.55284211\n",
      "Iteration 11, loss = 0.51519802\n",
      "Iteration 12, loss = 0.48415472\n",
      "Iteration 13, loss = 0.46163252\n",
      "Iteration 14, loss = 0.43906867\n",
      "Iteration 15, loss = 0.41692557\n",
      "Iteration 16, loss = 0.39854595\n",
      "Iteration 17, loss = 0.38181487\n",
      "Iteration 18, loss = 0.36903235\n",
      "Iteration 19, loss = 0.35597506\n",
      "Iteration 20, loss = 0.34808167\n",
      "Iteration 21, loss = 0.34247781\n",
      "Iteration 22, loss = 0.33553120\n",
      "Iteration 23, loss = 0.32825461\n",
      "Iteration 24, loss = 0.32660474\n",
      "Iteration 25, loss = 0.32072709\n",
      "Iteration 26, loss = 0.31698242\n",
      "Iteration 27, loss = 0.31760917\n",
      "Iteration 28, loss = 0.31224664\n",
      "Iteration 29, loss = 0.30882597\n",
      "Iteration 30, loss = 0.30743738\n",
      "Iteration 31, loss = 0.30460011\n",
      "Iteration 32, loss = 0.30510592\n",
      "Iteration 33, loss = 0.30746775\n",
      "Iteration 34, loss = 0.30620488\n",
      "Iteration 35, loss = 0.30463663\n",
      "Iteration 36, loss = 0.30107068\n",
      "Iteration 37, loss = 0.30061693\n",
      "Iteration 38, loss = 0.30372118\n",
      "Iteration 39, loss = 0.30827409\n",
      "Iteration 40, loss = 0.30595281\n",
      "Iteration 41, loss = 0.30213718\n",
      "Iteration 42, loss = 0.30778887\n",
      "Iteration 43, loss = 0.30983791\n",
      "Iteration 44, loss = 0.30463708\n",
      "Iteration 45, loss = 0.29709813\n",
      "Iteration 46, loss = 0.29435446\n",
      "Iteration 47, loss = 0.29858389\n",
      "Iteration 48, loss = 0.31421274\n",
      "Iteration 49, loss = 0.32572327\n",
      "Iteration 50, loss = 0.31725098\n",
      "Iteration 51, loss = 0.31367770\n",
      "Iteration 52, loss = 0.30832213\n",
      "Iteration 53, loss = 0.30456184\n",
      "Iteration 54, loss = 0.30802428\n",
      "Iteration 55, loss = 0.31331149\n",
      "Iteration 56, loss = 0.30582862\n",
      "Iteration 57, loss = 0.28969101\n",
      "Iteration 58, loss = 0.28757813\n",
      "Iteration 59, loss = 0.29963412\n",
      "Iteration 60, loss = 0.30946174\n",
      "Iteration 61, loss = 0.30680568\n",
      "Iteration 62, loss = 0.30442417\n",
      "Iteration 63, loss = 0.29409861\n",
      "Iteration 64, loss = 0.28790659\n",
      "Iteration 65, loss = 0.29189828\n",
      "Iteration 66, loss = 0.29288260\n",
      "Iteration 67, loss = 0.29373409\n",
      "Iteration 68, loss = 0.29555092\n",
      "Iteration 69, loss = 0.29347473\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.28627989\n",
      "Iteration 2, loss = 1.64352768\n",
      "Iteration 3, loss = 1.28721730\n",
      "Iteration 4, loss = 1.08369235\n",
      "Iteration 5, loss = 0.94176648\n",
      "Iteration 6, loss = 0.82857772\n",
      "Iteration 7, loss = 0.73379174\n",
      "Iteration 8, loss = 0.65961694\n",
      "Iteration 9, loss = 0.60628924\n",
      "Iteration 10, loss = 0.56100110\n",
      "Iteration 11, loss = 0.52193700\n",
      "Iteration 12, loss = 0.48724010\n",
      "Iteration 13, loss = 0.46108468\n",
      "Iteration 14, loss = 0.43792553\n",
      "Iteration 15, loss = 0.41715340\n",
      "Iteration 16, loss = 0.40093549\n",
      "Iteration 17, loss = 0.38694610\n",
      "Iteration 18, loss = 0.37318713\n",
      "Iteration 19, loss = 0.35705183\n",
      "Iteration 20, loss = 0.34456386\n",
      "Iteration 21, loss = 0.33507059\n",
      "Iteration 22, loss = 0.32850636\n",
      "Iteration 23, loss = 0.32436468\n",
      "Iteration 24, loss = 0.32284472\n",
      "Iteration 25, loss = 0.31975364\n",
      "Iteration 26, loss = 0.31530431\n",
      "Iteration 27, loss = 0.31123945\n",
      "Iteration 28, loss = 0.30560549\n",
      "Iteration 29, loss = 0.30336588\n",
      "Iteration 30, loss = 0.30276431\n",
      "Iteration 31, loss = 0.30199780\n",
      "Iteration 32, loss = 0.30509077\n",
      "Iteration 33, loss = 0.30619003\n",
      "Iteration 34, loss = 0.30189952\n",
      "Iteration 35, loss = 0.30046102\n",
      "Iteration 36, loss = 0.29916731\n",
      "Iteration 37, loss = 0.29743023\n",
      "Iteration 38, loss = 0.29913266\n",
      "Iteration 39, loss = 0.30163636\n",
      "Iteration 40, loss = 0.30351075\n",
      "Iteration 41, loss = 0.29698347\n",
      "Iteration 42, loss = 0.29906893\n",
      "Iteration 43, loss = 0.30055233\n",
      "Iteration 44, loss = 0.29616195\n",
      "Iteration 45, loss = 0.29291566\n",
      "Iteration 46, loss = 0.29319111\n",
      "Iteration 47, loss = 0.29823054\n",
      "Iteration 48, loss = 0.30656742\n",
      "Iteration 49, loss = 0.30900727\n",
      "Iteration 50, loss = 0.30090839\n",
      "Iteration 51, loss = 0.30110413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 52, loss = 0.30096157\n",
      "Iteration 53, loss = 0.29679679\n",
      "Iteration 54, loss = 0.29431932\n",
      "Iteration 55, loss = 0.29603196\n",
      "Iteration 56, loss = 0.29216414\n",
      "Iteration 57, loss = 0.28638217\n",
      "Iteration 58, loss = 0.28873801\n",
      "Iteration 59, loss = 0.28970021\n",
      "Iteration 60, loss = 0.28926641\n",
      "Iteration 61, loss = 0.28739577\n",
      "Iteration 62, loss = 0.28755022\n",
      "Iteration 63, loss = 0.28277254\n",
      "Iteration 64, loss = 0.27950726\n",
      "Iteration 65, loss = 0.28132933\n",
      "Iteration 66, loss = 0.28218630\n",
      "Iteration 67, loss = 0.28396270\n",
      "Iteration 68, loss = 0.28613282\n",
      "Iteration 69, loss = 0.29055813\n",
      "Iteration 70, loss = 0.29357982\n",
      "Iteration 71, loss = 0.29391161\n",
      "Iteration 72, loss = 0.29436798\n",
      "Iteration 73, loss = 0.28750406\n",
      "Iteration 74, loss = 0.27884034\n",
      "Iteration 75, loss = 0.27664130\n",
      "Iteration 76, loss = 0.28384097\n",
      "Iteration 77, loss = 0.29586510\n",
      "Iteration 78, loss = 0.29575575\n",
      "Iteration 79, loss = 0.28981763\n",
      "Iteration 80, loss = 0.28467960\n",
      "Iteration 81, loss = 0.28622787\n",
      "Iteration 82, loss = 0.29222902\n",
      "Iteration 83, loss = 0.29550502\n",
      "Iteration 84, loss = 0.29385142\n",
      "Iteration 85, loss = 0.28791186\n",
      "Iteration 86, loss = 0.28100484\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.29117345\n",
      "Iteration 2, loss = 1.65833164\n",
      "Iteration 3, loss = 1.30803412\n",
      "Iteration 4, loss = 1.10429968\n",
      "Iteration 5, loss = 0.95779120\n",
      "Iteration 6, loss = 0.83948007\n",
      "Iteration 7, loss = 0.74133381\n",
      "Iteration 8, loss = 0.66627991\n",
      "Iteration 9, loss = 0.61100996\n",
      "Iteration 10, loss = 0.56501292\n",
      "Iteration 11, loss = 0.52698935\n",
      "Iteration 12, loss = 0.49177323\n",
      "Iteration 13, loss = 0.46425942\n",
      "Iteration 14, loss = 0.43950413\n",
      "Iteration 15, loss = 0.41606753\n",
      "Iteration 16, loss = 0.39765987\n",
      "Iteration 17, loss = 0.38063979\n",
      "Iteration 18, loss = 0.36886114\n",
      "Iteration 19, loss = 0.35761201\n",
      "Iteration 20, loss = 0.35047184\n",
      "Iteration 21, loss = 0.34473789\n",
      "Iteration 22, loss = 0.33796655\n",
      "Iteration 23, loss = 0.32947074\n",
      "Iteration 24, loss = 0.32533974\n",
      "Iteration 25, loss = 0.32006055\n",
      "Iteration 26, loss = 0.31535023\n",
      "Iteration 27, loss = 0.31323412\n",
      "Iteration 28, loss = 0.31055797\n",
      "Iteration 29, loss = 0.31072027\n",
      "Iteration 30, loss = 0.30928669\n",
      "Iteration 31, loss = 0.30704023\n",
      "Iteration 32, loss = 0.30840018\n",
      "Iteration 33, loss = 0.31071766\n",
      "Iteration 34, loss = 0.30938173\n",
      "Iteration 35, loss = 0.30962832\n",
      "Iteration 36, loss = 0.30860938\n",
      "Iteration 37, loss = 0.30348842\n",
      "Iteration 38, loss = 0.30131971\n",
      "Iteration 39, loss = 0.30763789\n",
      "Iteration 40, loss = 0.30843153\n",
      "Iteration 41, loss = 0.30343966\n",
      "Iteration 42, loss = 0.30065715\n",
      "Iteration 43, loss = 0.30064302\n",
      "Iteration 44, loss = 0.29854493\n",
      "Iteration 45, loss = 0.29735103\n",
      "Iteration 46, loss = 0.29821436\n",
      "Iteration 47, loss = 0.30001323\n",
      "Iteration 48, loss = 0.30917407\n",
      "Iteration 49, loss = 0.31717112\n",
      "Iteration 50, loss = 0.31154920\n",
      "Iteration 51, loss = 0.30600754\n",
      "Iteration 52, loss = 0.30009774\n",
      "Iteration 53, loss = 0.29512919\n",
      "Iteration 54, loss = 0.29347898\n",
      "Iteration 55, loss = 0.29530076\n",
      "Iteration 56, loss = 0.29293941\n",
      "Iteration 57, loss = 0.28795349\n",
      "Iteration 58, loss = 0.29201806\n",
      "Iteration 59, loss = 0.29958412\n",
      "Iteration 60, loss = 0.30579616\n",
      "Iteration 61, loss = 0.30334273\n",
      "Iteration 62, loss = 0.29535791\n",
      "Iteration 63, loss = 0.28790964\n",
      "Iteration 64, loss = 0.28416072\n",
      "Iteration 65, loss = 0.28581061\n",
      "Iteration 66, loss = 0.28491936\n",
      "Iteration 67, loss = 0.28456702\n",
      "Iteration 68, loss = 0.28771714\n",
      "Iteration 69, loss = 0.29098309\n",
      "Iteration 70, loss = 0.28862589\n",
      "Iteration 71, loss = 0.28970847\n",
      "Iteration 72, loss = 0.29284758\n",
      "Iteration 73, loss = 0.29265359\n",
      "Iteration 74, loss = 0.28567072\n",
      "Iteration 75, loss = 0.28375201\n",
      "Iteration 76, loss = 0.28852741\n",
      "Iteration 77, loss = 0.29811319\n",
      "Iteration 78, loss = 0.29963529\n",
      "Iteration 79, loss = 0.29622766\n",
      "Iteration 80, loss = 0.29361931\n",
      "Iteration 81, loss = 0.29243714\n",
      "Iteration 82, loss = 0.29268488\n",
      "Iteration 83, loss = 0.29526059\n",
      "Iteration 84, loss = 0.29585500\n",
      "Iteration 85, loss = 0.29366183\n",
      "Iteration 86, loss = 0.28944498\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.29855899\n",
      "Iteration 2, loss = 1.66191199\n",
      "Iteration 3, loss = 1.30485865\n",
      "Iteration 4, loss = 1.09420750\n",
      "Iteration 5, loss = 0.94400531\n",
      "Iteration 6, loss = 0.82589375\n",
      "Iteration 7, loss = 0.72851202\n",
      "Iteration 8, loss = 0.65296766\n",
      "Iteration 9, loss = 0.59709023\n",
      "Iteration 10, loss = 0.55004255\n",
      "Iteration 11, loss = 0.51437983\n",
      "Iteration 12, loss = 0.48228222\n",
      "Iteration 13, loss = 0.45242286\n",
      "Iteration 14, loss = 0.42305242\n",
      "Iteration 15, loss = 0.39962199\n",
      "Iteration 16, loss = 0.38325204\n",
      "Iteration 17, loss = 0.36840863\n",
      "Iteration 18, loss = 0.35670131\n",
      "Iteration 19, loss = 0.34641367\n",
      "Iteration 20, loss = 0.34051229\n",
      "Iteration 21, loss = 0.33453446\n",
      "Iteration 22, loss = 0.32871558\n",
      "Iteration 23, loss = 0.32199727\n",
      "Iteration 24, loss = 0.31855586\n",
      "Iteration 25, loss = 0.31662774\n",
      "Iteration 26, loss = 0.31450854\n",
      "Iteration 27, loss = 0.30970648\n",
      "Iteration 28, loss = 0.30499055\n",
      "Iteration 29, loss = 0.30129800\n",
      "Iteration 30, loss = 0.29880949\n",
      "Iteration 31, loss = 0.29994357\n",
      "Iteration 32, loss = 0.30305121\n",
      "Iteration 33, loss = 0.30655009\n",
      "Iteration 34, loss = 0.30520240\n",
      "Iteration 35, loss = 0.30522745\n",
      "Iteration 36, loss = 0.30527558\n",
      "Iteration 37, loss = 0.29706055\n",
      "Iteration 38, loss = 0.30008448\n",
      "Iteration 39, loss = 0.30564969\n",
      "Iteration 40, loss = 0.30227191\n",
      "Iteration 41, loss = 0.29513281\n",
      "Iteration 42, loss = 0.29196924\n",
      "Iteration 43, loss = 0.29264271\n",
      "Iteration 44, loss = 0.29159818\n",
      "Iteration 45, loss = 0.29357251\n",
      "Iteration 46, loss = 0.29461977\n",
      "Iteration 47, loss = 0.29368061\n",
      "Iteration 48, loss = 0.29892056\n",
      "Iteration 49, loss = 0.30892110\n",
      "Iteration 50, loss = 0.30950776\n",
      "Iteration 51, loss = 0.29854444\n",
      "Iteration 52, loss = 0.28686548\n",
      "Iteration 53, loss = 0.28050685\n",
      "Iteration 54, loss = 0.27679543\n",
      "Iteration 55, loss = 0.28187747\n",
      "Iteration 56, loss = 0.28744850\n",
      "Iteration 57, loss = 0.28521868\n",
      "Iteration 58, loss = 0.28761248\n",
      "Iteration 59, loss = 0.28819988\n",
      "Iteration 60, loss = 0.29088878\n",
      "Iteration 61, loss = 0.29367494\n",
      "Iteration 62, loss = 0.29122468\n",
      "Iteration 63, loss = 0.28917645\n",
      "Iteration 64, loss = 0.28933461\n",
      "Iteration 65, loss = 0.29030000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.28013204\n",
      "Iteration 2, loss = 1.65861549\n",
      "Iteration 3, loss = 1.31520088\n",
      "Iteration 4, loss = 1.10840439\n",
      "Iteration 5, loss = 0.95736275\n",
      "Iteration 6, loss = 0.83664953\n",
      "Iteration 7, loss = 0.73871848\n",
      "Iteration 8, loss = 0.66442788\n",
      "Iteration 9, loss = 0.60786736\n",
      "Iteration 10, loss = 0.56016281\n",
      "Iteration 11, loss = 0.52148939\n",
      "Iteration 12, loss = 0.48626956\n",
      "Iteration 13, loss = 0.45484746\n",
      "Iteration 14, loss = 0.42536955\n",
      "Iteration 15, loss = 0.40182027\n",
      "Iteration 16, loss = 0.38449959\n",
      "Iteration 17, loss = 0.36897305\n",
      "Iteration 18, loss = 0.35731233\n",
      "Iteration 19, loss = 0.34761320\n",
      "Iteration 20, loss = 0.34320033\n",
      "Iteration 21, loss = 0.33741969\n",
      "Iteration 22, loss = 0.33173886\n",
      "Iteration 23, loss = 0.32389307\n",
      "Iteration 24, loss = 0.31784033\n",
      "Iteration 25, loss = 0.31245191\n",
      "Iteration 26, loss = 0.30937382\n",
      "Iteration 27, loss = 0.30522298\n",
      "Iteration 28, loss = 0.30507290\n",
      "Iteration 29, loss = 0.30440068\n",
      "Iteration 30, loss = 0.30252032\n",
      "Iteration 31, loss = 0.30567469\n",
      "Iteration 32, loss = 0.31278686\n",
      "Iteration 33, loss = 0.31819633\n",
      "Iteration 34, loss = 0.31288375\n",
      "Iteration 35, loss = 0.30849837\n",
      "Iteration 36, loss = 0.30801207\n",
      "Iteration 37, loss = 0.30489512\n",
      "Iteration 38, loss = 0.30570809\n",
      "Iteration 39, loss = 0.30431000\n",
      "Iteration 40, loss = 0.30018639\n",
      "Iteration 41, loss = 0.29377598\n",
      "Iteration 42, loss = 0.29123979\n",
      "Iteration 43, loss = 0.29228050\n",
      "Iteration 44, loss = 0.29147186\n",
      "Iteration 45, loss = 0.29230943\n",
      "Iteration 46, loss = 0.29599709\n",
      "Iteration 47, loss = 0.29933790\n",
      "Iteration 48, loss = 0.30241063\n",
      "Iteration 49, loss = 0.30360604\n",
      "Iteration 50, loss = 0.30206404\n",
      "Iteration 51, loss = 0.30017785\n",
      "Iteration 52, loss = 0.28819951\n",
      "Iteration 53, loss = 0.28206290\n",
      "Iteration 54, loss = 0.28379810\n",
      "Iteration 55, loss = 0.28781107\n",
      "Iteration 56, loss = 0.28860759\n",
      "Iteration 57, loss = 0.28323456\n",
      "Iteration 58, loss = 0.28832276\n",
      "Iteration 59, loss = 0.29305567\n",
      "Iteration 60, loss = 0.29697019\n",
      "Iteration 61, loss = 0.29757877\n",
      "Iteration 62, loss = 0.29322042\n",
      "Iteration 63, loss = 0.29526258\n",
      "Iteration 64, loss = 0.30027849\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.28742488\n",
      "Iteration 2, loss = 1.65866765\n",
      "Iteration 3, loss = 1.31849852\n",
      "Iteration 4, loss = 1.11729157\n",
      "Iteration 5, loss = 0.96750715\n",
      "Iteration 6, loss = 0.84791699\n",
      "Iteration 7, loss = 0.75292156\n",
      "Iteration 8, loss = 0.67892525\n",
      "Iteration 9, loss = 0.62230922\n",
      "Iteration 10, loss = 0.57514736\n",
      "Iteration 11, loss = 0.53551068\n",
      "Iteration 12, loss = 0.49839798\n",
      "Iteration 13, loss = 0.46711016\n",
      "Iteration 14, loss = 0.43811475\n",
      "Iteration 15, loss = 0.41541434\n",
      "Iteration 16, loss = 0.39842018\n",
      "Iteration 17, loss = 0.38195631\n",
      "Iteration 18, loss = 0.37108797\n",
      "Iteration 19, loss = 0.36220947\n",
      "Iteration 20, loss = 0.35676063\n",
      "Iteration 21, loss = 0.34907759\n",
      "Iteration 22, loss = 0.34243956\n",
      "Iteration 23, loss = 0.33575611\n",
      "Iteration 24, loss = 0.33147100\n",
      "Iteration 25, loss = 0.32897505\n",
      "Iteration 26, loss = 0.32339582\n",
      "Iteration 27, loss = 0.31628262\n",
      "Iteration 28, loss = 0.31424014\n",
      "Iteration 29, loss = 0.31547535\n",
      "Iteration 30, loss = 0.31630218\n",
      "Iteration 31, loss = 0.31692719\n",
      "Iteration 32, loss = 0.32042315\n",
      "Iteration 33, loss = 0.32666745\n",
      "Iteration 34, loss = 0.32236433\n",
      "Iteration 35, loss = 0.31888949\n",
      "Iteration 36, loss = 0.31731486\n",
      "Iteration 37, loss = 0.31316020\n",
      "Iteration 38, loss = 0.31589841\n",
      "Iteration 39, loss = 0.31769567\n",
      "Iteration 40, loss = 0.31578864\n",
      "Iteration 41, loss = 0.31011180\n",
      "Iteration 42, loss = 0.30640151\n",
      "Iteration 43, loss = 0.30446829\n",
      "Iteration 44, loss = 0.30042359\n",
      "Iteration 45, loss = 0.29860391\n",
      "Iteration 46, loss = 0.30383042\n",
      "Iteration 47, loss = 0.31431804\n",
      "Iteration 48, loss = 0.31909398\n",
      "Iteration 49, loss = 0.31073086\n",
      "Iteration 50, loss = 0.31062115\n",
      "Iteration 51, loss = 0.31254797\n",
      "Iteration 52, loss = 0.30593499\n",
      "Iteration 53, loss = 0.30248325\n",
      "Iteration 54, loss = 0.30256047\n",
      "Iteration 55, loss = 0.30215188\n",
      "Iteration 56, loss = 0.29876959\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.28454363\n",
      "Iteration 2, loss = 1.64395683\n",
      "Iteration 3, loss = 1.29973575\n",
      "Iteration 4, loss = 1.09501910\n",
      "Iteration 5, loss = 0.94066804\n",
      "Iteration 6, loss = 0.81779127\n",
      "Iteration 7, loss = 0.72208293\n",
      "Iteration 8, loss = 0.65040652\n",
      "Iteration 9, loss = 0.59837903\n",
      "Iteration 10, loss = 0.55512314\n",
      "Iteration 11, loss = 0.51764887\n",
      "Iteration 12, loss = 0.48269205\n",
      "Iteration 13, loss = 0.45204406\n",
      "Iteration 14, loss = 0.42537880\n",
      "Iteration 15, loss = 0.40459308\n",
      "Iteration 16, loss = 0.38803259\n",
      "Iteration 17, loss = 0.37146157\n",
      "Iteration 18, loss = 0.36181826\n",
      "Iteration 19, loss = 0.35427843\n",
      "Iteration 20, loss = 0.34945825\n",
      "Iteration 21, loss = 0.34341471\n",
      "Iteration 22, loss = 0.33768716\n",
      "Iteration 23, loss = 0.33072937\n",
      "Iteration 24, loss = 0.32672838\n",
      "Iteration 25, loss = 0.32212516\n",
      "Iteration 26, loss = 0.31722083\n",
      "Iteration 27, loss = 0.31129740\n",
      "Iteration 28, loss = 0.31209665\n",
      "Iteration 29, loss = 0.31445554\n",
      "Iteration 30, loss = 0.31161890\n",
      "Iteration 31, loss = 0.30898975\n",
      "Iteration 32, loss = 0.31232758\n",
      "Iteration 33, loss = 0.32054860\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 34, loss = 0.31865138\n",
      "Iteration 35, loss = 0.31699768\n",
      "Iteration 36, loss = 0.31776586\n",
      "Iteration 37, loss = 0.31367908\n",
      "Iteration 38, loss = 0.31463718\n",
      "Iteration 39, loss = 0.31201028\n",
      "Iteration 40, loss = 0.30499088\n",
      "Iteration 41, loss = 0.30027061\n",
      "Iteration 42, loss = 0.30182642\n",
      "Iteration 43, loss = 0.30420400\n",
      "Iteration 44, loss = 0.30238562\n",
      "Iteration 45, loss = 0.29992477\n",
      "Iteration 46, loss = 0.29900456\n",
      "Iteration 47, loss = 0.30854616\n",
      "Iteration 48, loss = 0.31999219\n",
      "Iteration 49, loss = 0.31445530\n",
      "Iteration 50, loss = 0.31064269\n",
      "Iteration 51, loss = 0.31341308\n",
      "Iteration 52, loss = 0.30743847\n",
      "Iteration 53, loss = 0.29951370\n",
      "Iteration 54, loss = 0.29474482\n",
      "Iteration 55, loss = 0.29498029\n",
      "Iteration 56, loss = 0.29892117\n",
      "Iteration 57, loss = 0.29094407\n",
      "Iteration 58, loss = 0.28955332\n",
      "Iteration 59, loss = 0.29154340\n",
      "Iteration 60, loss = 0.29842508\n",
      "Iteration 61, loss = 0.30162751\n",
      "Iteration 62, loss = 0.30261202\n",
      "Iteration 63, loss = 0.30463887\n",
      "Iteration 64, loss = 0.30762982\n",
      "Iteration 65, loss = 0.30903089\n",
      "Iteration 66, loss = 0.30875796\n",
      "Iteration 67, loss = 0.29964090\n",
      "Iteration 68, loss = 0.29892416\n",
      "Iteration 69, loss = 0.29856691\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.28283739\n",
      "Iteration 2, loss = 1.64290067\n",
      "Iteration 3, loss = 1.29808368\n",
      "Iteration 4, loss = 1.09313083\n",
      "Iteration 5, loss = 0.93741021\n",
      "Iteration 6, loss = 0.81399227\n",
      "Iteration 7, loss = 0.71807389\n",
      "Iteration 8, loss = 0.64620428\n",
      "Iteration 9, loss = 0.58997237\n",
      "Iteration 10, loss = 0.54446222\n",
      "Iteration 11, loss = 0.50647707\n",
      "Iteration 12, loss = 0.47125915\n",
      "Iteration 13, loss = 0.44030959\n",
      "Iteration 14, loss = 0.41372296\n",
      "Iteration 15, loss = 0.39319233\n",
      "Iteration 16, loss = 0.37766788\n",
      "Iteration 17, loss = 0.36168502\n",
      "Iteration 18, loss = 0.35204015\n",
      "Iteration 19, loss = 0.34470504\n",
      "Iteration 20, loss = 0.33702227\n",
      "Iteration 21, loss = 0.32836802\n",
      "Iteration 22, loss = 0.32341358\n",
      "Iteration 23, loss = 0.31780500\n",
      "Iteration 24, loss = 0.31583430\n",
      "Iteration 25, loss = 0.31450039\n",
      "Iteration 26, loss = 0.30928707\n",
      "Iteration 27, loss = 0.30261724\n",
      "Iteration 28, loss = 0.30222715\n",
      "Iteration 29, loss = 0.30616061\n",
      "Iteration 30, loss = 0.30596745\n",
      "Iteration 31, loss = 0.30330412\n",
      "Iteration 32, loss = 0.30506542\n",
      "Iteration 33, loss = 0.31199359\n",
      "Iteration 34, loss = 0.31485586\n",
      "Iteration 35, loss = 0.31397997\n",
      "Iteration 36, loss = 0.31444618\n",
      "Iteration 37, loss = 0.30609416\n",
      "Iteration 38, loss = 0.30735023\n",
      "Iteration 39, loss = 0.31014592\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.02249074\n",
      "Iteration 2, loss = 1.49064388\n",
      "Iteration 3, loss = 1.16721509\n",
      "Iteration 4, loss = 0.93234718\n",
      "Iteration 5, loss = 0.75614449\n",
      "Iteration 6, loss = 0.62952331\n",
      "Iteration 7, loss = 0.54074076\n",
      "Iteration 8, loss = 0.48266687\n",
      "Iteration 9, loss = 0.44087145\n",
      "Iteration 10, loss = 0.40794253\n",
      "Iteration 11, loss = 0.38362020\n",
      "Iteration 12, loss = 0.36104767\n",
      "Iteration 13, loss = 0.34440461\n",
      "Iteration 14, loss = 0.33478429\n",
      "Iteration 15, loss = 0.32325020\n",
      "Iteration 16, loss = 0.30912747\n",
      "Iteration 17, loss = 0.30262302\n",
      "Iteration 18, loss = 0.30436001\n",
      "Iteration 19, loss = 0.31287923\n",
      "Iteration 20, loss = 0.31965977\n",
      "Iteration 21, loss = 0.31663356\n",
      "Iteration 22, loss = 0.30525759\n",
      "Iteration 23, loss = 0.30163529\n",
      "Iteration 24, loss = 0.30048435\n",
      "Iteration 25, loss = 0.30130334\n",
      "Iteration 26, loss = 0.30072826\n",
      "Iteration 27, loss = 0.29949326\n",
      "Iteration 28, loss = 0.29935271\n",
      "Iteration 29, loss = 0.29689484\n",
      "Iteration 30, loss = 0.29023291\n",
      "Iteration 31, loss = 0.29124198\n",
      "Iteration 32, loss = 0.30470791\n",
      "Iteration 33, loss = 0.32631637\n",
      "Iteration 34, loss = 0.32399676\n",
      "Iteration 35, loss = 0.30583811\n",
      "Iteration 36, loss = 0.30223994\n",
      "Iteration 37, loss = 0.30539915\n",
      "Iteration 38, loss = 0.30381829\n",
      "Iteration 39, loss = 0.29808860\n",
      "Iteration 40, loss = 0.29270893\n",
      "Iteration 41, loss = 0.29715170\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.00566271\n",
      "Iteration 2, loss = 1.48859025\n",
      "Iteration 3, loss = 1.17889432\n",
      "Iteration 4, loss = 0.94728341\n",
      "Iteration 5, loss = 0.77472577\n",
      "Iteration 6, loss = 0.65228021\n",
      "Iteration 7, loss = 0.56472495\n",
      "Iteration 8, loss = 0.50391258\n",
      "Iteration 9, loss = 0.45809578\n",
      "Iteration 10, loss = 0.42024512\n",
      "Iteration 11, loss = 0.39402933\n",
      "Iteration 12, loss = 0.37426154\n",
      "Iteration 13, loss = 0.35683726\n",
      "Iteration 14, loss = 0.33855843\n",
      "Iteration 15, loss = 0.32973528\n",
      "Iteration 16, loss = 0.32535825\n",
      "Iteration 17, loss = 0.32397191\n",
      "Iteration 18, loss = 0.31721953\n",
      "Iteration 19, loss = 0.30699627\n",
      "Iteration 20, loss = 0.30539050\n",
      "Iteration 21, loss = 0.30700131\n",
      "Iteration 22, loss = 0.30737535\n",
      "Iteration 23, loss = 0.30713480\n",
      "Iteration 24, loss = 0.30946333\n",
      "Iteration 25, loss = 0.30718335\n",
      "Iteration 26, loss = 0.29883275\n",
      "Iteration 27, loss = 0.29445298\n",
      "Iteration 28, loss = 0.29323888\n",
      "Iteration 29, loss = 0.29236378\n",
      "Iteration 30, loss = 0.29350943\n",
      "Iteration 31, loss = 0.29231041\n",
      "Iteration 32, loss = 0.29092063\n",
      "Iteration 33, loss = 0.28880843\n",
      "Iteration 34, loss = 0.28812002\n",
      "Iteration 35, loss = 0.28966436\n",
      "Iteration 36, loss = 0.29507599\n",
      "Iteration 37, loss = 0.30035633\n",
      "Iteration 38, loss = 0.30004276\n",
      "Iteration 39, loss = 0.29879060\n",
      "Iteration 40, loss = 0.29335880\n",
      "Iteration 41, loss = 0.29017043\n",
      "Iteration 42, loss = 0.30144554\n",
      "Iteration 43, loss = 0.31081816\n",
      "Iteration 44, loss = 0.31422116\n",
      "Iteration 45, loss = 0.31341016\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.02344926\n",
      "Iteration 2, loss = 1.49955363\n",
      "Iteration 3, loss = 1.17944926\n",
      "Iteration 4, loss = 0.94426068\n",
      "Iteration 5, loss = 0.76636426\n",
      "Iteration 6, loss = 0.64015541\n",
      "Iteration 7, loss = 0.55348964\n",
      "Iteration 8, loss = 0.49531584\n",
      "Iteration 9, loss = 0.45235653\n",
      "Iteration 10, loss = 0.41677859\n",
      "Iteration 11, loss = 0.39161129\n",
      "Iteration 12, loss = 0.36867415\n",
      "Iteration 13, loss = 0.35135221\n",
      "Iteration 14, loss = 0.33529861\n",
      "Iteration 15, loss = 0.32656135\n",
      "Iteration 16, loss = 0.32037105\n",
      "Iteration 17, loss = 0.31704460\n",
      "Iteration 18, loss = 0.30836805\n",
      "Iteration 19, loss = 0.29930236\n",
      "Iteration 20, loss = 0.29948793\n",
      "Iteration 21, loss = 0.29954222\n",
      "Iteration 22, loss = 0.30156741\n",
      "Iteration 23, loss = 0.30588883\n",
      "Iteration 24, loss = 0.30711634\n",
      "Iteration 25, loss = 0.30085060\n",
      "Iteration 26, loss = 0.29202380\n",
      "Iteration 27, loss = 0.28811875\n",
      "Iteration 28, loss = 0.28967350\n",
      "Iteration 29, loss = 0.29161918\n",
      "Iteration 30, loss = 0.29316805\n",
      "Iteration 31, loss = 0.29450368\n",
      "Iteration 32, loss = 0.29496932\n",
      "Iteration 33, loss = 0.29550098\n",
      "Iteration 34, loss = 0.29169478\n",
      "Iteration 35, loss = 0.28819522\n",
      "Iteration 36, loss = 0.28952144\n",
      "Iteration 37, loss = 0.29201316\n",
      "Iteration 38, loss = 0.29336607\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.00710596\n",
      "Iteration 2, loss = 1.48530633\n",
      "Iteration 3, loss = 1.16470959\n",
      "Iteration 4, loss = 0.92765271\n",
      "Iteration 5, loss = 0.75127237\n",
      "Iteration 6, loss = 0.62709406\n",
      "Iteration 7, loss = 0.54403480\n",
      "Iteration 8, loss = 0.48876683\n",
      "Iteration 9, loss = 0.44519191\n",
      "Iteration 10, loss = 0.40897665\n",
      "Iteration 11, loss = 0.38640716\n",
      "Iteration 12, loss = 0.36499006\n",
      "Iteration 13, loss = 0.34446056\n",
      "Iteration 14, loss = 0.32648438\n",
      "Iteration 15, loss = 0.31701311\n",
      "Iteration 16, loss = 0.31164150\n",
      "Iteration 17, loss = 0.30925051\n",
      "Iteration 18, loss = 0.30255018\n",
      "Iteration 19, loss = 0.29456183\n",
      "Iteration 20, loss = 0.29508256\n",
      "Iteration 21, loss = 0.29288503\n",
      "Iteration 22, loss = 0.29237368\n",
      "Iteration 23, loss = 0.29413724\n",
      "Iteration 24, loss = 0.29535270\n",
      "Iteration 25, loss = 0.29514040\n",
      "Iteration 26, loss = 0.29065381\n",
      "Iteration 27, loss = 0.28600003\n",
      "Iteration 28, loss = 0.28659237\n",
      "Iteration 29, loss = 0.28534655\n",
      "Iteration 30, loss = 0.28547832\n",
      "Iteration 31, loss = 0.28853681\n",
      "Iteration 32, loss = 0.29284530\n",
      "Iteration 33, loss = 0.29678708\n",
      "Iteration 34, loss = 0.29768720\n",
      "Iteration 35, loss = 0.29119815\n",
      "Iteration 36, loss = 0.28548334\n",
      "Iteration 37, loss = 0.28807483\n",
      "Iteration 38, loss = 0.29296032\n",
      "Iteration 39, loss = 0.29181158\n",
      "Iteration 40, loss = 0.28899822\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.01825931\n",
      "Iteration 2, loss = 1.50172183\n",
      "Iteration 3, loss = 1.18583081\n",
      "Iteration 4, loss = 0.94326638\n",
      "Iteration 5, loss = 0.76165969\n",
      "Iteration 6, loss = 0.63352123\n",
      "Iteration 7, loss = 0.55086700\n",
      "Iteration 8, loss = 0.49673713\n",
      "Iteration 9, loss = 0.45424854\n",
      "Iteration 10, loss = 0.41921458\n",
      "Iteration 11, loss = 0.39256841\n",
      "Iteration 12, loss = 0.36884423\n",
      "Iteration 13, loss = 0.34945877\n",
      "Iteration 14, loss = 0.33246120\n",
      "Iteration 15, loss = 0.32313584\n",
      "Iteration 16, loss = 0.31613969\n",
      "Iteration 17, loss = 0.31116132\n",
      "Iteration 18, loss = 0.30300746\n",
      "Iteration 19, loss = 0.29558011\n",
      "Iteration 20, loss = 0.29932378\n",
      "Iteration 21, loss = 0.29632838\n",
      "Iteration 22, loss = 0.29379012\n",
      "Iteration 23, loss = 0.29387534\n",
      "Iteration 24, loss = 0.29533370\n",
      "Iteration 25, loss = 0.29483112\n",
      "Iteration 26, loss = 0.29200468\n",
      "Iteration 27, loss = 0.28616589\n",
      "Iteration 28, loss = 0.28165988\n",
      "Iteration 29, loss = 0.28461910\n",
      "Iteration 30, loss = 0.28978186\n",
      "Iteration 31, loss = 0.29237911\n",
      "Iteration 32, loss = 0.29362383\n",
      "Iteration 33, loss = 0.29610910\n",
      "Iteration 34, loss = 0.29980903\n",
      "Iteration 35, loss = 0.29751606\n",
      "Iteration 36, loss = 0.29332513\n",
      "Iteration 37, loss = 0.29463435\n",
      "Iteration 38, loss = 0.29605021\n",
      "Iteration 39, loss = 0.29500833\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.00905470\n",
      "Iteration 2, loss = 1.48723553\n",
      "Iteration 3, loss = 1.16115973\n",
      "Iteration 4, loss = 0.91943664\n",
      "Iteration 5, loss = 0.73871879\n",
      "Iteration 6, loss = 0.61340175\n",
      "Iteration 7, loss = 0.53177147\n",
      "Iteration 8, loss = 0.47667840\n",
      "Iteration 9, loss = 0.43397017\n",
      "Iteration 10, loss = 0.40252938\n",
      "Iteration 11, loss = 0.37553422\n",
      "Iteration 12, loss = 0.35205888\n",
      "Iteration 13, loss = 0.33439446\n",
      "Iteration 14, loss = 0.31912076\n",
      "Iteration 15, loss = 0.30761345\n",
      "Iteration 16, loss = 0.29927844\n",
      "Iteration 17, loss = 0.29286237\n",
      "Iteration 18, loss = 0.28713897\n",
      "Iteration 19, loss = 0.28440127\n",
      "Iteration 20, loss = 0.28696696\n",
      "Iteration 21, loss = 0.28614953\n",
      "Iteration 22, loss = 0.28406693\n",
      "Iteration 23, loss = 0.28255312\n",
      "Iteration 24, loss = 0.28094776\n",
      "Iteration 25, loss = 0.28234241\n",
      "Iteration 26, loss = 0.28253533\n",
      "Iteration 27, loss = 0.27894941\n",
      "Iteration 28, loss = 0.28038542\n",
      "Iteration 29, loss = 0.28870814\n",
      "Iteration 30, loss = 0.28974077\n",
      "Iteration 31, loss = 0.29085911\n",
      "Iteration 32, loss = 0.29246575\n",
      "Iteration 33, loss = 0.29509714\n",
      "Iteration 34, loss = 0.30259821\n",
      "Iteration 35, loss = 0.29673689\n",
      "Iteration 36, loss = 0.29381290\n",
      "Iteration 37, loss = 0.29391804\n",
      "Iteration 38, loss = 0.29109108\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.02765108\n",
      "Iteration 2, loss = 1.50136042\n",
      "Iteration 3, loss = 1.17250227\n",
      "Iteration 4, loss = 0.93021715\n",
      "Iteration 5, loss = 0.75196446\n",
      "Iteration 6, loss = 0.62652218\n",
      "Iteration 7, loss = 0.54430820\n",
      "Iteration 8, loss = 0.48652688\n",
      "Iteration 9, loss = 0.44107331\n",
      "Iteration 10, loss = 0.40805359\n",
      "Iteration 11, loss = 0.38244396\n",
      "Iteration 12, loss = 0.35794679\n",
      "Iteration 13, loss = 0.33799407\n",
      "Iteration 14, loss = 0.32271099\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15, loss = 0.31224171\n",
      "Iteration 16, loss = 0.30571735\n",
      "Iteration 17, loss = 0.30207533\n",
      "Iteration 18, loss = 0.29409369\n",
      "Iteration 19, loss = 0.29013891\n",
      "Iteration 20, loss = 0.28999883\n",
      "Iteration 21, loss = 0.29207161\n",
      "Iteration 22, loss = 0.29113366\n",
      "Iteration 23, loss = 0.28723654\n",
      "Iteration 24, loss = 0.28459901\n",
      "Iteration 25, loss = 0.28576708\n",
      "Iteration 26, loss = 0.28757063\n",
      "Iteration 27, loss = 0.28576140\n",
      "Iteration 28, loss = 0.28687041\n",
      "Iteration 29, loss = 0.29480817\n",
      "Iteration 30, loss = 0.29608952\n",
      "Iteration 31, loss = 0.30079525\n",
      "Iteration 32, loss = 0.30341777\n",
      "Iteration 33, loss = 0.30615320\n",
      "Iteration 34, loss = 0.31050761\n",
      "Iteration 35, loss = 0.30050657\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.02930566\n",
      "Iteration 2, loss = 1.50883417\n",
      "Iteration 3, loss = 1.18210525\n",
      "Iteration 4, loss = 0.93689278\n",
      "Iteration 5, loss = 0.75902057\n",
      "Iteration 6, loss = 0.63685358\n",
      "Iteration 7, loss = 0.55611101\n",
      "Iteration 8, loss = 0.50017455\n",
      "Iteration 9, loss = 0.45214673\n",
      "Iteration 10, loss = 0.41392574\n",
      "Iteration 11, loss = 0.38569556\n",
      "Iteration 12, loss = 0.36262793\n",
      "Iteration 13, loss = 0.34557245\n",
      "Iteration 14, loss = 0.33068206\n",
      "Iteration 15, loss = 0.31854169\n",
      "Iteration 16, loss = 0.31107525\n",
      "Iteration 17, loss = 0.30995244\n",
      "Iteration 18, loss = 0.30603549\n",
      "Iteration 19, loss = 0.30096341\n",
      "Iteration 20, loss = 0.30005620\n",
      "Iteration 21, loss = 0.30082449\n",
      "Iteration 22, loss = 0.29851447\n",
      "Iteration 23, loss = 0.29657176\n",
      "Iteration 24, loss = 0.29361383\n",
      "Iteration 25, loss = 0.29238861\n",
      "Iteration 26, loss = 0.29717440\n",
      "Iteration 27, loss = 0.29856875\n",
      "Iteration 28, loss = 0.29941938\n",
      "Iteration 29, loss = 0.30183022\n",
      "Iteration 30, loss = 0.29902481\n",
      "Iteration 31, loss = 0.30286528\n",
      "Iteration 32, loss = 0.30847028\n",
      "Iteration 33, loss = 0.31315415\n",
      "Iteration 34, loss = 0.31857630\n",
      "Iteration 35, loss = 0.31075436\n",
      "Iteration 36, loss = 0.30806121\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.01412763\n",
      "Iteration 2, loss = 1.48559432\n",
      "Iteration 3, loss = 1.15686363\n",
      "Iteration 4, loss = 0.90976045\n",
      "Iteration 5, loss = 0.73061136\n",
      "Iteration 6, loss = 0.61042402\n",
      "Iteration 7, loss = 0.53359178\n",
      "Iteration 8, loss = 0.48119314\n",
      "Iteration 9, loss = 0.43483524\n",
      "Iteration 10, loss = 0.39777146\n",
      "Iteration 11, loss = 0.37196904\n",
      "Iteration 12, loss = 0.35548848\n",
      "Iteration 13, loss = 0.34094768\n",
      "Iteration 14, loss = 0.32707044\n",
      "Iteration 15, loss = 0.31638525\n",
      "Iteration 16, loss = 0.31045758\n",
      "Iteration 17, loss = 0.30975551\n",
      "Iteration 18, loss = 0.30371026\n",
      "Iteration 19, loss = 0.29731782\n",
      "Iteration 20, loss = 0.29630277\n",
      "Iteration 21, loss = 0.29503097\n",
      "Iteration 22, loss = 0.29359658\n",
      "Iteration 23, loss = 0.29058974\n",
      "Iteration 24, loss = 0.28682027\n",
      "Iteration 25, loss = 0.28807361\n",
      "Iteration 26, loss = 0.29561386\n",
      "Iteration 27, loss = 0.29678510\n",
      "Iteration 28, loss = 0.29571468\n",
      "Iteration 29, loss = 0.29960742\n",
      "Iteration 30, loss = 0.29751473\n",
      "Iteration 31, loss = 0.30224097\n",
      "Iteration 32, loss = 0.30708711\n",
      "Iteration 33, loss = 0.30974767\n",
      "Iteration 34, loss = 0.31581566\n",
      "Iteration 35, loss = 0.30806307\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.02330087\n",
      "Iteration 2, loss = 1.48376463\n",
      "Iteration 3, loss = 1.14409163\n",
      "Iteration 4, loss = 0.89261493\n",
      "Iteration 5, loss = 0.71347332\n",
      "Iteration 6, loss = 0.59433814\n",
      "Iteration 7, loss = 0.51647587\n",
      "Iteration 8, loss = 0.46208400\n",
      "Iteration 9, loss = 0.41579800\n",
      "Iteration 10, loss = 0.37985631\n",
      "Iteration 11, loss = 0.35354347\n",
      "Iteration 12, loss = 0.33632471\n",
      "Iteration 13, loss = 0.32487155\n",
      "Iteration 14, loss = 0.31377009\n",
      "Iteration 15, loss = 0.30327154\n",
      "Iteration 16, loss = 0.29606878\n",
      "Iteration 17, loss = 0.29466081\n",
      "Iteration 18, loss = 0.29165349\n",
      "Iteration 19, loss = 0.28803214\n",
      "Iteration 20, loss = 0.28932895\n",
      "Iteration 21, loss = 0.28369678\n",
      "Iteration 22, loss = 0.28081403\n",
      "Iteration 23, loss = 0.28320727\n",
      "Iteration 24, loss = 0.28356090\n",
      "Iteration 25, loss = 0.27972550\n",
      "Iteration 26, loss = 0.28314228\n",
      "Iteration 27, loss = 0.28398631\n",
      "Iteration 28, loss = 0.28651094\n",
      "Iteration 29, loss = 0.29257300\n",
      "Iteration 30, loss = 0.29182821\n",
      "Iteration 31, loss = 0.29524435\n",
      "Iteration 32, loss = 0.29660053\n",
      "Iteration 33, loss = 0.29541701\n",
      "Iteration 34, loss = 0.29802364\n",
      "Iteration 35, loss = 0.29682909\n",
      "Iteration 36, loss = 0.29890321\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.04001186\n",
      "Iteration 2, loss = 1.50255355\n",
      "Iteration 3, loss = 1.21803944\n",
      "Iteration 4, loss = 1.00083756\n",
      "Iteration 5, loss = 0.82778368\n",
      "Iteration 6, loss = 0.69994290\n",
      "Iteration 7, loss = 0.60933590\n",
      "Iteration 8, loss = 0.54423549\n",
      "Iteration 9, loss = 0.49455766\n",
      "Iteration 10, loss = 0.45481529\n",
      "Iteration 11, loss = 0.42476800\n",
      "Iteration 12, loss = 0.40158932\n",
      "Iteration 13, loss = 0.38216230\n",
      "Iteration 14, loss = 0.36196594\n",
      "Iteration 15, loss = 0.34658752\n",
      "Iteration 16, loss = 0.33696054\n",
      "Iteration 17, loss = 0.33114147\n",
      "Iteration 18, loss = 0.32789863\n",
      "Iteration 19, loss = 0.32934334\n",
      "Iteration 20, loss = 0.32405733\n",
      "Iteration 21, loss = 0.31760340\n",
      "Iteration 22, loss = 0.31368580\n",
      "Iteration 23, loss = 0.30928694\n",
      "Iteration 24, loss = 0.30275518\n",
      "Iteration 25, loss = 0.30234850\n",
      "Iteration 26, loss = 0.31173739\n",
      "Iteration 27, loss = 0.30082858\n",
      "Iteration 28, loss = 0.29293946\n",
      "Iteration 29, loss = 0.29757457\n",
      "Iteration 30, loss = 0.30503740\n",
      "Iteration 31, loss = 0.31105289\n",
      "Iteration 32, loss = 0.31632463\n",
      "Iteration 33, loss = 0.30869973\n",
      "Iteration 34, loss = 0.29871072\n",
      "Iteration 35, loss = 0.29433211\n",
      "Iteration 36, loss = 0.29865327\n",
      "Iteration 37, loss = 0.30108458\n",
      "Iteration 38, loss = 0.29968184\n",
      "Iteration 39, loss = 0.28984951\n",
      "Iteration 40, loss = 0.28157251\n",
      "Iteration 41, loss = 0.28460446\n",
      "Iteration 42, loss = 0.29989445\n",
      "Iteration 43, loss = 0.29710472\n",
      "Iteration 44, loss = 0.29888116\n",
      "Iteration 45, loss = 0.30378929\n",
      "Iteration 46, loss = 0.29709125\n",
      "Iteration 47, loss = 0.29442034\n",
      "Iteration 48, loss = 0.29733614\n",
      "Iteration 49, loss = 0.29418415\n",
      "Iteration 50, loss = 0.28875030\n",
      "Iteration 51, loss = 0.29207563\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.03199112\n",
      "Iteration 2, loss = 1.47956952\n",
      "Iteration 3, loss = 1.18814977\n",
      "Iteration 4, loss = 0.94985029\n",
      "Iteration 5, loss = 0.77529652\n",
      "Iteration 6, loss = 0.65852752\n",
      "Iteration 7, loss = 0.57470411\n",
      "Iteration 8, loss = 0.51009093\n",
      "Iteration 9, loss = 0.46266518\n",
      "Iteration 10, loss = 0.42585775\n",
      "Iteration 11, loss = 0.39621018\n",
      "Iteration 12, loss = 0.37195221\n",
      "Iteration 13, loss = 0.35798145\n",
      "Iteration 14, loss = 0.34431226\n",
      "Iteration 15, loss = 0.33265005\n",
      "Iteration 16, loss = 0.31965310\n",
      "Iteration 17, loss = 0.31437803\n",
      "Iteration 18, loss = 0.31085332\n",
      "Iteration 19, loss = 0.30846774\n",
      "Iteration 20, loss = 0.30504189\n",
      "Iteration 21, loss = 0.30422852\n",
      "Iteration 22, loss = 0.30557605\n",
      "Iteration 23, loss = 0.30082534\n",
      "Iteration 24, loss = 0.29968134\n",
      "Iteration 25, loss = 0.30642949\n",
      "Iteration 26, loss = 0.30509429\n",
      "Iteration 27, loss = 0.29977721\n",
      "Iteration 28, loss = 0.30013119\n",
      "Iteration 29, loss = 0.30512674\n",
      "Iteration 30, loss = 0.30654244\n",
      "Iteration 31, loss = 0.30226829\n",
      "Iteration 32, loss = 0.29575396\n",
      "Iteration 33, loss = 0.28866013\n",
      "Iteration 34, loss = 0.28479274\n",
      "Iteration 35, loss = 0.29135484\n",
      "Iteration 36, loss = 0.29357506\n",
      "Iteration 37, loss = 0.29651604\n",
      "Iteration 38, loss = 0.29503955\n",
      "Iteration 39, loss = 0.29070384\n",
      "Iteration 40, loss = 0.29901345\n",
      "Iteration 41, loss = 0.30868993\n",
      "Iteration 42, loss = 0.30334303\n",
      "Iteration 43, loss = 0.30068142\n",
      "Iteration 44, loss = 0.30613928\n",
      "Iteration 45, loss = 0.31209202\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.03627468\n",
      "Iteration 2, loss = 1.49891064\n",
      "Iteration 3, loss = 1.19956335\n",
      "Iteration 4, loss = 0.95473048\n",
      "Iteration 5, loss = 0.77923008\n",
      "Iteration 6, loss = 0.66141550\n",
      "Iteration 7, loss = 0.57712768\n",
      "Iteration 8, loss = 0.50954646\n",
      "Iteration 9, loss = 0.45699860\n",
      "Iteration 10, loss = 0.41743053\n",
      "Iteration 11, loss = 0.38793302\n",
      "Iteration 12, loss = 0.36560088\n",
      "Iteration 13, loss = 0.35175599\n",
      "Iteration 14, loss = 0.33879966\n",
      "Iteration 15, loss = 0.33031017\n",
      "Iteration 16, loss = 0.31641004\n",
      "Iteration 17, loss = 0.31165240\n",
      "Iteration 18, loss = 0.30903441\n",
      "Iteration 19, loss = 0.30455792\n",
      "Iteration 20, loss = 0.30024293\n",
      "Iteration 21, loss = 0.29932443\n",
      "Iteration 22, loss = 0.30189615\n",
      "Iteration 23, loss = 0.29618798\n",
      "Iteration 24, loss = 0.29475710\n",
      "Iteration 25, loss = 0.29905964\n",
      "Iteration 26, loss = 0.30321978\n",
      "Iteration 27, loss = 0.30052743\n",
      "Iteration 28, loss = 0.29738030\n",
      "Iteration 29, loss = 0.29797963\n",
      "Iteration 30, loss = 0.30161711\n",
      "Iteration 31, loss = 0.30106164\n",
      "Iteration 32, loss = 0.29387931\n",
      "Iteration 33, loss = 0.28514286\n",
      "Iteration 34, loss = 0.28271647\n",
      "Iteration 35, loss = 0.28919367\n",
      "Iteration 36, loss = 0.29536702\n",
      "Iteration 37, loss = 0.29890029\n",
      "Iteration 38, loss = 0.29311498\n",
      "Iteration 39, loss = 0.29094657\n",
      "Iteration 40, loss = 0.29889238\n",
      "Iteration 41, loss = 0.30573193\n",
      "Iteration 42, loss = 0.29886282\n",
      "Iteration 43, loss = 0.29282044\n",
      "Iteration 44, loss = 0.29453124\n",
      "Iteration 45, loss = 0.30631264\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.02337547\n",
      "Iteration 2, loss = 1.46346897\n",
      "Iteration 3, loss = 1.18153001\n",
      "Iteration 4, loss = 0.95271176\n",
      "Iteration 5, loss = 0.77491743\n",
      "Iteration 6, loss = 0.65341807\n",
      "Iteration 7, loss = 0.57652318\n",
      "Iteration 8, loss = 0.51604220\n",
      "Iteration 9, loss = 0.46834495\n",
      "Iteration 10, loss = 0.42553222\n",
      "Iteration 11, loss = 0.39389301\n",
      "Iteration 12, loss = 0.37051578\n",
      "Iteration 13, loss = 0.35758705\n",
      "Iteration 14, loss = 0.34452921\n",
      "Iteration 15, loss = 0.33156753\n",
      "Iteration 16, loss = 0.31484896\n",
      "Iteration 17, loss = 0.30871882\n",
      "Iteration 18, loss = 0.30926412\n",
      "Iteration 19, loss = 0.30879189\n",
      "Iteration 20, loss = 0.30000344\n",
      "Iteration 21, loss = 0.29381875\n",
      "Iteration 22, loss = 0.29657430\n",
      "Iteration 23, loss = 0.29411152\n",
      "Iteration 24, loss = 0.29005594\n",
      "Iteration 25, loss = 0.29001889\n",
      "Iteration 26, loss = 0.29587318\n",
      "Iteration 27, loss = 0.29773632\n",
      "Iteration 28, loss = 0.29693609\n",
      "Iteration 29, loss = 0.29560750\n",
      "Iteration 30, loss = 0.29809290\n",
      "Iteration 31, loss = 0.29999850\n",
      "Iteration 32, loss = 0.29392936\n",
      "Iteration 33, loss = 0.28475921\n",
      "Iteration 34, loss = 0.27948198\n",
      "Iteration 35, loss = 0.28131712\n",
      "Iteration 36, loss = 0.28636924\n",
      "Iteration 37, loss = 0.29088808\n",
      "Iteration 38, loss = 0.28767708\n",
      "Iteration 39, loss = 0.28665255\n",
      "Iteration 40, loss = 0.29478105\n",
      "Iteration 41, loss = 0.30345516\n",
      "Iteration 42, loss = 0.29315378\n",
      "Iteration 43, loss = 0.28875629\n",
      "Iteration 44, loss = 0.28783080\n",
      "Iteration 45, loss = 0.29794394\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.03093946\n",
      "Iteration 2, loss = 1.48657466\n",
      "Iteration 3, loss = 1.19866066\n",
      "Iteration 4, loss = 0.95856400\n",
      "Iteration 5, loss = 0.77862323\n",
      "Iteration 6, loss = 0.65729680\n",
      "Iteration 7, loss = 0.57986024\n",
      "Iteration 8, loss = 0.51508082\n",
      "Iteration 9, loss = 0.46488200\n",
      "Iteration 10, loss = 0.42098481\n",
      "Iteration 11, loss = 0.38935963\n",
      "Iteration 12, loss = 0.36640258\n",
      "Iteration 13, loss = 0.35462685\n",
      "Iteration 14, loss = 0.34344050\n",
      "Iteration 15, loss = 0.33378108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16, loss = 0.31859901\n",
      "Iteration 17, loss = 0.31132626\n",
      "Iteration 18, loss = 0.31281516\n",
      "Iteration 19, loss = 0.31750382\n",
      "Iteration 20, loss = 0.31034555\n",
      "Iteration 21, loss = 0.29828877\n",
      "Iteration 22, loss = 0.29773648\n",
      "Iteration 23, loss = 0.29596719\n",
      "Iteration 24, loss = 0.29661920\n",
      "Iteration 25, loss = 0.29494026\n",
      "Iteration 26, loss = 0.29445999\n",
      "Iteration 27, loss = 0.29333071\n",
      "Iteration 28, loss = 0.29404177\n",
      "Iteration 29, loss = 0.29531076\n",
      "Iteration 30, loss = 0.29827006\n",
      "Iteration 31, loss = 0.30205705\n",
      "Iteration 32, loss = 0.30000885\n",
      "Iteration 33, loss = 0.29217275\n",
      "Iteration 34, loss = 0.28591633\n",
      "Iteration 35, loss = 0.28683560\n",
      "Iteration 36, loss = 0.28987051\n",
      "Iteration 37, loss = 0.29326790\n",
      "Iteration 38, loss = 0.29191316\n",
      "Iteration 39, loss = 0.29146284\n",
      "Iteration 40, loss = 0.29846025\n",
      "Iteration 41, loss = 0.30530609\n",
      "Iteration 42, loss = 0.29444712\n",
      "Iteration 43, loss = 0.29382427\n",
      "Iteration 44, loss = 0.29718278\n",
      "Iteration 45, loss = 0.30736551\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.03385082\n",
      "Iteration 2, loss = 1.48996144\n",
      "Iteration 3, loss = 1.20893623\n",
      "Iteration 4, loss = 0.97158059\n",
      "Iteration 5, loss = 0.78668394\n",
      "Iteration 6, loss = 0.65736236\n",
      "Iteration 7, loss = 0.57864450\n",
      "Iteration 8, loss = 0.51389862\n",
      "Iteration 9, loss = 0.46463386\n",
      "Iteration 10, loss = 0.42176852\n",
      "Iteration 11, loss = 0.38943709\n",
      "Iteration 12, loss = 0.36490515\n",
      "Iteration 13, loss = 0.35075307\n",
      "Iteration 14, loss = 0.33773866\n",
      "Iteration 15, loss = 0.33032217\n",
      "Iteration 16, loss = 0.31263460\n",
      "Iteration 17, loss = 0.30498492\n",
      "Iteration 18, loss = 0.30628737\n",
      "Iteration 19, loss = 0.31164973\n",
      "Iteration 20, loss = 0.30718382\n",
      "Iteration 21, loss = 0.29570688\n",
      "Iteration 22, loss = 0.28858859\n",
      "Iteration 23, loss = 0.28978118\n",
      "Iteration 24, loss = 0.29241129\n",
      "Iteration 25, loss = 0.29090509\n",
      "Iteration 26, loss = 0.28935771\n",
      "Iteration 27, loss = 0.28893423\n",
      "Iteration 28, loss = 0.29222588\n",
      "Iteration 29, loss = 0.29344825\n",
      "Iteration 30, loss = 0.29413250\n",
      "Iteration 31, loss = 0.29378594\n",
      "Iteration 32, loss = 0.29060143\n",
      "Iteration 33, loss = 0.28588130\n",
      "Iteration 34, loss = 0.28293655\n",
      "Iteration 35, loss = 0.28590886\n",
      "Iteration 36, loss = 0.28950024\n",
      "Iteration 37, loss = 0.28896511\n",
      "Iteration 38, loss = 0.28387625\n",
      "Iteration 39, loss = 0.28466720\n",
      "Iteration 40, loss = 0.29191323\n",
      "Iteration 41, loss = 0.29890997\n",
      "Iteration 42, loss = 0.28962391\n",
      "Iteration 43, loss = 0.29067334\n",
      "Iteration 44, loss = 0.29726993\n",
      "Iteration 45, loss = 0.29999853\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.03576590\n",
      "Iteration 2, loss = 1.50379594\n",
      "Iteration 3, loss = 1.21226970\n",
      "Iteration 4, loss = 0.96910291\n",
      "Iteration 5, loss = 0.78515948\n",
      "Iteration 6, loss = 0.65738738\n",
      "Iteration 7, loss = 0.57577346\n",
      "Iteration 8, loss = 0.50778807\n",
      "Iteration 9, loss = 0.45505508\n",
      "Iteration 10, loss = 0.41113748\n",
      "Iteration 11, loss = 0.37884605\n",
      "Iteration 12, loss = 0.35797000\n",
      "Iteration 13, loss = 0.34769390\n",
      "Iteration 14, loss = 0.33571890\n",
      "Iteration 15, loss = 0.32426689\n",
      "Iteration 16, loss = 0.30632937\n",
      "Iteration 17, loss = 0.30032482\n",
      "Iteration 18, loss = 0.30641421\n",
      "Iteration 19, loss = 0.30905964\n",
      "Iteration 20, loss = 0.30162867\n",
      "Iteration 21, loss = 0.29135165\n",
      "Iteration 22, loss = 0.28821469\n",
      "Iteration 23, loss = 0.29097265\n",
      "Iteration 24, loss = 0.29161416\n",
      "Iteration 25, loss = 0.28734507\n",
      "Iteration 26, loss = 0.28649780\n",
      "Iteration 27, loss = 0.29201710\n",
      "Iteration 28, loss = 0.29503049\n",
      "Iteration 29, loss = 0.29512504\n",
      "Iteration 30, loss = 0.29189853\n",
      "Iteration 31, loss = 0.28911415\n",
      "Iteration 32, loss = 0.28814644\n",
      "Iteration 33, loss = 0.29011147\n",
      "Iteration 34, loss = 0.28910615\n",
      "Iteration 35, loss = 0.28773807\n",
      "Iteration 36, loss = 0.28839319\n",
      "Iteration 37, loss = 0.28898360\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.04113788\n",
      "Iteration 2, loss = 1.50923314\n",
      "Iteration 3, loss = 1.23189854\n",
      "Iteration 4, loss = 0.99608152\n",
      "Iteration 5, loss = 0.81043151\n",
      "Iteration 6, loss = 0.67973621\n",
      "Iteration 7, loss = 0.60100758\n",
      "Iteration 8, loss = 0.53686145\n",
      "Iteration 9, loss = 0.48447517\n",
      "Iteration 10, loss = 0.43877598\n",
      "Iteration 11, loss = 0.40480700\n",
      "Iteration 12, loss = 0.38189597\n",
      "Iteration 13, loss = 0.36793828\n",
      "Iteration 14, loss = 0.35482874\n",
      "Iteration 15, loss = 0.34153410\n",
      "Iteration 16, loss = 0.32057443\n",
      "Iteration 17, loss = 0.31278460\n",
      "Iteration 18, loss = 0.31872969\n",
      "Iteration 19, loss = 0.32231906\n",
      "Iteration 20, loss = 0.31846632\n",
      "Iteration 21, loss = 0.30807911\n",
      "Iteration 22, loss = 0.30109823\n",
      "Iteration 23, loss = 0.30355527\n",
      "Iteration 24, loss = 0.30453179\n",
      "Iteration 25, loss = 0.30119843\n",
      "Iteration 26, loss = 0.30084341\n",
      "Iteration 27, loss = 0.30755245\n",
      "Iteration 28, loss = 0.31314063\n",
      "Iteration 29, loss = 0.31892051\n",
      "Iteration 30, loss = 0.30683707\n",
      "Iteration 31, loss = 0.29660929\n",
      "Iteration 32, loss = 0.29735184\n",
      "Iteration 33, loss = 0.30431096\n",
      "Iteration 34, loss = 0.30595619\n",
      "Iteration 35, loss = 0.30382940\n",
      "Iteration 36, loss = 0.30092745\n",
      "Iteration 37, loss = 0.29941530\n",
      "Iteration 38, loss = 0.29966197\n",
      "Iteration 39, loss = 0.30071772\n",
      "Iteration 40, loss = 0.29958689\n",
      "Iteration 41, loss = 0.30082524\n",
      "Iteration 42, loss = 0.29623509\n",
      "Iteration 43, loss = 0.30346397\n",
      "Iteration 44, loss = 0.30927825\n",
      "Iteration 45, loss = 0.30291972\n",
      "Iteration 46, loss = 0.29277436\n",
      "Iteration 47, loss = 0.29140717\n",
      "Iteration 48, loss = 0.29569520\n",
      "Iteration 49, loss = 0.29844637\n",
      "Iteration 50, loss = 0.30078812\n",
      "Iteration 51, loss = 0.30028358\n",
      "Iteration 52, loss = 0.29670918\n",
      "Iteration 53, loss = 0.29379999\n",
      "Iteration 54, loss = 0.29385899\n",
      "Iteration 55, loss = 0.29524229\n",
      "Iteration 56, loss = 0.29873309\n",
      "Iteration 57, loss = 0.30120473\n",
      "Iteration 58, loss = 0.29969668\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.03149758\n",
      "Iteration 2, loss = 1.48983572\n",
      "Iteration 3, loss = 1.22019344\n",
      "Iteration 4, loss = 0.99007390\n",
      "Iteration 5, loss = 0.80703606\n",
      "Iteration 6, loss = 0.67859814\n",
      "Iteration 7, loss = 0.59940948\n",
      "Iteration 8, loss = 0.53863821\n",
      "Iteration 9, loss = 0.48579609\n",
      "Iteration 10, loss = 0.44176901\n",
      "Iteration 11, loss = 0.40719985\n",
      "Iteration 12, loss = 0.38254894\n",
      "Iteration 13, loss = 0.36599600\n",
      "Iteration 14, loss = 0.35099522\n",
      "Iteration 15, loss = 0.33818552\n",
      "Iteration 16, loss = 0.31965122\n",
      "Iteration 17, loss = 0.31531409\n",
      "Iteration 18, loss = 0.32113280\n",
      "Iteration 19, loss = 0.31991747\n",
      "Iteration 20, loss = 0.31698322\n",
      "Iteration 21, loss = 0.30672059\n",
      "Iteration 22, loss = 0.29697693\n",
      "Iteration 23, loss = 0.29830227\n",
      "Iteration 24, loss = 0.30215143\n",
      "Iteration 25, loss = 0.30064776\n",
      "Iteration 26, loss = 0.30117054\n",
      "Iteration 27, loss = 0.30706133\n",
      "Iteration 28, loss = 0.31021182\n",
      "Iteration 29, loss = 0.31600216\n",
      "Iteration 30, loss = 0.30389519\n",
      "Iteration 31, loss = 0.29180470\n",
      "Iteration 32, loss = 0.29305935\n",
      "Iteration 33, loss = 0.30338483\n",
      "Iteration 34, loss = 0.30809248\n",
      "Iteration 35, loss = 0.30295172\n",
      "Iteration 36, loss = 0.29577721\n",
      "Iteration 37, loss = 0.29259228\n",
      "Iteration 38, loss = 0.29331105\n",
      "Iteration 39, loss = 0.29647910\n",
      "Iteration 40, loss = 0.29662017\n",
      "Iteration 41, loss = 0.29660548\n",
      "Iteration 42, loss = 0.29248680\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.03793230\n",
      "Iteration 2, loss = 1.50039759\n",
      "Iteration 3, loss = 1.21519244\n",
      "Iteration 4, loss = 0.97704184\n",
      "Iteration 5, loss = 0.79159861\n",
      "Iteration 6, loss = 0.66039833\n",
      "Iteration 7, loss = 0.57538518\n",
      "Iteration 8, loss = 0.51602924\n",
      "Iteration 9, loss = 0.46451276\n",
      "Iteration 10, loss = 0.42073767\n",
      "Iteration 11, loss = 0.38607251\n",
      "Iteration 12, loss = 0.36176277\n",
      "Iteration 13, loss = 0.34590857\n",
      "Iteration 14, loss = 0.33204517\n",
      "Iteration 15, loss = 0.32272193\n",
      "Iteration 16, loss = 0.30782994\n",
      "Iteration 17, loss = 0.30484607\n",
      "Iteration 18, loss = 0.30981111\n",
      "Iteration 19, loss = 0.30920518\n",
      "Iteration 20, loss = 0.30887604\n",
      "Iteration 21, loss = 0.30175868\n",
      "Iteration 22, loss = 0.28992793\n",
      "Iteration 23, loss = 0.28841630\n",
      "Iteration 24, loss = 0.28372463\n",
      "Iteration 25, loss = 0.28678123\n",
      "Iteration 26, loss = 0.29684760\n",
      "Iteration 27, loss = 0.30738348\n",
      "Iteration 28, loss = 0.30097241\n",
      "Iteration 29, loss = 0.30432466\n",
      "Iteration 30, loss = 0.29497057\n",
      "Iteration 31, loss = 0.28426634\n",
      "Iteration 32, loss = 0.28385426\n",
      "Iteration 33, loss = 0.29164599\n",
      "Iteration 34, loss = 0.29739617\n",
      "Iteration 35, loss = 0.29608578\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.32375967\n",
      "Iteration 2, loss = 1.62688171\n",
      "Iteration 3, loss = 1.32759873\n",
      "Iteration 4, loss = 1.10294326\n",
      "Iteration 5, loss = 0.89843383\n",
      "Iteration 6, loss = 0.74137484\n",
      "Iteration 7, loss = 0.63129644\n",
      "Iteration 8, loss = 0.56166910\n",
      "Iteration 9, loss = 0.52361495\n",
      "Iteration 10, loss = 0.49349635\n",
      "Iteration 11, loss = 0.46165892\n",
      "Iteration 12, loss = 0.43155924\n",
      "Iteration 13, loss = 0.40414315\n",
      "Iteration 14, loss = 0.38315751\n",
      "Iteration 15, loss = 0.35871023\n",
      "Iteration 16, loss = 0.34984343\n",
      "Iteration 17, loss = 0.34730867\n",
      "Iteration 18, loss = 0.34685244\n",
      "Iteration 19, loss = 0.34416311\n",
      "Iteration 20, loss = 0.33092441\n",
      "Iteration 21, loss = 0.31423451\n",
      "Iteration 22, loss = 0.30754541\n",
      "Iteration 23, loss = 0.30473388\n",
      "Iteration 24, loss = 0.30826483\n",
      "Iteration 25, loss = 0.31429473\n",
      "Iteration 26, loss = 0.30959749\n",
      "Iteration 27, loss = 0.30729879\n",
      "Iteration 28, loss = 0.29828700\n",
      "Iteration 29, loss = 0.29186234\n",
      "Iteration 30, loss = 0.29993785\n",
      "Iteration 31, loss = 0.30514593\n",
      "Iteration 32, loss = 0.30634462\n",
      "Iteration 33, loss = 0.30371760\n",
      "Iteration 34, loss = 0.30723601\n",
      "Iteration 35, loss = 0.31160087\n",
      "Iteration 36, loss = 0.31507040\n",
      "Iteration 37, loss = 0.30706663\n",
      "Iteration 38, loss = 0.30663671\n",
      "Iteration 39, loss = 0.30856479\n",
      "Iteration 40, loss = 0.30366721\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.30445938\n",
      "Iteration 2, loss = 1.61274243\n",
      "Iteration 3, loss = 1.30608405\n",
      "Iteration 4, loss = 1.06797494\n",
      "Iteration 5, loss = 0.86779326\n",
      "Iteration 6, loss = 0.72503945\n",
      "Iteration 7, loss = 0.62658464\n",
      "Iteration 8, loss = 0.55856482\n",
      "Iteration 9, loss = 0.50621811\n",
      "Iteration 10, loss = 0.46228188\n",
      "Iteration 11, loss = 0.42395446\n",
      "Iteration 12, loss = 0.39450214\n",
      "Iteration 13, loss = 0.37676755\n",
      "Iteration 14, loss = 0.36290863\n",
      "Iteration 15, loss = 0.34648036\n",
      "Iteration 16, loss = 0.33132812\n",
      "Iteration 17, loss = 0.32358362\n",
      "Iteration 18, loss = 0.31963837\n",
      "Iteration 19, loss = 0.31341049\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 20, loss = 0.30570296\n",
      "Iteration 21, loss = 0.30238095\n",
      "Iteration 22, loss = 0.29718022\n",
      "Iteration 23, loss = 0.29855246\n",
      "Iteration 24, loss = 0.29988034\n",
      "Iteration 25, loss = 0.29618484\n",
      "Iteration 26, loss = 0.28949945\n",
      "Iteration 27, loss = 0.28671694\n",
      "Iteration 28, loss = 0.29566824\n",
      "Iteration 29, loss = 0.29678681\n",
      "Iteration 30, loss = 0.29200816\n",
      "Iteration 31, loss = 0.29161865\n",
      "Iteration 32, loss = 0.28841048\n",
      "Iteration 33, loss = 0.28758168\n",
      "Iteration 34, loss = 0.28937047\n",
      "Iteration 35, loss = 0.29257517\n",
      "Iteration 36, loss = 0.29080796\n",
      "Iteration 37, loss = 0.29031419\n",
      "Iteration 38, loss = 0.28981218\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.32677968\n",
      "Iteration 2, loss = 1.62830863\n",
      "Iteration 3, loss = 1.30933866\n",
      "Iteration 4, loss = 1.06371376\n",
      "Iteration 5, loss = 0.86279921\n",
      "Iteration 6, loss = 0.71613478\n",
      "Iteration 7, loss = 0.61143368\n",
      "Iteration 8, loss = 0.54007490\n",
      "Iteration 9, loss = 0.48966977\n",
      "Iteration 10, loss = 0.44602524\n",
      "Iteration 11, loss = 0.40985773\n",
      "Iteration 12, loss = 0.38682951\n",
      "Iteration 13, loss = 0.37405468\n",
      "Iteration 14, loss = 0.35845885\n",
      "Iteration 15, loss = 0.33912481\n",
      "Iteration 16, loss = 0.32384710\n",
      "Iteration 17, loss = 0.31892594\n",
      "Iteration 18, loss = 0.31255972\n",
      "Iteration 19, loss = 0.30462779\n",
      "Iteration 20, loss = 0.29948624\n",
      "Iteration 21, loss = 0.30016262\n",
      "Iteration 22, loss = 0.30095677\n",
      "Iteration 23, loss = 0.30281608\n",
      "Iteration 24, loss = 0.30429228\n",
      "Iteration 25, loss = 0.30184166\n",
      "Iteration 26, loss = 0.29473235\n",
      "Iteration 27, loss = 0.28702576\n",
      "Iteration 28, loss = 0.29076521\n",
      "Iteration 29, loss = 0.28987229\n",
      "Iteration 30, loss = 0.28868762\n",
      "Iteration 31, loss = 0.29379803\n",
      "Iteration 32, loss = 0.29199491\n",
      "Iteration 33, loss = 0.28931814\n",
      "Iteration 34, loss = 0.29172956\n",
      "Iteration 35, loss = 0.29217216\n",
      "Iteration 36, loss = 0.28975866\n",
      "Iteration 37, loss = 0.28938692\n",
      "Iteration 38, loss = 0.28895560\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.29976324\n",
      "Iteration 2, loss = 1.61790381\n",
      "Iteration 3, loss = 1.31980825\n",
      "Iteration 4, loss = 1.07155821\n",
      "Iteration 5, loss = 0.86035737\n",
      "Iteration 6, loss = 0.70887878\n",
      "Iteration 7, loss = 0.61074195\n",
      "Iteration 8, loss = 0.54701448\n",
      "Iteration 9, loss = 0.49744980\n",
      "Iteration 10, loss = 0.45055163\n",
      "Iteration 11, loss = 0.40874704\n",
      "Iteration 12, loss = 0.37888036\n",
      "Iteration 13, loss = 0.36258875\n",
      "Iteration 14, loss = 0.34812255\n",
      "Iteration 15, loss = 0.33342675\n",
      "Iteration 16, loss = 0.32055922\n",
      "Iteration 17, loss = 0.31378312\n",
      "Iteration 18, loss = 0.30650480\n",
      "Iteration 19, loss = 0.29828642\n",
      "Iteration 20, loss = 0.29394324\n",
      "Iteration 21, loss = 0.29643711\n",
      "Iteration 22, loss = 0.29741096\n",
      "Iteration 23, loss = 0.29285807\n",
      "Iteration 24, loss = 0.29097122\n",
      "Iteration 25, loss = 0.29294198\n",
      "Iteration 26, loss = 0.29309423\n",
      "Iteration 27, loss = 0.28460919\n",
      "Iteration 28, loss = 0.28367841\n",
      "Iteration 29, loss = 0.28315854\n",
      "Iteration 30, loss = 0.28254118\n",
      "Iteration 31, loss = 0.28709058\n",
      "Iteration 32, loss = 0.28787013\n",
      "Iteration 33, loss = 0.28557673\n",
      "Iteration 34, loss = 0.28419117\n",
      "Iteration 35, loss = 0.28226150\n",
      "Iteration 36, loss = 0.27719557\n",
      "Iteration 37, loss = 0.27484687\n",
      "Iteration 38, loss = 0.27493973\n",
      "Iteration 39, loss = 0.28304402\n",
      "Iteration 40, loss = 0.28959346\n",
      "Iteration 41, loss = 0.28749576\n",
      "Iteration 42, loss = 0.28913114\n",
      "Iteration 43, loss = 0.28365220\n",
      "Iteration 44, loss = 0.28470299\n",
      "Iteration 45, loss = 0.29777099\n",
      "Iteration 46, loss = 0.28543444\n",
      "Iteration 47, loss = 0.27521592\n",
      "Iteration 48, loss = 0.28208918\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.31262122\n",
      "Iteration 2, loss = 1.62118116\n",
      "Iteration 3, loss = 1.31815267\n",
      "Iteration 4, loss = 1.07312823\n",
      "Iteration 5, loss = 0.86437709\n",
      "Iteration 6, loss = 0.71421777\n",
      "Iteration 7, loss = 0.61299745\n",
      "Iteration 8, loss = 0.54582881\n",
      "Iteration 9, loss = 0.49520435\n",
      "Iteration 10, loss = 0.44896227\n",
      "Iteration 11, loss = 0.40941977\n",
      "Iteration 12, loss = 0.38206763\n",
      "Iteration 13, loss = 0.36737626\n",
      "Iteration 14, loss = 0.35133334\n",
      "Iteration 15, loss = 0.33500175\n",
      "Iteration 16, loss = 0.32061498\n",
      "Iteration 17, loss = 0.31620795\n",
      "Iteration 18, loss = 0.31364123\n",
      "Iteration 19, loss = 0.30789911\n",
      "Iteration 20, loss = 0.30088097\n",
      "Iteration 21, loss = 0.29614952\n",
      "Iteration 22, loss = 0.29411648\n",
      "Iteration 23, loss = 0.29320217\n",
      "Iteration 24, loss = 0.29496346\n",
      "Iteration 25, loss = 0.29612368\n",
      "Iteration 26, loss = 0.29280926\n",
      "Iteration 27, loss = 0.28741331\n",
      "Iteration 28, loss = 0.29347265\n",
      "Iteration 29, loss = 0.29252164\n",
      "Iteration 30, loss = 0.28904844\n",
      "Iteration 31, loss = 0.29426881\n",
      "Iteration 32, loss = 0.29623520\n",
      "Iteration 33, loss = 0.29317256\n",
      "Iteration 34, loss = 0.28851075\n",
      "Iteration 35, loss = 0.28627982\n",
      "Iteration 36, loss = 0.28257427\n",
      "Iteration 37, loss = 0.28035652\n",
      "Iteration 38, loss = 0.27944801\n",
      "Iteration 39, loss = 0.28483901\n",
      "Iteration 40, loss = 0.29142580\n",
      "Iteration 41, loss = 0.28479681\n",
      "Iteration 42, loss = 0.28538389\n",
      "Iteration 43, loss = 0.29454618\n",
      "Iteration 44, loss = 0.29934136\n",
      "Iteration 45, loss = 0.29819505\n",
      "Iteration 46, loss = 0.28214816\n",
      "Iteration 47, loss = 0.27374318\n",
      "Iteration 48, loss = 0.28270830\n",
      "Iteration 49, loss = 0.30941034\n",
      "Iteration 50, loss = 0.30876814\n",
      "Iteration 51, loss = 0.29622425\n",
      "Iteration 52, loss = 0.28995563\n",
      "Iteration 53, loss = 0.28502514\n",
      "Iteration 54, loss = 0.28469391\n",
      "Iteration 55, loss = 0.28722362\n",
      "Iteration 56, loss = 0.28431084\n",
      "Iteration 57, loss = 0.28800142\n",
      "Iteration 58, loss = 0.29237790\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.31486271\n",
      "Iteration 2, loss = 1.61739972\n",
      "Iteration 3, loss = 1.30690870\n",
      "Iteration 4, loss = 1.05917857\n",
      "Iteration 5, loss = 0.84800121\n",
      "Iteration 6, loss = 0.69722749\n",
      "Iteration 7, loss = 0.59712409\n",
      "Iteration 8, loss = 0.52947121\n",
      "Iteration 9, loss = 0.47983665\n",
      "Iteration 10, loss = 0.43439306\n",
      "Iteration 11, loss = 0.39611465\n",
      "Iteration 12, loss = 0.36969297\n",
      "Iteration 13, loss = 0.35362778\n",
      "Iteration 14, loss = 0.33953732\n",
      "Iteration 15, loss = 0.32560743\n",
      "Iteration 16, loss = 0.31329135\n",
      "Iteration 17, loss = 0.30634484\n",
      "Iteration 18, loss = 0.30202351\n",
      "Iteration 19, loss = 0.29704226\n",
      "Iteration 20, loss = 0.29446258\n",
      "Iteration 21, loss = 0.28899418\n",
      "Iteration 22, loss = 0.28383025\n",
      "Iteration 23, loss = 0.27750198\n",
      "Iteration 24, loss = 0.27754107\n",
      "Iteration 25, loss = 0.28181068\n",
      "Iteration 26, loss = 0.28693249\n",
      "Iteration 27, loss = 0.28017067\n",
      "Iteration 28, loss = 0.28497973\n",
      "Iteration 29, loss = 0.28887916\n",
      "Iteration 30, loss = 0.28694233\n",
      "Iteration 31, loss = 0.29385938\n",
      "Iteration 32, loss = 0.29417851\n",
      "Iteration 33, loss = 0.28928202\n",
      "Iteration 34, loss = 0.28515867\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.31974774\n",
      "Iteration 2, loss = 1.64504450\n",
      "Iteration 3, loss = 1.32431760\n",
      "Iteration 4, loss = 1.06811950\n",
      "Iteration 5, loss = 0.85879065\n",
      "Iteration 6, loss = 0.71163118\n",
      "Iteration 7, loss = 0.61242237\n",
      "Iteration 8, loss = 0.54202384\n",
      "Iteration 9, loss = 0.48699589\n",
      "Iteration 10, loss = 0.43874507\n",
      "Iteration 11, loss = 0.40223069\n",
      "Iteration 12, loss = 0.37846496\n",
      "Iteration 13, loss = 0.36320959\n",
      "Iteration 14, loss = 0.34514221\n",
      "Iteration 15, loss = 0.32873725\n",
      "Iteration 16, loss = 0.31559423\n",
      "Iteration 17, loss = 0.30921752\n",
      "Iteration 18, loss = 0.30425325\n",
      "Iteration 19, loss = 0.29850684\n",
      "Iteration 20, loss = 0.29454672\n",
      "Iteration 21, loss = 0.28690114\n",
      "Iteration 22, loss = 0.28184858\n",
      "Iteration 23, loss = 0.28151263\n",
      "Iteration 24, loss = 0.28562089\n",
      "Iteration 25, loss = 0.28969967\n",
      "Iteration 26, loss = 0.29007427\n",
      "Iteration 27, loss = 0.28232870\n",
      "Iteration 28, loss = 0.28594943\n",
      "Iteration 29, loss = 0.29074324\n",
      "Iteration 30, loss = 0.28902581\n",
      "Iteration 31, loss = 0.29407111\n",
      "Iteration 32, loss = 0.29241890\n",
      "Iteration 33, loss = 0.28358161\n",
      "Iteration 34, loss = 0.27806409\n",
      "Iteration 35, loss = 0.27674859\n",
      "Iteration 36, loss = 0.27700127\n",
      "Iteration 37, loss = 0.27462810\n",
      "Iteration 38, loss = 0.27468873\n",
      "Iteration 39, loss = 0.28212682\n",
      "Iteration 40, loss = 0.28809976\n",
      "Iteration 41, loss = 0.27976441\n",
      "Iteration 42, loss = 0.28408648\n",
      "Iteration 43, loss = 0.29434251\n",
      "Iteration 44, loss = 0.29304442\n",
      "Iteration 45, loss = 0.28225298\n",
      "Iteration 46, loss = 0.27533244\n",
      "Iteration 47, loss = 0.28580267\n",
      "Iteration 48, loss = 0.28712199\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.32882976\n",
      "Iteration 2, loss = 1.65054366\n",
      "Iteration 3, loss = 1.33623191\n",
      "Iteration 4, loss = 1.08084128\n",
      "Iteration 5, loss = 0.87275258\n",
      "Iteration 6, loss = 0.72801011\n",
      "Iteration 7, loss = 0.63051312\n",
      "Iteration 8, loss = 0.56138878\n",
      "Iteration 9, loss = 0.50515680\n",
      "Iteration 10, loss = 0.45523408\n",
      "Iteration 11, loss = 0.41741806\n",
      "Iteration 12, loss = 0.39376469\n",
      "Iteration 13, loss = 0.38076645\n",
      "Iteration 14, loss = 0.36590900\n",
      "Iteration 15, loss = 0.34819870\n",
      "Iteration 16, loss = 0.33025616\n",
      "Iteration 17, loss = 0.31984003\n",
      "Iteration 18, loss = 0.31497361\n",
      "Iteration 19, loss = 0.31150089\n",
      "Iteration 20, loss = 0.31101808\n",
      "Iteration 21, loss = 0.30237492\n",
      "Iteration 22, loss = 0.29651275\n",
      "Iteration 23, loss = 0.29789426\n",
      "Iteration 24, loss = 0.30174360\n",
      "Iteration 25, loss = 0.30070392\n",
      "Iteration 26, loss = 0.29707887\n",
      "Iteration 27, loss = 0.29117905\n",
      "Iteration 28, loss = 0.29592805\n",
      "Iteration 29, loss = 0.29977941\n",
      "Iteration 30, loss = 0.29720774\n",
      "Iteration 31, loss = 0.30111916\n",
      "Iteration 32, loss = 0.30082442\n",
      "Iteration 33, loss = 0.29628833\n",
      "Iteration 34, loss = 0.29099341\n",
      "Iteration 35, loss = 0.28703934\n",
      "Iteration 36, loss = 0.28435177\n",
      "Iteration 37, loss = 0.27953394\n",
      "Iteration 38, loss = 0.28312603\n",
      "Iteration 39, loss = 0.29513701\n",
      "Iteration 40, loss = 0.30210073\n",
      "Iteration 41, loss = 0.29127401\n",
      "Iteration 42, loss = 0.29527750\n",
      "Iteration 43, loss = 0.30738768\n",
      "Iteration 44, loss = 0.31065566\n",
      "Iteration 45, loss = 0.28945338\n",
      "Iteration 46, loss = 0.28601551\n",
      "Iteration 47, loss = 0.31095658\n",
      "Iteration 48, loss = 0.31502375\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.31836015\n",
      "Iteration 2, loss = 1.62949032\n",
      "Iteration 3, loss = 1.31633800\n",
      "Iteration 4, loss = 1.06491508\n",
      "Iteration 5, loss = 0.85955916\n",
      "Iteration 6, loss = 0.71786747\n",
      "Iteration 7, loss = 0.62329617\n",
      "Iteration 8, loss = 0.55713282\n",
      "Iteration 9, loss = 0.50079545\n",
      "Iteration 10, loss = 0.45102444\n",
      "Iteration 11, loss = 0.41457910\n",
      "Iteration 12, loss = 0.39124779\n",
      "Iteration 13, loss = 0.37682465\n",
      "Iteration 14, loss = 0.36173770\n",
      "Iteration 15, loss = 0.34426068\n",
      "Iteration 16, loss = 0.32719783\n",
      "Iteration 17, loss = 0.31696979\n",
      "Iteration 18, loss = 0.31223694\n",
      "Iteration 19, loss = 0.31066811\n",
      "Iteration 20, loss = 0.31148386\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 21, loss = 0.30178892\n",
      "Iteration 22, loss = 0.29360846\n",
      "Iteration 23, loss = 0.29560372\n",
      "Iteration 24, loss = 0.30058109\n",
      "Iteration 25, loss = 0.29878978\n",
      "Iteration 26, loss = 0.29344102\n",
      "Iteration 27, loss = 0.28894757\n",
      "Iteration 28, loss = 0.29615651\n",
      "Iteration 29, loss = 0.29839750\n",
      "Iteration 30, loss = 0.29295026\n",
      "Iteration 31, loss = 0.29542312\n",
      "Iteration 32, loss = 0.29938670\n",
      "Iteration 33, loss = 0.29790825\n",
      "Iteration 34, loss = 0.29349302\n",
      "Iteration 35, loss = 0.28804348\n",
      "Iteration 36, loss = 0.28206705\n",
      "Iteration 37, loss = 0.27946117\n",
      "Iteration 38, loss = 0.28853735\n",
      "Iteration 39, loss = 0.30209381\n",
      "Iteration 40, loss = 0.30721839\n",
      "Iteration 41, loss = 0.29496396\n",
      "Iteration 42, loss = 0.29529214\n",
      "Iteration 43, loss = 0.30316078\n",
      "Iteration 44, loss = 0.30541882\n",
      "Iteration 45, loss = 0.28280313\n",
      "Iteration 46, loss = 0.28216061\n",
      "Iteration 47, loss = 0.30871834\n",
      "Iteration 48, loss = 0.31587333\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.32728072\n",
      "Iteration 2, loss = 1.63004437\n",
      "Iteration 3, loss = 1.31401844\n",
      "Iteration 4, loss = 1.06837781\n",
      "Iteration 5, loss = 0.86188697\n",
      "Iteration 6, loss = 0.71171252\n",
      "Iteration 7, loss = 0.60588458\n",
      "Iteration 8, loss = 0.53167991\n",
      "Iteration 9, loss = 0.47739181\n",
      "Iteration 10, loss = 0.43306608\n",
      "Iteration 11, loss = 0.39955053\n",
      "Iteration 12, loss = 0.37444913\n",
      "Iteration 13, loss = 0.35886412\n",
      "Iteration 14, loss = 0.34442240\n",
      "Iteration 15, loss = 0.32976105\n",
      "Iteration 16, loss = 0.31600869\n",
      "Iteration 17, loss = 0.30562548\n",
      "Iteration 18, loss = 0.30024825\n",
      "Iteration 19, loss = 0.29960524\n",
      "Iteration 20, loss = 0.30438764\n",
      "Iteration 21, loss = 0.29650436\n",
      "Iteration 22, loss = 0.28859179\n",
      "Iteration 23, loss = 0.28823038\n",
      "Iteration 24, loss = 0.28826293\n",
      "Iteration 25, loss = 0.28748058\n",
      "Iteration 26, loss = 0.28615217\n",
      "Iteration 27, loss = 0.28306565\n",
      "Iteration 28, loss = 0.29090099\n",
      "Iteration 29, loss = 0.29533552\n",
      "Iteration 30, loss = 0.29100969\n",
      "Iteration 31, loss = 0.28966331\n",
      "Iteration 32, loss = 0.28958457\n",
      "Iteration 33, loss = 0.28869401\n",
      "Iteration 34, loss = 0.28942163\n",
      "Iteration 35, loss = 0.28444706\n",
      "Iteration 36, loss = 0.28144936\n",
      "Iteration 37, loss = 0.28053867\n",
      "Iteration 38, loss = 0.28946901\n",
      "Iteration 39, loss = 0.29885091\n",
      "Iteration 40, loss = 0.30064750\n",
      "Iteration 41, loss = 0.28701401\n",
      "Iteration 42, loss = 0.28314496\n",
      "Iteration 43, loss = 0.28736071\n",
      "Iteration 44, loss = 0.29284755\n",
      "Iteration 45, loss = 0.27763644\n",
      "Iteration 46, loss = 0.27500224\n",
      "Iteration 47, loss = 0.28975248\n",
      "Iteration 48, loss = 0.29590137\n",
      "Iteration 49, loss = 0.30808628\n",
      "Iteration 50, loss = 0.31449979\n",
      "Iteration 51, loss = 0.30919544\n",
      "Iteration 52, loss = 0.29063614\n",
      "Iteration 53, loss = 0.27890677\n",
      "Iteration 54, loss = 0.28159449\n",
      "Iteration 55, loss = 0.28082932\n",
      "Iteration 56, loss = 0.28316885\n",
      "Iteration 57, loss = 0.28977937\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.24877712\n",
      "Iteration 2, loss = 1.56797131\n",
      "Iteration 3, loss = 1.26985363\n",
      "Iteration 4, loss = 1.00903289\n",
      "Iteration 5, loss = 0.80382008\n",
      "Iteration 6, loss = 0.66191925\n",
      "Iteration 7, loss = 0.57243039\n",
      "Iteration 8, loss = 0.50843679\n",
      "Iteration 9, loss = 0.46193618\n",
      "Iteration 10, loss = 0.42728172\n",
      "Iteration 11, loss = 0.39971586\n",
      "Iteration 12, loss = 0.37837554\n",
      "Iteration 13, loss = 0.36466043\n",
      "Iteration 14, loss = 0.35731618\n",
      "Iteration 15, loss = 0.34516052\n",
      "Iteration 16, loss = 0.33239619\n",
      "Iteration 17, loss = 0.31731473\n",
      "Iteration 18, loss = 0.31406447\n",
      "Iteration 19, loss = 0.32250090\n",
      "Iteration 20, loss = 0.32699255\n",
      "Iteration 21, loss = 0.32026174\n",
      "Iteration 22, loss = 0.30722997\n",
      "Iteration 23, loss = 0.29101181\n",
      "Iteration 24, loss = 0.28726837\n",
      "Iteration 25, loss = 0.30117347\n",
      "Iteration 26, loss = 0.30258878\n",
      "Iteration 27, loss = 0.30256544\n",
      "Iteration 28, loss = 0.29790821\n",
      "Iteration 29, loss = 0.28663788\n",
      "Iteration 30, loss = 0.29395238\n",
      "Iteration 31, loss = 0.30264319\n",
      "Iteration 32, loss = 0.30490609\n",
      "Iteration 33, loss = 0.29986529\n",
      "Iteration 34, loss = 0.29612063\n",
      "Iteration 35, loss = 0.29636229\n",
      "Iteration 36, loss = 0.30432175\n",
      "Iteration 37, loss = 0.31479834\n",
      "Iteration 38, loss = 0.31586623\n",
      "Iteration 39, loss = 0.30507315\n",
      "Iteration 40, loss = 0.30205191\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.23746126\n",
      "Iteration 2, loss = 1.55392512\n",
      "Iteration 3, loss = 1.25613598\n",
      "Iteration 4, loss = 1.00989664\n",
      "Iteration 5, loss = 0.80979798\n",
      "Iteration 6, loss = 0.67159576\n",
      "Iteration 7, loss = 0.58375316\n",
      "Iteration 8, loss = 0.52404370\n",
      "Iteration 9, loss = 0.47949221\n",
      "Iteration 10, loss = 0.44322390\n",
      "Iteration 11, loss = 0.41211529\n",
      "Iteration 12, loss = 0.38733380\n",
      "Iteration 13, loss = 0.36991145\n",
      "Iteration 14, loss = 0.35340337\n",
      "Iteration 15, loss = 0.34982607\n",
      "Iteration 16, loss = 0.34671982\n",
      "Iteration 17, loss = 0.33660775\n",
      "Iteration 18, loss = 0.32505376\n",
      "Iteration 19, loss = 0.31300675\n",
      "Iteration 20, loss = 0.30099682\n",
      "Iteration 21, loss = 0.29739925\n",
      "Iteration 22, loss = 0.29776776\n",
      "Iteration 23, loss = 0.30309383\n",
      "Iteration 24, loss = 0.30332226\n",
      "Iteration 25, loss = 0.29521839\n",
      "Iteration 26, loss = 0.27978395\n",
      "Iteration 27, loss = 0.27842864\n",
      "Iteration 28, loss = 0.29540261\n",
      "Iteration 29, loss = 0.30779568\n",
      "Iteration 30, loss = 0.30761011\n",
      "Iteration 31, loss = 0.30530704\n",
      "Iteration 32, loss = 0.29035822\n",
      "Iteration 33, loss = 0.29616831\n",
      "Iteration 34, loss = 0.30369034\n",
      "Iteration 35, loss = 0.30254593\n",
      "Iteration 36, loss = 0.29520962\n",
      "Iteration 37, loss = 0.28933879\n",
      "Iteration 38, loss = 0.29355124\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.24587357\n",
      "Iteration 2, loss = 1.55729946\n",
      "Iteration 3, loss = 1.25186900\n",
      "Iteration 4, loss = 1.00799481\n",
      "Iteration 5, loss = 0.81032090\n",
      "Iteration 6, loss = 0.66774872\n",
      "Iteration 7, loss = 0.57461298\n",
      "Iteration 8, loss = 0.51354379\n",
      "Iteration 9, loss = 0.47194997\n",
      "Iteration 10, loss = 0.43906689\n",
      "Iteration 11, loss = 0.41004674\n",
      "Iteration 12, loss = 0.38405957\n",
      "Iteration 13, loss = 0.36545634\n",
      "Iteration 14, loss = 0.34952154\n",
      "Iteration 15, loss = 0.34681152\n",
      "Iteration 16, loss = 0.34025739\n",
      "Iteration 17, loss = 0.32892068\n",
      "Iteration 18, loss = 0.32166126\n",
      "Iteration 19, loss = 0.31462537\n",
      "Iteration 20, loss = 0.30556317\n",
      "Iteration 21, loss = 0.30380715\n",
      "Iteration 22, loss = 0.30362845\n",
      "Iteration 23, loss = 0.30437879\n",
      "Iteration 24, loss = 0.30461645\n",
      "Iteration 25, loss = 0.29694723\n",
      "Iteration 26, loss = 0.28378381\n",
      "Iteration 27, loss = 0.28141910\n",
      "Iteration 28, loss = 0.29219992\n",
      "Iteration 29, loss = 0.30379342\n",
      "Iteration 30, loss = 0.30409846\n",
      "Iteration 31, loss = 0.29926764\n",
      "Iteration 32, loss = 0.28934412\n",
      "Iteration 33, loss = 0.29058973\n",
      "Iteration 34, loss = 0.29172753\n",
      "Iteration 35, loss = 0.29205393\n",
      "Iteration 36, loss = 0.29122458\n",
      "Iteration 37, loss = 0.28891243\n",
      "Iteration 38, loss = 0.29082684\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.23011901\n",
      "Iteration 2, loss = 1.55270603\n",
      "Iteration 3, loss = 1.24519179\n",
      "Iteration 4, loss = 1.00111873\n",
      "Iteration 5, loss = 0.80877084\n",
      "Iteration 6, loss = 0.67077797\n",
      "Iteration 7, loss = 0.58028069\n",
      "Iteration 8, loss = 0.52266807\n",
      "Iteration 9, loss = 0.48257773\n",
      "Iteration 10, loss = 0.44766323\n",
      "Iteration 11, loss = 0.41475607\n",
      "Iteration 12, loss = 0.38589534\n",
      "Iteration 13, loss = 0.36446683\n",
      "Iteration 14, loss = 0.34583205\n",
      "Iteration 15, loss = 0.34142971\n",
      "Iteration 16, loss = 0.33421536\n",
      "Iteration 17, loss = 0.32472031\n",
      "Iteration 18, loss = 0.31882124\n",
      "Iteration 19, loss = 0.31314580\n",
      "Iteration 20, loss = 0.30511088\n",
      "Iteration 21, loss = 0.30380135\n",
      "Iteration 22, loss = 0.30099827\n",
      "Iteration 23, loss = 0.29859087\n",
      "Iteration 24, loss = 0.29889720\n",
      "Iteration 25, loss = 0.29782653\n",
      "Iteration 26, loss = 0.28500923\n",
      "Iteration 27, loss = 0.27760093\n",
      "Iteration 28, loss = 0.28154272\n",
      "Iteration 29, loss = 0.29348512\n",
      "Iteration 30, loss = 0.29979005\n",
      "Iteration 31, loss = 0.30021384\n",
      "Iteration 32, loss = 0.29771897\n",
      "Iteration 33, loss = 0.29898455\n",
      "Iteration 34, loss = 0.29377427\n",
      "Iteration 35, loss = 0.28466993\n",
      "Iteration 36, loss = 0.28087951\n",
      "Iteration 37, loss = 0.28294229\n",
      "Iteration 38, loss = 0.28777005\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.24334309\n",
      "Iteration 2, loss = 1.57308672\n",
      "Iteration 3, loss = 1.28692975\n",
      "Iteration 4, loss = 1.03791356\n",
      "Iteration 5, loss = 0.83752757\n",
      "Iteration 6, loss = 0.69920902\n",
      "Iteration 7, loss = 0.60657850\n",
      "Iteration 8, loss = 0.54385770\n",
      "Iteration 9, loss = 0.50232831\n",
      "Iteration 10, loss = 0.46601255\n",
      "Iteration 11, loss = 0.43140461\n",
      "Iteration 12, loss = 0.40044141\n",
      "Iteration 13, loss = 0.37709761\n",
      "Iteration 14, loss = 0.35540124\n",
      "Iteration 15, loss = 0.34627169\n",
      "Iteration 16, loss = 0.33871228\n",
      "Iteration 17, loss = 0.33233021\n",
      "Iteration 18, loss = 0.32887227\n",
      "Iteration 19, loss = 0.32267844\n",
      "Iteration 20, loss = 0.31216028\n",
      "Iteration 21, loss = 0.30725152\n",
      "Iteration 22, loss = 0.30819254\n",
      "Iteration 23, loss = 0.30492153\n",
      "Iteration 24, loss = 0.30200960\n",
      "Iteration 25, loss = 0.29841113\n",
      "Iteration 26, loss = 0.28631177\n",
      "Iteration 27, loss = 0.27994430\n",
      "Iteration 28, loss = 0.28715605\n",
      "Iteration 29, loss = 0.30281021\n",
      "Iteration 30, loss = 0.30978117\n",
      "Iteration 31, loss = 0.30492010\n",
      "Iteration 32, loss = 0.29585077\n",
      "Iteration 33, loss = 0.29173921\n",
      "Iteration 34, loss = 0.28776012\n",
      "Iteration 35, loss = 0.28315574\n",
      "Iteration 36, loss = 0.28822569\n",
      "Iteration 37, loss = 0.29688656\n",
      "Iteration 38, loss = 0.29653097\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.23143707\n",
      "Iteration 2, loss = 1.56562237\n",
      "Iteration 3, loss = 1.27262717\n",
      "Iteration 4, loss = 1.01723650\n",
      "Iteration 5, loss = 0.81733371\n",
      "Iteration 6, loss = 0.68052879\n",
      "Iteration 7, loss = 0.59128516\n",
      "Iteration 8, loss = 0.52947346\n",
      "Iteration 9, loss = 0.48565983\n",
      "Iteration 10, loss = 0.44914751\n",
      "Iteration 11, loss = 0.41543928\n",
      "Iteration 12, loss = 0.38705552\n",
      "Iteration 13, loss = 0.36398902\n",
      "Iteration 14, loss = 0.34513877\n",
      "Iteration 15, loss = 0.33431711\n",
      "Iteration 16, loss = 0.32090521\n",
      "Iteration 17, loss = 0.31342536\n",
      "Iteration 18, loss = 0.31341252\n",
      "Iteration 19, loss = 0.30773862\n",
      "Iteration 20, loss = 0.30098246\n",
      "Iteration 21, loss = 0.30491091\n",
      "Iteration 22, loss = 0.31230125\n",
      "Iteration 23, loss = 0.30269597\n",
      "Iteration 24, loss = 0.29011596\n",
      "Iteration 25, loss = 0.28747461\n",
      "Iteration 26, loss = 0.28312129\n",
      "Iteration 27, loss = 0.27649201\n",
      "Iteration 28, loss = 0.27336487\n",
      "Iteration 29, loss = 0.27744063\n",
      "Iteration 30, loss = 0.28504740\n",
      "Iteration 31, loss = 0.28965854\n",
      "Iteration 32, loss = 0.28854675\n",
      "Iteration 33, loss = 0.28818090\n",
      "Iteration 34, loss = 0.28569474\n",
      "Iteration 35, loss = 0.28284379\n",
      "Iteration 36, loss = 0.28761491\n",
      "Iteration 37, loss = 0.29337915\n",
      "Iteration 38, loss = 0.28535452\n",
      "Iteration 39, loss = 0.28090599\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.25539142\n",
      "Iteration 2, loss = 1.59229075\n",
      "Iteration 3, loss = 1.30784388\n",
      "Iteration 4, loss = 1.04863957\n",
      "Iteration 5, loss = 0.84575271\n",
      "Iteration 6, loss = 0.70919464\n",
      "Iteration 7, loss = 0.61849586\n",
      "Iteration 8, loss = 0.55249475\n",
      "Iteration 9, loss = 0.50278397\n",
      "Iteration 10, loss = 0.46333641\n",
      "Iteration 11, loss = 0.42736363\n",
      "Iteration 12, loss = 0.39827002\n",
      "Iteration 13, loss = 0.37426147\n",
      "Iteration 14, loss = 0.35460369\n",
      "Iteration 15, loss = 0.34688711\n",
      "Iteration 16, loss = 0.33393061\n",
      "Iteration 17, loss = 0.32383731\n",
      "Iteration 18, loss = 0.31932436\n",
      "Iteration 19, loss = 0.31046747\n",
      "Iteration 20, loss = 0.30865060\n",
      "Iteration 21, loss = 0.31349828\n",
      "Iteration 22, loss = 0.31397487\n",
      "Iteration 23, loss = 0.30096001\n",
      "Iteration 24, loss = 0.29005058\n",
      "Iteration 25, loss = 0.29418900\n",
      "Iteration 26, loss = 0.29406151\n",
      "Iteration 27, loss = 0.28896451\n",
      "Iteration 28, loss = 0.28524959\n",
      "Iteration 29, loss = 0.28834974\n",
      "Iteration 30, loss = 0.29133953\n",
      "Iteration 31, loss = 0.29441058\n",
      "Iteration 32, loss = 0.29699781\n",
      "Iteration 33, loss = 0.29872080\n",
      "Iteration 34, loss = 0.29311321\n",
      "Iteration 35, loss = 0.28642618\n",
      "Iteration 36, loss = 0.28436341\n",
      "Iteration 37, loss = 0.29349846\n",
      "Iteration 38, loss = 0.29920883\n",
      "Iteration 39, loss = 0.29709691\n",
      "Iteration 40, loss = 0.29155169\n",
      "Iteration 41, loss = 0.28706249\n",
      "Iteration 42, loss = 0.28306779\n",
      "Iteration 43, loss = 0.29031169\n",
      "Iteration 44, loss = 0.28486744\n",
      "Iteration 45, loss = 0.28821432\n",
      "Iteration 46, loss = 0.28253468\n",
      "Iteration 47, loss = 0.28345454\n",
      "Iteration 48, loss = 0.28284091\n",
      "Iteration 49, loss = 0.27642863\n",
      "Iteration 50, loss = 0.27942997\n",
      "Iteration 51, loss = 0.28111009\n",
      "Iteration 52, loss = 0.28947969\n",
      "Iteration 53, loss = 0.29172214\n",
      "Iteration 54, loss = 0.28411227\n",
      "Iteration 55, loss = 0.27288974\n",
      "Iteration 56, loss = 0.27584832\n",
      "Iteration 57, loss = 0.28218582\n",
      "Iteration 58, loss = 0.29390346\n",
      "Iteration 59, loss = 0.28242064\n",
      "Iteration 60, loss = 0.27371627\n",
      "Iteration 61, loss = 0.28439911\n",
      "Iteration 62, loss = 0.29538496\n",
      "Iteration 63, loss = 0.29183153\n",
      "Iteration 64, loss = 0.28602385\n",
      "Iteration 65, loss = 0.27794356\n",
      "Iteration 66, loss = 0.27490763\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.25929022\n",
      "Iteration 2, loss = 1.59981735\n",
      "Iteration 3, loss = 1.31636678\n",
      "Iteration 4, loss = 1.05599812\n",
      "Iteration 5, loss = 0.85208337\n",
      "Iteration 6, loss = 0.71473206\n",
      "Iteration 7, loss = 0.62426415\n",
      "Iteration 8, loss = 0.56032829\n",
      "Iteration 9, loss = 0.51275513\n",
      "Iteration 10, loss = 0.47100963\n",
      "Iteration 11, loss = 0.43232100\n",
      "Iteration 12, loss = 0.40157827\n",
      "Iteration 13, loss = 0.37507020\n",
      "Iteration 14, loss = 0.35889334\n",
      "Iteration 15, loss = 0.35167518\n",
      "Iteration 16, loss = 0.34147479\n",
      "Iteration 17, loss = 0.33304588\n",
      "Iteration 18, loss = 0.32613142\n",
      "Iteration 19, loss = 0.31862784\n",
      "Iteration 20, loss = 0.32112362\n",
      "Iteration 21, loss = 0.32819508\n",
      "Iteration 22, loss = 0.32607928\n",
      "Iteration 23, loss = 0.30984221\n",
      "Iteration 24, loss = 0.30518916\n",
      "Iteration 25, loss = 0.32115773\n",
      "Iteration 26, loss = 0.32667181\n",
      "Iteration 27, loss = 0.31200128\n",
      "Iteration 28, loss = 0.29641685\n",
      "Iteration 29, loss = 0.29683623\n",
      "Iteration 30, loss = 0.31090942\n",
      "Iteration 31, loss = 0.32324088\n",
      "Iteration 32, loss = 0.32153208\n",
      "Iteration 33, loss = 0.31300165\n",
      "Iteration 34, loss = 0.29852322\n",
      "Iteration 35, loss = 0.28979026\n",
      "Iteration 36, loss = 0.29343284\n",
      "Iteration 37, loss = 0.30902230\n",
      "Iteration 38, loss = 0.31301931\n",
      "Iteration 39, loss = 0.30415404\n",
      "Iteration 40, loss = 0.29529291\n",
      "Iteration 41, loss = 0.29361828\n",
      "Iteration 42, loss = 0.29033495\n",
      "Iteration 43, loss = 0.29826369\n",
      "Iteration 44, loss = 0.29484509\n",
      "Iteration 45, loss = 0.29954905\n",
      "Iteration 46, loss = 0.29418202\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.24149948\n",
      "Iteration 2, loss = 1.58040481\n",
      "Iteration 3, loss = 1.28636629\n",
      "Iteration 4, loss = 1.02651455\n",
      "Iteration 5, loss = 0.82968323\n",
      "Iteration 6, loss = 0.69847631\n",
      "Iteration 7, loss = 0.61292997\n",
      "Iteration 8, loss = 0.55106133\n",
      "Iteration 9, loss = 0.50361697\n",
      "Iteration 10, loss = 0.46307483\n",
      "Iteration 11, loss = 0.42418282\n",
      "Iteration 12, loss = 0.39409338\n",
      "Iteration 13, loss = 0.36750506\n",
      "Iteration 14, loss = 0.35152357\n",
      "Iteration 15, loss = 0.34487267\n",
      "Iteration 16, loss = 0.33410817\n",
      "Iteration 17, loss = 0.32550330\n",
      "Iteration 18, loss = 0.31868485\n",
      "Iteration 19, loss = 0.31483418\n",
      "Iteration 20, loss = 0.31571585\n",
      "Iteration 21, loss = 0.31815914\n",
      "Iteration 22, loss = 0.31604604\n",
      "Iteration 23, loss = 0.30404744\n",
      "Iteration 24, loss = 0.30175775\n",
      "Iteration 25, loss = 0.31888270\n",
      "Iteration 26, loss = 0.32734789\n",
      "Iteration 27, loss = 0.31055443\n",
      "Iteration 28, loss = 0.29377236\n",
      "Iteration 29, loss = 0.29929976\n",
      "Iteration 30, loss = 0.31148743\n",
      "Iteration 31, loss = 0.31661006\n",
      "Iteration 32, loss = 0.31187888\n",
      "Iteration 33, loss = 0.30403839\n",
      "Iteration 34, loss = 0.29120816\n",
      "Iteration 35, loss = 0.28351264\n",
      "Iteration 36, loss = 0.28771981\n",
      "Iteration 37, loss = 0.30726682\n",
      "Iteration 38, loss = 0.31426624\n",
      "Iteration 39, loss = 0.30464456\n",
      "Iteration 40, loss = 0.29769560\n",
      "Iteration 41, loss = 0.29728556\n",
      "Iteration 42, loss = 0.29166932\n",
      "Iteration 43, loss = 0.29831490\n",
      "Iteration 44, loss = 0.29346848\n",
      "Iteration 45, loss = 0.29458160\n",
      "Iteration 46, loss = 0.29003180\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.25853234\n",
      "Iteration 2, loss = 1.58463724\n",
      "Iteration 3, loss = 1.28916235\n",
      "Iteration 4, loss = 1.02245538\n",
      "Iteration 5, loss = 0.80797692\n",
      "Iteration 6, loss = 0.66816477\n",
      "Iteration 7, loss = 0.58208871\n",
      "Iteration 8, loss = 0.52447138\n",
      "Iteration 9, loss = 0.48060964\n",
      "Iteration 10, loss = 0.43637788\n",
      "Iteration 11, loss = 0.39571517\n",
      "Iteration 12, loss = 0.36731739\n",
      "Iteration 13, loss = 0.34390153\n",
      "Iteration 14, loss = 0.32945136\n",
      "Iteration 15, loss = 0.32213760\n",
      "Iteration 16, loss = 0.31397744\n",
      "Iteration 17, loss = 0.30755242\n",
      "Iteration 18, loss = 0.30445831\n",
      "Iteration 19, loss = 0.30600265\n",
      "Iteration 20, loss = 0.30654157\n",
      "Iteration 21, loss = 0.30294934\n",
      "Iteration 22, loss = 0.30158150\n",
      "Iteration 23, loss = 0.29349070\n",
      "Iteration 24, loss = 0.29519017\n",
      "Iteration 25, loss = 0.30990469\n",
      "Iteration 26, loss = 0.31167277\n",
      "Iteration 27, loss = 0.29097841\n",
      "Iteration 28, loss = 0.27306177\n",
      "Iteration 29, loss = 0.28176485\n",
      "Iteration 30, loss = 0.29590521\n",
      "Iteration 31, loss = 0.29634304\n",
      "Iteration 32, loss = 0.28904707\n",
      "Iteration 33, loss = 0.28681882\n",
      "Iteration 34, loss = 0.28220585\n",
      "Iteration 35, loss = 0.28032756\n",
      "Iteration 36, loss = 0.28455447\n",
      "Iteration 37, loss = 0.29264826\n",
      "Iteration 38, loss = 0.29962172\n",
      "Iteration 39, loss = 0.29636795\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.15038062\n",
      "Iteration 2, loss = 1.45927914\n",
      "Iteration 3, loss = 1.13323552\n",
      "Iteration 4, loss = 0.88933020\n",
      "Iteration 5, loss = 0.72173148\n",
      "Iteration 6, loss = 0.61490822\n",
      "Iteration 7, loss = 0.53783652\n",
      "Iteration 8, loss = 0.48081158\n",
      "Iteration 9, loss = 0.44097904\n",
      "Iteration 10, loss = 0.41159012\n",
      "Iteration 11, loss = 0.39399605\n",
      "Iteration 12, loss = 0.37330713\n",
      "Iteration 13, loss = 0.35618833\n",
      "Iteration 14, loss = 0.34052912\n",
      "Iteration 15, loss = 0.32910804\n",
      "Iteration 16, loss = 0.31744604\n",
      "Iteration 17, loss = 0.30967201\n",
      "Iteration 18, loss = 0.30379877\n",
      "Iteration 19, loss = 0.29491416\n",
      "Iteration 20, loss = 0.28760366\n",
      "Iteration 21, loss = 0.28832001\n",
      "Iteration 22, loss = 0.29186673\n",
      "Iteration 23, loss = 0.30220126\n",
      "Iteration 24, loss = 0.30103907\n",
      "Iteration 25, loss = 0.30153250\n",
      "Iteration 26, loss = 0.29860122\n",
      "Iteration 27, loss = 0.29163351\n",
      "Iteration 28, loss = 0.27965789\n",
      "Iteration 29, loss = 0.28401864\n",
      "Iteration 30, loss = 0.29275941\n",
      "Iteration 31, loss = 0.29973127\n",
      "Iteration 32, loss = 0.29285630\n",
      "Iteration 33, loss = 0.28808646\n",
      "Iteration 34, loss = 0.30093023\n",
      "Iteration 35, loss = 0.30151457\n",
      "Iteration 36, loss = 0.30358898\n",
      "Iteration 37, loss = 0.30814601\n",
      "Iteration 38, loss = 0.31297183\n",
      "Iteration 39, loss = 0.31997560\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.13371664\n",
      "Iteration 2, loss = 1.40594435\n",
      "Iteration 3, loss = 1.08834383\n",
      "Iteration 4, loss = 0.88197015\n",
      "Iteration 5, loss = 0.72622068\n",
      "Iteration 6, loss = 0.61539883\n",
      "Iteration 7, loss = 0.54449363\n",
      "Iteration 8, loss = 0.50348888\n",
      "Iteration 9, loss = 0.47095938\n",
      "Iteration 10, loss = 0.44653069\n",
      "Iteration 11, loss = 0.42151080\n",
      "Iteration 12, loss = 0.39284773\n",
      "Iteration 13, loss = 0.36885073\n",
      "Iteration 14, loss = 0.36054905\n",
      "Iteration 15, loss = 0.35382440\n",
      "Iteration 16, loss = 0.34820770\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 17, loss = 0.33249261\n",
      "Iteration 18, loss = 0.31455513\n",
      "Iteration 19, loss = 0.30815624\n",
      "Iteration 20, loss = 0.31310802\n",
      "Iteration 21, loss = 0.31398473\n",
      "Iteration 22, loss = 0.30402175\n",
      "Iteration 23, loss = 0.29538532\n",
      "Iteration 24, loss = 0.29395461\n",
      "Iteration 25, loss = 0.30075941\n",
      "Iteration 26, loss = 0.30651156\n",
      "Iteration 27, loss = 0.29991489\n",
      "Iteration 28, loss = 0.29238540\n",
      "Iteration 29, loss = 0.29103451\n",
      "Iteration 30, loss = 0.28959508\n",
      "Iteration 31, loss = 0.28838649\n",
      "Iteration 32, loss = 0.28978125\n",
      "Iteration 33, loss = 0.29244835\n",
      "Iteration 34, loss = 0.30159562\n",
      "Iteration 35, loss = 0.30108667\n",
      "Iteration 36, loss = 0.29387732\n",
      "Iteration 37, loss = 0.29264326\n",
      "Iteration 38, loss = 0.29157500\n",
      "Iteration 39, loss = 0.29066291\n",
      "Iteration 40, loss = 0.28775977\n",
      "Iteration 41, loss = 0.29004545\n",
      "Iteration 42, loss = 0.29562718\n",
      "Iteration 43, loss = 0.28892044\n",
      "Iteration 44, loss = 0.28367048\n",
      "Iteration 45, loss = 0.27599634\n",
      "Iteration 46, loss = 0.27746415\n",
      "Iteration 47, loss = 0.28210665\n",
      "Iteration 48, loss = 0.29416952\n",
      "Iteration 49, loss = 0.29595628\n",
      "Iteration 50, loss = 0.28772172\n",
      "Iteration 51, loss = 0.29121867\n",
      "Iteration 52, loss = 0.29002063\n",
      "Iteration 53, loss = 0.29002399\n",
      "Iteration 54, loss = 0.28612101\n",
      "Iteration 55, loss = 0.28229410\n",
      "Iteration 56, loss = 0.28802687\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.12992080\n",
      "Iteration 2, loss = 1.43823774\n",
      "Iteration 3, loss = 1.11950933\n",
      "Iteration 4, loss = 0.88612962\n",
      "Iteration 5, loss = 0.71346159\n",
      "Iteration 6, loss = 0.59716699\n",
      "Iteration 7, loss = 0.52808533\n",
      "Iteration 8, loss = 0.48533632\n",
      "Iteration 9, loss = 0.45243492\n",
      "Iteration 10, loss = 0.43039259\n",
      "Iteration 11, loss = 0.41137433\n",
      "Iteration 12, loss = 0.38892914\n",
      "Iteration 13, loss = 0.36615889\n",
      "Iteration 14, loss = 0.35392903\n",
      "Iteration 15, loss = 0.34109098\n",
      "Iteration 16, loss = 0.33460524\n",
      "Iteration 17, loss = 0.32720264\n",
      "Iteration 18, loss = 0.31269348\n",
      "Iteration 19, loss = 0.30225623\n",
      "Iteration 20, loss = 0.30021676\n",
      "Iteration 21, loss = 0.30329180\n",
      "Iteration 22, loss = 0.29818694\n",
      "Iteration 23, loss = 0.28796097\n",
      "Iteration 24, loss = 0.28910852\n",
      "Iteration 25, loss = 0.29963058\n",
      "Iteration 26, loss = 0.30650044\n",
      "Iteration 27, loss = 0.30293652\n",
      "Iteration 28, loss = 0.29604957\n",
      "Iteration 29, loss = 0.28700873\n",
      "Iteration 30, loss = 0.28034425\n",
      "Iteration 31, loss = 0.28407800\n",
      "Iteration 32, loss = 0.29570444\n",
      "Iteration 33, loss = 0.29720267\n",
      "Iteration 34, loss = 0.29737140\n",
      "Iteration 35, loss = 0.29044129\n",
      "Iteration 36, loss = 0.28355552\n",
      "Iteration 37, loss = 0.28222642\n",
      "Iteration 38, loss = 0.27943659\n",
      "Iteration 39, loss = 0.28246339\n",
      "Iteration 40, loss = 0.28074008\n",
      "Iteration 41, loss = 0.28173983\n",
      "Iteration 42, loss = 0.29134071\n",
      "Iteration 43, loss = 0.28739947\n",
      "Iteration 44, loss = 0.28437679\n",
      "Iteration 45, loss = 0.27823928\n",
      "Iteration 46, loss = 0.27653810\n",
      "Iteration 47, loss = 0.27285465\n",
      "Iteration 48, loss = 0.28452059\n",
      "Iteration 49, loss = 0.30128913\n",
      "Iteration 50, loss = 0.30696207\n",
      "Iteration 51, loss = 0.30278442\n",
      "Iteration 52, loss = 0.29365623\n",
      "Iteration 53, loss = 0.28690535\n",
      "Iteration 54, loss = 0.30087011\n",
      "Iteration 55, loss = 0.29690923\n",
      "Iteration 56, loss = 0.28960163\n",
      "Iteration 57, loss = 0.30125153\n",
      "Iteration 58, loss = 0.29372710\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.15172479\n",
      "Iteration 2, loss = 1.41660192\n",
      "Iteration 3, loss = 1.07272126\n",
      "Iteration 4, loss = 0.84747191\n",
      "Iteration 5, loss = 0.69588259\n",
      "Iteration 6, loss = 0.59097051\n",
      "Iteration 7, loss = 0.52216849\n",
      "Iteration 8, loss = 0.47799063\n",
      "Iteration 9, loss = 0.44421032\n",
      "Iteration 10, loss = 0.42394712\n",
      "Iteration 11, loss = 0.41118361\n",
      "Iteration 12, loss = 0.39113818\n",
      "Iteration 13, loss = 0.36750904\n",
      "Iteration 14, loss = 0.34848623\n",
      "Iteration 15, loss = 0.32819834\n",
      "Iteration 16, loss = 0.32196722\n",
      "Iteration 17, loss = 0.32256770\n",
      "Iteration 18, loss = 0.31252438\n",
      "Iteration 19, loss = 0.29564963\n",
      "Iteration 20, loss = 0.29159303\n",
      "Iteration 21, loss = 0.30073817\n",
      "Iteration 22, loss = 0.30233154\n",
      "Iteration 23, loss = 0.29213489\n",
      "Iteration 24, loss = 0.28955339\n",
      "Iteration 25, loss = 0.29720252\n",
      "Iteration 26, loss = 0.29835722\n",
      "Iteration 27, loss = 0.29990769\n",
      "Iteration 28, loss = 0.30012007\n",
      "Iteration 29, loss = 0.29409030\n",
      "Iteration 30, loss = 0.28946813\n",
      "Iteration 31, loss = 0.28827156\n",
      "Iteration 32, loss = 0.29068213\n",
      "Iteration 33, loss = 0.28459763\n",
      "Iteration 34, loss = 0.28218742\n",
      "Iteration 35, loss = 0.28331831\n",
      "Iteration 36, loss = 0.28599322\n",
      "Iteration 37, loss = 0.28826273\n",
      "Iteration 38, loss = 0.28569988\n",
      "Iteration 39, loss = 0.28542973\n",
      "Iteration 40, loss = 0.28885125\n",
      "Iteration 41, loss = 0.28721725\n",
      "Iteration 42, loss = 0.28599078\n",
      "Iteration 43, loss = 0.27931129\n",
      "Iteration 44, loss = 0.27930564\n",
      "Iteration 45, loss = 0.27572665\n",
      "Iteration 46, loss = 0.27458186\n",
      "Iteration 47, loss = 0.27190381\n",
      "Iteration 48, loss = 0.27948446\n",
      "Iteration 49, loss = 0.28907375\n",
      "Iteration 50, loss = 0.29141848\n",
      "Iteration 51, loss = 0.28904587\n",
      "Iteration 52, loss = 0.28559101\n",
      "Iteration 53, loss = 0.27972083\n",
      "Iteration 54, loss = 0.28138032\n",
      "Iteration 55, loss = 0.27687780\n",
      "Iteration 56, loss = 0.27670797\n",
      "Iteration 57, loss = 0.29689530\n",
      "Iteration 58, loss = 0.28639020\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.13955655\n",
      "Iteration 2, loss = 1.42492730\n",
      "Iteration 3, loss = 1.08334226\n",
      "Iteration 4, loss = 0.85568974\n",
      "Iteration 5, loss = 0.70587944\n",
      "Iteration 6, loss = 0.60019272\n",
      "Iteration 7, loss = 0.52811478\n",
      "Iteration 8, loss = 0.48212954\n",
      "Iteration 9, loss = 0.44911492\n",
      "Iteration 10, loss = 0.42676917\n",
      "Iteration 11, loss = 0.40976662\n",
      "Iteration 12, loss = 0.38570422\n",
      "Iteration 13, loss = 0.36233930\n",
      "Iteration 14, loss = 0.34888333\n",
      "Iteration 15, loss = 0.33269031\n",
      "Iteration 16, loss = 0.32193261\n",
      "Iteration 17, loss = 0.31702260\n",
      "Iteration 18, loss = 0.31071831\n",
      "Iteration 19, loss = 0.30303012\n",
      "Iteration 20, loss = 0.29675623\n",
      "Iteration 21, loss = 0.29616987\n",
      "Iteration 22, loss = 0.29414565\n",
      "Iteration 23, loss = 0.29262947\n",
      "Iteration 24, loss = 0.29074879\n",
      "Iteration 25, loss = 0.29036765\n",
      "Iteration 26, loss = 0.28987145\n",
      "Iteration 27, loss = 0.29356652\n",
      "Iteration 28, loss = 0.29221467\n",
      "Iteration 29, loss = 0.28725483\n",
      "Iteration 30, loss = 0.28903501\n",
      "Iteration 31, loss = 0.29324359\n",
      "Iteration 32, loss = 0.28627179\n",
      "Iteration 33, loss = 0.27818280\n",
      "Iteration 34, loss = 0.28191160\n",
      "Iteration 35, loss = 0.28542373\n",
      "Iteration 36, loss = 0.28611031\n",
      "Iteration 37, loss = 0.29344078\n",
      "Iteration 38, loss = 0.28899938\n",
      "Iteration 39, loss = 0.28279251\n",
      "Iteration 40, loss = 0.28194440\n",
      "Iteration 41, loss = 0.28280196\n",
      "Iteration 42, loss = 0.28503574\n",
      "Iteration 43, loss = 0.27912769\n",
      "Iteration 44, loss = 0.27754473\n",
      "Iteration 45, loss = 0.27454176\n",
      "Iteration 46, loss = 0.27744720\n",
      "Iteration 47, loss = 0.27740127\n",
      "Iteration 48, loss = 0.28269645\n",
      "Iteration 49, loss = 0.27907419\n",
      "Iteration 50, loss = 0.27460421\n",
      "Iteration 51, loss = 0.28984888\n",
      "Iteration 52, loss = 0.28342763\n",
      "Iteration 53, loss = 0.27431742\n",
      "Iteration 54, loss = 0.28253532\n",
      "Iteration 55, loss = 0.29475204\n",
      "Iteration 56, loss = 0.30226423\n",
      "Iteration 57, loss = 0.30876952\n",
      "Iteration 58, loss = 0.28975524\n",
      "Iteration 59, loss = 0.28132746\n",
      "Iteration 60, loss = 0.30490428\n",
      "Iteration 61, loss = 0.31031874\n",
      "Iteration 62, loss = 0.29046039\n",
      "Iteration 63, loss = 0.28694435\n",
      "Iteration 64, loss = 0.29154925\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.13951492\n",
      "Iteration 2, loss = 1.42040609\n",
      "Iteration 3, loss = 1.07908743\n",
      "Iteration 4, loss = 0.84660298\n",
      "Iteration 5, loss = 0.69129633\n",
      "Iteration 6, loss = 0.58244950\n",
      "Iteration 7, loss = 0.51070643\n",
      "Iteration 8, loss = 0.46519940\n",
      "Iteration 9, loss = 0.43440797\n",
      "Iteration 10, loss = 0.41352477\n",
      "Iteration 11, loss = 0.39539737\n",
      "Iteration 12, loss = 0.37200241\n",
      "Iteration 13, loss = 0.35368666\n",
      "Iteration 14, loss = 0.33873408\n",
      "Iteration 15, loss = 0.32566376\n",
      "Iteration 16, loss = 0.31877994\n",
      "Iteration 17, loss = 0.31488869\n",
      "Iteration 18, loss = 0.30729104\n",
      "Iteration 19, loss = 0.29795637\n",
      "Iteration 20, loss = 0.28697290\n",
      "Iteration 21, loss = 0.28072178\n",
      "Iteration 22, loss = 0.28007353\n",
      "Iteration 23, loss = 0.28294305\n",
      "Iteration 24, loss = 0.28005866\n",
      "Iteration 25, loss = 0.27957632\n",
      "Iteration 26, loss = 0.28061701\n",
      "Iteration 27, loss = 0.28822049\n",
      "Iteration 28, loss = 0.29173799\n",
      "Iteration 29, loss = 0.28806023\n",
      "Iteration 30, loss = 0.28587653\n",
      "Iteration 31, loss = 0.28215102\n",
      "Iteration 32, loss = 0.27654644\n",
      "Iteration 33, loss = 0.27151294\n",
      "Iteration 34, loss = 0.27699920\n",
      "Iteration 35, loss = 0.28180692\n",
      "Iteration 36, loss = 0.28040684\n",
      "Iteration 37, loss = 0.28898437\n",
      "Iteration 38, loss = 0.29221324\n",
      "Iteration 39, loss = 0.28533395\n",
      "Iteration 40, loss = 0.27916986\n",
      "Iteration 41, loss = 0.28118452\n",
      "Iteration 42, loss = 0.28344824\n",
      "Iteration 43, loss = 0.28034141\n",
      "Iteration 44, loss = 0.27982458\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.14725917\n",
      "Iteration 2, loss = 1.44253855\n",
      "Iteration 3, loss = 1.10836825\n",
      "Iteration 4, loss = 0.86803216\n",
      "Iteration 5, loss = 0.70504442\n",
      "Iteration 6, loss = 0.59399572\n",
      "Iteration 7, loss = 0.52457126\n",
      "Iteration 8, loss = 0.47797710\n",
      "Iteration 9, loss = 0.44090296\n",
      "Iteration 10, loss = 0.41343722\n",
      "Iteration 11, loss = 0.39502743\n",
      "Iteration 12, loss = 0.37065450\n",
      "Iteration 13, loss = 0.35111882\n",
      "Iteration 14, loss = 0.33979155\n",
      "Iteration 15, loss = 0.32574602\n",
      "Iteration 16, loss = 0.31594335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 17, loss = 0.30813813\n",
      "Iteration 18, loss = 0.30477257\n",
      "Iteration 19, loss = 0.30191022\n",
      "Iteration 20, loss = 0.29121501\n",
      "Iteration 21, loss = 0.28511321\n",
      "Iteration 22, loss = 0.28664886\n",
      "Iteration 23, loss = 0.28839817\n",
      "Iteration 24, loss = 0.28556944\n",
      "Iteration 25, loss = 0.28928193\n",
      "Iteration 26, loss = 0.29405590\n",
      "Iteration 27, loss = 0.29649425\n",
      "Iteration 28, loss = 0.29349673\n",
      "Iteration 29, loss = 0.28720913\n",
      "Iteration 30, loss = 0.28404666\n",
      "Iteration 31, loss = 0.27927927\n",
      "Iteration 32, loss = 0.27881847\n",
      "Iteration 33, loss = 0.27752939\n",
      "Iteration 34, loss = 0.28057581\n",
      "Iteration 35, loss = 0.27981518\n",
      "Iteration 36, loss = 0.28324885\n",
      "Iteration 37, loss = 0.29890551\n",
      "Iteration 38, loss = 0.30674774\n",
      "Iteration 39, loss = 0.29948778\n",
      "Iteration 40, loss = 0.28739490\n",
      "Iteration 41, loss = 0.28450210\n",
      "Iteration 42, loss = 0.28874325\n",
      "Iteration 43, loss = 0.28767418\n",
      "Iteration 44, loss = 0.28492734\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.14308112\n",
      "Iteration 2, loss = 1.46158781\n",
      "Iteration 3, loss = 1.12349481\n",
      "Iteration 4, loss = 0.88030055\n",
      "Iteration 5, loss = 0.71774606\n",
      "Iteration 6, loss = 0.60717842\n",
      "Iteration 7, loss = 0.53765099\n",
      "Iteration 8, loss = 0.49420851\n",
      "Iteration 9, loss = 0.45825423\n",
      "Iteration 10, loss = 0.42712704\n",
      "Iteration 11, loss = 0.40366261\n",
      "Iteration 12, loss = 0.37793030\n",
      "Iteration 13, loss = 0.36048683\n",
      "Iteration 14, loss = 0.35238767\n",
      "Iteration 15, loss = 0.33938403\n",
      "Iteration 16, loss = 0.32760221\n",
      "Iteration 17, loss = 0.31606289\n",
      "Iteration 18, loss = 0.31404236\n",
      "Iteration 19, loss = 0.31538518\n",
      "Iteration 20, loss = 0.30859177\n",
      "Iteration 21, loss = 0.30230460\n",
      "Iteration 22, loss = 0.30034452\n",
      "Iteration 23, loss = 0.30066179\n",
      "Iteration 24, loss = 0.29705936\n",
      "Iteration 25, loss = 0.30143562\n",
      "Iteration 26, loss = 0.30668590\n",
      "Iteration 27, loss = 0.30899421\n",
      "Iteration 28, loss = 0.30491731\n",
      "Iteration 29, loss = 0.29622888\n",
      "Iteration 30, loss = 0.29281352\n",
      "Iteration 31, loss = 0.28722766\n",
      "Iteration 32, loss = 0.28693959\n",
      "Iteration 33, loss = 0.28620975\n",
      "Iteration 34, loss = 0.29109140\n",
      "Iteration 35, loss = 0.29409982\n",
      "Iteration 36, loss = 0.30013908\n",
      "Iteration 37, loss = 0.30669677\n",
      "Iteration 38, loss = 0.30562044\n",
      "Iteration 39, loss = 0.29923103\n",
      "Iteration 40, loss = 0.29602378\n",
      "Iteration 41, loss = 0.29289571\n",
      "Iteration 42, loss = 0.29212078\n",
      "Iteration 43, loss = 0.29069977\n",
      "Iteration 44, loss = 0.29500191\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.14676380\n",
      "Iteration 2, loss = 1.44657262\n",
      "Iteration 3, loss = 1.09829125\n",
      "Iteration 4, loss = 0.86082024\n",
      "Iteration 5, loss = 0.70335122\n",
      "Iteration 6, loss = 0.59440356\n",
      "Iteration 7, loss = 0.52522328\n",
      "Iteration 8, loss = 0.48268315\n",
      "Iteration 9, loss = 0.44804788\n",
      "Iteration 10, loss = 0.41970300\n",
      "Iteration 11, loss = 0.39822684\n",
      "Iteration 12, loss = 0.37668945\n",
      "Iteration 13, loss = 0.35797848\n",
      "Iteration 14, loss = 0.34871133\n",
      "Iteration 15, loss = 0.33602946\n",
      "Iteration 16, loss = 0.32425917\n",
      "Iteration 17, loss = 0.31255708\n",
      "Iteration 18, loss = 0.31289235\n",
      "Iteration 19, loss = 0.31640463\n",
      "Iteration 20, loss = 0.31152089\n",
      "Iteration 21, loss = 0.30287903\n",
      "Iteration 22, loss = 0.29773772\n",
      "Iteration 23, loss = 0.29767998\n",
      "Iteration 24, loss = 0.29665119\n",
      "Iteration 25, loss = 0.30144079\n",
      "Iteration 26, loss = 0.30740023\n",
      "Iteration 27, loss = 0.30885777\n",
      "Iteration 28, loss = 0.30359917\n",
      "Iteration 29, loss = 0.29418999\n",
      "Iteration 30, loss = 0.29090490\n",
      "Iteration 31, loss = 0.28564511\n",
      "Iteration 32, loss = 0.28221847\n",
      "Iteration 33, loss = 0.28247655\n",
      "Iteration 34, loss = 0.28760190\n",
      "Iteration 35, loss = 0.28891431\n",
      "Iteration 36, loss = 0.29177694\n",
      "Iteration 37, loss = 0.29923497\n",
      "Iteration 38, loss = 0.30163787\n",
      "Iteration 39, loss = 0.29563634\n",
      "Iteration 40, loss = 0.29135243\n",
      "Iteration 41, loss = 0.28947922\n",
      "Iteration 42, loss = 0.28720939\n",
      "Iteration 43, loss = 0.28137800\n",
      "Iteration 44, loss = 0.28440750\n",
      "Iteration 45, loss = 0.29434858\n",
      "Iteration 46, loss = 0.29338888\n",
      "Iteration 47, loss = 0.28623053\n",
      "Iteration 48, loss = 0.28457551\n",
      "Iteration 49, loss = 0.28674621\n",
      "Iteration 50, loss = 0.28594742\n",
      "Iteration 51, loss = 0.29063446\n",
      "Iteration 52, loss = 0.29182737\n",
      "Iteration 53, loss = 0.28939627\n",
      "Iteration 54, loss = 0.28977847\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.14245537\n",
      "Iteration 2, loss = 1.42501329\n",
      "Iteration 3, loss = 1.08137704\n",
      "Iteration 4, loss = 0.84242947\n",
      "Iteration 5, loss = 0.68264918\n",
      "Iteration 6, loss = 0.57330747\n",
      "Iteration 7, loss = 0.50908449\n",
      "Iteration 8, loss = 0.47334551\n",
      "Iteration 9, loss = 0.43825950\n",
      "Iteration 10, loss = 0.40533101\n",
      "Iteration 11, loss = 0.38148321\n",
      "Iteration 12, loss = 0.35617648\n",
      "Iteration 13, loss = 0.33953390\n",
      "Iteration 14, loss = 0.33263365\n",
      "Iteration 15, loss = 0.31979592\n",
      "Iteration 16, loss = 0.31298481\n",
      "Iteration 17, loss = 0.30385527\n",
      "Iteration 18, loss = 0.30156835\n",
      "Iteration 19, loss = 0.30364878\n",
      "Iteration 20, loss = 0.29786524\n",
      "Iteration 21, loss = 0.28960076\n",
      "Iteration 22, loss = 0.28932659\n",
      "Iteration 23, loss = 0.29938843\n",
      "Iteration 24, loss = 0.29853374\n",
      "Iteration 25, loss = 0.29737400\n",
      "Iteration 26, loss = 0.29849680\n",
      "Iteration 27, loss = 0.29800933\n",
      "Iteration 28, loss = 0.29341254\n",
      "Iteration 29, loss = 0.28443970\n",
      "Iteration 30, loss = 0.27737794\n",
      "Iteration 31, loss = 0.27476557\n",
      "Iteration 32, loss = 0.27713949\n",
      "Iteration 33, loss = 0.27747300\n",
      "Iteration 34, loss = 0.28224731\n",
      "Iteration 35, loss = 0.28136592\n",
      "Iteration 36, loss = 0.28273162\n",
      "Iteration 37, loss = 0.28542313\n",
      "Iteration 38, loss = 0.28816350\n",
      "Iteration 39, loss = 0.28705933\n",
      "Iteration 40, loss = 0.28593555\n",
      "Iteration 41, loss = 0.28257491\n",
      "Iteration 42, loss = 0.27984794\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.13747442\n",
      "Iteration 2, loss = 1.43901001\n",
      "Iteration 3, loss = 1.09772520\n",
      "Iteration 4, loss = 0.83241762\n",
      "Iteration 5, loss = 0.66025313\n",
      "Iteration 6, loss = 0.55961144\n",
      "Iteration 7, loss = 0.49921464\n",
      "Iteration 8, loss = 0.45779276\n",
      "Iteration 9, loss = 0.43151903\n",
      "Iteration 10, loss = 0.40145325\n",
      "Iteration 11, loss = 0.36989887\n",
      "Iteration 12, loss = 0.35034396\n",
      "Iteration 13, loss = 0.33532747\n",
      "Iteration 14, loss = 0.32450894\n",
      "Iteration 15, loss = 0.32301144\n",
      "Iteration 16, loss = 0.31807490\n",
      "Iteration 17, loss = 0.30816821\n",
      "Iteration 18, loss = 0.30496897\n",
      "Iteration 19, loss = 0.30571092\n",
      "Iteration 20, loss = 0.29860901\n",
      "Iteration 21, loss = 0.29443378\n",
      "Iteration 22, loss = 0.30089996\n",
      "Iteration 23, loss = 0.30894684\n",
      "Iteration 24, loss = 0.30148674\n",
      "Iteration 25, loss = 0.29404337\n",
      "Iteration 26, loss = 0.29286922\n",
      "Iteration 27, loss = 0.29576461\n",
      "Iteration 28, loss = 0.30397622\n",
      "Iteration 29, loss = 0.29796795\n",
      "Iteration 30, loss = 0.29311423\n",
      "Iteration 31, loss = 0.30789089\n",
      "Iteration 32, loss = 0.30474158\n",
      "Iteration 33, loss = 0.29717724\n",
      "Iteration 34, loss = 0.30946773\n",
      "Iteration 35, loss = 0.30044435\n",
      "Iteration 36, loss = 0.28686088\n",
      "Iteration 37, loss = 0.28151603\n",
      "Iteration 38, loss = 0.28756067\n",
      "Iteration 39, loss = 0.29815895\n",
      "Iteration 40, loss = 0.30914479\n",
      "Iteration 41, loss = 0.31593403\n",
      "Iteration 42, loss = 0.29671507\n",
      "Iteration 43, loss = 0.28274954\n",
      "Iteration 44, loss = 0.29114340\n",
      "Iteration 45, loss = 0.30167830\n",
      "Iteration 46, loss = 0.30423072\n",
      "Iteration 47, loss = 0.29878374\n",
      "Iteration 48, loss = 0.28745587\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.12148056\n",
      "Iteration 2, loss = 1.44128442\n",
      "Iteration 3, loss = 1.12787542\n",
      "Iteration 4, loss = 0.85211559\n",
      "Iteration 5, loss = 0.68136615\n",
      "Iteration 6, loss = 0.60607275\n",
      "Iteration 7, loss = 0.54837857\n",
      "Iteration 8, loss = 0.49469351\n",
      "Iteration 9, loss = 0.44574401\n",
      "Iteration 10, loss = 0.41348446\n",
      "Iteration 11, loss = 0.38746042\n",
      "Iteration 12, loss = 0.36586912\n",
      "Iteration 13, loss = 0.34703190\n",
      "Iteration 14, loss = 0.33019945\n",
      "Iteration 15, loss = 0.32284333\n",
      "Iteration 16, loss = 0.31777750\n",
      "Iteration 17, loss = 0.31000735\n",
      "Iteration 18, loss = 0.30727442\n",
      "Iteration 19, loss = 0.30778097\n",
      "Iteration 20, loss = 0.30468466\n",
      "Iteration 21, loss = 0.30250606\n",
      "Iteration 22, loss = 0.29615982\n",
      "Iteration 23, loss = 0.28855624\n",
      "Iteration 24, loss = 0.28668440\n",
      "Iteration 25, loss = 0.29158227\n",
      "Iteration 26, loss = 0.29921542\n",
      "Iteration 27, loss = 0.29865993\n",
      "Iteration 28, loss = 0.29647179\n",
      "Iteration 29, loss = 0.29198773\n",
      "Iteration 30, loss = 0.28236094\n",
      "Iteration 31, loss = 0.27251501\n",
      "Iteration 32, loss = 0.29081813\n",
      "Iteration 33, loss = 0.29991147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 34, loss = 0.30003303\n",
      "Iteration 35, loss = 0.29142585\n",
      "Iteration 36, loss = 0.28477165\n",
      "Iteration 37, loss = 0.28375256\n",
      "Iteration 38, loss = 0.28550534\n",
      "Iteration 39, loss = 0.27833372\n",
      "Iteration 40, loss = 0.29090582\n",
      "Iteration 41, loss = 0.30273469\n",
      "Iteration 42, loss = 0.29635086\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.13320649\n",
      "Iteration 2, loss = 1.43664663\n",
      "Iteration 3, loss = 1.11070934\n",
      "Iteration 4, loss = 0.83292046\n",
      "Iteration 5, loss = 0.65462620\n",
      "Iteration 6, loss = 0.56682131\n",
      "Iteration 7, loss = 0.50638502\n",
      "Iteration 8, loss = 0.46106801\n",
      "Iteration 9, loss = 0.42166494\n",
      "Iteration 10, loss = 0.39374053\n",
      "Iteration 11, loss = 0.36812210\n",
      "Iteration 12, loss = 0.34773007\n",
      "Iteration 13, loss = 0.33479792\n",
      "Iteration 14, loss = 0.32514111\n",
      "Iteration 15, loss = 0.31636249\n",
      "Iteration 16, loss = 0.31203232\n",
      "Iteration 17, loss = 0.30366399\n",
      "Iteration 18, loss = 0.30122887\n",
      "Iteration 19, loss = 0.30345225\n",
      "Iteration 20, loss = 0.30600755\n",
      "Iteration 21, loss = 0.30852598\n",
      "Iteration 22, loss = 0.29896321\n",
      "Iteration 23, loss = 0.28722423\n",
      "Iteration 24, loss = 0.27860375\n",
      "Iteration 25, loss = 0.27874153\n",
      "Iteration 26, loss = 0.28962908\n",
      "Iteration 27, loss = 0.29548636\n",
      "Iteration 28, loss = 0.29227828\n",
      "Iteration 29, loss = 0.28416045\n",
      "Iteration 30, loss = 0.27411010\n",
      "Iteration 31, loss = 0.27175468\n",
      "Iteration 32, loss = 0.28077316\n",
      "Iteration 33, loss = 0.28411553\n",
      "Iteration 34, loss = 0.28421282\n",
      "Iteration 35, loss = 0.27973820\n",
      "Iteration 36, loss = 0.27643835\n",
      "Iteration 37, loss = 0.27956054\n",
      "Iteration 38, loss = 0.28520785\n",
      "Iteration 39, loss = 0.27981251\n",
      "Iteration 40, loss = 0.29054793\n",
      "Iteration 41, loss = 0.29963232\n",
      "Iteration 42, loss = 0.29280194\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.10964092\n",
      "Iteration 2, loss = 1.44075308\n",
      "Iteration 3, loss = 1.15217402\n",
      "Iteration 4, loss = 0.87284572\n",
      "Iteration 5, loss = 0.68556283\n",
      "Iteration 6, loss = 0.60153732\n",
      "Iteration 7, loss = 0.55021564\n",
      "Iteration 8, loss = 0.50060323\n",
      "Iteration 9, loss = 0.44616173\n",
      "Iteration 10, loss = 0.40623932\n",
      "Iteration 11, loss = 0.37848592\n",
      "Iteration 12, loss = 0.35863334\n",
      "Iteration 13, loss = 0.34501109\n",
      "Iteration 14, loss = 0.33239768\n",
      "Iteration 15, loss = 0.32105261\n",
      "Iteration 16, loss = 0.31331611\n",
      "Iteration 17, loss = 0.30297533\n",
      "Iteration 18, loss = 0.30152812\n",
      "Iteration 19, loss = 0.30368918\n",
      "Iteration 20, loss = 0.30203413\n",
      "Iteration 21, loss = 0.29993087\n",
      "Iteration 22, loss = 0.28995116\n",
      "Iteration 23, loss = 0.27995143\n",
      "Iteration 24, loss = 0.27953196\n",
      "Iteration 25, loss = 0.27418737\n",
      "Iteration 26, loss = 0.28311013\n",
      "Iteration 27, loss = 0.29717565\n",
      "Iteration 28, loss = 0.29841227\n",
      "Iteration 29, loss = 0.28707826\n",
      "Iteration 30, loss = 0.27419075\n",
      "Iteration 31, loss = 0.26516575\n",
      "Iteration 32, loss = 0.27051165\n",
      "Iteration 33, loss = 0.27266374\n",
      "Iteration 34, loss = 0.27637282\n",
      "Iteration 35, loss = 0.28402319\n",
      "Iteration 36, loss = 0.28369282\n",
      "Iteration 37, loss = 0.28444239\n",
      "Iteration 38, loss = 0.27616265\n",
      "Iteration 39, loss = 0.27166883\n",
      "Iteration 40, loss = 0.29077518\n",
      "Iteration 41, loss = 0.30632816\n",
      "Iteration 42, loss = 0.29736952\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.11173550\n",
      "Iteration 2, loss = 1.44261930\n",
      "Iteration 3, loss = 1.14847815\n",
      "Iteration 4, loss = 0.87722143\n",
      "Iteration 5, loss = 0.69109956\n",
      "Iteration 6, loss = 0.60742342\n",
      "Iteration 7, loss = 0.54957915\n",
      "Iteration 8, loss = 0.49466468\n",
      "Iteration 9, loss = 0.43974662\n",
      "Iteration 10, loss = 0.40341428\n",
      "Iteration 11, loss = 0.37790917\n",
      "Iteration 12, loss = 0.35904125\n",
      "Iteration 13, loss = 0.34371547\n",
      "Iteration 14, loss = 0.32963609\n",
      "Iteration 15, loss = 0.32014995\n",
      "Iteration 16, loss = 0.31703652\n",
      "Iteration 17, loss = 0.30934796\n",
      "Iteration 18, loss = 0.30401482\n",
      "Iteration 19, loss = 0.29807721\n",
      "Iteration 20, loss = 0.29645713\n",
      "Iteration 21, loss = 0.30185022\n",
      "Iteration 22, loss = 0.29680700\n",
      "Iteration 23, loss = 0.28573770\n",
      "Iteration 24, loss = 0.28035187\n",
      "Iteration 25, loss = 0.27711246\n",
      "Iteration 26, loss = 0.28485967\n",
      "Iteration 27, loss = 0.29496837\n",
      "Iteration 28, loss = 0.29857764\n",
      "Iteration 29, loss = 0.29225001\n",
      "Iteration 30, loss = 0.28437234\n",
      "Iteration 31, loss = 0.27369957\n",
      "Iteration 32, loss = 0.27882370\n",
      "Iteration 33, loss = 0.28752398\n",
      "Iteration 34, loss = 0.29615714\n",
      "Iteration 35, loss = 0.29506368\n",
      "Iteration 36, loss = 0.28472409\n",
      "Iteration 37, loss = 0.27642784\n",
      "Iteration 38, loss = 0.27078382\n",
      "Iteration 39, loss = 0.27473423\n",
      "Iteration 40, loss = 0.30053253\n",
      "Iteration 41, loss = 0.31526692\n",
      "Iteration 42, loss = 0.30597276\n",
      "Iteration 43, loss = 0.28934536\n",
      "Iteration 44, loss = 0.28211028\n",
      "Iteration 45, loss = 0.27942269\n",
      "Iteration 46, loss = 0.28390532\n",
      "Iteration 47, loss = 0.28934393\n",
      "Iteration 48, loss = 0.28206739\n",
      "Iteration 49, loss = 0.27788060\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.13124074\n",
      "Iteration 2, loss = 1.44173843\n",
      "Iteration 3, loss = 1.14855207\n",
      "Iteration 4, loss = 0.88585728\n",
      "Iteration 5, loss = 0.69436217\n",
      "Iteration 6, loss = 0.59415183\n",
      "Iteration 7, loss = 0.53927302\n",
      "Iteration 8, loss = 0.49201041\n",
      "Iteration 9, loss = 0.43342698\n",
      "Iteration 10, loss = 0.39256177\n",
      "Iteration 11, loss = 0.36681900\n",
      "Iteration 12, loss = 0.35141620\n",
      "Iteration 13, loss = 0.33917580\n",
      "Iteration 14, loss = 0.32952321\n",
      "Iteration 15, loss = 0.32378435\n",
      "Iteration 16, loss = 0.31679683\n",
      "Iteration 17, loss = 0.30625600\n",
      "Iteration 18, loss = 0.29836738\n",
      "Iteration 19, loss = 0.29197668\n",
      "Iteration 20, loss = 0.29124934\n",
      "Iteration 21, loss = 0.29373175\n",
      "Iteration 22, loss = 0.28683158\n",
      "Iteration 23, loss = 0.27833967\n",
      "Iteration 24, loss = 0.27752100\n",
      "Iteration 25, loss = 0.27579908\n",
      "Iteration 26, loss = 0.28426801\n",
      "Iteration 27, loss = 0.29271333\n",
      "Iteration 28, loss = 0.29539903\n",
      "Iteration 29, loss = 0.29335765\n",
      "Iteration 30, loss = 0.28788923\n",
      "Iteration 31, loss = 0.27806215\n",
      "Iteration 32, loss = 0.27193781\n",
      "Iteration 33, loss = 0.28349418\n",
      "Iteration 34, loss = 0.28994884\n",
      "Iteration 35, loss = 0.28378702\n",
      "Iteration 36, loss = 0.27483460\n",
      "Iteration 37, loss = 0.27357880\n",
      "Iteration 38, loss = 0.27862538\n",
      "Iteration 39, loss = 0.28542290\n",
      "Iteration 40, loss = 0.29667832\n",
      "Iteration 41, loss = 0.29394103\n",
      "Iteration 42, loss = 0.28607994\n",
      "Iteration 43, loss = 0.28604118\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.12397815\n",
      "Iteration 2, loss = 1.45327513\n",
      "Iteration 3, loss = 1.15636099\n",
      "Iteration 4, loss = 0.89173997\n",
      "Iteration 5, loss = 0.70236911\n",
      "Iteration 6, loss = 0.59778744\n",
      "Iteration 7, loss = 0.53041165\n",
      "Iteration 8, loss = 0.47463039\n",
      "Iteration 9, loss = 0.42244318\n",
      "Iteration 10, loss = 0.38909286\n",
      "Iteration 11, loss = 0.36886324\n",
      "Iteration 12, loss = 0.34728077\n",
      "Iteration 13, loss = 0.33292202\n",
      "Iteration 14, loss = 0.32474966\n",
      "Iteration 15, loss = 0.32507859\n",
      "Iteration 16, loss = 0.31793062\n",
      "Iteration 17, loss = 0.30712309\n",
      "Iteration 18, loss = 0.29494199\n",
      "Iteration 19, loss = 0.28860151\n",
      "Iteration 20, loss = 0.29013674\n",
      "Iteration 21, loss = 0.29417773\n",
      "Iteration 22, loss = 0.28535545\n",
      "Iteration 23, loss = 0.27517690\n",
      "Iteration 24, loss = 0.27483646\n",
      "Iteration 25, loss = 0.27664549\n",
      "Iteration 26, loss = 0.28379424\n",
      "Iteration 27, loss = 0.29121374\n",
      "Iteration 28, loss = 0.29547241\n",
      "Iteration 29, loss = 0.29048778\n",
      "Iteration 30, loss = 0.28134661\n",
      "Iteration 31, loss = 0.27229673\n",
      "Iteration 32, loss = 0.27166219\n",
      "Iteration 33, loss = 0.27605558\n",
      "Iteration 34, loss = 0.27984967\n",
      "Iteration 35, loss = 0.27757229\n",
      "Iteration 36, loss = 0.26856249\n",
      "Iteration 37, loss = 0.26446380\n",
      "Iteration 38, loss = 0.27106319\n",
      "Iteration 39, loss = 0.27483900\n",
      "Iteration 40, loss = 0.27989763\n",
      "Iteration 41, loss = 0.28294113\n",
      "Iteration 42, loss = 0.28235896\n",
      "Iteration 43, loss = 0.28221227\n",
      "Iteration 44, loss = 0.27988340\n",
      "Iteration 45, loss = 0.27272146\n",
      "Iteration 46, loss = 0.27382182\n",
      "Iteration 47, loss = 0.27936780\n",
      "Iteration 48, loss = 0.27961292\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.12112825\n",
      "Iteration 2, loss = 1.45913566\n",
      "Iteration 3, loss = 1.16700572\n",
      "Iteration 4, loss = 0.89793924\n",
      "Iteration 5, loss = 0.70699495\n",
      "Iteration 6, loss = 0.60249253\n",
      "Iteration 7, loss = 0.53440947\n",
      "Iteration 8, loss = 0.47814317\n",
      "Iteration 9, loss = 0.42738668\n",
      "Iteration 10, loss = 0.39274109\n",
      "Iteration 11, loss = 0.37342014\n",
      "Iteration 12, loss = 0.35247902\n",
      "Iteration 13, loss = 0.33862088\n",
      "Iteration 14, loss = 0.33029638\n",
      "Iteration 15, loss = 0.33011910\n",
      "Iteration 16, loss = 0.32709714\n",
      "Iteration 17, loss = 0.32349160\n",
      "Iteration 18, loss = 0.30691337\n",
      "Iteration 19, loss = 0.29860408\n",
      "Iteration 20, loss = 0.30500408\n",
      "Iteration 21, loss = 0.30919440\n",
      "Iteration 22, loss = 0.29891121\n",
      "Iteration 23, loss = 0.28517801\n",
      "Iteration 24, loss = 0.27935842\n",
      "Iteration 25, loss = 0.27972851\n",
      "Iteration 26, loss = 0.28768643\n",
      "Iteration 27, loss = 0.29540709\n",
      "Iteration 28, loss = 0.29970609\n",
      "Iteration 29, loss = 0.29849721\n",
      "Iteration 30, loss = 0.29286614\n",
      "Iteration 31, loss = 0.28450293\n",
      "Iteration 32, loss = 0.28023320\n",
      "Iteration 33, loss = 0.28458724\n",
      "Iteration 34, loss = 0.29048541\n",
      "Iteration 35, loss = 0.29198917\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.11592199\n",
      "Iteration 2, loss = 1.44050718\n",
      "Iteration 3, loss = 1.14375914\n",
      "Iteration 4, loss = 0.86861474\n",
      "Iteration 5, loss = 0.68190159\n",
      "Iteration 6, loss = 0.58270936\n",
      "Iteration 7, loss = 0.51858707\n",
      "Iteration 8, loss = 0.46501740\n",
      "Iteration 9, loss = 0.41943685\n",
      "Iteration 10, loss = 0.38519461\n",
      "Iteration 11, loss = 0.36510171\n",
      "Iteration 12, loss = 0.34470965\n",
      "Iteration 13, loss = 0.33279584\n",
      "Iteration 14, loss = 0.32503872\n",
      "Iteration 15, loss = 0.32319133\n",
      "Iteration 16, loss = 0.32185155\n",
      "Iteration 17, loss = 0.31908033\n",
      "Iteration 18, loss = 0.30301014\n",
      "Iteration 19, loss = 0.29348107\n",
      "Iteration 20, loss = 0.29702828\n",
      "Iteration 21, loss = 0.30068606\n",
      "Iteration 22, loss = 0.29377286\n",
      "Iteration 23, loss = 0.28292440\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 24, loss = 0.27685131\n",
      "Iteration 25, loss = 0.27516569\n",
      "Iteration 26, loss = 0.28017160\n",
      "Iteration 27, loss = 0.28992842\n",
      "Iteration 28, loss = 0.29480041\n",
      "Iteration 29, loss = 0.29522786\n",
      "Iteration 30, loss = 0.29170916\n",
      "Iteration 31, loss = 0.28263128\n",
      "Iteration 32, loss = 0.27678522\n",
      "Iteration 33, loss = 0.27709879\n",
      "Iteration 34, loss = 0.28242328\n",
      "Iteration 35, loss = 0.28343209\n",
      "Iteration 36, loss = 0.28022799\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.12753305\n",
      "Iteration 2, loss = 1.43639027\n",
      "Iteration 3, loss = 1.12748491\n",
      "Iteration 4, loss = 0.86187936\n",
      "Iteration 5, loss = 0.67649636\n",
      "Iteration 6, loss = 0.57217780\n",
      "Iteration 7, loss = 0.50754230\n",
      "Iteration 8, loss = 0.45559953\n",
      "Iteration 9, loss = 0.40945924\n",
      "Iteration 10, loss = 0.37485132\n",
      "Iteration 11, loss = 0.35467252\n",
      "Iteration 12, loss = 0.33463223\n",
      "Iteration 13, loss = 0.32105067\n",
      "Iteration 14, loss = 0.31363023\n",
      "Iteration 15, loss = 0.31017348\n",
      "Iteration 16, loss = 0.30789908\n",
      "Iteration 17, loss = 0.30731002\n",
      "Iteration 18, loss = 0.29747587\n",
      "Iteration 19, loss = 0.29348812\n",
      "Iteration 20, loss = 0.29754882\n",
      "Iteration 21, loss = 0.29593572\n",
      "Iteration 22, loss = 0.28721044\n",
      "Iteration 23, loss = 0.27740202\n",
      "Iteration 24, loss = 0.27323625\n",
      "Iteration 25, loss = 0.27018931\n",
      "Iteration 26, loss = 0.27286574\n",
      "Iteration 27, loss = 0.27848961\n",
      "Iteration 28, loss = 0.28432847\n",
      "Iteration 29, loss = 0.28758143\n",
      "Iteration 30, loss = 0.28805930\n",
      "Iteration 31, loss = 0.28394592\n",
      "Iteration 32, loss = 0.27327485\n",
      "Iteration 33, loss = 0.27508954\n",
      "Iteration 34, loss = 0.27822012\n",
      "Iteration 35, loss = 0.27655870\n",
      "Iteration 36, loss = 0.27538301\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.10883913\n",
      "Iteration 2, loss = 1.52906614\n",
      "Iteration 3, loss = 1.16300936\n",
      "Iteration 4, loss = 0.86893483\n",
      "Iteration 5, loss = 0.69222208\n",
      "Iteration 6, loss = 0.58729068\n",
      "Iteration 7, loss = 0.51458271\n",
      "Iteration 8, loss = 0.45951083\n",
      "Iteration 9, loss = 0.41802974\n",
      "Iteration 10, loss = 0.38815610\n",
      "Iteration 11, loss = 0.37187273\n",
      "Iteration 12, loss = 0.35411601\n",
      "Iteration 13, loss = 0.33862779\n",
      "Iteration 14, loss = 0.32775845\n",
      "Iteration 15, loss = 0.31496627\n",
      "Iteration 16, loss = 0.30712075\n",
      "Iteration 17, loss = 0.30343843\n",
      "Iteration 18, loss = 0.29872868\n",
      "Iteration 19, loss = 0.29333771\n",
      "Iteration 20, loss = 0.29472155\n",
      "Iteration 21, loss = 0.29784678\n",
      "Iteration 22, loss = 0.29495217\n",
      "Iteration 23, loss = 0.28907514\n",
      "Iteration 24, loss = 0.29061759\n",
      "Iteration 25, loss = 0.29378257\n",
      "Iteration 26, loss = 0.29849929\n",
      "Iteration 27, loss = 0.29534997\n",
      "Iteration 28, loss = 0.29527233\n",
      "Iteration 29, loss = 0.28696389\n",
      "Iteration 30, loss = 0.28151752\n",
      "Iteration 31, loss = 0.28942473\n",
      "Iteration 32, loss = 0.29835047\n",
      "Iteration 33, loss = 0.30435818\n",
      "Iteration 34, loss = 0.30289380\n",
      "Iteration 35, loss = 0.29658774\n",
      "Iteration 36, loss = 0.31042114\n",
      "Iteration 37, loss = 0.30879812\n",
      "Iteration 38, loss = 0.29202630\n",
      "Iteration 39, loss = 0.28106663\n",
      "Iteration 40, loss = 0.28076476\n",
      "Iteration 41, loss = 0.28477999\n",
      "Iteration 42, loss = 0.28755300\n",
      "Iteration 43, loss = 0.28688434\n",
      "Iteration 44, loss = 0.29135679\n",
      "Iteration 45, loss = 0.29219238\n",
      "Iteration 46, loss = 0.29721547\n",
      "Iteration 47, loss = 0.30765659\n",
      "Iteration 48, loss = 0.30315927\n",
      "Iteration 49, loss = 0.29318217\n",
      "Iteration 50, loss = 0.28557854\n",
      "Iteration 51, loss = 0.27718972\n",
      "Iteration 52, loss = 0.27672154\n",
      "Iteration 53, loss = 0.28712364\n",
      "Iteration 54, loss = 0.29121639\n",
      "Iteration 55, loss = 0.29529395\n",
      "Iteration 56, loss = 0.30268624\n",
      "Iteration 57, loss = 0.29654991\n",
      "Iteration 58, loss = 0.28405907\n",
      "Iteration 59, loss = 0.28775996\n",
      "Iteration 60, loss = 0.29485070\n",
      "Iteration 61, loss = 0.29771944\n",
      "Iteration 62, loss = 0.29294356\n",
      "Iteration 63, loss = 0.28812279\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.15205712\n",
      "Iteration 2, loss = 1.47903279\n",
      "Iteration 3, loss = 1.11640368\n",
      "Iteration 4, loss = 0.86356720\n",
      "Iteration 5, loss = 0.68470411\n",
      "Iteration 6, loss = 0.57277005\n",
      "Iteration 7, loss = 0.50505308\n",
      "Iteration 8, loss = 0.45266152\n",
      "Iteration 9, loss = 0.41307011\n",
      "Iteration 10, loss = 0.38594701\n",
      "Iteration 11, loss = 0.36358998\n",
      "Iteration 12, loss = 0.33838072\n",
      "Iteration 13, loss = 0.32408901\n",
      "Iteration 14, loss = 0.32079805\n",
      "Iteration 15, loss = 0.32545202\n",
      "Iteration 16, loss = 0.32124746\n",
      "Iteration 17, loss = 0.30715030\n",
      "Iteration 18, loss = 0.30211971\n",
      "Iteration 19, loss = 0.30016953\n",
      "Iteration 20, loss = 0.29215999\n",
      "Iteration 21, loss = 0.28997172\n",
      "Iteration 22, loss = 0.28586934\n",
      "Iteration 23, loss = 0.28231933\n",
      "Iteration 24, loss = 0.28450347\n",
      "Iteration 25, loss = 0.29004126\n",
      "Iteration 26, loss = 0.28888941\n",
      "Iteration 27, loss = 0.28332577\n",
      "Iteration 28, loss = 0.28140212\n",
      "Iteration 29, loss = 0.28568056\n",
      "Iteration 30, loss = 0.29139086\n",
      "Iteration 31, loss = 0.29098459\n",
      "Iteration 32, loss = 0.28390244\n",
      "Iteration 33, loss = 0.28688119\n",
      "Iteration 34, loss = 0.29965892\n",
      "Iteration 35, loss = 0.29219505\n",
      "Iteration 36, loss = 0.30634915\n",
      "Iteration 37, loss = 0.31439172\n",
      "Iteration 38, loss = 0.30078608\n",
      "Iteration 39, loss = 0.29229713\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.13589828\n",
      "Iteration 2, loss = 1.48706288\n",
      "Iteration 3, loss = 1.11833231\n",
      "Iteration 4, loss = 0.85223063\n",
      "Iteration 5, loss = 0.67063604\n",
      "Iteration 6, loss = 0.55341973\n",
      "Iteration 7, loss = 0.47913525\n",
      "Iteration 8, loss = 0.42695745\n",
      "Iteration 9, loss = 0.39720801\n",
      "Iteration 10, loss = 0.37676966\n",
      "Iteration 11, loss = 0.35476583\n",
      "Iteration 12, loss = 0.33514857\n",
      "Iteration 13, loss = 0.32392181\n",
      "Iteration 14, loss = 0.32084799\n",
      "Iteration 15, loss = 0.32220358\n",
      "Iteration 16, loss = 0.31459224\n",
      "Iteration 17, loss = 0.30237046\n",
      "Iteration 18, loss = 0.30359690\n",
      "Iteration 19, loss = 0.30615377\n",
      "Iteration 20, loss = 0.29549865\n",
      "Iteration 21, loss = 0.28858414\n",
      "Iteration 22, loss = 0.28478751\n",
      "Iteration 23, loss = 0.28308090\n",
      "Iteration 24, loss = 0.28349522\n",
      "Iteration 25, loss = 0.28567248\n",
      "Iteration 26, loss = 0.28838407\n",
      "Iteration 27, loss = 0.28764449\n",
      "Iteration 28, loss = 0.27996760\n",
      "Iteration 29, loss = 0.27606167\n",
      "Iteration 30, loss = 0.28055229\n",
      "Iteration 31, loss = 0.29155383\n",
      "Iteration 32, loss = 0.28868708\n",
      "Iteration 33, loss = 0.29057198\n",
      "Iteration 34, loss = 0.29829171\n",
      "Iteration 35, loss = 0.28837971\n",
      "Iteration 36, loss = 0.29812458\n",
      "Iteration 37, loss = 0.30324688\n",
      "Iteration 38, loss = 0.29066067\n",
      "Iteration 39, loss = 0.28499160\n",
      "Iteration 40, loss = 0.28156595\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.13851533\n",
      "Iteration 2, loss = 1.46944485\n",
      "Iteration 3, loss = 1.13221016\n",
      "Iteration 4, loss = 0.87049432\n",
      "Iteration 5, loss = 0.68455194\n",
      "Iteration 6, loss = 0.57050452\n",
      "Iteration 7, loss = 0.49906304\n",
      "Iteration 8, loss = 0.44570387\n",
      "Iteration 9, loss = 0.41020340\n",
      "Iteration 10, loss = 0.38835063\n",
      "Iteration 11, loss = 0.36797165\n",
      "Iteration 12, loss = 0.34446511\n",
      "Iteration 13, loss = 0.32729073\n",
      "Iteration 14, loss = 0.32109915\n",
      "Iteration 15, loss = 0.32163931\n",
      "Iteration 16, loss = 0.31912289\n",
      "Iteration 17, loss = 0.30635897\n",
      "Iteration 18, loss = 0.29777834\n",
      "Iteration 19, loss = 0.29171761\n",
      "Iteration 20, loss = 0.28095532\n",
      "Iteration 21, loss = 0.28085634\n",
      "Iteration 22, loss = 0.28457125\n",
      "Iteration 23, loss = 0.28401615\n",
      "Iteration 24, loss = 0.27780951\n",
      "Iteration 25, loss = 0.27739951\n",
      "Iteration 26, loss = 0.29575533\n",
      "Iteration 27, loss = 0.29092631\n",
      "Iteration 28, loss = 0.27840979\n",
      "Iteration 29, loss = 0.27943949\n",
      "Iteration 30, loss = 0.27693410\n",
      "Iteration 31, loss = 0.27758760\n",
      "Iteration 32, loss = 0.27634657\n",
      "Iteration 33, loss = 0.28429941\n",
      "Iteration 34, loss = 0.29618170\n",
      "Iteration 35, loss = 0.28690963\n",
      "Iteration 36, loss = 0.28553856\n",
      "Iteration 37, loss = 0.28672225\n",
      "Iteration 38, loss = 0.27972665\n",
      "Iteration 39, loss = 0.27306587\n",
      "Iteration 40, loss = 0.27085296\n",
      "Iteration 41, loss = 0.26846854\n",
      "Iteration 42, loss = 0.27424937\n",
      "Iteration 43, loss = 0.27686604\n",
      "Iteration 44, loss = 0.27362225\n",
      "Iteration 45, loss = 0.27274685\n",
      "Iteration 46, loss = 0.27749189\n",
      "Iteration 47, loss = 0.28308871\n",
      "Iteration 48, loss = 0.27237449\n",
      "Iteration 49, loss = 0.27926390\n",
      "Iteration 50, loss = 0.28644185\n",
      "Iteration 51, loss = 0.27807034\n",
      "Iteration 52, loss = 0.28342196\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.11962332\n",
      "Iteration 2, loss = 1.47470405\n",
      "Iteration 3, loss = 1.11447431\n",
      "Iteration 4, loss = 0.84222518\n",
      "Iteration 5, loss = 0.66285297\n",
      "Iteration 6, loss = 0.56059204\n",
      "Iteration 7, loss = 0.48494922\n",
      "Iteration 8, loss = 0.43335705\n",
      "Iteration 9, loss = 0.40227185\n",
      "Iteration 10, loss = 0.37943912\n",
      "Iteration 11, loss = 0.35693950\n",
      "Iteration 12, loss = 0.33151051\n",
      "Iteration 13, loss = 0.31884589\n",
      "Iteration 14, loss = 0.31642583\n",
      "Iteration 15, loss = 0.31773916\n",
      "Iteration 16, loss = 0.31463857\n",
      "Iteration 17, loss = 0.30375628\n",
      "Iteration 18, loss = 0.29476717\n",
      "Iteration 19, loss = 0.29133285\n",
      "Iteration 20, loss = 0.28425462\n",
      "Iteration 21, loss = 0.28526332\n",
      "Iteration 22, loss = 0.29050777\n",
      "Iteration 23, loss = 0.28920247\n",
      "Iteration 24, loss = 0.28531595\n",
      "Iteration 25, loss = 0.28110555\n",
      "Iteration 26, loss = 0.29479464\n",
      "Iteration 27, loss = 0.29766891\n",
      "Iteration 28, loss = 0.28864630\n",
      "Iteration 29, loss = 0.28771117\n",
      "Iteration 30, loss = 0.28224471\n",
      "Iteration 31, loss = 0.27616598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 32, loss = 0.27660113\n",
      "Iteration 33, loss = 0.29060391\n",
      "Iteration 34, loss = 0.30720245\n",
      "Iteration 35, loss = 0.30211700\n",
      "Iteration 36, loss = 0.29375545\n",
      "Iteration 37, loss = 0.28183541\n",
      "Iteration 38, loss = 0.28303201\n",
      "Iteration 39, loss = 0.29080755\n",
      "Iteration 40, loss = 0.28967077\n",
      "Iteration 41, loss = 0.27943109\n",
      "Iteration 42, loss = 0.27537027\n",
      "Iteration 43, loss = 0.28018973\n",
      "Iteration 44, loss = 0.28258988\n",
      "Iteration 45, loss = 0.27876250\n",
      "Iteration 46, loss = 0.28006787\n",
      "Iteration 47, loss = 0.28607897\n",
      "Iteration 48, loss = 0.27701946\n",
      "Iteration 49, loss = 0.28907138\n",
      "Iteration 50, loss = 0.30300448\n",
      "Iteration 51, loss = 0.29323221\n",
      "Iteration 52, loss = 0.28897090\n",
      "Iteration 53, loss = 0.28497414\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.12550861\n",
      "Iteration 2, loss = 1.48078797\n",
      "Iteration 3, loss = 1.12524867\n",
      "Iteration 4, loss = 0.85311850\n",
      "Iteration 5, loss = 0.67072195\n",
      "Iteration 6, loss = 0.56353966\n",
      "Iteration 7, loss = 0.48318586\n",
      "Iteration 8, loss = 0.42526324\n",
      "Iteration 9, loss = 0.39186252\n",
      "Iteration 10, loss = 0.37091846\n",
      "Iteration 11, loss = 0.35046535\n",
      "Iteration 12, loss = 0.32734619\n",
      "Iteration 13, loss = 0.31123007\n",
      "Iteration 14, loss = 0.30441576\n",
      "Iteration 15, loss = 0.30476765\n",
      "Iteration 16, loss = 0.30279024\n",
      "Iteration 17, loss = 0.29631802\n",
      "Iteration 18, loss = 0.28972272\n",
      "Iteration 19, loss = 0.28708333\n",
      "Iteration 20, loss = 0.28059347\n",
      "Iteration 21, loss = 0.27791731\n",
      "Iteration 22, loss = 0.28014439\n",
      "Iteration 23, loss = 0.27661578\n",
      "Iteration 24, loss = 0.27501412\n",
      "Iteration 25, loss = 0.26876907\n",
      "Iteration 26, loss = 0.28099197\n",
      "Iteration 27, loss = 0.28192025\n",
      "Iteration 28, loss = 0.27831963\n",
      "Iteration 29, loss = 0.28435640\n",
      "Iteration 30, loss = 0.28355067\n",
      "Iteration 31, loss = 0.27292670\n",
      "Iteration 32, loss = 0.27756318\n",
      "Iteration 33, loss = 0.29095155\n",
      "Iteration 34, loss = 0.29719551\n",
      "Iteration 35, loss = 0.28651921\n",
      "Iteration 36, loss = 0.28528558\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.10274795\n",
      "Iteration 2, loss = 1.45349013\n",
      "Iteration 3, loss = 1.07589283\n",
      "Iteration 4, loss = 0.80687644\n",
      "Iteration 5, loss = 0.64307764\n",
      "Iteration 6, loss = 0.53771552\n",
      "Iteration 7, loss = 0.45905000\n",
      "Iteration 8, loss = 0.41449633\n",
      "Iteration 9, loss = 0.39339035\n",
      "Iteration 10, loss = 0.37473384\n",
      "Iteration 11, loss = 0.34893653\n",
      "Iteration 12, loss = 0.32490193\n",
      "Iteration 13, loss = 0.31053807\n",
      "Iteration 14, loss = 0.30174336\n",
      "Iteration 15, loss = 0.30163743\n",
      "Iteration 16, loss = 0.30007364\n",
      "Iteration 17, loss = 0.29212229\n",
      "Iteration 18, loss = 0.28812911\n",
      "Iteration 19, loss = 0.28811616\n",
      "Iteration 20, loss = 0.28394662\n",
      "Iteration 21, loss = 0.28298811\n",
      "Iteration 22, loss = 0.28690544\n",
      "Iteration 23, loss = 0.28115670\n",
      "Iteration 24, loss = 0.27765308\n",
      "Iteration 25, loss = 0.27137922\n",
      "Iteration 26, loss = 0.28637678\n",
      "Iteration 27, loss = 0.28851979\n",
      "Iteration 28, loss = 0.27704509\n",
      "Iteration 29, loss = 0.27693966\n",
      "Iteration 30, loss = 0.28186025\n",
      "Iteration 31, loss = 0.27562847\n",
      "Iteration 32, loss = 0.28089353\n",
      "Iteration 33, loss = 0.29334996\n",
      "Iteration 34, loss = 0.30029524\n",
      "Iteration 35, loss = 0.29201603\n",
      "Iteration 36, loss = 0.29031142\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.10426096\n",
      "Iteration 2, loss = 1.47216913\n",
      "Iteration 3, loss = 1.09435257\n",
      "Iteration 4, loss = 0.82021917\n",
      "Iteration 5, loss = 0.65235674\n",
      "Iteration 6, loss = 0.54924433\n",
      "Iteration 7, loss = 0.47098970\n",
      "Iteration 8, loss = 0.42532306\n",
      "Iteration 9, loss = 0.40281602\n",
      "Iteration 10, loss = 0.38326635\n",
      "Iteration 11, loss = 0.35908903\n",
      "Iteration 12, loss = 0.33564057\n",
      "Iteration 13, loss = 0.31937064\n",
      "Iteration 14, loss = 0.31020773\n",
      "Iteration 15, loss = 0.30990881\n",
      "Iteration 16, loss = 0.30874480\n",
      "Iteration 17, loss = 0.30389848\n",
      "Iteration 18, loss = 0.30036596\n",
      "Iteration 19, loss = 0.30032911\n",
      "Iteration 20, loss = 0.29768682\n",
      "Iteration 21, loss = 0.29421851\n",
      "Iteration 22, loss = 0.29185316\n",
      "Iteration 23, loss = 0.28277149\n",
      "Iteration 24, loss = 0.28196245\n",
      "Iteration 25, loss = 0.28008129\n",
      "Iteration 26, loss = 0.29140021\n",
      "Iteration 27, loss = 0.28862141\n",
      "Iteration 28, loss = 0.28228937\n",
      "Iteration 29, loss = 0.28245249\n",
      "Iteration 30, loss = 0.28879874\n",
      "Iteration 31, loss = 0.28779839\n",
      "Iteration 32, loss = 0.28937140\n",
      "Iteration 33, loss = 0.30163447\n",
      "Iteration 34, loss = 0.30742385\n",
      "Iteration 35, loss = 0.29817199\n",
      "Iteration 36, loss = 0.29241811\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.11052682\n",
      "Iteration 2, loss = 1.46018315\n",
      "Iteration 3, loss = 1.08472989\n",
      "Iteration 4, loss = 0.81223332\n",
      "Iteration 5, loss = 0.64094715\n",
      "Iteration 6, loss = 0.53970804\n",
      "Iteration 7, loss = 0.46426853\n",
      "Iteration 8, loss = 0.41423196\n",
      "Iteration 9, loss = 0.38672502\n",
      "Iteration 10, loss = 0.36999849\n",
      "Iteration 11, loss = 0.35280314\n",
      "Iteration 12, loss = 0.33193760\n",
      "Iteration 13, loss = 0.31749364\n",
      "Iteration 14, loss = 0.30674466\n",
      "Iteration 15, loss = 0.30262782\n",
      "Iteration 16, loss = 0.30160049\n",
      "Iteration 17, loss = 0.29846775\n",
      "Iteration 18, loss = 0.29363825\n",
      "Iteration 19, loss = 0.29222083\n",
      "Iteration 20, loss = 0.29117826\n",
      "Iteration 21, loss = 0.28495276\n",
      "Iteration 22, loss = 0.28428070\n",
      "Iteration 23, loss = 0.28599081\n",
      "Iteration 24, loss = 0.28349596\n",
      "Iteration 25, loss = 0.27652780\n",
      "Iteration 26, loss = 0.28255649\n",
      "Iteration 27, loss = 0.28495258\n",
      "Iteration 28, loss = 0.27972879\n",
      "Iteration 29, loss = 0.27509202\n",
      "Iteration 30, loss = 0.28155740\n",
      "Iteration 31, loss = 0.28239973\n",
      "Iteration 32, loss = 0.28487628\n",
      "Iteration 33, loss = 0.29409510\n",
      "Iteration 34, loss = 0.30154331\n",
      "Iteration 35, loss = 0.29333252\n",
      "Iteration 36, loss = 0.28890933\n",
      "Iteration 37, loss = 0.28768216\n",
      "Iteration 38, loss = 0.29313522\n",
      "Iteration 39, loss = 0.31035208\n",
      "Iteration 40, loss = 0.30720232\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.09191913\n",
      "Iteration 2, loss = 1.46714253\n",
      "Iteration 3, loss = 1.07529954\n",
      "Iteration 4, loss = 0.79981928\n",
      "Iteration 5, loss = 0.63411842\n",
      "Iteration 6, loss = 0.53708742\n",
      "Iteration 7, loss = 0.46239611\n",
      "Iteration 8, loss = 0.40842843\n",
      "Iteration 9, loss = 0.37795582\n",
      "Iteration 10, loss = 0.36145365\n",
      "Iteration 11, loss = 0.34553411\n",
      "Iteration 12, loss = 0.32747795\n",
      "Iteration 13, loss = 0.30907861\n",
      "Iteration 14, loss = 0.29374952\n",
      "Iteration 15, loss = 0.29255178\n",
      "Iteration 16, loss = 0.29897330\n",
      "Iteration 17, loss = 0.29250658\n",
      "Iteration 18, loss = 0.28153919\n",
      "Iteration 19, loss = 0.28216608\n",
      "Iteration 20, loss = 0.28658991\n",
      "Iteration 21, loss = 0.27853165\n",
      "Iteration 22, loss = 0.27454570\n",
      "Iteration 23, loss = 0.27819751\n",
      "Iteration 24, loss = 0.27641900\n",
      "Iteration 25, loss = 0.26414149\n",
      "Iteration 26, loss = 0.27105773\n",
      "Iteration 27, loss = 0.27563299\n",
      "Iteration 28, loss = 0.27083329\n",
      "Iteration 29, loss = 0.26862608\n",
      "Iteration 30, loss = 0.27125466\n",
      "Iteration 31, loss = 0.27521765\n",
      "Iteration 32, loss = 0.27491257\n",
      "Iteration 33, loss = 0.28395237\n",
      "Iteration 34, loss = 0.29068742\n",
      "Iteration 35, loss = 0.28514504\n",
      "Iteration 36, loss = 0.28224432\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.17221797\n",
      "Iteration 2, loss = 1.43189516\n",
      "Iteration 3, loss = 1.08248885\n",
      "Iteration 4, loss = 0.78328875\n",
      "Iteration 5, loss = 0.60138888\n",
      "Iteration 6, loss = 0.52129944\n",
      "Iteration 7, loss = 0.47853979\n",
      "Iteration 8, loss = 0.44484735\n",
      "Iteration 9, loss = 0.41244073\n",
      "Iteration 10, loss = 0.38129346\n",
      "Iteration 11, loss = 0.35541859\n",
      "Iteration 12, loss = 0.34224996\n",
      "Iteration 13, loss = 0.34192469\n",
      "Iteration 14, loss = 0.34587780\n",
      "Iteration 15, loss = 0.33955728\n",
      "Iteration 16, loss = 0.31620433\n",
      "Iteration 17, loss = 0.29531599\n",
      "Iteration 18, loss = 0.30570072\n",
      "Iteration 19, loss = 0.30191149\n",
      "Iteration 20, loss = 0.31270951\n",
      "Iteration 21, loss = 0.30998358\n",
      "Iteration 22, loss = 0.30621423\n",
      "Iteration 23, loss = 0.30190497\n",
      "Iteration 24, loss = 0.30245164\n",
      "Iteration 25, loss = 0.30311841\n",
      "Iteration 26, loss = 0.29954201\n",
      "Iteration 27, loss = 0.29805889\n",
      "Iteration 28, loss = 0.29113748\n",
      "Iteration 29, loss = 0.28805534\n",
      "Iteration 30, loss = 0.28804649\n",
      "Iteration 31, loss = 0.29376154\n",
      "Iteration 32, loss = 0.30011665\n",
      "Iteration 33, loss = 0.30128669\n",
      "Iteration 34, loss = 0.29770226\n",
      "Iteration 35, loss = 0.30120728\n",
      "Iteration 36, loss = 0.29436239\n",
      "Iteration 37, loss = 0.28486033\n",
      "Iteration 38, loss = 0.29316500\n",
      "Iteration 39, loss = 0.30228952\n",
      "Iteration 40, loss = 0.29975936\n",
      "Iteration 41, loss = 0.28446098\n",
      "Iteration 42, loss = 0.29041131\n",
      "Iteration 43, loss = 0.31164687\n",
      "Iteration 44, loss = 0.29677403\n",
      "Iteration 45, loss = 0.29025873\n",
      "Iteration 46, loss = 0.30001337\n",
      "Iteration 47, loss = 0.30922654\n",
      "Iteration 48, loss = 0.30140512\n",
      "Iteration 49, loss = 0.28494994\n",
      "Iteration 50, loss = 0.27544411\n",
      "Iteration 51, loss = 0.27514180\n",
      "Iteration 52, loss = 0.28323231\n",
      "Iteration 53, loss = 0.30472310\n",
      "Iteration 54, loss = 0.30347842\n",
      "Iteration 55, loss = 0.28373671\n",
      "Iteration 56, loss = 0.29008840\n",
      "Iteration 57, loss = 0.32943524\n",
      "Iteration 58, loss = 0.33338697\n",
      "Iteration 59, loss = 0.30494681\n",
      "Iteration 60, loss = 0.29091722\n",
      "Iteration 61, loss = 0.28734653\n",
      "Iteration 62, loss = 0.28632804\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.17358252\n",
      "Iteration 2, loss = 1.40662401\n",
      "Iteration 3, loss = 1.04423173\n",
      "Iteration 4, loss = 0.74989179\n",
      "Iteration 5, loss = 0.60355134\n",
      "Iteration 6, loss = 0.54409831\n",
      "Iteration 7, loss = 0.49793431\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8, loss = 0.44647823\n",
      "Iteration 9, loss = 0.40244519\n",
      "Iteration 10, loss = 0.37384131\n",
      "Iteration 11, loss = 0.35734879\n",
      "Iteration 12, loss = 0.34203756\n",
      "Iteration 13, loss = 0.33663479\n",
      "Iteration 14, loss = 0.33950345\n",
      "Iteration 15, loss = 0.33306852\n",
      "Iteration 16, loss = 0.32208642\n",
      "Iteration 17, loss = 0.31576430\n",
      "Iteration 18, loss = 0.31057935\n",
      "Iteration 19, loss = 0.31424800\n",
      "Iteration 20, loss = 0.32461282\n",
      "Iteration 21, loss = 0.31043993\n",
      "Iteration 22, loss = 0.29697880\n",
      "Iteration 23, loss = 0.28656348\n",
      "Iteration 24, loss = 0.28603760\n",
      "Iteration 25, loss = 0.29585546\n",
      "Iteration 26, loss = 0.30549650\n",
      "Iteration 27, loss = 0.29867863\n",
      "Iteration 28, loss = 0.29088612\n",
      "Iteration 29, loss = 0.29580944\n",
      "Iteration 30, loss = 0.29385454\n",
      "Iteration 31, loss = 0.29578312\n",
      "Iteration 32, loss = 0.29658407\n",
      "Iteration 33, loss = 0.29253578\n",
      "Iteration 34, loss = 0.30204118\n",
      "Iteration 35, loss = 0.30318190\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.16542185\n",
      "Iteration 2, loss = 1.42276893\n",
      "Iteration 3, loss = 1.04121449\n",
      "Iteration 4, loss = 0.74458977\n",
      "Iteration 5, loss = 0.59391807\n",
      "Iteration 6, loss = 0.52788981\n",
      "Iteration 7, loss = 0.48074114\n",
      "Iteration 8, loss = 0.43425947\n",
      "Iteration 9, loss = 0.39748339\n",
      "Iteration 10, loss = 0.37419148\n",
      "Iteration 11, loss = 0.35763676\n",
      "Iteration 12, loss = 0.34117698\n",
      "Iteration 13, loss = 0.33535626\n",
      "Iteration 14, loss = 0.34040780\n",
      "Iteration 15, loss = 0.33760333\n",
      "Iteration 16, loss = 0.32755052\n",
      "Iteration 17, loss = 0.31661903\n",
      "Iteration 18, loss = 0.30363088\n",
      "Iteration 19, loss = 0.30294035\n",
      "Iteration 20, loss = 0.31538802\n",
      "Iteration 21, loss = 0.30561758\n",
      "Iteration 22, loss = 0.29222771\n",
      "Iteration 23, loss = 0.28430924\n",
      "Iteration 24, loss = 0.28888805\n",
      "Iteration 25, loss = 0.30101445\n",
      "Iteration 26, loss = 0.31231723\n",
      "Iteration 27, loss = 0.30711316\n",
      "Iteration 28, loss = 0.29859829\n",
      "Iteration 29, loss = 0.29329622\n",
      "Iteration 30, loss = 0.29750337\n",
      "Iteration 31, loss = 0.30032506\n",
      "Iteration 32, loss = 0.29536524\n",
      "Iteration 33, loss = 0.28694751\n",
      "Iteration 34, loss = 0.29896188\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.16744020\n",
      "Iteration 2, loss = 1.39292942\n",
      "Iteration 3, loss = 1.04706169\n",
      "Iteration 4, loss = 0.76837622\n",
      "Iteration 5, loss = 0.61373878\n",
      "Iteration 6, loss = 0.54105467\n",
      "Iteration 7, loss = 0.48837734\n",
      "Iteration 8, loss = 0.44380768\n",
      "Iteration 9, loss = 0.41057214\n",
      "Iteration 10, loss = 0.38805397\n",
      "Iteration 11, loss = 0.36978219\n",
      "Iteration 12, loss = 0.35294883\n",
      "Iteration 13, loss = 0.34282288\n",
      "Iteration 14, loss = 0.33547148\n",
      "Iteration 15, loss = 0.32748965\n",
      "Iteration 16, loss = 0.31739027\n",
      "Iteration 17, loss = 0.31023833\n",
      "Iteration 18, loss = 0.30138235\n",
      "Iteration 19, loss = 0.30417559\n",
      "Iteration 20, loss = 0.31420348\n",
      "Iteration 21, loss = 0.30246207\n",
      "Iteration 22, loss = 0.28978713\n",
      "Iteration 23, loss = 0.27584397\n",
      "Iteration 24, loss = 0.27874511\n",
      "Iteration 25, loss = 0.29244838\n",
      "Iteration 26, loss = 0.30000654\n",
      "Iteration 27, loss = 0.29874975\n",
      "Iteration 28, loss = 0.28815773\n",
      "Iteration 29, loss = 0.28309654\n",
      "Iteration 30, loss = 0.28515078\n",
      "Iteration 31, loss = 0.28578899\n",
      "Iteration 32, loss = 0.28479916\n",
      "Iteration 33, loss = 0.27812447\n",
      "Iteration 34, loss = 0.28405047\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.17677930\n",
      "Iteration 2, loss = 1.42777439\n",
      "Iteration 3, loss = 1.07431800\n",
      "Iteration 4, loss = 0.78635590\n",
      "Iteration 5, loss = 0.62713790\n",
      "Iteration 6, loss = 0.54777880\n",
      "Iteration 7, loss = 0.49328760\n",
      "Iteration 8, loss = 0.44772023\n",
      "Iteration 9, loss = 0.41172225\n",
      "Iteration 10, loss = 0.38527423\n",
      "Iteration 11, loss = 0.36548177\n",
      "Iteration 12, loss = 0.34782403\n",
      "Iteration 13, loss = 0.33580843\n",
      "Iteration 14, loss = 0.32930177\n",
      "Iteration 15, loss = 0.32198485\n",
      "Iteration 16, loss = 0.30552012\n",
      "Iteration 17, loss = 0.29565402\n",
      "Iteration 18, loss = 0.29389840\n",
      "Iteration 19, loss = 0.29866880\n",
      "Iteration 20, loss = 0.30481747\n",
      "Iteration 21, loss = 0.29498314\n",
      "Iteration 22, loss = 0.29273863\n",
      "Iteration 23, loss = 0.28718166\n",
      "Iteration 24, loss = 0.29414173\n",
      "Iteration 25, loss = 0.30704153\n",
      "Iteration 26, loss = 0.31179122\n",
      "Iteration 27, loss = 0.30508325\n",
      "Iteration 28, loss = 0.30128936\n",
      "Iteration 29, loss = 0.29474351\n",
      "Iteration 30, loss = 0.28942641\n",
      "Iteration 31, loss = 0.28512360\n",
      "Iteration 32, loss = 0.28571167\n",
      "Iteration 33, loss = 0.28152533\n",
      "Iteration 34, loss = 0.29104180\n",
      "Iteration 35, loss = 0.30085269\n",
      "Iteration 36, loss = 0.30937487\n",
      "Iteration 37, loss = 0.31012590\n",
      "Iteration 38, loss = 0.30070899\n",
      "Iteration 39, loss = 0.29336460\n",
      "Iteration 40, loss = 0.29584622\n",
      "Iteration 41, loss = 0.29915494\n",
      "Iteration 42, loss = 0.29722173\n",
      "Iteration 43, loss = 0.28475558\n",
      "Iteration 44, loss = 0.27514134\n",
      "Iteration 45, loss = 0.27410853\n",
      "Iteration 46, loss = 0.27957135\n",
      "Iteration 47, loss = 0.27947063\n",
      "Iteration 48, loss = 0.27592867\n",
      "Iteration 49, loss = 0.27732586\n",
      "Iteration 50, loss = 0.27949622\n",
      "Iteration 51, loss = 0.27536435\n",
      "Iteration 52, loss = 0.26512122\n",
      "Iteration 53, loss = 0.26196852\n",
      "Iteration 54, loss = 0.27660928\n",
      "Iteration 55, loss = 0.28225874\n",
      "Iteration 56, loss = 0.28369377\n",
      "Iteration 57, loss = 0.28554438\n",
      "Iteration 58, loss = 0.29114272\n",
      "Iteration 59, loss = 0.29133787\n",
      "Iteration 60, loss = 0.27670385\n",
      "Iteration 61, loss = 0.26821820\n",
      "Iteration 62, loss = 0.27163134\n",
      "Iteration 63, loss = 0.29319071\n",
      "Iteration 64, loss = 0.30822474\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.18640803\n",
      "Iteration 2, loss = 1.41577612\n",
      "Iteration 3, loss = 1.06136651\n",
      "Iteration 4, loss = 0.77765091\n",
      "Iteration 5, loss = 0.61726431\n",
      "Iteration 6, loss = 0.53360652\n",
      "Iteration 7, loss = 0.47423753\n",
      "Iteration 8, loss = 0.42717699\n",
      "Iteration 9, loss = 0.39260764\n",
      "Iteration 10, loss = 0.37289744\n",
      "Iteration 11, loss = 0.35742025\n",
      "Iteration 12, loss = 0.34264048\n",
      "Iteration 13, loss = 0.32985494\n",
      "Iteration 14, loss = 0.32190000\n",
      "Iteration 15, loss = 0.30632630\n",
      "Iteration 16, loss = 0.28826865\n",
      "Iteration 17, loss = 0.28888485\n",
      "Iteration 18, loss = 0.30243413\n",
      "Iteration 19, loss = 0.30672892\n",
      "Iteration 20, loss = 0.29774371\n",
      "Iteration 21, loss = 0.28513613\n",
      "Iteration 22, loss = 0.28101145\n",
      "Iteration 23, loss = 0.28571998\n",
      "Iteration 24, loss = 0.29763850\n",
      "Iteration 25, loss = 0.29473391\n",
      "Iteration 26, loss = 0.28666144\n",
      "Iteration 27, loss = 0.28115298\n",
      "Iteration 28, loss = 0.29020902\n",
      "Iteration 29, loss = 0.30134794\n",
      "Iteration 30, loss = 0.28867816\n",
      "Iteration 31, loss = 0.27295789\n",
      "Iteration 32, loss = 0.28004549\n",
      "Iteration 33, loss = 0.28483294\n",
      "Iteration 34, loss = 0.27656923\n",
      "Iteration 35, loss = 0.26834330\n",
      "Iteration 36, loss = 0.28100486\n",
      "Iteration 37, loss = 0.29499188\n",
      "Iteration 38, loss = 0.29872173\n",
      "Iteration 39, loss = 0.28695656\n",
      "Iteration 40, loss = 0.28736915\n",
      "Iteration 41, loss = 0.29925145\n",
      "Iteration 42, loss = 0.30286337\n",
      "Iteration 43, loss = 0.28017793\n",
      "Iteration 44, loss = 0.27768122\n",
      "Iteration 45, loss = 0.28786950\n",
      "Iteration 46, loss = 0.29349105\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.17922316\n",
      "Iteration 2, loss = 1.44611747\n",
      "Iteration 3, loss = 1.08657949\n",
      "Iteration 4, loss = 0.80023391\n",
      "Iteration 5, loss = 0.63780971\n",
      "Iteration 6, loss = 0.55251631\n",
      "Iteration 7, loss = 0.49050430\n",
      "Iteration 8, loss = 0.44274083\n",
      "Iteration 9, loss = 0.40790805\n",
      "Iteration 10, loss = 0.38659513\n",
      "Iteration 11, loss = 0.36859991\n",
      "Iteration 12, loss = 0.35120859\n",
      "Iteration 13, loss = 0.33518284\n",
      "Iteration 14, loss = 0.32657552\n",
      "Iteration 15, loss = 0.31381553\n",
      "Iteration 16, loss = 0.29989665\n",
      "Iteration 17, loss = 0.30031140\n",
      "Iteration 18, loss = 0.30520282\n",
      "Iteration 19, loss = 0.29724958\n",
      "Iteration 20, loss = 0.28872650\n",
      "Iteration 21, loss = 0.28763530\n",
      "Iteration 22, loss = 0.29138710\n",
      "Iteration 23, loss = 0.29687951\n",
      "Iteration 24, loss = 0.29937065\n",
      "Iteration 25, loss = 0.29234082\n",
      "Iteration 26, loss = 0.28821838\n",
      "Iteration 27, loss = 0.28392003\n",
      "Iteration 28, loss = 0.29270073\n",
      "Iteration 29, loss = 0.29586605\n",
      "Iteration 30, loss = 0.29063897\n",
      "Iteration 31, loss = 0.27999124\n",
      "Iteration 32, loss = 0.27683230\n",
      "Iteration 33, loss = 0.27844252\n",
      "Iteration 34, loss = 0.27684750\n",
      "Iteration 35, loss = 0.27709149\n",
      "Iteration 36, loss = 0.28836526\n",
      "Iteration 37, loss = 0.28974875\n",
      "Iteration 38, loss = 0.28852296\n",
      "Iteration 39, loss = 0.29220589\n",
      "Iteration 40, loss = 0.29686553\n",
      "Iteration 41, loss = 0.28882823\n",
      "Iteration 42, loss = 0.28322216\n",
      "Iteration 43, loss = 0.27539055\n",
      "Iteration 44, loss = 0.27129079\n",
      "Iteration 45, loss = 0.27157841\n",
      "Iteration 46, loss = 0.28092072\n",
      "Iteration 47, loss = 0.28091417\n",
      "Iteration 48, loss = 0.27411041\n",
      "Iteration 49, loss = 0.28337738\n",
      "Iteration 50, loss = 0.28652903\n",
      "Iteration 51, loss = 0.28106862\n",
      "Iteration 52, loss = 0.28238005\n",
      "Iteration 53, loss = 0.28591705\n",
      "Iteration 54, loss = 0.28274644\n",
      "Iteration 55, loss = 0.27868274\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.18341605\n",
      "Iteration 2, loss = 1.45173722\n",
      "Iteration 3, loss = 1.09647286\n",
      "Iteration 4, loss = 0.80810498\n",
      "Iteration 5, loss = 0.64670291\n",
      "Iteration 6, loss = 0.56329685\n",
      "Iteration 7, loss = 0.50514954\n",
      "Iteration 8, loss = 0.45796364\n",
      "Iteration 9, loss = 0.42173967\n",
      "Iteration 10, loss = 0.40129022\n",
      "Iteration 11, loss = 0.38672244\n",
      "Iteration 12, loss = 0.36984233\n",
      "Iteration 13, loss = 0.35475003\n",
      "Iteration 14, loss = 0.34352603\n",
      "Iteration 15, loss = 0.32619002\n",
      "Iteration 16, loss = 0.31051146\n",
      "Iteration 17, loss = 0.30884443\n",
      "Iteration 18, loss = 0.31686917\n",
      "Iteration 19, loss = 0.30944397\n",
      "Iteration 20, loss = 0.29921774\n",
      "Iteration 21, loss = 0.29854598\n",
      "Iteration 22, loss = 0.30343008\n",
      "Iteration 23, loss = 0.30701395\n",
      "Iteration 24, loss = 0.30945614\n",
      "Iteration 25, loss = 0.30217425\n",
      "Iteration 26, loss = 0.29887036\n",
      "Iteration 27, loss = 0.29695828\n",
      "Iteration 28, loss = 0.30788003\n",
      "Iteration 29, loss = 0.31728568\n",
      "Iteration 30, loss = 0.30739944\n",
      "Iteration 31, loss = 0.28925306\n",
      "Iteration 32, loss = 0.28842833\n",
      "Iteration 33, loss = 0.29135377\n",
      "Iteration 34, loss = 0.28576909\n",
      "Iteration 35, loss = 0.29040881\n",
      "Iteration 36, loss = 0.30496643\n",
      "Iteration 37, loss = 0.30231262\n",
      "Iteration 38, loss = 0.30244648\n",
      "Iteration 39, loss = 0.30717947\n",
      "Iteration 40, loss = 0.31335202\n",
      "Iteration 41, loss = 0.30539087\n",
      "Iteration 42, loss = 0.29425810\n",
      "Iteration 43, loss = 0.28421035\n",
      "Iteration 44, loss = 0.28383910\n",
      "Iteration 45, loss = 0.28257386\n",
      "Iteration 46, loss = 0.28853218\n",
      "Iteration 47, loss = 0.28972536\n",
      "Iteration 48, loss = 0.28103695\n",
      "Iteration 49, loss = 0.28745235\n",
      "Iteration 50, loss = 0.28985391\n",
      "Iteration 51, loss = 0.28843584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 52, loss = 0.29048658\n",
      "Iteration 53, loss = 0.29440940\n",
      "Iteration 54, loss = 0.29359259\n",
      "Iteration 55, loss = 0.28732250\n",
      "Iteration 56, loss = 0.27588872\n",
      "Iteration 57, loss = 0.28683313\n",
      "Iteration 58, loss = 0.31463193\n",
      "Iteration 59, loss = 0.32984173\n",
      "Iteration 60, loss = 0.31347000\n",
      "Iteration 61, loss = 0.29193965\n",
      "Iteration 62, loss = 0.29592236\n",
      "Iteration 63, loss = 0.31481236\n",
      "Iteration 64, loss = 0.32205434\n",
      "Iteration 65, loss = 0.31649146\n",
      "Iteration 66, loss = 0.29910950\n",
      "Iteration 67, loss = 0.28950970\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.18267990\n",
      "Iteration 2, loss = 1.43400295\n",
      "Iteration 3, loss = 1.07961864\n",
      "Iteration 4, loss = 0.79011367\n",
      "Iteration 5, loss = 0.62551471\n",
      "Iteration 6, loss = 0.54677495\n",
      "Iteration 7, loss = 0.49630374\n",
      "Iteration 8, loss = 0.45271488\n",
      "Iteration 9, loss = 0.41956299\n",
      "Iteration 10, loss = 0.39524472\n",
      "Iteration 11, loss = 0.37764793\n",
      "Iteration 12, loss = 0.36249830\n",
      "Iteration 13, loss = 0.35159917\n",
      "Iteration 14, loss = 0.34216487\n",
      "Iteration 15, loss = 0.32863342\n",
      "Iteration 16, loss = 0.31480932\n",
      "Iteration 17, loss = 0.30537934\n",
      "Iteration 18, loss = 0.30450011\n",
      "Iteration 19, loss = 0.29691465\n",
      "Iteration 20, loss = 0.28992664\n",
      "Iteration 21, loss = 0.29026652\n",
      "Iteration 22, loss = 0.29180970\n",
      "Iteration 23, loss = 0.29284191\n",
      "Iteration 24, loss = 0.30093757\n",
      "Iteration 25, loss = 0.30507034\n",
      "Iteration 26, loss = 0.30823210\n",
      "Iteration 27, loss = 0.30382069\n",
      "Iteration 28, loss = 0.30909916\n",
      "Iteration 29, loss = 0.31672758\n",
      "Iteration 30, loss = 0.31128809\n",
      "Iteration 31, loss = 0.29097396\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.17356999\n",
      "Iteration 2, loss = 1.42759004\n",
      "Iteration 3, loss = 1.05320079\n",
      "Iteration 4, loss = 0.76050898\n",
      "Iteration 5, loss = 0.60142430\n",
      "Iteration 6, loss = 0.52174170\n",
      "Iteration 7, loss = 0.47063386\n",
      "Iteration 8, loss = 0.42982060\n",
      "Iteration 9, loss = 0.39931206\n",
      "Iteration 10, loss = 0.37814071\n",
      "Iteration 11, loss = 0.36169557\n",
      "Iteration 12, loss = 0.34572506\n",
      "Iteration 13, loss = 0.33332567\n",
      "Iteration 14, loss = 0.32405354\n",
      "Iteration 15, loss = 0.30920591\n",
      "Iteration 16, loss = 0.29748363\n",
      "Iteration 17, loss = 0.28995817\n",
      "Iteration 18, loss = 0.29236523\n",
      "Iteration 19, loss = 0.28905750\n",
      "Iteration 20, loss = 0.28571741\n",
      "Iteration 21, loss = 0.28734779\n",
      "Iteration 22, loss = 0.28602327\n",
      "Iteration 23, loss = 0.28701903\n",
      "Iteration 24, loss = 0.28991096\n",
      "Iteration 25, loss = 0.29378358\n",
      "Iteration 26, loss = 0.30269150\n",
      "Iteration 27, loss = 0.29876862\n",
      "Iteration 28, loss = 0.29616081\n",
      "Iteration 29, loss = 0.29413689\n",
      "Iteration 30, loss = 0.29264352\n",
      "Iteration 31, loss = 0.28108257\n",
      "Iteration 32, loss = 0.26914030\n",
      "Iteration 33, loss = 0.26594946\n",
      "Iteration 34, loss = 0.26771197\n",
      "Iteration 35, loss = 0.27556327\n",
      "Iteration 36, loss = 0.28178856\n",
      "Iteration 37, loss = 0.27795691\n",
      "Iteration 38, loss = 0.28327480\n",
      "Iteration 39, loss = 0.28684191\n",
      "Iteration 40, loss = 0.28169159\n",
      "Iteration 41, loss = 0.27666791\n",
      "Iteration 42, loss = 0.27635434\n",
      "Iteration 43, loss = 0.26967735\n",
      "Iteration 44, loss = 0.27131065\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.24746494\n",
      "Iteration 2, loss = 1.56566656\n",
      "Iteration 3, loss = 1.14941542\n",
      "Iteration 4, loss = 0.86559250\n",
      "Iteration 5, loss = 0.71733990\n",
      "Iteration 6, loss = 0.59607939\n",
      "Iteration 7, loss = 0.50364719\n",
      "Iteration 8, loss = 0.44661161\n",
      "Iteration 9, loss = 0.41564100\n",
      "Iteration 10, loss = 0.38515149\n",
      "Iteration 11, loss = 0.35623320\n",
      "Iteration 12, loss = 0.33855383\n",
      "Iteration 13, loss = 0.33081730\n",
      "Iteration 14, loss = 0.31954141\n",
      "Iteration 15, loss = 0.31097913\n",
      "Iteration 16, loss = 0.30456265\n",
      "Iteration 17, loss = 0.29969908\n",
      "Iteration 18, loss = 0.29637083\n",
      "Iteration 19, loss = 0.29198652\n",
      "Iteration 20, loss = 0.29381354\n",
      "Iteration 21, loss = 0.29566725\n",
      "Iteration 22, loss = 0.29182542\n",
      "Iteration 23, loss = 0.29392149\n",
      "Iteration 24, loss = 0.29422475\n",
      "Iteration 25, loss = 0.29134310\n",
      "Iteration 26, loss = 0.28882124\n",
      "Iteration 27, loss = 0.28487516\n",
      "Iteration 28, loss = 0.28755779\n",
      "Iteration 29, loss = 0.29582709\n",
      "Iteration 30, loss = 0.29808092\n",
      "Iteration 31, loss = 0.28890772\n",
      "Iteration 32, loss = 0.28092717\n",
      "Iteration 33, loss = 0.28344528\n",
      "Iteration 34, loss = 0.28976553\n",
      "Iteration 35, loss = 0.29877152\n",
      "Iteration 36, loss = 0.31147863\n",
      "Iteration 37, loss = 0.29889610\n",
      "Iteration 38, loss = 0.28449579\n",
      "Iteration 39, loss = 0.28554108\n",
      "Iteration 40, loss = 0.29475453\n",
      "Iteration 41, loss = 0.29790001\n",
      "Iteration 42, loss = 0.28436055\n",
      "Iteration 43, loss = 0.28827867\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.25733716\n",
      "Iteration 2, loss = 1.51945216\n",
      "Iteration 3, loss = 1.10735907\n",
      "Iteration 4, loss = 0.83583007\n",
      "Iteration 5, loss = 0.70264897\n",
      "Iteration 6, loss = 0.59733213\n",
      "Iteration 7, loss = 0.51513385\n",
      "Iteration 8, loss = 0.44867368\n",
      "Iteration 9, loss = 0.39957698\n",
      "Iteration 10, loss = 0.37466913\n",
      "Iteration 11, loss = 0.36665195\n",
      "Iteration 12, loss = 0.36091920\n",
      "Iteration 13, loss = 0.34073924\n",
      "Iteration 14, loss = 0.31869783\n",
      "Iteration 15, loss = 0.30899517\n",
      "Iteration 16, loss = 0.31056346\n",
      "Iteration 17, loss = 0.30687020\n",
      "Iteration 18, loss = 0.29965837\n",
      "Iteration 19, loss = 0.29163768\n",
      "Iteration 20, loss = 0.28900690\n",
      "Iteration 21, loss = 0.29306021\n",
      "Iteration 22, loss = 0.29871981\n",
      "Iteration 23, loss = 0.30065942\n",
      "Iteration 24, loss = 0.30206704\n",
      "Iteration 25, loss = 0.29715752\n",
      "Iteration 26, loss = 0.29551584\n",
      "Iteration 27, loss = 0.29095018\n",
      "Iteration 28, loss = 0.28915147\n",
      "Iteration 29, loss = 0.28839719\n",
      "Iteration 30, loss = 0.27880359\n",
      "Iteration 31, loss = 0.27599308\n",
      "Iteration 32, loss = 0.28238398\n",
      "Iteration 33, loss = 0.29354672\n",
      "Iteration 34, loss = 0.29911941\n",
      "Iteration 35, loss = 0.29533025\n",
      "Iteration 36, loss = 0.29471266\n",
      "Iteration 37, loss = 0.28946756\n",
      "Iteration 38, loss = 0.28017163\n",
      "Iteration 39, loss = 0.28084392\n",
      "Iteration 40, loss = 0.28465736\n",
      "Iteration 41, loss = 0.29343333\n",
      "Iteration 42, loss = 0.30685153\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.27117863\n",
      "Iteration 2, loss = 1.51273123\n",
      "Iteration 3, loss = 1.11125068\n",
      "Iteration 4, loss = 0.84267987\n",
      "Iteration 5, loss = 0.67176401\n",
      "Iteration 6, loss = 0.55211377\n",
      "Iteration 7, loss = 0.48071066\n",
      "Iteration 8, loss = 0.42890580\n",
      "Iteration 9, loss = 0.38944380\n",
      "Iteration 10, loss = 0.36672173\n",
      "Iteration 11, loss = 0.35330675\n",
      "Iteration 12, loss = 0.33840718\n",
      "Iteration 13, loss = 0.32480630\n",
      "Iteration 14, loss = 0.31168896\n",
      "Iteration 15, loss = 0.30174677\n",
      "Iteration 16, loss = 0.29558546\n",
      "Iteration 17, loss = 0.28944846\n",
      "Iteration 18, loss = 0.28743972\n",
      "Iteration 19, loss = 0.28507181\n",
      "Iteration 20, loss = 0.28490688\n",
      "Iteration 21, loss = 0.28829690\n",
      "Iteration 22, loss = 0.28396622\n",
      "Iteration 23, loss = 0.28874499\n",
      "Iteration 24, loss = 0.29194637\n",
      "Iteration 25, loss = 0.29062441\n",
      "Iteration 26, loss = 0.29734258\n",
      "Iteration 27, loss = 0.29145898\n",
      "Iteration 28, loss = 0.29370140\n",
      "Iteration 29, loss = 0.29709686\n",
      "Iteration 30, loss = 0.29281391\n",
      "Iteration 31, loss = 0.29239805\n",
      "Iteration 32, loss = 0.29384852\n",
      "Iteration 33, loss = 0.28744807\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.26026351\n",
      "Iteration 2, loss = 1.52323668\n",
      "Iteration 3, loss = 1.10022433\n",
      "Iteration 4, loss = 0.82505213\n",
      "Iteration 5, loss = 0.68113480\n",
      "Iteration 6, loss = 0.57936626\n",
      "Iteration 7, loss = 0.49859614\n",
      "Iteration 8, loss = 0.43857481\n",
      "Iteration 9, loss = 0.39680136\n",
      "Iteration 10, loss = 0.37499746\n",
      "Iteration 11, loss = 0.35525211\n",
      "Iteration 12, loss = 0.33382330\n",
      "Iteration 13, loss = 0.31883413\n",
      "Iteration 14, loss = 0.30687765\n",
      "Iteration 15, loss = 0.29995672\n",
      "Iteration 16, loss = 0.29604647\n",
      "Iteration 17, loss = 0.28836436\n",
      "Iteration 18, loss = 0.28315519\n",
      "Iteration 19, loss = 0.27971530\n",
      "Iteration 20, loss = 0.27873689\n",
      "Iteration 21, loss = 0.28109014\n",
      "Iteration 22, loss = 0.27751708\n",
      "Iteration 23, loss = 0.27961035\n",
      "Iteration 24, loss = 0.27650106\n",
      "Iteration 25, loss = 0.27721193\n",
      "Iteration 26, loss = 0.28269384\n",
      "Iteration 27, loss = 0.27957034\n",
      "Iteration 28, loss = 0.28307261\n",
      "Iteration 29, loss = 0.28658272\n",
      "Iteration 30, loss = 0.28398660\n",
      "Iteration 31, loss = 0.28646732\n",
      "Iteration 32, loss = 0.28532043\n",
      "Iteration 33, loss = 0.27716570\n",
      "Iteration 34, loss = 0.27771523\n",
      "Iteration 35, loss = 0.28095722\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.25093192\n",
      "Iteration 2, loss = 1.53396411\n",
      "Iteration 3, loss = 1.10125981\n",
      "Iteration 4, loss = 0.82963351\n",
      "Iteration 5, loss = 0.68640466\n",
      "Iteration 6, loss = 0.58145889\n",
      "Iteration 7, loss = 0.50336353\n",
      "Iteration 8, loss = 0.44890374\n",
      "Iteration 9, loss = 0.40826438\n",
      "Iteration 10, loss = 0.38543338\n",
      "Iteration 11, loss = 0.36458384\n",
      "Iteration 12, loss = 0.34765538\n",
      "Iteration 13, loss = 0.33681108\n",
      "Iteration 14, loss = 0.31835076\n",
      "Iteration 15, loss = 0.31023525\n",
      "Iteration 16, loss = 0.30963559\n",
      "Iteration 17, loss = 0.30297039\n",
      "Iteration 18, loss = 0.29453606\n",
      "Iteration 19, loss = 0.28460472\n",
      "Iteration 20, loss = 0.28349575\n",
      "Iteration 21, loss = 0.28873285\n",
      "Iteration 22, loss = 0.28360296\n",
      "Iteration 23, loss = 0.28327057\n",
      "Iteration 24, loss = 0.28851426\n",
      "Iteration 25, loss = 0.29322497\n",
      "Iteration 26, loss = 0.29060389\n",
      "Iteration 27, loss = 0.28742388\n",
      "Iteration 28, loss = 0.28585566\n",
      "Iteration 29, loss = 0.28199549\n",
      "Iteration 30, loss = 0.27702408\n",
      "Iteration 31, loss = 0.28416616\n",
      "Iteration 32, loss = 0.29146252\n",
      "Iteration 33, loss = 0.29397709\n",
      "Iteration 34, loss = 0.28554716\n",
      "Iteration 35, loss = 0.28707050\n",
      "Iteration 36, loss = 0.29310908\n",
      "Iteration 37, loss = 0.28502745\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 38, loss = 0.28038324\n",
      "Iteration 39, loss = 0.28476396\n",
      "Iteration 40, loss = 0.28001872\n",
      "Iteration 41, loss = 0.28237023\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.25769935\n",
      "Iteration 2, loss = 1.54505988\n",
      "Iteration 3, loss = 1.10955479\n",
      "Iteration 4, loss = 0.83405678\n",
      "Iteration 5, loss = 0.68131468\n",
      "Iteration 6, loss = 0.57139249\n",
      "Iteration 7, loss = 0.49232819\n",
      "Iteration 8, loss = 0.43807020\n",
      "Iteration 9, loss = 0.39950458\n",
      "Iteration 10, loss = 0.37779914\n",
      "Iteration 11, loss = 0.35833109\n",
      "Iteration 12, loss = 0.33959010\n",
      "Iteration 13, loss = 0.32850100\n",
      "Iteration 14, loss = 0.31035523\n",
      "Iteration 15, loss = 0.29907109\n",
      "Iteration 16, loss = 0.30148765\n",
      "Iteration 17, loss = 0.30202350\n",
      "Iteration 18, loss = 0.29700758\n",
      "Iteration 19, loss = 0.28517499\n",
      "Iteration 20, loss = 0.28293341\n",
      "Iteration 21, loss = 0.28947356\n",
      "Iteration 22, loss = 0.28975684\n",
      "Iteration 23, loss = 0.28569656\n",
      "Iteration 24, loss = 0.27911647\n",
      "Iteration 25, loss = 0.28184763\n",
      "Iteration 26, loss = 0.28113204\n",
      "Iteration 27, loss = 0.27680950\n",
      "Iteration 28, loss = 0.28030900\n",
      "Iteration 29, loss = 0.27855949\n",
      "Iteration 30, loss = 0.27506314\n",
      "Iteration 31, loss = 0.28004024\n",
      "Iteration 32, loss = 0.28542030\n",
      "Iteration 33, loss = 0.28365063\n",
      "Iteration 34, loss = 0.27868904\n",
      "Iteration 35, loss = 0.28603252\n",
      "Iteration 36, loss = 0.28313365\n",
      "Iteration 37, loss = 0.27404512\n",
      "Iteration 38, loss = 0.27529333\n",
      "Iteration 39, loss = 0.27827318\n",
      "Iteration 40, loss = 0.26980570\n",
      "Iteration 41, loss = 0.26773575\n",
      "Iteration 42, loss = 0.28492745\n",
      "Iteration 43, loss = 0.28349909\n",
      "Iteration 44, loss = 0.27572602\n",
      "Iteration 45, loss = 0.27639675\n",
      "Iteration 46, loss = 0.27768303\n",
      "Iteration 47, loss = 0.26880857\n",
      "Iteration 48, loss = 0.26788240\n",
      "Iteration 49, loss = 0.27334333\n",
      "Iteration 50, loss = 0.27490252\n",
      "Iteration 51, loss = 0.27000093\n",
      "Iteration 52, loss = 0.26417297\n",
      "Iteration 53, loss = 0.26566010\n",
      "Iteration 54, loss = 0.27738306\n",
      "Iteration 55, loss = 0.27104810\n",
      "Iteration 56, loss = 0.28135464\n",
      "Iteration 57, loss = 0.28820066\n",
      "Iteration 58, loss = 0.28429611\n",
      "Iteration 59, loss = 0.27898158\n",
      "Iteration 60, loss = 0.28918593\n",
      "Iteration 61, loss = 0.28069647\n",
      "Iteration 62, loss = 0.27661878\n",
      "Iteration 63, loss = 0.27482639\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.24181960\n",
      "Iteration 2, loss = 1.53579560\n",
      "Iteration 3, loss = 1.09321140\n",
      "Iteration 4, loss = 0.82323326\n",
      "Iteration 5, loss = 0.67721002\n",
      "Iteration 6, loss = 0.56299367\n",
      "Iteration 7, loss = 0.48135051\n",
      "Iteration 8, loss = 0.43355287\n",
      "Iteration 9, loss = 0.39813508\n",
      "Iteration 10, loss = 0.37427650\n",
      "Iteration 11, loss = 0.35310401\n",
      "Iteration 12, loss = 0.33606637\n",
      "Iteration 13, loss = 0.32486326\n",
      "Iteration 14, loss = 0.30674928\n",
      "Iteration 15, loss = 0.29803637\n",
      "Iteration 16, loss = 0.30319150\n",
      "Iteration 17, loss = 0.29868656\n",
      "Iteration 18, loss = 0.29445987\n",
      "Iteration 19, loss = 0.28847464\n",
      "Iteration 20, loss = 0.28757398\n",
      "Iteration 21, loss = 0.29238446\n",
      "Iteration 22, loss = 0.28702854\n",
      "Iteration 23, loss = 0.27682605\n",
      "Iteration 24, loss = 0.27309388\n",
      "Iteration 25, loss = 0.28162840\n",
      "Iteration 26, loss = 0.28784823\n",
      "Iteration 27, loss = 0.28226299\n",
      "Iteration 28, loss = 0.28251542\n",
      "Iteration 29, loss = 0.27906911\n",
      "Iteration 30, loss = 0.27416766\n",
      "Iteration 31, loss = 0.28050951\n",
      "Iteration 32, loss = 0.28355573\n",
      "Iteration 33, loss = 0.28590109\n",
      "Iteration 34, loss = 0.27625012\n",
      "Iteration 35, loss = 0.28549137\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.23658005\n",
      "Iteration 2, loss = 1.55370337\n",
      "Iteration 3, loss = 1.10595903\n",
      "Iteration 4, loss = 0.84037660\n",
      "Iteration 5, loss = 0.69955363\n",
      "Iteration 6, loss = 0.58802809\n",
      "Iteration 7, loss = 0.50728451\n",
      "Iteration 8, loss = 0.45036417\n",
      "Iteration 9, loss = 0.41036042\n",
      "Iteration 10, loss = 0.39010748\n",
      "Iteration 11, loss = 0.37340346\n",
      "Iteration 12, loss = 0.35502449\n",
      "Iteration 13, loss = 0.33960849\n",
      "Iteration 14, loss = 0.31888626\n",
      "Iteration 15, loss = 0.31567070\n",
      "Iteration 16, loss = 0.32229627\n",
      "Iteration 17, loss = 0.30799400\n",
      "Iteration 18, loss = 0.30014135\n",
      "Iteration 19, loss = 0.30099937\n",
      "Iteration 20, loss = 0.30518014\n",
      "Iteration 21, loss = 0.30576874\n",
      "Iteration 22, loss = 0.29316388\n",
      "Iteration 23, loss = 0.29269716\n",
      "Iteration 24, loss = 0.29363973\n",
      "Iteration 25, loss = 0.30161613\n",
      "Iteration 26, loss = 0.29396596\n",
      "Iteration 27, loss = 0.29102475\n",
      "Iteration 28, loss = 0.30399966\n",
      "Iteration 29, loss = 0.30435778\n",
      "Iteration 30, loss = 0.29263705\n",
      "Iteration 31, loss = 0.29293911\n",
      "Iteration 32, loss = 0.30581930\n",
      "Iteration 33, loss = 0.31559920\n",
      "Iteration 34, loss = 0.29455636\n",
      "Iteration 35, loss = 0.30476291\n",
      "Iteration 36, loss = 0.32134747\n",
      "Iteration 37, loss = 0.30481041\n",
      "Iteration 38, loss = 0.29799001\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.23795014\n",
      "Iteration 2, loss = 1.55495487\n",
      "Iteration 3, loss = 1.11950118\n",
      "Iteration 4, loss = 0.83432410\n",
      "Iteration 5, loss = 0.69125015\n",
      "Iteration 6, loss = 0.58807769\n",
      "Iteration 7, loss = 0.50757057\n",
      "Iteration 8, loss = 0.45006203\n",
      "Iteration 9, loss = 0.41229954\n",
      "Iteration 10, loss = 0.39047019\n",
      "Iteration 11, loss = 0.36795578\n",
      "Iteration 12, loss = 0.34900092\n",
      "Iteration 13, loss = 0.33839405\n",
      "Iteration 14, loss = 0.32606715\n",
      "Iteration 15, loss = 0.32462175\n",
      "Iteration 16, loss = 0.32579643\n",
      "Iteration 17, loss = 0.30714214\n",
      "Iteration 18, loss = 0.29701883\n",
      "Iteration 19, loss = 0.29828378\n",
      "Iteration 20, loss = 0.30120034\n",
      "Iteration 21, loss = 0.30076862\n",
      "Iteration 22, loss = 0.28874193\n",
      "Iteration 23, loss = 0.28389294\n",
      "Iteration 24, loss = 0.29033487\n",
      "Iteration 25, loss = 0.30636461\n",
      "Iteration 26, loss = 0.29820347\n",
      "Iteration 27, loss = 0.28955620\n",
      "Iteration 28, loss = 0.29751459\n",
      "Iteration 29, loss = 0.29561807\n",
      "Iteration 30, loss = 0.28817584\n",
      "Iteration 31, loss = 0.29266913\n",
      "Iteration 32, loss = 0.29716511\n",
      "Iteration 33, loss = 0.30221856\n",
      "Iteration 34, loss = 0.28298927\n",
      "Iteration 35, loss = 0.29424383\n",
      "Iteration 36, loss = 0.31267731\n",
      "Iteration 37, loss = 0.30039879\n",
      "Iteration 38, loss = 0.29441182\n",
      "Iteration 39, loss = 0.29871165\n",
      "Iteration 40, loss = 0.28666215\n",
      "Iteration 41, loss = 0.27882352\n",
      "Iteration 42, loss = 0.28779819\n",
      "Iteration 43, loss = 0.28824834\n",
      "Iteration 44, loss = 0.28231713\n",
      "Iteration 45, loss = 0.28149696\n",
      "Iteration 46, loss = 0.28024258\n",
      "Iteration 47, loss = 0.27082341\n",
      "Iteration 48, loss = 0.27733827\n",
      "Iteration 49, loss = 0.28932459\n",
      "Iteration 50, loss = 0.28799218\n",
      "Iteration 51, loss = 0.27836801\n",
      "Iteration 52, loss = 0.27485527\n",
      "Iteration 53, loss = 0.28001939\n",
      "Iteration 54, loss = 0.29143262\n",
      "Iteration 55, loss = 0.29283830\n",
      "Iteration 56, loss = 0.29432844\n",
      "Iteration 57, loss = 0.28667004\n",
      "Iteration 58, loss = 0.28278214\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.23717621\n",
      "Iteration 2, loss = 1.55046066\n",
      "Iteration 3, loss = 1.09887459\n",
      "Iteration 4, loss = 0.82458005\n",
      "Iteration 5, loss = 0.68422745\n",
      "Iteration 6, loss = 0.57376478\n",
      "Iteration 7, loss = 0.49098222\n",
      "Iteration 8, loss = 0.43727031\n",
      "Iteration 9, loss = 0.40005374\n",
      "Iteration 10, loss = 0.38101027\n",
      "Iteration 11, loss = 0.35910114\n",
      "Iteration 12, loss = 0.34171727\n",
      "Iteration 13, loss = 0.32955642\n",
      "Iteration 14, loss = 0.31393049\n",
      "Iteration 15, loss = 0.30830591\n",
      "Iteration 16, loss = 0.30835010\n",
      "Iteration 17, loss = 0.29538568\n",
      "Iteration 18, loss = 0.29089494\n",
      "Iteration 19, loss = 0.29004302\n",
      "Iteration 20, loss = 0.29197070\n",
      "Iteration 21, loss = 0.28902862\n",
      "Iteration 22, loss = 0.28172545\n",
      "Iteration 23, loss = 0.28198729\n",
      "Iteration 24, loss = 0.28436010\n",
      "Iteration 25, loss = 0.29521301\n",
      "Iteration 26, loss = 0.29317252\n",
      "Iteration 27, loss = 0.28490980\n",
      "Iteration 28, loss = 0.28430988\n",
      "Iteration 29, loss = 0.27817595\n",
      "Iteration 30, loss = 0.27502713\n",
      "Iteration 31, loss = 0.28539565\n",
      "Iteration 32, loss = 0.29485742\n",
      "Iteration 33, loss = 0.29672535\n",
      "Iteration 34, loss = 0.28091691\n",
      "Iteration 35, loss = 0.28932433\n",
      "Iteration 36, loss = 0.30233461\n",
      "Iteration 37, loss = 0.28749582\n",
      "Iteration 38, loss = 0.28010904\n",
      "Iteration 39, loss = 0.28316052\n",
      "Iteration 40, loss = 0.28525900\n",
      "Iteration 41, loss = 0.27958870\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.13911608\n",
      "Iteration 2, loss = 1.37546350\n",
      "Iteration 3, loss = 0.98953034\n",
      "Iteration 4, loss = 0.71089545\n",
      "Iteration 5, loss = 0.56992214\n",
      "Iteration 6, loss = 0.50170556\n",
      "Iteration 7, loss = 0.46292439\n",
      "Iteration 8, loss = 0.43434812\n",
      "Iteration 9, loss = 0.40565203\n",
      "Iteration 10, loss = 0.37787850\n",
      "Iteration 11, loss = 0.35443916\n",
      "Iteration 12, loss = 0.33968590\n",
      "Iteration 13, loss = 0.33285609\n",
      "Iteration 14, loss = 0.32604586\n",
      "Iteration 15, loss = 0.31359897\n",
      "Iteration 16, loss = 0.30123493\n",
      "Iteration 17, loss = 0.29710548\n",
      "Iteration 18, loss = 0.29748213\n",
      "Iteration 19, loss = 0.31092617\n",
      "Iteration 20, loss = 0.31714691\n",
      "Iteration 21, loss = 0.30462807\n",
      "Iteration 22, loss = 0.30289986\n",
      "Iteration 23, loss = 0.30263617\n",
      "Iteration 24, loss = 0.29225733\n",
      "Iteration 25, loss = 0.29097960\n",
      "Iteration 26, loss = 0.30400796\n",
      "Iteration 27, loss = 0.30939247\n",
      "Iteration 28, loss = 0.29770293\n",
      "Iteration 29, loss = 0.28707264\n",
      "Iteration 30, loss = 0.29613017\n",
      "Iteration 31, loss = 0.31708977\n",
      "Iteration 32, loss = 0.30899990\n",
      "Iteration 33, loss = 0.30836254\n",
      "Iteration 34, loss = 0.31331070\n",
      "Iteration 35, loss = 0.30404686\n",
      "Iteration 36, loss = 0.28862879\n",
      "Iteration 37, loss = 0.28542414\n",
      "Iteration 38, loss = 0.29387623\n",
      "Iteration 39, loss = 0.29345639\n",
      "Iteration 40, loss = 0.30833986\n",
      "Iteration 41, loss = 0.30426742\n",
      "Iteration 42, loss = 0.30005897\n",
      "Iteration 43, loss = 0.30647740\n",
      "Iteration 44, loss = 0.30685530\n",
      "Iteration 45, loss = 0.30227186\n",
      "Iteration 46, loss = 0.29883586\n",
      "Iteration 47, loss = 0.29641849\n",
      "Iteration 48, loss = 0.28914290\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.14478173\n",
      "Iteration 2, loss = 1.38290986\n",
      "Iteration 3, loss = 0.99882062\n",
      "Iteration 4, loss = 0.72876610\n",
      "Iteration 5, loss = 0.61055989\n",
      "Iteration 6, loss = 0.54629977\n",
      "Iteration 7, loss = 0.48543835\n",
      "Iteration 8, loss = 0.43009987\n",
      "Iteration 9, loss = 0.39107825\n",
      "Iteration 10, loss = 0.37524039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11, loss = 0.36744664\n",
      "Iteration 12, loss = 0.35958557\n",
      "Iteration 13, loss = 0.34694144\n",
      "Iteration 14, loss = 0.32955424\n",
      "Iteration 15, loss = 0.31523449\n",
      "Iteration 16, loss = 0.31215853\n",
      "Iteration 17, loss = 0.31450895\n",
      "Iteration 18, loss = 0.31172666\n",
      "Iteration 19, loss = 0.30496039\n",
      "Iteration 20, loss = 0.29783806\n",
      "Iteration 21, loss = 0.29092638\n",
      "Iteration 22, loss = 0.29092348\n",
      "Iteration 23, loss = 0.29085867\n",
      "Iteration 24, loss = 0.28618483\n",
      "Iteration 25, loss = 0.28348772\n",
      "Iteration 26, loss = 0.28061903\n",
      "Iteration 27, loss = 0.27691336\n",
      "Iteration 28, loss = 0.28244655\n",
      "Iteration 29, loss = 0.28816148\n",
      "Iteration 30, loss = 0.27838607\n",
      "Iteration 31, loss = 0.28739596\n",
      "Iteration 32, loss = 0.30006949\n",
      "Iteration 33, loss = 0.30008680\n",
      "Iteration 34, loss = 0.29309054\n",
      "Iteration 35, loss = 0.28895416\n",
      "Iteration 36, loss = 0.29244138\n",
      "Iteration 37, loss = 0.30305879\n",
      "Iteration 38, loss = 0.32155362\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.14106389\n",
      "Iteration 2, loss = 1.38074840\n",
      "Iteration 3, loss = 0.99742267\n",
      "Iteration 4, loss = 0.73184160\n",
      "Iteration 5, loss = 0.59799275\n",
      "Iteration 6, loss = 0.51879565\n",
      "Iteration 7, loss = 0.45665610\n",
      "Iteration 8, loss = 0.40857582\n",
      "Iteration 9, loss = 0.37258412\n",
      "Iteration 10, loss = 0.36009516\n",
      "Iteration 11, loss = 0.35344984\n",
      "Iteration 12, loss = 0.34278785\n",
      "Iteration 13, loss = 0.33090781\n",
      "Iteration 14, loss = 0.31969839\n",
      "Iteration 15, loss = 0.31260027\n",
      "Iteration 16, loss = 0.30633494\n",
      "Iteration 17, loss = 0.30417468\n",
      "Iteration 18, loss = 0.30412255\n",
      "Iteration 19, loss = 0.30091155\n",
      "Iteration 20, loss = 0.29320021\n",
      "Iteration 21, loss = 0.28541928\n",
      "Iteration 22, loss = 0.28781562\n",
      "Iteration 23, loss = 0.29066244\n",
      "Iteration 24, loss = 0.28681772\n",
      "Iteration 25, loss = 0.28097100\n",
      "Iteration 26, loss = 0.27524095\n",
      "Iteration 27, loss = 0.27266203\n",
      "Iteration 28, loss = 0.27201784\n",
      "Iteration 29, loss = 0.27661997\n",
      "Iteration 30, loss = 0.28024157\n",
      "Iteration 31, loss = 0.29178552\n",
      "Iteration 32, loss = 0.29025080\n",
      "Iteration 33, loss = 0.28663942\n",
      "Iteration 34, loss = 0.29202062\n",
      "Iteration 35, loss = 0.28509671\n",
      "Iteration 36, loss = 0.28980876\n",
      "Iteration 37, loss = 0.29021878\n",
      "Iteration 38, loss = 0.29571657\n",
      "Iteration 39, loss = 0.29076454\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.13977475\n",
      "Iteration 2, loss = 1.36044252\n",
      "Iteration 3, loss = 0.97535655\n",
      "Iteration 4, loss = 0.70057822\n",
      "Iteration 5, loss = 0.57679094\n",
      "Iteration 6, loss = 0.51479843\n",
      "Iteration 7, loss = 0.45698025\n",
      "Iteration 8, loss = 0.40670274\n",
      "Iteration 9, loss = 0.37002266\n",
      "Iteration 10, loss = 0.35331941\n",
      "Iteration 11, loss = 0.34416325\n",
      "Iteration 12, loss = 0.33476194\n",
      "Iteration 13, loss = 0.32791823\n",
      "Iteration 14, loss = 0.31501875\n",
      "Iteration 15, loss = 0.29871144\n",
      "Iteration 16, loss = 0.29251302\n",
      "Iteration 17, loss = 0.29922341\n",
      "Iteration 18, loss = 0.30700659\n",
      "Iteration 19, loss = 0.30373583\n",
      "Iteration 20, loss = 0.29070552\n",
      "Iteration 21, loss = 0.27709120\n",
      "Iteration 22, loss = 0.27776389\n",
      "Iteration 23, loss = 0.28910796\n",
      "Iteration 24, loss = 0.28698526\n",
      "Iteration 25, loss = 0.27530398\n",
      "Iteration 26, loss = 0.27086414\n",
      "Iteration 27, loss = 0.27188334\n",
      "Iteration 28, loss = 0.27420062\n",
      "Iteration 29, loss = 0.26988747\n",
      "Iteration 30, loss = 0.26376234\n",
      "Iteration 31, loss = 0.27932402\n",
      "Iteration 32, loss = 0.28833705\n",
      "Iteration 33, loss = 0.28009793\n",
      "Iteration 34, loss = 0.27641524\n",
      "Iteration 35, loss = 0.27495534\n",
      "Iteration 36, loss = 0.29000237\n",
      "Iteration 37, loss = 0.29689794\n",
      "Iteration 38, loss = 0.29797133\n",
      "Iteration 39, loss = 0.28431924\n",
      "Iteration 40, loss = 0.28181382\n",
      "Iteration 41, loss = 0.28805050\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.14298109\n",
      "Iteration 2, loss = 1.38427154\n",
      "Iteration 3, loss = 1.00003238\n",
      "Iteration 4, loss = 0.72705094\n",
      "Iteration 5, loss = 0.59833712\n",
      "Iteration 6, loss = 0.52785571\n",
      "Iteration 7, loss = 0.46204295\n",
      "Iteration 8, loss = 0.40788289\n",
      "Iteration 9, loss = 0.37370606\n",
      "Iteration 10, loss = 0.35667881\n",
      "Iteration 11, loss = 0.34393888\n",
      "Iteration 12, loss = 0.33247586\n",
      "Iteration 13, loss = 0.32251740\n",
      "Iteration 14, loss = 0.31278667\n",
      "Iteration 15, loss = 0.30332892\n",
      "Iteration 16, loss = 0.30050487\n",
      "Iteration 17, loss = 0.31049964\n",
      "Iteration 18, loss = 0.31246042\n",
      "Iteration 19, loss = 0.30268485\n",
      "Iteration 20, loss = 0.29036456\n",
      "Iteration 21, loss = 0.28081486\n",
      "Iteration 22, loss = 0.28635396\n",
      "Iteration 23, loss = 0.29536346\n",
      "Iteration 24, loss = 0.28564020\n",
      "Iteration 25, loss = 0.26761701\n",
      "Iteration 26, loss = 0.27367389\n",
      "Iteration 27, loss = 0.28110210\n",
      "Iteration 28, loss = 0.27405677\n",
      "Iteration 29, loss = 0.27373433\n",
      "Iteration 30, loss = 0.28197478\n",
      "Iteration 31, loss = 0.29597035\n",
      "Iteration 32, loss = 0.28951863\n",
      "Iteration 33, loss = 0.27767877\n",
      "Iteration 34, loss = 0.27542079\n",
      "Iteration 35, loss = 0.28575602\n",
      "Iteration 36, loss = 0.30426044\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.15022519\n",
      "Iteration 2, loss = 1.38047176\n",
      "Iteration 3, loss = 1.00339289\n",
      "Iteration 4, loss = 0.72138211\n",
      "Iteration 5, loss = 0.57947304\n",
      "Iteration 6, loss = 0.51331723\n",
      "Iteration 7, loss = 0.45596566\n",
      "Iteration 8, loss = 0.40533845\n",
      "Iteration 9, loss = 0.37080679\n",
      "Iteration 10, loss = 0.34486509\n",
      "Iteration 11, loss = 0.33116603\n",
      "Iteration 12, loss = 0.32048588\n",
      "Iteration 13, loss = 0.31481199\n",
      "Iteration 14, loss = 0.30672228\n",
      "Iteration 15, loss = 0.29509136\n",
      "Iteration 16, loss = 0.28279362\n",
      "Iteration 17, loss = 0.28472625\n",
      "Iteration 18, loss = 0.29309906\n",
      "Iteration 19, loss = 0.29350571\n",
      "Iteration 20, loss = 0.28808763\n",
      "Iteration 21, loss = 0.27720209\n",
      "Iteration 22, loss = 0.27664247\n",
      "Iteration 23, loss = 0.28027972\n",
      "Iteration 24, loss = 0.27520708\n",
      "Iteration 25, loss = 0.26255119\n",
      "Iteration 26, loss = 0.27278907\n",
      "Iteration 27, loss = 0.28937312\n",
      "Iteration 28, loss = 0.29181244\n",
      "Iteration 29, loss = 0.28323337\n",
      "Iteration 30, loss = 0.27615295\n",
      "Iteration 31, loss = 0.28025017\n",
      "Iteration 32, loss = 0.28818184\n",
      "Iteration 33, loss = 0.29152959\n",
      "Iteration 34, loss = 0.28716089\n",
      "Iteration 35, loss = 0.28376037\n",
      "Iteration 36, loss = 0.29172493\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.13734758\n",
      "Iteration 2, loss = 1.38364007\n",
      "Iteration 3, loss = 0.99340822\n",
      "Iteration 4, loss = 0.71452505\n",
      "Iteration 5, loss = 0.58354509\n",
      "Iteration 6, loss = 0.51325551\n",
      "Iteration 7, loss = 0.44981663\n",
      "Iteration 8, loss = 0.40017392\n",
      "Iteration 9, loss = 0.36799078\n",
      "Iteration 10, loss = 0.34316708\n",
      "Iteration 11, loss = 0.32928987\n",
      "Iteration 12, loss = 0.31658014\n",
      "Iteration 13, loss = 0.30802528\n",
      "Iteration 14, loss = 0.29920113\n",
      "Iteration 15, loss = 0.29124775\n",
      "Iteration 16, loss = 0.28971363\n",
      "Iteration 17, loss = 0.28749358\n",
      "Iteration 18, loss = 0.29036884\n",
      "Iteration 19, loss = 0.29468681\n",
      "Iteration 20, loss = 0.29364157\n",
      "Iteration 21, loss = 0.28072285\n",
      "Iteration 22, loss = 0.27713316\n",
      "Iteration 23, loss = 0.27547842\n",
      "Iteration 24, loss = 0.27228026\n",
      "Iteration 25, loss = 0.26713765\n",
      "Iteration 26, loss = 0.27691490\n",
      "Iteration 27, loss = 0.29050883\n",
      "Iteration 28, loss = 0.29017071\n",
      "Iteration 29, loss = 0.28559123\n",
      "Iteration 30, loss = 0.29444345\n",
      "Iteration 31, loss = 0.30186320\n",
      "Iteration 32, loss = 0.29792642\n",
      "Iteration 33, loss = 0.29491070\n",
      "Iteration 34, loss = 0.31063380\n",
      "Iteration 35, loss = 0.30157239\n",
      "Iteration 36, loss = 0.28487950\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.13928988\n",
      "Iteration 2, loss = 1.39877103\n",
      "Iteration 3, loss = 1.02208920\n",
      "Iteration 4, loss = 0.73709216\n",
      "Iteration 5, loss = 0.60739701\n",
      "Iteration 6, loss = 0.53878042\n",
      "Iteration 7, loss = 0.47570026\n",
      "Iteration 8, loss = 0.42313229\n",
      "Iteration 9, loss = 0.38676090\n",
      "Iteration 10, loss = 0.36019776\n",
      "Iteration 11, loss = 0.34820463\n",
      "Iteration 12, loss = 0.33826699\n",
      "Iteration 13, loss = 0.32992875\n",
      "Iteration 14, loss = 0.31664401\n",
      "Iteration 15, loss = 0.30215366\n",
      "Iteration 16, loss = 0.29919469\n",
      "Iteration 17, loss = 0.30047397\n",
      "Iteration 18, loss = 0.30876728\n",
      "Iteration 19, loss = 0.31167617\n",
      "Iteration 20, loss = 0.30562793\n",
      "Iteration 21, loss = 0.29029374\n",
      "Iteration 22, loss = 0.28740175\n",
      "Iteration 23, loss = 0.29098315\n",
      "Iteration 24, loss = 0.28923847\n",
      "Iteration 25, loss = 0.27979159\n",
      "Iteration 26, loss = 0.28574644\n",
      "Iteration 27, loss = 0.30116759\n",
      "Iteration 28, loss = 0.30470106\n",
      "Iteration 29, loss = 0.30296551\n",
      "Iteration 30, loss = 0.30480863\n",
      "Iteration 31, loss = 0.30681653\n",
      "Iteration 32, loss = 0.30597792\n",
      "Iteration 33, loss = 0.30568152\n",
      "Iteration 34, loss = 0.31271090\n",
      "Iteration 35, loss = 0.29985343\n",
      "Iteration 36, loss = 0.29298507\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.14100334\n",
      "Iteration 2, loss = 1.38139260\n",
      "Iteration 3, loss = 0.99792292\n",
      "Iteration 4, loss = 0.71740478\n",
      "Iteration 5, loss = 0.59238299\n",
      "Iteration 6, loss = 0.52851140\n",
      "Iteration 7, loss = 0.47038032\n",
      "Iteration 8, loss = 0.42219941\n",
      "Iteration 9, loss = 0.38349951\n",
      "Iteration 10, loss = 0.35868315\n",
      "Iteration 11, loss = 0.34852411\n",
      "Iteration 12, loss = 0.33651083\n",
      "Iteration 13, loss = 0.32947169\n",
      "Iteration 14, loss = 0.31687143\n",
      "Iteration 15, loss = 0.30281525\n",
      "Iteration 16, loss = 0.29797557\n",
      "Iteration 17, loss = 0.29329091\n",
      "Iteration 18, loss = 0.29773476\n",
      "Iteration 19, loss = 0.30374147\n",
      "Iteration 20, loss = 0.30310599\n",
      "Iteration 21, loss = 0.28749328\n",
      "Iteration 22, loss = 0.27822904\n",
      "Iteration 23, loss = 0.27896374\n",
      "Iteration 24, loss = 0.28191080\n",
      "Iteration 25, loss = 0.27849323\n",
      "Iteration 26, loss = 0.28342256\n",
      "Iteration 27, loss = 0.29640497\n",
      "Iteration 28, loss = 0.29861945\n",
      "Iteration 29, loss = 0.29554784\n",
      "Iteration 30, loss = 0.30175495\n",
      "Iteration 31, loss = 0.30274101\n",
      "Iteration 32, loss = 0.30657237\n",
      "Iteration 33, loss = 0.31296283\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.12755344\n",
      "Iteration 2, loss = 1.38097092\n",
      "Iteration 3, loss = 0.99394921\n",
      "Iteration 4, loss = 0.70978399\n",
      "Iteration 5, loss = 0.57829292\n",
      "Iteration 6, loss = 0.51290740\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7, loss = 0.45718039\n",
      "Iteration 8, loss = 0.41361989\n",
      "Iteration 9, loss = 0.37645562\n",
      "Iteration 10, loss = 0.34856215\n",
      "Iteration 11, loss = 0.33606504\n",
      "Iteration 12, loss = 0.32245937\n",
      "Iteration 13, loss = 0.31340854\n",
      "Iteration 14, loss = 0.30183627\n",
      "Iteration 15, loss = 0.28915601\n",
      "Iteration 16, loss = 0.29045020\n",
      "Iteration 17, loss = 0.28538377\n",
      "Iteration 18, loss = 0.28748887\n",
      "Iteration 19, loss = 0.29345916\n",
      "Iteration 20, loss = 0.29500523\n",
      "Iteration 21, loss = 0.28249380\n",
      "Iteration 22, loss = 0.27467229\n",
      "Iteration 23, loss = 0.27314396\n",
      "Iteration 24, loss = 0.27449872\n",
      "Iteration 25, loss = 0.27199123\n",
      "Iteration 26, loss = 0.28185478\n",
      "Iteration 27, loss = 0.29174565\n",
      "Iteration 28, loss = 0.28519623\n",
      "Iteration 29, loss = 0.28434394\n",
      "Iteration 30, loss = 0.29062509\n",
      "Iteration 31, loss = 0.28986298\n",
      "Iteration 32, loss = 0.29228146\n",
      "Iteration 33, loss = 0.29411463\n",
      "Iteration 34, loss = 0.30058013\n",
      "Iteration 35, loss = 0.29009931\n",
      "Iteration 36, loss = 0.28138915\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.14745509\n",
      "Iteration 2, loss = 1.34628893\n",
      "Iteration 3, loss = 0.95570932\n",
      "Iteration 4, loss = 0.71142885\n",
      "Iteration 5, loss = 0.59053046\n",
      "Iteration 6, loss = 0.50740081\n",
      "Iteration 7, loss = 0.44211976\n",
      "Iteration 8, loss = 0.39837616\n",
      "Iteration 9, loss = 0.37652013\n",
      "Iteration 10, loss = 0.36240659\n",
      "Iteration 11, loss = 0.34545230\n",
      "Iteration 12, loss = 0.33284855\n",
      "Iteration 13, loss = 0.31967251\n",
      "Iteration 14, loss = 0.30992925\n",
      "Iteration 15, loss = 0.29877925\n",
      "Iteration 16, loss = 0.30401224\n",
      "Iteration 17, loss = 0.31560537\n",
      "Iteration 18, loss = 0.31395314\n",
      "Iteration 19, loss = 0.30882545\n",
      "Iteration 20, loss = 0.30903554\n",
      "Iteration 21, loss = 0.29904821\n",
      "Iteration 22, loss = 0.28993526\n",
      "Iteration 23, loss = 0.29613403\n",
      "Iteration 24, loss = 0.30199731\n",
      "Iteration 25, loss = 0.30351698\n",
      "Iteration 26, loss = 0.29661205\n",
      "Iteration 27, loss = 0.29144964\n",
      "Iteration 28, loss = 0.29216277\n",
      "Iteration 29, loss = 0.29885088\n",
      "Iteration 30, loss = 0.30329417\n",
      "Iteration 31, loss = 0.30044928\n",
      "Iteration 32, loss = 0.29570433\n",
      "Iteration 33, loss = 0.29816180\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.12436054\n",
      "Iteration 2, loss = 1.39394415\n",
      "Iteration 3, loss = 1.02271597\n",
      "Iteration 4, loss = 0.74036924\n",
      "Iteration 5, loss = 0.60437947\n",
      "Iteration 6, loss = 0.53349297\n",
      "Iteration 7, loss = 0.47439610\n",
      "Iteration 8, loss = 0.41875109\n",
      "Iteration 9, loss = 0.38029556\n",
      "Iteration 10, loss = 0.35773907\n",
      "Iteration 11, loss = 0.34817616\n",
      "Iteration 12, loss = 0.33581684\n",
      "Iteration 13, loss = 0.32377347\n",
      "Iteration 14, loss = 0.31140528\n",
      "Iteration 15, loss = 0.29469563\n",
      "Iteration 16, loss = 0.29097494\n",
      "Iteration 17, loss = 0.30033761\n",
      "Iteration 18, loss = 0.30938124\n",
      "Iteration 19, loss = 0.29628989\n",
      "Iteration 20, loss = 0.29378036\n",
      "Iteration 21, loss = 0.29846325\n",
      "Iteration 22, loss = 0.31225875\n",
      "Iteration 23, loss = 0.31133981\n",
      "Iteration 24, loss = 0.30335076\n",
      "Iteration 25, loss = 0.29480587\n",
      "Iteration 26, loss = 0.29132678\n",
      "Iteration 27, loss = 0.29024063\n",
      "Iteration 28, loss = 0.29002623\n",
      "Iteration 29, loss = 0.30721859\n",
      "Iteration 30, loss = 0.31312513\n",
      "Iteration 31, loss = 0.30274020\n",
      "Iteration 32, loss = 0.29234092\n",
      "Iteration 33, loss = 0.30031900\n",
      "Iteration 34, loss = 0.31255453\n",
      "Iteration 35, loss = 0.29984467\n",
      "Iteration 36, loss = 0.27852306\n",
      "Iteration 37, loss = 0.26537210\n",
      "Iteration 38, loss = 0.27895282\n",
      "Iteration 39, loss = 0.29055615\n",
      "Iteration 40, loss = 0.28365744\n",
      "Iteration 41, loss = 0.29028790\n",
      "Iteration 42, loss = 0.30524090\n",
      "Iteration 43, loss = 0.31710140\n",
      "Iteration 44, loss = 0.29913652\n",
      "Iteration 45, loss = 0.29834618\n",
      "Iteration 46, loss = 0.30590786\n",
      "Iteration 47, loss = 0.29890006\n",
      "Iteration 48, loss = 0.29321127\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.11443703\n",
      "Iteration 2, loss = 1.42862214\n",
      "Iteration 3, loss = 1.05061937\n",
      "Iteration 4, loss = 0.75059969\n",
      "Iteration 5, loss = 0.59758396\n",
      "Iteration 6, loss = 0.52445774\n",
      "Iteration 7, loss = 0.47229723\n",
      "Iteration 8, loss = 0.42557836\n",
      "Iteration 9, loss = 0.38955102\n",
      "Iteration 10, loss = 0.36468616\n",
      "Iteration 11, loss = 0.34835476\n",
      "Iteration 12, loss = 0.33235566\n",
      "Iteration 13, loss = 0.31939009\n",
      "Iteration 14, loss = 0.31253408\n",
      "Iteration 15, loss = 0.30556811\n",
      "Iteration 16, loss = 0.29858644\n",
      "Iteration 17, loss = 0.30213855\n",
      "Iteration 18, loss = 0.31086981\n",
      "Iteration 19, loss = 0.29118966\n",
      "Iteration 20, loss = 0.28470230\n",
      "Iteration 21, loss = 0.28562095\n",
      "Iteration 22, loss = 0.29721029\n",
      "Iteration 23, loss = 0.29741102\n",
      "Iteration 24, loss = 0.29243477\n",
      "Iteration 25, loss = 0.29404398\n",
      "Iteration 26, loss = 0.29765046\n",
      "Iteration 27, loss = 0.29830861\n",
      "Iteration 28, loss = 0.29307300\n",
      "Iteration 29, loss = 0.30372275\n",
      "Iteration 30, loss = 0.31814527\n",
      "Iteration 31, loss = 0.31789436\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.15707546\n",
      "Iteration 2, loss = 1.37853683\n",
      "Iteration 3, loss = 1.03094220\n",
      "Iteration 4, loss = 0.75289569\n",
      "Iteration 5, loss = 0.59889495\n",
      "Iteration 6, loss = 0.51979224\n",
      "Iteration 7, loss = 0.47231234\n",
      "Iteration 8, loss = 0.43267556\n",
      "Iteration 9, loss = 0.39744148\n",
      "Iteration 10, loss = 0.36929867\n",
      "Iteration 11, loss = 0.35019013\n",
      "Iteration 12, loss = 0.33429072\n",
      "Iteration 13, loss = 0.32113663\n",
      "Iteration 14, loss = 0.30984910\n",
      "Iteration 15, loss = 0.30303135\n",
      "Iteration 16, loss = 0.29904395\n",
      "Iteration 17, loss = 0.30073825\n",
      "Iteration 18, loss = 0.30474455\n",
      "Iteration 19, loss = 0.28684859\n",
      "Iteration 20, loss = 0.28395390\n",
      "Iteration 21, loss = 0.28838782\n",
      "Iteration 22, loss = 0.30109895\n",
      "Iteration 23, loss = 0.29476347\n",
      "Iteration 24, loss = 0.28710615\n",
      "Iteration 25, loss = 0.28959335\n",
      "Iteration 26, loss = 0.29275962\n",
      "Iteration 27, loss = 0.29377081\n",
      "Iteration 28, loss = 0.29236977\n",
      "Iteration 29, loss = 0.29096203\n",
      "Iteration 30, loss = 0.29070616\n",
      "Iteration 31, loss = 0.28933913\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.12909611\n",
      "Iteration 2, loss = 1.40322944\n",
      "Iteration 3, loss = 1.03577944\n",
      "Iteration 4, loss = 0.76123412\n",
      "Iteration 5, loss = 0.61735878\n",
      "Iteration 6, loss = 0.54078170\n",
      "Iteration 7, loss = 0.48867537\n",
      "Iteration 8, loss = 0.43989186\n",
      "Iteration 9, loss = 0.40500066\n",
      "Iteration 10, loss = 0.37651977\n",
      "Iteration 11, loss = 0.35602984\n",
      "Iteration 12, loss = 0.33963001\n",
      "Iteration 13, loss = 0.32851395\n",
      "Iteration 14, loss = 0.32106529\n",
      "Iteration 15, loss = 0.31355860\n",
      "Iteration 16, loss = 0.30084959\n",
      "Iteration 17, loss = 0.29944029\n",
      "Iteration 18, loss = 0.31239404\n",
      "Iteration 19, loss = 0.30421575\n",
      "Iteration 20, loss = 0.28710773\n",
      "Iteration 21, loss = 0.27648308\n",
      "Iteration 22, loss = 0.28852793\n",
      "Iteration 23, loss = 0.29397182\n",
      "Iteration 24, loss = 0.29213564\n",
      "Iteration 25, loss = 0.29112669\n",
      "Iteration 26, loss = 0.29539292\n",
      "Iteration 27, loss = 0.29502831\n",
      "Iteration 28, loss = 0.29265845\n",
      "Iteration 29, loss = 0.28573404\n",
      "Iteration 30, loss = 0.29583771\n",
      "Iteration 31, loss = 0.29557051\n",
      "Iteration 32, loss = 0.29449310\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.15242906\n",
      "Iteration 2, loss = 1.37881913\n",
      "Iteration 3, loss = 1.00281080\n",
      "Iteration 4, loss = 0.73288636\n",
      "Iteration 5, loss = 0.58897033\n",
      "Iteration 6, loss = 0.50927442\n",
      "Iteration 7, loss = 0.46007948\n",
      "Iteration 8, loss = 0.41455318\n",
      "Iteration 9, loss = 0.37929203\n",
      "Iteration 10, loss = 0.35387110\n",
      "Iteration 11, loss = 0.33547767\n",
      "Iteration 12, loss = 0.31929797\n",
      "Iteration 13, loss = 0.30557910\n",
      "Iteration 14, loss = 0.29595463\n",
      "Iteration 15, loss = 0.29724331\n",
      "Iteration 16, loss = 0.29181021\n",
      "Iteration 17, loss = 0.29290283\n",
      "Iteration 18, loss = 0.29943576\n",
      "Iteration 19, loss = 0.28774845\n",
      "Iteration 20, loss = 0.27754636\n",
      "Iteration 21, loss = 0.27807266\n",
      "Iteration 22, loss = 0.29766231\n",
      "Iteration 23, loss = 0.28913553\n",
      "Iteration 24, loss = 0.28233003\n",
      "Iteration 25, loss = 0.29277941\n",
      "Iteration 26, loss = 0.29535009\n",
      "Iteration 27, loss = 0.28117511\n",
      "Iteration 28, loss = 0.27270697\n",
      "Iteration 29, loss = 0.27541356\n",
      "Iteration 30, loss = 0.28264544\n",
      "Iteration 31, loss = 0.27908436\n",
      "Iteration 32, loss = 0.27926732\n",
      "Iteration 33, loss = 0.27908103\n",
      "Iteration 34, loss = 0.27962245\n",
      "Iteration 35, loss = 0.28056085\n",
      "Iteration 36, loss = 0.28610893\n",
      "Iteration 37, loss = 0.27496915\n",
      "Iteration 38, loss = 0.27108273\n",
      "Iteration 39, loss = 0.28403521\n",
      "Iteration 40, loss = 0.27685441\n",
      "Iteration 41, loss = 0.27889030\n",
      "Iteration 42, loss = 0.27726004\n",
      "Iteration 43, loss = 0.28876430\n",
      "Iteration 44, loss = 0.29228009\n",
      "Iteration 45, loss = 0.28689904\n",
      "Iteration 46, loss = 0.28963409\n",
      "Iteration 47, loss = 0.28524492\n",
      "Iteration 48, loss = 0.28153385\n",
      "Iteration 49, loss = 0.28597230\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.13586452\n",
      "Iteration 2, loss = 1.36269999\n",
      "Iteration 3, loss = 0.96653695\n",
      "Iteration 4, loss = 0.70279147\n",
      "Iteration 5, loss = 0.57052248\n",
      "Iteration 6, loss = 0.49799897\n",
      "Iteration 7, loss = 0.44863871\n",
      "Iteration 8, loss = 0.40353261\n",
      "Iteration 9, loss = 0.36896638\n",
      "Iteration 10, loss = 0.34541247\n",
      "Iteration 11, loss = 0.33044116\n",
      "Iteration 12, loss = 0.31720193\n",
      "Iteration 13, loss = 0.30582606\n",
      "Iteration 14, loss = 0.29602054\n",
      "Iteration 15, loss = 0.29690038\n",
      "Iteration 16, loss = 0.28412774\n",
      "Iteration 17, loss = 0.28697632\n",
      "Iteration 18, loss = 0.30565780\n",
      "Iteration 19, loss = 0.30275601\n",
      "Iteration 20, loss = 0.28235521\n",
      "Iteration 21, loss = 0.27084833\n",
      "Iteration 22, loss = 0.28959232\n",
      "Iteration 23, loss = 0.29152100\n",
      "Iteration 24, loss = 0.28722183\n",
      "Iteration 25, loss = 0.29625359\n",
      "Iteration 26, loss = 0.29753621\n",
      "Iteration 27, loss = 0.28588706\n",
      "Iteration 28, loss = 0.28032039\n",
      "Iteration 29, loss = 0.28523281\n",
      "Iteration 30, loss = 0.28928314\n",
      "Iteration 31, loss = 0.28065085\n",
      "Iteration 32, loss = 0.28208148\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.13220381\n",
      "Iteration 2, loss = 1.36501914\n",
      "Iteration 3, loss = 0.97437988\n",
      "Iteration 4, loss = 0.71757821\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5, loss = 0.59128249\n",
      "Iteration 6, loss = 0.51898271\n",
      "Iteration 7, loss = 0.46662997\n",
      "Iteration 8, loss = 0.42008609\n",
      "Iteration 9, loss = 0.38501962\n",
      "Iteration 10, loss = 0.36076468\n",
      "Iteration 11, loss = 0.35141491\n",
      "Iteration 12, loss = 0.34471515\n",
      "Iteration 13, loss = 0.32789887\n",
      "Iteration 14, loss = 0.31246703\n",
      "Iteration 15, loss = 0.31223211\n",
      "Iteration 16, loss = 0.29934638\n",
      "Iteration 17, loss = 0.29549720\n",
      "Iteration 18, loss = 0.30814219\n",
      "Iteration 19, loss = 0.30274285\n",
      "Iteration 20, loss = 0.28851813\n",
      "Iteration 21, loss = 0.28213780\n",
      "Iteration 22, loss = 0.30258040\n",
      "Iteration 23, loss = 0.30266358\n",
      "Iteration 24, loss = 0.29180770\n",
      "Iteration 25, loss = 0.30647835\n",
      "Iteration 26, loss = 0.31620000\n",
      "Iteration 27, loss = 0.31251491\n",
      "Iteration 28, loss = 0.30582408\n",
      "Iteration 29, loss = 0.30567344\n",
      "Iteration 30, loss = 0.30236800\n",
      "Iteration 31, loss = 0.29082610\n",
      "Iteration 32, loss = 0.29116315\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.13983787\n",
      "Iteration 2, loss = 1.33988648\n",
      "Iteration 3, loss = 0.94747842\n",
      "Iteration 4, loss = 0.69346438\n",
      "Iteration 5, loss = 0.56653828\n",
      "Iteration 6, loss = 0.49938456\n",
      "Iteration 7, loss = 0.45513763\n",
      "Iteration 8, loss = 0.41311734\n",
      "Iteration 9, loss = 0.37906993\n",
      "Iteration 10, loss = 0.35115531\n",
      "Iteration 11, loss = 0.33855602\n",
      "Iteration 12, loss = 0.33440005\n",
      "Iteration 13, loss = 0.32024736\n",
      "Iteration 14, loss = 0.30560711\n",
      "Iteration 15, loss = 0.30830072\n",
      "Iteration 16, loss = 0.29704068\n",
      "Iteration 17, loss = 0.29369477\n",
      "Iteration 18, loss = 0.30259333\n",
      "Iteration 19, loss = 0.29241311\n",
      "Iteration 20, loss = 0.28848145\n",
      "Iteration 21, loss = 0.29233308\n",
      "Iteration 22, loss = 0.30641693\n",
      "Iteration 23, loss = 0.29608261\n",
      "Iteration 24, loss = 0.28224644\n",
      "Iteration 25, loss = 0.29520141\n",
      "Iteration 26, loss = 0.30545571\n",
      "Iteration 27, loss = 0.30219309\n",
      "Iteration 28, loss = 0.29625938\n",
      "Iteration 29, loss = 0.29813379\n",
      "Iteration 30, loss = 0.29134470\n",
      "Iteration 31, loss = 0.28782589\n",
      "Iteration 32, loss = 0.29257418\n",
      "Iteration 33, loss = 0.29265042\n",
      "Iteration 34, loss = 0.28703589\n",
      "Iteration 35, loss = 0.29094154\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.12240436\n",
      "Iteration 2, loss = 1.34953733\n",
      "Iteration 3, loss = 0.94113526\n",
      "Iteration 4, loss = 0.68126919\n",
      "Iteration 5, loss = 0.55495425\n",
      "Iteration 6, loss = 0.48719827\n",
      "Iteration 7, loss = 0.44348527\n",
      "Iteration 8, loss = 0.40127840\n",
      "Iteration 9, loss = 0.36427482\n",
      "Iteration 10, loss = 0.33655117\n",
      "Iteration 11, loss = 0.32773427\n",
      "Iteration 12, loss = 0.32526892\n",
      "Iteration 13, loss = 0.31052077\n",
      "Iteration 14, loss = 0.29420504\n",
      "Iteration 15, loss = 0.29498015\n",
      "Iteration 16, loss = 0.28948830\n",
      "Iteration 17, loss = 0.28881611\n",
      "Iteration 18, loss = 0.29857898\n",
      "Iteration 19, loss = 0.29259421\n",
      "Iteration 20, loss = 0.28408372\n",
      "Iteration 21, loss = 0.27318650\n",
      "Iteration 22, loss = 0.28754151\n",
      "Iteration 23, loss = 0.30504115\n",
      "Iteration 24, loss = 0.30726704\n",
      "Iteration 25, loss = 0.30400285\n",
      "Iteration 26, loss = 0.28965808\n",
      "Iteration 27, loss = 0.27875276\n",
      "Iteration 28, loss = 0.28493814\n",
      "Iteration 29, loss = 0.29903648\n",
      "Iteration 30, loss = 0.27911165\n",
      "Iteration 31, loss = 0.28391559\n",
      "Iteration 32, loss = 0.28789297\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.23411645\n",
      "Iteration 2, loss = 1.41321841\n",
      "Iteration 3, loss = 1.10541470\n",
      "Iteration 4, loss = 0.81629649\n",
      "Iteration 5, loss = 0.63269659\n",
      "Iteration 6, loss = 0.54659620\n",
      "Iteration 7, loss = 0.50005204\n",
      "Iteration 8, loss = 0.46141574\n",
      "Iteration 9, loss = 0.42318994\n",
      "Iteration 10, loss = 0.38743360\n",
      "Iteration 11, loss = 0.36064106\n",
      "Iteration 12, loss = 0.34124735\n",
      "Iteration 13, loss = 0.33692266\n",
      "Iteration 14, loss = 0.32518380\n",
      "Iteration 15, loss = 0.31402318\n",
      "Iteration 16, loss = 0.30882368\n",
      "Iteration 17, loss = 0.30629441\n",
      "Iteration 18, loss = 0.30678127\n",
      "Iteration 19, loss = 0.31361045\n",
      "Iteration 20, loss = 0.31470298\n",
      "Iteration 21, loss = 0.31123867\n",
      "Iteration 22, loss = 0.29318917\n",
      "Iteration 23, loss = 0.29267022\n",
      "Iteration 24, loss = 0.30681949\n",
      "Iteration 25, loss = 0.29964791\n",
      "Iteration 26, loss = 0.29710175\n",
      "Iteration 27, loss = 0.31304894\n",
      "Iteration 28, loss = 0.29600359\n",
      "Iteration 29, loss = 0.28869451\n",
      "Iteration 30, loss = 0.30462345\n",
      "Iteration 31, loss = 0.31589122\n",
      "Iteration 32, loss = 0.29984069\n",
      "Iteration 33, loss = 0.29538014\n",
      "Iteration 34, loss = 0.30278597\n",
      "Iteration 35, loss = 0.30023741\n",
      "Iteration 36, loss = 0.30093258\n",
      "Iteration 37, loss = 0.29524869\n",
      "Iteration 38, loss = 0.29436218\n",
      "Iteration 39, loss = 0.29182714\n",
      "Iteration 40, loss = 0.27986449\n",
      "Iteration 41, loss = 0.27842220\n",
      "Iteration 42, loss = 0.28146454\n",
      "Iteration 43, loss = 0.27682426\n",
      "Iteration 44, loss = 0.27504366\n",
      "Iteration 45, loss = 0.28756664\n",
      "Iteration 46, loss = 0.29519484\n",
      "Iteration 47, loss = 0.29498491\n",
      "Iteration 48, loss = 0.28144838\n",
      "Iteration 49, loss = 0.28247076\n",
      "Iteration 50, loss = 0.29407975\n",
      "Iteration 51, loss = 0.29944614\n",
      "Iteration 52, loss = 0.29418114\n",
      "Iteration 53, loss = 0.28651022\n",
      "Iteration 54, loss = 0.28323902\n",
      "Iteration 55, loss = 0.28217773\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.19004533\n",
      "Iteration 2, loss = 1.37678924\n",
      "Iteration 3, loss = 0.99518079\n",
      "Iteration 4, loss = 0.69948487\n",
      "Iteration 5, loss = 0.55038630\n",
      "Iteration 6, loss = 0.48563994\n",
      "Iteration 7, loss = 0.43994477\n",
      "Iteration 8, loss = 0.40037647\n",
      "Iteration 9, loss = 0.36989983\n",
      "Iteration 10, loss = 0.34379704\n",
      "Iteration 11, loss = 0.32033604\n",
      "Iteration 12, loss = 0.31179693\n",
      "Iteration 13, loss = 0.31052407\n",
      "Iteration 14, loss = 0.31025125\n",
      "Iteration 15, loss = 0.31777277\n",
      "Iteration 16, loss = 0.30829842\n",
      "Iteration 17, loss = 0.30173538\n",
      "Iteration 18, loss = 0.29977083\n",
      "Iteration 19, loss = 0.30270652\n",
      "Iteration 20, loss = 0.31646392\n",
      "Iteration 21, loss = 0.30769915\n",
      "Iteration 22, loss = 0.29205305\n",
      "Iteration 23, loss = 0.30366986\n",
      "Iteration 24, loss = 0.30521042\n",
      "Iteration 25, loss = 0.29086987\n",
      "Iteration 26, loss = 0.28736552\n",
      "Iteration 27, loss = 0.28904396\n",
      "Iteration 28, loss = 0.28823810\n",
      "Iteration 29, loss = 0.28879672\n",
      "Iteration 30, loss = 0.29463783\n",
      "Iteration 31, loss = 0.29384447\n",
      "Iteration 32, loss = 0.28771069\n",
      "Iteration 33, loss = 0.29986394\n",
      "Iteration 34, loss = 0.28940889\n",
      "Iteration 35, loss = 0.29108262\n",
      "Iteration 36, loss = 0.29669387\n",
      "Iteration 37, loss = 0.29422304\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.21166169\n",
      "Iteration 2, loss = 1.42693093\n",
      "Iteration 3, loss = 1.03427831\n",
      "Iteration 4, loss = 0.72746925\n",
      "Iteration 5, loss = 0.55124779\n",
      "Iteration 6, loss = 0.48049984\n",
      "Iteration 7, loss = 0.44113647\n",
      "Iteration 8, loss = 0.40690896\n",
      "Iteration 9, loss = 0.38023342\n",
      "Iteration 10, loss = 0.35453938\n",
      "Iteration 11, loss = 0.32997876\n",
      "Iteration 12, loss = 0.31966496\n",
      "Iteration 13, loss = 0.31925363\n",
      "Iteration 14, loss = 0.31997961\n",
      "Iteration 15, loss = 0.32698525\n",
      "Iteration 16, loss = 0.31297995\n",
      "Iteration 17, loss = 0.30486917\n",
      "Iteration 18, loss = 0.30387824\n",
      "Iteration 19, loss = 0.30396094\n",
      "Iteration 20, loss = 0.30929503\n",
      "Iteration 21, loss = 0.30127798\n",
      "Iteration 22, loss = 0.29127361\n",
      "Iteration 23, loss = 0.29293033\n",
      "Iteration 24, loss = 0.29179899\n",
      "Iteration 25, loss = 0.28649467\n",
      "Iteration 26, loss = 0.28686794\n",
      "Iteration 27, loss = 0.28904146\n",
      "Iteration 28, loss = 0.28748656\n",
      "Iteration 29, loss = 0.28728606\n",
      "Iteration 30, loss = 0.28770008\n",
      "Iteration 31, loss = 0.28785055\n",
      "Iteration 32, loss = 0.28037477\n",
      "Iteration 33, loss = 0.29361571\n",
      "Iteration 34, loss = 0.29321804\n",
      "Iteration 35, loss = 0.29386070\n",
      "Iteration 36, loss = 0.29318616\n",
      "Iteration 37, loss = 0.29005811\n",
      "Iteration 38, loss = 0.29274943\n",
      "Iteration 39, loss = 0.29865376\n",
      "Iteration 40, loss = 0.29501774\n",
      "Iteration 41, loss = 0.28654227\n",
      "Iteration 42, loss = 0.28278509\n",
      "Iteration 43, loss = 0.28204350\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.15623845\n",
      "Iteration 2, loss = 1.36566451\n",
      "Iteration 3, loss = 0.98268554\n",
      "Iteration 4, loss = 0.68289526\n",
      "Iteration 5, loss = 0.53125580\n",
      "Iteration 6, loss = 0.46770568\n",
      "Iteration 7, loss = 0.42434038\n",
      "Iteration 8, loss = 0.38735777\n",
      "Iteration 9, loss = 0.36559158\n",
      "Iteration 10, loss = 0.34929761\n",
      "Iteration 11, loss = 0.32612319\n",
      "Iteration 12, loss = 0.31169333\n",
      "Iteration 13, loss = 0.30629700\n",
      "Iteration 14, loss = 0.30666558\n",
      "Iteration 15, loss = 0.31070896\n",
      "Iteration 16, loss = 0.30139762\n",
      "Iteration 17, loss = 0.29379103\n",
      "Iteration 18, loss = 0.29452449\n",
      "Iteration 19, loss = 0.29165703\n",
      "Iteration 20, loss = 0.30131425\n",
      "Iteration 21, loss = 0.29986931\n",
      "Iteration 22, loss = 0.29011337\n",
      "Iteration 23, loss = 0.28757875\n",
      "Iteration 24, loss = 0.28324632\n",
      "Iteration 25, loss = 0.27617254\n",
      "Iteration 26, loss = 0.27737950\n",
      "Iteration 27, loss = 0.27249561\n",
      "Iteration 28, loss = 0.26644194\n",
      "Iteration 29, loss = 0.27275917\n",
      "Iteration 30, loss = 0.27795956\n",
      "Iteration 31, loss = 0.28013366\n",
      "Iteration 32, loss = 0.27197975\n",
      "Iteration 33, loss = 0.28717889\n",
      "Iteration 34, loss = 0.28539319\n",
      "Iteration 35, loss = 0.28849316\n",
      "Iteration 36, loss = 0.29213738\n",
      "Iteration 37, loss = 0.28458688\n",
      "Iteration 38, loss = 0.28538439\n",
      "Iteration 39, loss = 0.29273048\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.19001689\n",
      "Iteration 2, loss = 1.38187507\n",
      "Iteration 3, loss = 0.98938242\n",
      "Iteration 4, loss = 0.68306155\n",
      "Iteration 5, loss = 0.53175815\n",
      "Iteration 6, loss = 0.47319130\n",
      "Iteration 7, loss = 0.43226264\n",
      "Iteration 8, loss = 0.39492574\n",
      "Iteration 9, loss = 0.36886791\n",
      "Iteration 10, loss = 0.34838574\n",
      "Iteration 11, loss = 0.32447595\n",
      "Iteration 12, loss = 0.30923176\n",
      "Iteration 13, loss = 0.30648721\n",
      "Iteration 14, loss = 0.30820244\n",
      "Iteration 15, loss = 0.30531779\n",
      "Iteration 16, loss = 0.29348961\n",
      "Iteration 17, loss = 0.29021357\n",
      "Iteration 18, loss = 0.29351116\n",
      "Iteration 19, loss = 0.29520609\n",
      "Iteration 20, loss = 0.30063029\n",
      "Iteration 21, loss = 0.29329731\n",
      "Iteration 22, loss = 0.29524373\n",
      "Iteration 23, loss = 0.29764077\n",
      "Iteration 24, loss = 0.28405371\n",
      "Iteration 25, loss = 0.27538810\n",
      "Iteration 26, loss = 0.27991777\n",
      "Iteration 27, loss = 0.27487901\n",
      "Iteration 28, loss = 0.27481540\n",
      "Iteration 29, loss = 0.28531137\n",
      "Iteration 30, loss = 0.28114844\n",
      "Iteration 31, loss = 0.27290121\n",
      "Iteration 32, loss = 0.26815213\n",
      "Iteration 33, loss = 0.29389056\n",
      "Iteration 34, loss = 0.29203078\n",
      "Iteration 35, loss = 0.29252912\n",
      "Iteration 36, loss = 0.29080046\n",
      "Iteration 37, loss = 0.28877345\n",
      "Iteration 38, loss = 0.30198381\n",
      "Iteration 39, loss = 0.30884811\n",
      "Iteration 40, loss = 0.30322400\n",
      "Iteration 41, loss = 0.28869587\n",
      "Iteration 42, loss = 0.27947704\n",
      "Iteration 43, loss = 0.28223317\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.18755793\n",
      "Iteration 2, loss = 1.35947833\n",
      "Iteration 3, loss = 0.98478163\n",
      "Iteration 4, loss = 0.68147976\n",
      "Iteration 5, loss = 0.51861958\n",
      "Iteration 6, loss = 0.45953578\n",
      "Iteration 7, loss = 0.41720070\n",
      "Iteration 8, loss = 0.37971482\n",
      "Iteration 9, loss = 0.35287488\n",
      "Iteration 10, loss = 0.33477817\n",
      "Iteration 11, loss = 0.31504956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12, loss = 0.30509427\n",
      "Iteration 13, loss = 0.30393070\n",
      "Iteration 14, loss = 0.30623675\n",
      "Iteration 15, loss = 0.30220292\n",
      "Iteration 16, loss = 0.28849716\n",
      "Iteration 17, loss = 0.28138837\n",
      "Iteration 18, loss = 0.28006101\n",
      "Iteration 19, loss = 0.28172537\n",
      "Iteration 20, loss = 0.28704801\n",
      "Iteration 21, loss = 0.28546145\n",
      "Iteration 22, loss = 0.29104116\n",
      "Iteration 23, loss = 0.28731734\n",
      "Iteration 24, loss = 0.27125643\n",
      "Iteration 25, loss = 0.27419196\n",
      "Iteration 26, loss = 0.27967573\n",
      "Iteration 27, loss = 0.27704550\n",
      "Iteration 28, loss = 0.28170563\n",
      "Iteration 29, loss = 0.27836083\n",
      "Iteration 30, loss = 0.26654545\n",
      "Iteration 31, loss = 0.26705313\n",
      "Iteration 32, loss = 0.26291433\n",
      "Iteration 33, loss = 0.26896828\n",
      "Iteration 34, loss = 0.27146325\n",
      "Iteration 35, loss = 0.27800416\n",
      "Iteration 36, loss = 0.28056145\n",
      "Iteration 37, loss = 0.28021200\n",
      "Iteration 38, loss = 0.28376716\n",
      "Iteration 39, loss = 0.28622173\n",
      "Iteration 40, loss = 0.28208104\n",
      "Iteration 41, loss = 0.27812578\n",
      "Iteration 42, loss = 0.28186548\n",
      "Iteration 43, loss = 0.28391506\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.20382694\n",
      "Iteration 2, loss = 1.36994879\n",
      "Iteration 3, loss = 0.95654708\n",
      "Iteration 4, loss = 0.66398679\n",
      "Iteration 5, loss = 0.52437628\n",
      "Iteration 6, loss = 0.45672312\n",
      "Iteration 7, loss = 0.40294971\n",
      "Iteration 8, loss = 0.36602596\n",
      "Iteration 9, loss = 0.34655859\n",
      "Iteration 10, loss = 0.33378514\n",
      "Iteration 11, loss = 0.31790193\n",
      "Iteration 12, loss = 0.31380982\n",
      "Iteration 13, loss = 0.31340698\n",
      "Iteration 14, loss = 0.30762976\n",
      "Iteration 15, loss = 0.29468476\n",
      "Iteration 16, loss = 0.28504682\n",
      "Iteration 17, loss = 0.29035829\n",
      "Iteration 18, loss = 0.29486361\n",
      "Iteration 19, loss = 0.28885943\n",
      "Iteration 20, loss = 0.29611354\n",
      "Iteration 21, loss = 0.31049064\n",
      "Iteration 22, loss = 0.30911214\n",
      "Iteration 23, loss = 0.29098214\n",
      "Iteration 24, loss = 0.29444046\n",
      "Iteration 25, loss = 0.30996785\n",
      "Iteration 26, loss = 0.30629217\n",
      "Iteration 27, loss = 0.28974776\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.20533321\n",
      "Iteration 2, loss = 1.37394961\n",
      "Iteration 3, loss = 0.95432435\n",
      "Iteration 4, loss = 0.67371376\n",
      "Iteration 5, loss = 0.54239946\n",
      "Iteration 6, loss = 0.47026931\n",
      "Iteration 7, loss = 0.41155294\n",
      "Iteration 8, loss = 0.37307678\n",
      "Iteration 9, loss = 0.35464568\n",
      "Iteration 10, loss = 0.34516841\n",
      "Iteration 11, loss = 0.33127274\n",
      "Iteration 12, loss = 0.32603392\n",
      "Iteration 13, loss = 0.32121028\n",
      "Iteration 14, loss = 0.31437051\n",
      "Iteration 15, loss = 0.30453133\n",
      "Iteration 16, loss = 0.29525048\n",
      "Iteration 17, loss = 0.29728955\n",
      "Iteration 18, loss = 0.30039032\n",
      "Iteration 19, loss = 0.30166064\n",
      "Iteration 20, loss = 0.31046160\n",
      "Iteration 21, loss = 0.31652459\n",
      "Iteration 22, loss = 0.30743463\n",
      "Iteration 23, loss = 0.29801264\n",
      "Iteration 24, loss = 0.30787380\n",
      "Iteration 25, loss = 0.31698460\n",
      "Iteration 26, loss = 0.30696360\n",
      "Iteration 27, loss = 0.29127919\n",
      "Iteration 28, loss = 0.28872528\n",
      "Iteration 29, loss = 0.29032487\n",
      "Iteration 30, loss = 0.30906758\n",
      "Iteration 31, loss = 0.28466051\n",
      "Iteration 32, loss = 0.28476626\n",
      "Iteration 33, loss = 0.31492643\n",
      "Iteration 34, loss = 0.32403731\n",
      "Iteration 35, loss = 0.30734884\n",
      "Iteration 36, loss = 0.29503853\n",
      "Iteration 37, loss = 0.29831684\n",
      "Iteration 38, loss = 0.31732644\n",
      "Iteration 39, loss = 0.33185661\n",
      "Iteration 40, loss = 0.31173512\n",
      "Iteration 41, loss = 0.28856711\n",
      "Iteration 42, loss = 0.28903626\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.19305340\n",
      "Iteration 2, loss = 1.35354906\n",
      "Iteration 3, loss = 0.93117045\n",
      "Iteration 4, loss = 0.65300281\n",
      "Iteration 5, loss = 0.52053089\n",
      "Iteration 6, loss = 0.45275304\n",
      "Iteration 7, loss = 0.40431923\n",
      "Iteration 8, loss = 0.36859951\n",
      "Iteration 9, loss = 0.34694673\n",
      "Iteration 10, loss = 0.34090794\n",
      "Iteration 11, loss = 0.33346569\n",
      "Iteration 12, loss = 0.32445056\n",
      "Iteration 13, loss = 0.31614689\n",
      "Iteration 14, loss = 0.30590202\n",
      "Iteration 15, loss = 0.29607814\n",
      "Iteration 16, loss = 0.28718369\n",
      "Iteration 17, loss = 0.28934824\n",
      "Iteration 18, loss = 0.29313898\n",
      "Iteration 19, loss = 0.29318316\n",
      "Iteration 20, loss = 0.29851377\n",
      "Iteration 21, loss = 0.30222441\n",
      "Iteration 22, loss = 0.29788099\n",
      "Iteration 23, loss = 0.29143590\n",
      "Iteration 24, loss = 0.30405842\n",
      "Iteration 25, loss = 0.31007804\n",
      "Iteration 26, loss = 0.29786036\n",
      "Iteration 27, loss = 0.28453088\n",
      "Iteration 28, loss = 0.28514533\n",
      "Iteration 29, loss = 0.28453688\n",
      "Iteration 30, loss = 0.30301659\n",
      "Iteration 31, loss = 0.28286741\n",
      "Iteration 32, loss = 0.28104300\n",
      "Iteration 33, loss = 0.29772637\n",
      "Iteration 34, loss = 0.30262346\n",
      "Iteration 35, loss = 0.29795030\n",
      "Iteration 36, loss = 0.29227858\n",
      "Iteration 37, loss = 0.29146092\n",
      "Iteration 38, loss = 0.30494635\n",
      "Iteration 39, loss = 0.31545760\n",
      "Iteration 40, loss = 0.30116463\n",
      "Iteration 41, loss = 0.28577444\n",
      "Iteration 42, loss = 0.28490768\n",
      "Iteration 43, loss = 0.29563653\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.21231092\n",
      "Iteration 2, loss = 1.37518754\n",
      "Iteration 3, loss = 0.94960425\n",
      "Iteration 4, loss = 0.66587454\n",
      "Iteration 5, loss = 0.52393670\n",
      "Iteration 6, loss = 0.45695464\n",
      "Iteration 7, loss = 0.41000932\n",
      "Iteration 8, loss = 0.36709283\n",
      "Iteration 9, loss = 0.34177275\n",
      "Iteration 10, loss = 0.34106452\n",
      "Iteration 11, loss = 0.33078035\n",
      "Iteration 12, loss = 0.31508971\n",
      "Iteration 13, loss = 0.30486776\n",
      "Iteration 14, loss = 0.29673024\n",
      "Iteration 15, loss = 0.28963192\n",
      "Iteration 16, loss = 0.27964106\n",
      "Iteration 17, loss = 0.27641348\n",
      "Iteration 18, loss = 0.28050270\n",
      "Iteration 19, loss = 0.28303580\n",
      "Iteration 20, loss = 0.28969326\n",
      "Iteration 21, loss = 0.29360803\n",
      "Iteration 22, loss = 0.29585553\n",
      "Iteration 23, loss = 0.28740111\n",
      "Iteration 24, loss = 0.29422591\n",
      "Iteration 25, loss = 0.30099453\n",
      "Iteration 26, loss = 0.28858576\n",
      "Iteration 27, loss = 0.27347739\n",
      "Iteration 28, loss = 0.27464269\n",
      "Iteration 29, loss = 0.27501851\n",
      "Iteration 30, loss = 0.29764185\n",
      "Iteration 31, loss = 0.27748262\n",
      "Iteration 32, loss = 0.27629372\n",
      "Iteration 33, loss = 0.28864261\n",
      "Iteration 34, loss = 0.29307860\n",
      "Iteration 35, loss = 0.30571160\n",
      "Iteration 36, loss = 0.31847160\n",
      "Iteration 37, loss = 0.31270334\n",
      "Iteration 38, loss = 0.30593971\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.50496865\n",
      "Iteration 2, loss = 1.43012951\n",
      "Iteration 3, loss = 1.15714872\n",
      "Iteration 4, loss = 0.82568842\n",
      "Iteration 5, loss = 0.61556940\n",
      "Iteration 6, loss = 0.55245041\n",
      "Iteration 7, loss = 0.53277889\n",
      "Iteration 8, loss = 0.49579685\n",
      "Iteration 9, loss = 0.45315546\n",
      "Iteration 10, loss = 0.41031198\n",
      "Iteration 11, loss = 0.38383110\n",
      "Iteration 12, loss = 0.37606898\n",
      "Iteration 13, loss = 0.36892528\n",
      "Iteration 14, loss = 0.35966758\n",
      "Iteration 15, loss = 0.35027335\n",
      "Iteration 16, loss = 0.33209222\n",
      "Iteration 17, loss = 0.32092100\n",
      "Iteration 18, loss = 0.31547556\n",
      "Iteration 19, loss = 0.30860922\n",
      "Iteration 20, loss = 0.30903456\n",
      "Iteration 21, loss = 0.31667363\n",
      "Iteration 22, loss = 0.31323039\n",
      "Iteration 23, loss = 0.30069774\n",
      "Iteration 24, loss = 0.29311609\n",
      "Iteration 25, loss = 0.29766966\n",
      "Iteration 26, loss = 0.30690729\n",
      "Iteration 27, loss = 0.30619867\n",
      "Iteration 28, loss = 0.29145854\n",
      "Iteration 29, loss = 0.28269602\n",
      "Iteration 30, loss = 0.28851537\n",
      "Iteration 31, loss = 0.29160132\n",
      "Iteration 32, loss = 0.29916506\n",
      "Iteration 33, loss = 0.31717531\n",
      "Iteration 34, loss = 0.31361433\n",
      "Iteration 35, loss = 0.29206392\n",
      "Iteration 36, loss = 0.29445776\n",
      "Iteration 37, loss = 0.30296244\n",
      "Iteration 38, loss = 0.29792893\n",
      "Iteration 39, loss = 0.30079494\n",
      "Iteration 40, loss = 0.29778629\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.51402013\n",
      "Iteration 2, loss = 1.40370989\n",
      "Iteration 3, loss = 1.10688730\n",
      "Iteration 4, loss = 0.81408979\n",
      "Iteration 5, loss = 0.61149850\n",
      "Iteration 6, loss = 0.53714091\n",
      "Iteration 7, loss = 0.51738199\n",
      "Iteration 8, loss = 0.48946162\n",
      "Iteration 9, loss = 0.44689077\n",
      "Iteration 10, loss = 0.40840101\n",
      "Iteration 11, loss = 0.37754733\n",
      "Iteration 12, loss = 0.36150787\n",
      "Iteration 13, loss = 0.35364326\n",
      "Iteration 14, loss = 0.34569499\n",
      "Iteration 15, loss = 0.33220868\n",
      "Iteration 16, loss = 0.32982204\n",
      "Iteration 17, loss = 0.33018663\n",
      "Iteration 18, loss = 0.32180220\n",
      "Iteration 19, loss = 0.31732007\n",
      "Iteration 20, loss = 0.31938405\n",
      "Iteration 21, loss = 0.30363992\n",
      "Iteration 22, loss = 0.29576913\n",
      "Iteration 23, loss = 0.29662284\n",
      "Iteration 24, loss = 0.30173560\n",
      "Iteration 25, loss = 0.30527977\n",
      "Iteration 26, loss = 0.28757589\n",
      "Iteration 27, loss = 0.29917620\n",
      "Iteration 28, loss = 0.29930331\n",
      "Iteration 29, loss = 0.28642796\n",
      "Iteration 30, loss = 0.28761190\n",
      "Iteration 31, loss = 0.28195561\n",
      "Iteration 32, loss = 0.28567335\n",
      "Iteration 33, loss = 0.29179284\n",
      "Iteration 34, loss = 0.28897027\n",
      "Iteration 35, loss = 0.30037889\n",
      "Iteration 36, loss = 0.31139457\n",
      "Iteration 37, loss = 0.30355859\n",
      "Iteration 38, loss = 0.28742165\n",
      "Iteration 39, loss = 0.27468411\n",
      "Iteration 40, loss = 0.27798283\n",
      "Iteration 41, loss = 0.28182831\n",
      "Iteration 42, loss = 0.27973686\n",
      "Iteration 43, loss = 0.27878826\n",
      "Iteration 44, loss = 0.28095394\n",
      "Iteration 45, loss = 0.27812432\n",
      "Iteration 46, loss = 0.28386241\n",
      "Iteration 47, loss = 0.29482250\n",
      "Iteration 48, loss = 0.30024350\n",
      "Iteration 49, loss = 0.29048662\n",
      "Iteration 50, loss = 0.28692136\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.50682840\n",
      "Iteration 2, loss = 1.42360575\n",
      "Iteration 3, loss = 1.10204448\n",
      "Iteration 4, loss = 0.78582204\n",
      "Iteration 5, loss = 0.59130720\n",
      "Iteration 6, loss = 0.52614714\n",
      "Iteration 7, loss = 0.50227768\n",
      "Iteration 8, loss = 0.47262739\n",
      "Iteration 9, loss = 0.43638660\n",
      "Iteration 10, loss = 0.40606373\n",
      "Iteration 11, loss = 0.38154447\n",
      "Iteration 12, loss = 0.36122196\n",
      "Iteration 13, loss = 0.34833819\n",
      "Iteration 14, loss = 0.34362691\n",
      "Iteration 15, loss = 0.33205290\n",
      "Iteration 16, loss = 0.32529660\n",
      "Iteration 17, loss = 0.31947399\n",
      "Iteration 18, loss = 0.31300773\n",
      "Iteration 19, loss = 0.31541076\n",
      "Iteration 20, loss = 0.31842004\n",
      "Iteration 21, loss = 0.29635015\n",
      "Iteration 22, loss = 0.28379747\n",
      "Iteration 23, loss = 0.29752039\n",
      "Iteration 24, loss = 0.30849309\n",
      "Iteration 25, loss = 0.30341391\n",
      "Iteration 26, loss = 0.28318034\n",
      "Iteration 27, loss = 0.29829404\n",
      "Iteration 28, loss = 0.30676125\n",
      "Iteration 29, loss = 0.30135093\n",
      "Iteration 30, loss = 0.30398587\n",
      "Iteration 31, loss = 0.29552933\n",
      "Iteration 32, loss = 0.28655619\n",
      "Iteration 33, loss = 0.28893189\n",
      "Iteration 34, loss = 0.29952223\n",
      "Iteration 35, loss = 0.31188309\n",
      "Iteration 36, loss = 0.30284692\n",
      "Iteration 37, loss = 0.29577030\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.51813112\n",
      "Iteration 2, loss = 1.41979385\n",
      "Iteration 3, loss = 1.10468239\n",
      "Iteration 4, loss = 0.79107045\n",
      "Iteration 5, loss = 0.59780120\n",
      "Iteration 6, loss = 0.52958025\n",
      "Iteration 7, loss = 0.50551214\n",
      "Iteration 8, loss = 0.47532357\n",
      "Iteration 9, loss = 0.43408185\n",
      "Iteration 10, loss = 0.39834342\n",
      "Iteration 11, loss = 0.37455992\n",
      "Iteration 12, loss = 0.36534398\n",
      "Iteration 13, loss = 0.34903632\n",
      "Iteration 14, loss = 0.33055019\n",
      "Iteration 15, loss = 0.31872414\n",
      "Iteration 16, loss = 0.31726107\n",
      "Iteration 17, loss = 0.31542371\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 18, loss = 0.30595739\n",
      "Iteration 19, loss = 0.30411498\n",
      "Iteration 20, loss = 0.31193892\n",
      "Iteration 21, loss = 0.29423898\n",
      "Iteration 22, loss = 0.28676034\n",
      "Iteration 23, loss = 0.29280764\n",
      "Iteration 24, loss = 0.30006879\n",
      "Iteration 25, loss = 0.29841878\n",
      "Iteration 26, loss = 0.28242370\n",
      "Iteration 27, loss = 0.28613152\n",
      "Iteration 28, loss = 0.28728417\n",
      "Iteration 29, loss = 0.28575445\n",
      "Iteration 30, loss = 0.29132379\n",
      "Iteration 31, loss = 0.28673846\n",
      "Iteration 32, loss = 0.28202738\n",
      "Iteration 33, loss = 0.28225596\n",
      "Iteration 34, loss = 0.28427520\n",
      "Iteration 35, loss = 0.30206107\n",
      "Iteration 36, loss = 0.29829502\n",
      "Iteration 37, loss = 0.28825743\n",
      "Iteration 38, loss = 0.28193660\n",
      "Iteration 39, loss = 0.27290993\n",
      "Iteration 40, loss = 0.27148006\n",
      "Iteration 41, loss = 0.26913083\n",
      "Iteration 42, loss = 0.27307847\n",
      "Iteration 43, loss = 0.28650649\n",
      "Iteration 44, loss = 0.29012002\n",
      "Iteration 45, loss = 0.28062329\n",
      "Iteration 46, loss = 0.28171383\n",
      "Iteration 47, loss = 0.29055541\n",
      "Iteration 48, loss = 0.28942097\n",
      "Iteration 49, loss = 0.28291344\n",
      "Iteration 50, loss = 0.27886291\n",
      "Iteration 51, loss = 0.27402226\n",
      "Iteration 52, loss = 0.26938848\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.50739755\n",
      "Iteration 2, loss = 1.42889295\n",
      "Iteration 3, loss = 1.12746033\n",
      "Iteration 4, loss = 0.81226167\n",
      "Iteration 5, loss = 0.60957808\n",
      "Iteration 6, loss = 0.54024403\n",
      "Iteration 7, loss = 0.51636034\n",
      "Iteration 8, loss = 0.48390085\n",
      "Iteration 9, loss = 0.43972676\n",
      "Iteration 10, loss = 0.39994562\n",
      "Iteration 11, loss = 0.37218633\n",
      "Iteration 12, loss = 0.36492672\n",
      "Iteration 13, loss = 0.35706621\n",
      "Iteration 14, loss = 0.34112213\n",
      "Iteration 15, loss = 0.32475483\n",
      "Iteration 16, loss = 0.32265258\n",
      "Iteration 17, loss = 0.32525904\n",
      "Iteration 18, loss = 0.31419440\n",
      "Iteration 19, loss = 0.31182297\n",
      "Iteration 20, loss = 0.30946169\n",
      "Iteration 21, loss = 0.30337495\n",
      "Iteration 22, loss = 0.30372362\n",
      "Iteration 23, loss = 0.31144147\n",
      "Iteration 24, loss = 0.30933486\n",
      "Iteration 25, loss = 0.29799339\n",
      "Iteration 26, loss = 0.27950792\n",
      "Iteration 27, loss = 0.29367792\n",
      "Iteration 28, loss = 0.30127300\n",
      "Iteration 29, loss = 0.29439568\n",
      "Iteration 30, loss = 0.29467623\n",
      "Iteration 31, loss = 0.29180191\n",
      "Iteration 32, loss = 0.28937262\n",
      "Iteration 33, loss = 0.29040504\n",
      "Iteration 34, loss = 0.29392613\n",
      "Iteration 35, loss = 0.31063443\n",
      "Iteration 36, loss = 0.30389947\n",
      "Iteration 37, loss = 0.28645228\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.51298619\n",
      "Iteration 2, loss = 1.40984985\n",
      "Iteration 3, loss = 1.09173642\n",
      "Iteration 4, loss = 0.77347295\n",
      "Iteration 5, loss = 0.58318863\n",
      "Iteration 6, loss = 0.51707475\n",
      "Iteration 7, loss = 0.49115455\n",
      "Iteration 8, loss = 0.45406757\n",
      "Iteration 9, loss = 0.41087180\n",
      "Iteration 10, loss = 0.37668426\n",
      "Iteration 11, loss = 0.35316330\n",
      "Iteration 12, loss = 0.34542482\n",
      "Iteration 13, loss = 0.33487205\n",
      "Iteration 14, loss = 0.32257076\n",
      "Iteration 15, loss = 0.31163768\n",
      "Iteration 16, loss = 0.30426460\n",
      "Iteration 17, loss = 0.30243040\n",
      "Iteration 18, loss = 0.29836790\n",
      "Iteration 19, loss = 0.29584715\n",
      "Iteration 20, loss = 0.29025503\n",
      "Iteration 21, loss = 0.29510813\n",
      "Iteration 22, loss = 0.29451658\n",
      "Iteration 23, loss = 0.28708323\n",
      "Iteration 24, loss = 0.28558322\n",
      "Iteration 25, loss = 0.28734060\n",
      "Iteration 26, loss = 0.28670938\n",
      "Iteration 27, loss = 0.28537109\n",
      "Iteration 28, loss = 0.28643084\n",
      "Iteration 29, loss = 0.28901879\n",
      "Iteration 30, loss = 0.27751987\n",
      "Iteration 31, loss = 0.27762883\n",
      "Iteration 32, loss = 0.28935901\n",
      "Iteration 33, loss = 0.29244370\n",
      "Iteration 34, loss = 0.28941602\n",
      "Iteration 35, loss = 0.29878497\n",
      "Iteration 36, loss = 0.28721458\n",
      "Iteration 37, loss = 0.27505962\n",
      "Iteration 38, loss = 0.27324331\n",
      "Iteration 39, loss = 0.27373329\n",
      "Iteration 40, loss = 0.27721630\n",
      "Iteration 41, loss = 0.27063113\n",
      "Iteration 42, loss = 0.27303621\n",
      "Iteration 43, loss = 0.28277623\n",
      "Iteration 44, loss = 0.29800629\n",
      "Iteration 45, loss = 0.29419791\n",
      "Iteration 46, loss = 0.28706193\n",
      "Iteration 47, loss = 0.29468487\n",
      "Iteration 48, loss = 0.30425388\n",
      "Iteration 49, loss = 0.30128415\n",
      "Iteration 50, loss = 0.29344102\n",
      "Iteration 51, loss = 0.28872894\n",
      "Iteration 52, loss = 0.28213789\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.49829641\n",
      "Iteration 2, loss = 1.43034632\n",
      "Iteration 3, loss = 1.09776105\n",
      "Iteration 4, loss = 0.76753355\n",
      "Iteration 5, loss = 0.58408953\n",
      "Iteration 6, loss = 0.52290120\n",
      "Iteration 7, loss = 0.49438679\n",
      "Iteration 8, loss = 0.45107801\n",
      "Iteration 9, loss = 0.40734450\n",
      "Iteration 10, loss = 0.38069661\n",
      "Iteration 11, loss = 0.36693689\n",
      "Iteration 12, loss = 0.36177872\n",
      "Iteration 13, loss = 0.34656643\n",
      "Iteration 14, loss = 0.33349391\n",
      "Iteration 15, loss = 0.32497712\n",
      "Iteration 16, loss = 0.31085015\n",
      "Iteration 17, loss = 0.29962422\n",
      "Iteration 18, loss = 0.30050329\n",
      "Iteration 19, loss = 0.30618668\n",
      "Iteration 20, loss = 0.29147022\n",
      "Iteration 21, loss = 0.29258442\n",
      "Iteration 22, loss = 0.29205435\n",
      "Iteration 23, loss = 0.29851903\n",
      "Iteration 24, loss = 0.30735238\n",
      "Iteration 25, loss = 0.29972108\n",
      "Iteration 26, loss = 0.28127094\n",
      "Iteration 27, loss = 0.29610021\n",
      "Iteration 28, loss = 0.30753157\n",
      "Iteration 29, loss = 0.29900654\n",
      "Iteration 30, loss = 0.28927765\n",
      "Iteration 31, loss = 0.29880346\n",
      "Iteration 32, loss = 0.29933665\n",
      "Iteration 33, loss = 0.28730833\n",
      "Iteration 34, loss = 0.28283640\n",
      "Iteration 35, loss = 0.29914878\n",
      "Iteration 36, loss = 0.30011264\n",
      "Iteration 37, loss = 0.28196699\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.50095694\n",
      "Iteration 2, loss = 1.43410722\n",
      "Iteration 3, loss = 1.11208494\n",
      "Iteration 4, loss = 0.78928493\n",
      "Iteration 5, loss = 0.60676783\n",
      "Iteration 6, loss = 0.54009555\n",
      "Iteration 7, loss = 0.50787494\n",
      "Iteration 8, loss = 0.46416895\n",
      "Iteration 9, loss = 0.42075002\n",
      "Iteration 10, loss = 0.39355168\n",
      "Iteration 11, loss = 0.38172555\n",
      "Iteration 12, loss = 0.37600762\n",
      "Iteration 13, loss = 0.35873574\n",
      "Iteration 14, loss = 0.34368015\n",
      "Iteration 15, loss = 0.33960090\n",
      "Iteration 16, loss = 0.32800057\n",
      "Iteration 17, loss = 0.31354736\n",
      "Iteration 18, loss = 0.30914869\n",
      "Iteration 19, loss = 0.31281850\n",
      "Iteration 20, loss = 0.30256197\n",
      "Iteration 21, loss = 0.30260885\n",
      "Iteration 22, loss = 0.29843437\n",
      "Iteration 23, loss = 0.30741056\n",
      "Iteration 24, loss = 0.31841411\n",
      "Iteration 25, loss = 0.31591630\n",
      "Iteration 26, loss = 0.29951444\n",
      "Iteration 27, loss = 0.30643659\n",
      "Iteration 28, loss = 0.32675469\n",
      "Iteration 29, loss = 0.32593729\n",
      "Iteration 30, loss = 0.31368189\n",
      "Iteration 31, loss = 0.31998219\n",
      "Iteration 32, loss = 0.31750391\n",
      "Iteration 33, loss = 0.30379552\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.50729175\n",
      "Iteration 2, loss = 1.41367401\n",
      "Iteration 3, loss = 1.08640733\n",
      "Iteration 4, loss = 0.76697095\n",
      "Iteration 5, loss = 0.59101189\n",
      "Iteration 6, loss = 0.52514625\n",
      "Iteration 7, loss = 0.49481007\n",
      "Iteration 8, loss = 0.45520506\n",
      "Iteration 9, loss = 0.41307021\n",
      "Iteration 10, loss = 0.38542167\n",
      "Iteration 11, loss = 0.37791916\n",
      "Iteration 12, loss = 0.37633152\n",
      "Iteration 13, loss = 0.35799093\n",
      "Iteration 14, loss = 0.33910237\n",
      "Iteration 15, loss = 0.33324173\n",
      "Iteration 16, loss = 0.32364370\n",
      "Iteration 17, loss = 0.31361807\n",
      "Iteration 18, loss = 0.31142799\n",
      "Iteration 19, loss = 0.31089124\n",
      "Iteration 20, loss = 0.29563553\n",
      "Iteration 21, loss = 0.29267425\n",
      "Iteration 22, loss = 0.29214724\n",
      "Iteration 23, loss = 0.29948603\n",
      "Iteration 24, loss = 0.30688732\n",
      "Iteration 25, loss = 0.30553792\n",
      "Iteration 26, loss = 0.29269812\n",
      "Iteration 27, loss = 0.30258047\n",
      "Iteration 28, loss = 0.32317135\n",
      "Iteration 29, loss = 0.31931030\n",
      "Iteration 30, loss = 0.30653508\n",
      "Iteration 31, loss = 0.31217492\n",
      "Iteration 32, loss = 0.30973146\n",
      "Iteration 33, loss = 0.29299772\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.50394996\n",
      "Iteration 2, loss = 1.42092114\n",
      "Iteration 3, loss = 1.07820330\n",
      "Iteration 4, loss = 0.74646837\n",
      "Iteration 5, loss = 0.58078534\n",
      "Iteration 6, loss = 0.52409705\n",
      "Iteration 7, loss = 0.48989584\n",
      "Iteration 8, loss = 0.44322333\n",
      "Iteration 9, loss = 0.39967746\n",
      "Iteration 10, loss = 0.37322640\n",
      "Iteration 11, loss = 0.36446434\n",
      "Iteration 12, loss = 0.36321679\n",
      "Iteration 13, loss = 0.34577126\n",
      "Iteration 14, loss = 0.32864028\n",
      "Iteration 15, loss = 0.32477060\n",
      "Iteration 16, loss = 0.31741228\n",
      "Iteration 17, loss = 0.30545606\n",
      "Iteration 18, loss = 0.30579779\n",
      "Iteration 19, loss = 0.30918949\n",
      "Iteration 20, loss = 0.29966270\n",
      "Iteration 21, loss = 0.29359449\n",
      "Iteration 22, loss = 0.28429601\n",
      "Iteration 23, loss = 0.28649980\n",
      "Iteration 24, loss = 0.29529709\n",
      "Iteration 25, loss = 0.29059950\n",
      "Iteration 26, loss = 0.27793012\n",
      "Iteration 27, loss = 0.28252609\n",
      "Iteration 28, loss = 0.30124063\n",
      "Iteration 29, loss = 0.30378277\n",
      "Iteration 30, loss = 0.29177260\n",
      "Iteration 31, loss = 0.29631650\n",
      "Iteration 32, loss = 0.29456322\n",
      "Iteration 33, loss = 0.28184471\n",
      "Iteration 34, loss = 0.27878396\n",
      "Iteration 35, loss = 0.28688641\n",
      "Iteration 36, loss = 0.28552957\n",
      "Iteration 37, loss = 0.26936389\n",
      "Iteration 38, loss = 0.27067748\n",
      "Iteration 39, loss = 0.27249092\n",
      "Iteration 40, loss = 0.27211583\n",
      "Iteration 41, loss = 0.26990803\n",
      "Iteration 42, loss = 0.26398216\n",
      "Iteration 43, loss = 0.26867361\n",
      "Iteration 44, loss = 0.28110831\n",
      "Iteration 45, loss = 0.28738348\n",
      "Iteration 46, loss = 0.27952878\n",
      "Iteration 47, loss = 0.27539200\n",
      "Iteration 48, loss = 0.29586240\n",
      "Iteration 49, loss = 0.30010081\n",
      "Iteration 50, loss = 0.27234882\n",
      "Iteration 51, loss = 0.30947164\n",
      "Iteration 52, loss = 0.30894330\n",
      "Iteration 53, loss = 0.27841220\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.24802771\n",
      "Iteration 2, loss = 1.33894780\n",
      "Iteration 3, loss = 0.92237908\n",
      "Iteration 4, loss = 0.67529999\n",
      "Iteration 5, loss = 0.55500533\n",
      "Iteration 6, loss = 0.50944191\n",
      "Iteration 7, loss = 0.47382148\n",
      "Iteration 8, loss = 0.42755433\n",
      "Iteration 9, loss = 0.38349943\n",
      "Iteration 10, loss = 0.36222188\n",
      "Iteration 11, loss = 0.36323696\n",
      "Iteration 12, loss = 0.36608677\n",
      "Iteration 13, loss = 0.34674181\n",
      "Iteration 14, loss = 0.32876322\n",
      "Iteration 15, loss = 0.32044626\n",
      "Iteration 16, loss = 0.31503611\n",
      "Iteration 17, loss = 0.30880035\n",
      "Iteration 18, loss = 0.29890573\n",
      "Iteration 19, loss = 0.30136558\n",
      "Iteration 20, loss = 0.30541730\n",
      "Iteration 21, loss = 0.29365249\n",
      "Iteration 22, loss = 0.28860091\n",
      "Iteration 23, loss = 0.30537268\n",
      "Iteration 24, loss = 0.32888013\n",
      "Iteration 25, loss = 0.32533019\n",
      "Iteration 26, loss = 0.32554523\n",
      "Iteration 27, loss = 0.31507133\n",
      "Iteration 28, loss = 0.29603731\n",
      "Iteration 29, loss = 0.30867095\n",
      "Iteration 30, loss = 0.32931060\n",
      "Iteration 31, loss = 0.32333463\n",
      "Iteration 32, loss = 0.30646088\n",
      "Iteration 33, loss = 0.33186146\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.23048231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, loss = 1.33862380\n",
      "Iteration 3, loss = 1.05355372\n",
      "Iteration 4, loss = 0.79323654\n",
      "Iteration 5, loss = 0.58849644\n",
      "Iteration 6, loss = 0.50916877\n",
      "Iteration 7, loss = 0.48302931\n",
      "Iteration 8, loss = 0.45612413\n",
      "Iteration 9, loss = 0.41583078\n",
      "Iteration 10, loss = 0.38026679\n",
      "Iteration 11, loss = 0.35144917\n",
      "Iteration 12, loss = 0.33593956\n",
      "Iteration 13, loss = 0.32562202\n",
      "Iteration 14, loss = 0.32143156\n",
      "Iteration 15, loss = 0.31273495\n",
      "Iteration 16, loss = 0.30107998\n",
      "Iteration 17, loss = 0.29403583\n",
      "Iteration 18, loss = 0.29458407\n",
      "Iteration 19, loss = 0.29676711\n",
      "Iteration 20, loss = 0.29937183\n",
      "Iteration 21, loss = 0.29973767\n",
      "Iteration 22, loss = 0.29296339\n",
      "Iteration 23, loss = 0.29154502\n",
      "Iteration 24, loss = 0.29116301\n",
      "Iteration 25, loss = 0.28781372\n",
      "Iteration 26, loss = 0.28700606\n",
      "Iteration 27, loss = 0.28768621\n",
      "Iteration 28, loss = 0.28657374\n",
      "Iteration 29, loss = 0.28257648\n",
      "Iteration 30, loss = 0.29418951\n",
      "Iteration 31, loss = 0.30794480\n",
      "Iteration 32, loss = 0.29253165\n",
      "Iteration 33, loss = 0.28408884\n",
      "Iteration 34, loss = 0.30849658\n",
      "Iteration 35, loss = 0.33585224\n",
      "Iteration 36, loss = 0.30413469\n",
      "Iteration 37, loss = 0.27988272\n",
      "Iteration 38, loss = 0.30567727\n",
      "Iteration 39, loss = 0.30409725\n",
      "Iteration 40, loss = 0.29205140\n",
      "Iteration 41, loss = 0.29154606\n",
      "Iteration 42, loss = 0.29356460\n",
      "Iteration 43, loss = 0.29443529\n",
      "Iteration 44, loss = 0.29213445\n",
      "Iteration 45, loss = 0.28444629\n",
      "Iteration 46, loss = 0.28554157\n",
      "Iteration 47, loss = 0.28632412\n",
      "Iteration 48, loss = 0.29602805\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.23460659\n",
      "Iteration 2, loss = 1.32533793\n",
      "Iteration 3, loss = 0.99040592\n",
      "Iteration 4, loss = 0.71451258\n",
      "Iteration 5, loss = 0.53915742\n",
      "Iteration 6, loss = 0.48252566\n",
      "Iteration 7, loss = 0.45563545\n",
      "Iteration 8, loss = 0.42276293\n",
      "Iteration 9, loss = 0.38609978\n",
      "Iteration 10, loss = 0.35811942\n",
      "Iteration 11, loss = 0.34004287\n",
      "Iteration 12, loss = 0.32473432\n",
      "Iteration 13, loss = 0.31482128\n",
      "Iteration 14, loss = 0.31409978\n",
      "Iteration 15, loss = 0.30937808\n",
      "Iteration 16, loss = 0.29681812\n",
      "Iteration 17, loss = 0.29100741\n",
      "Iteration 18, loss = 0.29462647\n",
      "Iteration 19, loss = 0.29963644\n",
      "Iteration 20, loss = 0.29713377\n",
      "Iteration 21, loss = 0.30054843\n",
      "Iteration 22, loss = 0.29061475\n",
      "Iteration 23, loss = 0.28546302\n",
      "Iteration 24, loss = 0.28720753\n",
      "Iteration 25, loss = 0.28708994\n",
      "Iteration 26, loss = 0.28737510\n",
      "Iteration 27, loss = 0.28424116\n",
      "Iteration 28, loss = 0.28100526\n",
      "Iteration 29, loss = 0.27888775\n",
      "Iteration 30, loss = 0.27949553\n",
      "Iteration 31, loss = 0.28587447\n",
      "Iteration 32, loss = 0.28198942\n",
      "Iteration 33, loss = 0.28229397\n",
      "Iteration 34, loss = 0.30132175\n",
      "Iteration 35, loss = 0.31251715\n",
      "Iteration 36, loss = 0.29526509\n",
      "Iteration 37, loss = 0.27957209\n",
      "Iteration 38, loss = 0.27666039\n",
      "Iteration 39, loss = 0.27526179\n",
      "Iteration 40, loss = 0.28089991\n",
      "Iteration 41, loss = 0.28195706\n",
      "Iteration 42, loss = 0.28179342\n",
      "Iteration 43, loss = 0.28338639\n",
      "Iteration 44, loss = 0.28281247\n",
      "Iteration 45, loss = 0.27176553\n",
      "Iteration 46, loss = 0.27132187\n",
      "Iteration 47, loss = 0.27615211\n",
      "Iteration 48, loss = 0.29293962\n",
      "Iteration 49, loss = 0.29743176\n",
      "Iteration 50, loss = 0.28695270\n",
      "Iteration 51, loss = 0.28186293\n",
      "Iteration 52, loss = 0.28471415\n",
      "Iteration 53, loss = 0.28518628\n",
      "Iteration 54, loss = 0.27319441\n",
      "Iteration 55, loss = 0.28909090\n",
      "Iteration 56, loss = 0.32108134\n",
      "Iteration 57, loss = 0.31535071\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.20404470\n",
      "Iteration 2, loss = 1.32173565\n",
      "Iteration 3, loss = 0.95219144\n",
      "Iteration 4, loss = 0.67771797\n",
      "Iteration 5, loss = 0.53323880\n",
      "Iteration 6, loss = 0.50108356\n",
      "Iteration 7, loss = 0.48164834\n",
      "Iteration 8, loss = 0.43712859\n",
      "Iteration 9, loss = 0.38871170\n",
      "Iteration 10, loss = 0.35807022\n",
      "Iteration 11, loss = 0.34117976\n",
      "Iteration 12, loss = 0.32785494\n",
      "Iteration 13, loss = 0.31523589\n",
      "Iteration 14, loss = 0.30699629\n",
      "Iteration 15, loss = 0.29616220\n",
      "Iteration 16, loss = 0.29013670\n",
      "Iteration 17, loss = 0.28481029\n",
      "Iteration 18, loss = 0.28091778\n",
      "Iteration 19, loss = 0.28826864\n",
      "Iteration 20, loss = 0.29403158\n",
      "Iteration 21, loss = 0.29023233\n",
      "Iteration 22, loss = 0.27835683\n",
      "Iteration 23, loss = 0.27573734\n",
      "Iteration 24, loss = 0.27459151\n",
      "Iteration 25, loss = 0.27252299\n",
      "Iteration 26, loss = 0.27310831\n",
      "Iteration 27, loss = 0.27323317\n",
      "Iteration 28, loss = 0.27382386\n",
      "Iteration 29, loss = 0.27960965\n",
      "Iteration 30, loss = 0.27411693\n",
      "Iteration 31, loss = 0.28876789\n",
      "Iteration 32, loss = 0.28940107\n",
      "Iteration 33, loss = 0.27527878\n",
      "Iteration 34, loss = 0.29530783\n",
      "Iteration 35, loss = 0.31975528\n",
      "Iteration 36, loss = 0.30477228\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.23165643\n",
      "Iteration 2, loss = 1.32952568\n",
      "Iteration 3, loss = 0.98311950\n",
      "Iteration 4, loss = 0.71198827\n",
      "Iteration 5, loss = 0.55994943\n",
      "Iteration 6, loss = 0.52475153\n",
      "Iteration 7, loss = 0.50690859\n",
      "Iteration 8, loss = 0.45863557\n",
      "Iteration 9, loss = 0.40325860\n",
      "Iteration 10, loss = 0.36787965\n",
      "Iteration 11, loss = 0.35077060\n",
      "Iteration 12, loss = 0.33368646\n",
      "Iteration 13, loss = 0.32453428\n",
      "Iteration 14, loss = 0.32207249\n",
      "Iteration 15, loss = 0.31537222\n",
      "Iteration 16, loss = 0.31012615\n",
      "Iteration 17, loss = 0.29793033\n",
      "Iteration 18, loss = 0.29319011\n",
      "Iteration 19, loss = 0.30224768\n",
      "Iteration 20, loss = 0.31531297\n",
      "Iteration 21, loss = 0.31564563\n",
      "Iteration 22, loss = 0.29323855\n",
      "Iteration 23, loss = 0.28450064\n",
      "Iteration 24, loss = 0.28544683\n",
      "Iteration 25, loss = 0.28979898\n",
      "Iteration 26, loss = 0.29008966\n",
      "Iteration 27, loss = 0.28641180\n",
      "Iteration 28, loss = 0.28528988\n",
      "Iteration 29, loss = 0.29120932\n",
      "Iteration 30, loss = 0.28949212\n",
      "Iteration 31, loss = 0.28994449\n",
      "Iteration 32, loss = 0.28275507\n",
      "Iteration 33, loss = 0.27920083\n",
      "Iteration 34, loss = 0.28128118\n",
      "Iteration 35, loss = 0.28712856\n",
      "Iteration 36, loss = 0.28725094\n",
      "Iteration 37, loss = 0.27265197\n",
      "Iteration 38, loss = 0.25991952\n",
      "Iteration 39, loss = 0.26936748\n",
      "Iteration 40, loss = 0.28353528\n",
      "Iteration 41, loss = 0.27599759\n",
      "Iteration 42, loss = 0.27673989\n",
      "Iteration 43, loss = 0.28233174\n",
      "Iteration 44, loss = 0.27875141\n",
      "Iteration 45, loss = 0.28361570\n",
      "Iteration 46, loss = 0.28486207\n",
      "Iteration 47, loss = 0.28308607\n",
      "Iteration 48, loss = 0.30256440\n",
      "Iteration 49, loss = 0.30607683\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.23017422\n",
      "Iteration 2, loss = 1.31691305\n",
      "Iteration 3, loss = 0.97758804\n",
      "Iteration 4, loss = 0.69888237\n",
      "Iteration 5, loss = 0.54387724\n",
      "Iteration 6, loss = 0.49801523\n",
      "Iteration 7, loss = 0.48091554\n",
      "Iteration 8, loss = 0.44393057\n",
      "Iteration 9, loss = 0.39753989\n",
      "Iteration 10, loss = 0.36093140\n",
      "Iteration 11, loss = 0.34018205\n",
      "Iteration 12, loss = 0.32456081\n",
      "Iteration 13, loss = 0.31458818\n",
      "Iteration 14, loss = 0.31150866\n",
      "Iteration 15, loss = 0.30757880\n",
      "Iteration 16, loss = 0.30513921\n",
      "Iteration 17, loss = 0.29038289\n",
      "Iteration 18, loss = 0.27985704\n",
      "Iteration 19, loss = 0.28304172\n",
      "Iteration 20, loss = 0.29396148\n",
      "Iteration 21, loss = 0.29661673\n",
      "Iteration 22, loss = 0.28217858\n",
      "Iteration 23, loss = 0.27740481\n",
      "Iteration 24, loss = 0.27842412\n",
      "Iteration 25, loss = 0.28685973\n",
      "Iteration 26, loss = 0.28898320\n",
      "Iteration 27, loss = 0.28333088\n",
      "Iteration 28, loss = 0.28305751\n",
      "Iteration 29, loss = 0.28933128\n",
      "Iteration 30, loss = 0.28220732\n",
      "Iteration 31, loss = 0.27864159\n",
      "Iteration 32, loss = 0.27545460\n",
      "Iteration 33, loss = 0.28095264\n",
      "Iteration 34, loss = 0.28364778\n",
      "Iteration 35, loss = 0.28041086\n",
      "Iteration 36, loss = 0.27692106\n",
      "Iteration 37, loss = 0.26831184\n",
      "Iteration 38, loss = 0.25523515\n",
      "Iteration 39, loss = 0.26419649\n",
      "Iteration 40, loss = 0.28428192\n",
      "Iteration 41, loss = 0.27012244\n",
      "Iteration 42, loss = 0.28092226\n",
      "Iteration 43, loss = 0.29836785\n",
      "Iteration 44, loss = 0.30253237\n",
      "Iteration 45, loss = 0.28378423\n",
      "Iteration 46, loss = 0.27463117\n",
      "Iteration 47, loss = 0.28789521\n",
      "Iteration 48, loss = 0.30739634\n",
      "Iteration 49, loss = 0.29781112\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.23235488\n",
      "Iteration 2, loss = 1.32705920\n",
      "Iteration 3, loss = 0.95632908\n",
      "Iteration 4, loss = 0.68327969\n",
      "Iteration 5, loss = 0.54440764\n",
      "Iteration 6, loss = 0.50596456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7, loss = 0.48771287\n",
      "Iteration 8, loss = 0.44273855\n",
      "Iteration 9, loss = 0.39345189\n",
      "Iteration 10, loss = 0.35785453\n",
      "Iteration 11, loss = 0.34434851\n",
      "Iteration 12, loss = 0.32976460\n",
      "Iteration 13, loss = 0.32092764\n",
      "Iteration 14, loss = 0.31480056\n",
      "Iteration 15, loss = 0.30392647\n",
      "Iteration 16, loss = 0.29694200\n",
      "Iteration 17, loss = 0.29169108\n",
      "Iteration 18, loss = 0.28937647\n",
      "Iteration 19, loss = 0.28562176\n",
      "Iteration 20, loss = 0.28737325\n",
      "Iteration 21, loss = 0.30226067\n",
      "Iteration 22, loss = 0.30683068\n",
      "Iteration 23, loss = 0.29974997\n",
      "Iteration 24, loss = 0.29429169\n",
      "Iteration 25, loss = 0.29749880\n",
      "Iteration 26, loss = 0.29383081\n",
      "Iteration 27, loss = 0.29176050\n",
      "Iteration 28, loss = 0.29694017\n",
      "Iteration 29, loss = 0.29236172\n",
      "Iteration 30, loss = 0.27568842\n",
      "Iteration 31, loss = 0.29062270\n",
      "Iteration 32, loss = 0.28376549\n",
      "Iteration 33, loss = 0.28929973\n",
      "Iteration 34, loss = 0.29517560\n",
      "Iteration 35, loss = 0.30125266\n",
      "Iteration 36, loss = 0.29837531\n",
      "Iteration 37, loss = 0.28299794\n",
      "Iteration 38, loss = 0.27257778\n",
      "Iteration 39, loss = 0.27965334\n",
      "Iteration 40, loss = 0.28015381\n",
      "Iteration 41, loss = 0.28123853\n",
      "Iteration 42, loss = 0.30186222\n",
      "Iteration 43, loss = 0.31212941\n",
      "Iteration 44, loss = 0.29739335\n",
      "Iteration 45, loss = 0.27090161\n",
      "Iteration 46, loss = 0.27523121\n",
      "Iteration 47, loss = 0.31576038\n",
      "Iteration 48, loss = 0.32753592\n",
      "Iteration 49, loss = 0.31082727\n",
      "Iteration 50, loss = 0.28843416\n",
      "Iteration 51, loss = 0.27743533\n",
      "Iteration 52, loss = 0.27465775\n",
      "Iteration 53, loss = 0.27584691\n",
      "Iteration 54, loss = 0.27524013\n",
      "Iteration 55, loss = 0.29053498\n",
      "Iteration 56, loss = 0.31126142\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.23705536\n",
      "Iteration 2, loss = 1.33193191\n",
      "Iteration 3, loss = 0.98799825\n",
      "Iteration 4, loss = 0.72017512\n",
      "Iteration 5, loss = 0.56780765\n",
      "Iteration 6, loss = 0.52097514\n",
      "Iteration 7, loss = 0.50782497\n",
      "Iteration 8, loss = 0.45892454\n",
      "Iteration 9, loss = 0.40752752\n",
      "Iteration 10, loss = 0.37280566\n",
      "Iteration 11, loss = 0.35513870\n",
      "Iteration 12, loss = 0.34061696\n",
      "Iteration 13, loss = 0.33552558\n",
      "Iteration 14, loss = 0.32847591\n",
      "Iteration 15, loss = 0.31597062\n",
      "Iteration 16, loss = 0.30928084\n",
      "Iteration 17, loss = 0.30230203\n",
      "Iteration 18, loss = 0.30145189\n",
      "Iteration 19, loss = 0.29424695\n",
      "Iteration 20, loss = 0.29811252\n",
      "Iteration 21, loss = 0.31455131\n",
      "Iteration 22, loss = 0.32169589\n",
      "Iteration 23, loss = 0.30246366\n",
      "Iteration 24, loss = 0.28460621\n",
      "Iteration 25, loss = 0.30327591\n",
      "Iteration 26, loss = 0.31611366\n",
      "Iteration 27, loss = 0.31348334\n",
      "Iteration 28, loss = 0.30694258\n",
      "Iteration 29, loss = 0.29478546\n",
      "Iteration 30, loss = 0.29086133\n",
      "Iteration 31, loss = 0.31058620\n",
      "Iteration 32, loss = 0.30261233\n",
      "Iteration 33, loss = 0.30072930\n",
      "Iteration 34, loss = 0.29632560\n",
      "Iteration 35, loss = 0.29346062\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.23175948\n",
      "Iteration 2, loss = 1.31600413\n",
      "Iteration 3, loss = 0.95588778\n",
      "Iteration 4, loss = 0.69601862\n",
      "Iteration 5, loss = 0.55913643\n",
      "Iteration 6, loss = 0.51202152\n",
      "Iteration 7, loss = 0.49153664\n",
      "Iteration 8, loss = 0.44222888\n",
      "Iteration 9, loss = 0.39365575\n",
      "Iteration 10, loss = 0.36213929\n",
      "Iteration 11, loss = 0.34651228\n",
      "Iteration 12, loss = 0.33456776\n",
      "Iteration 13, loss = 0.33159658\n",
      "Iteration 14, loss = 0.32166186\n",
      "Iteration 15, loss = 0.30484835\n",
      "Iteration 16, loss = 0.29689462\n",
      "Iteration 17, loss = 0.29418131\n",
      "Iteration 18, loss = 0.29831804\n",
      "Iteration 19, loss = 0.28532442\n",
      "Iteration 20, loss = 0.29240931\n",
      "Iteration 21, loss = 0.31382834\n",
      "Iteration 22, loss = 0.31919985\n",
      "Iteration 23, loss = 0.29768226\n",
      "Iteration 24, loss = 0.28110676\n",
      "Iteration 25, loss = 0.29271349\n",
      "Iteration 26, loss = 0.29973802\n",
      "Iteration 27, loss = 0.29587233\n",
      "Iteration 28, loss = 0.30117132\n",
      "Iteration 29, loss = 0.30377094\n",
      "Iteration 30, loss = 0.29218214\n",
      "Iteration 31, loss = 0.30167679\n",
      "Iteration 32, loss = 0.28799453\n",
      "Iteration 33, loss = 0.29280072\n",
      "Iteration 34, loss = 0.29221657\n",
      "Iteration 35, loss = 0.29599354\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.23851618\n",
      "Iteration 2, loss = 1.32173019\n",
      "Iteration 3, loss = 0.95540677\n",
      "Iteration 4, loss = 0.68369059\n",
      "Iteration 5, loss = 0.54609673\n",
      "Iteration 6, loss = 0.50450262\n",
      "Iteration 7, loss = 0.48916644\n",
      "Iteration 8, loss = 0.44122721\n",
      "Iteration 9, loss = 0.38991347\n",
      "Iteration 10, loss = 0.35249665\n",
      "Iteration 11, loss = 0.34021172\n",
      "Iteration 12, loss = 0.33035265\n",
      "Iteration 13, loss = 0.32347835\n",
      "Iteration 14, loss = 0.31154388\n",
      "Iteration 15, loss = 0.29794875\n",
      "Iteration 16, loss = 0.29446005\n",
      "Iteration 17, loss = 0.28873443\n",
      "Iteration 18, loss = 0.28723792\n",
      "Iteration 19, loss = 0.27495916\n",
      "Iteration 20, loss = 0.28079107\n",
      "Iteration 21, loss = 0.29463572\n",
      "Iteration 22, loss = 0.30084437\n",
      "Iteration 23, loss = 0.28049160\n",
      "Iteration 24, loss = 0.26752046\n",
      "Iteration 25, loss = 0.28954899\n",
      "Iteration 26, loss = 0.30159020\n",
      "Iteration 27, loss = 0.28642118\n",
      "Iteration 28, loss = 0.28337512\n",
      "Iteration 29, loss = 0.28557309\n",
      "Iteration 30, loss = 0.28534709\n",
      "Iteration 31, loss = 0.29435077\n",
      "Iteration 32, loss = 0.28010559\n",
      "Iteration 33, loss = 0.28580006\n",
      "Iteration 34, loss = 0.29499056\n",
      "Iteration 35, loss = 0.29058570\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.36078575\n",
      "Iteration 2, loss = 1.39665938\n",
      "Iteration 3, loss = 1.01471426\n",
      "Iteration 4, loss = 0.75603829\n",
      "Iteration 5, loss = 0.57877379\n",
      "Iteration 6, loss = 0.51090763\n",
      "Iteration 7, loss = 0.50669821\n",
      "Iteration 8, loss = 0.48077849\n",
      "Iteration 9, loss = 0.43586210\n",
      "Iteration 10, loss = 0.41035317\n",
      "Iteration 11, loss = 0.38756343\n",
      "Iteration 12, loss = 0.36712975\n",
      "Iteration 13, loss = 0.35049873\n",
      "Iteration 14, loss = 0.34068464\n",
      "Iteration 15, loss = 0.33430006\n",
      "Iteration 16, loss = 0.33158034\n",
      "Iteration 17, loss = 0.31634072\n",
      "Iteration 18, loss = 0.29749676\n",
      "Iteration 19, loss = 0.29314760\n",
      "Iteration 20, loss = 0.29464836\n",
      "Iteration 21, loss = 0.28957642\n",
      "Iteration 22, loss = 0.28692771\n",
      "Iteration 23, loss = 0.29585945\n",
      "Iteration 24, loss = 0.30577019\n",
      "Iteration 25, loss = 0.30096866\n",
      "Iteration 26, loss = 0.30922567\n",
      "Iteration 27, loss = 0.31615193\n",
      "Iteration 28, loss = 0.31981116\n",
      "Iteration 29, loss = 0.30362802\n",
      "Iteration 30, loss = 0.29596663\n",
      "Iteration 31, loss = 0.30259273\n",
      "Iteration 32, loss = 0.31262134\n",
      "Iteration 33, loss = 0.28837361\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.33188147\n",
      "Iteration 2, loss = 1.39221447\n",
      "Iteration 3, loss = 0.98395410\n",
      "Iteration 4, loss = 0.71452988\n",
      "Iteration 5, loss = 0.56110614\n",
      "Iteration 6, loss = 0.49507360\n",
      "Iteration 7, loss = 0.46072307\n",
      "Iteration 8, loss = 0.43132163\n",
      "Iteration 9, loss = 0.40060742\n",
      "Iteration 10, loss = 0.37399581\n",
      "Iteration 11, loss = 0.35796900\n",
      "Iteration 12, loss = 0.35000424\n",
      "Iteration 13, loss = 0.33301132\n",
      "Iteration 14, loss = 0.32308981\n",
      "Iteration 15, loss = 0.33234927\n",
      "Iteration 16, loss = 0.33440452\n",
      "Iteration 17, loss = 0.32343999\n",
      "Iteration 18, loss = 0.30323654\n",
      "Iteration 19, loss = 0.30672444\n",
      "Iteration 20, loss = 0.31685695\n",
      "Iteration 21, loss = 0.30232959\n",
      "Iteration 22, loss = 0.28958050\n",
      "Iteration 23, loss = 0.29527696\n",
      "Iteration 24, loss = 0.29191026\n",
      "Iteration 25, loss = 0.28765655\n",
      "Iteration 26, loss = 0.28747517\n",
      "Iteration 27, loss = 0.29968972\n",
      "Iteration 28, loss = 0.29837847\n",
      "Iteration 29, loss = 0.28905678\n",
      "Iteration 30, loss = 0.28900558\n",
      "Iteration 31, loss = 0.29050630\n",
      "Iteration 32, loss = 0.28736090\n",
      "Iteration 33, loss = 0.28720275\n",
      "Iteration 34, loss = 0.28944233\n",
      "Iteration 35, loss = 0.29626414\n",
      "Iteration 36, loss = 0.29336260\n",
      "Iteration 37, loss = 0.27909745\n",
      "Iteration 38, loss = 0.28575677\n",
      "Iteration 39, loss = 0.28955521\n",
      "Iteration 40, loss = 0.28445695\n",
      "Iteration 41, loss = 0.29397127\n",
      "Iteration 42, loss = 0.30132697\n",
      "Iteration 43, loss = 0.29519271\n",
      "Iteration 44, loss = 0.28668864\n",
      "Iteration 45, loss = 0.28184634\n",
      "Iteration 46, loss = 0.27549357\n",
      "Iteration 47, loss = 0.28797994\n",
      "Iteration 48, loss = 0.31697185\n",
      "Iteration 49, loss = 0.28836042\n",
      "Iteration 50, loss = 0.29087337\n",
      "Iteration 51, loss = 0.30078531\n",
      "Iteration 52, loss = 0.28913707\n",
      "Iteration 53, loss = 0.28079665\n",
      "Iteration 54, loss = 0.28551192\n",
      "Iteration 55, loss = 0.28739309\n",
      "Iteration 56, loss = 0.27603291\n",
      "Iteration 57, loss = 0.27608394\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.35369430\n",
      "Iteration 2, loss = 1.39152825\n",
      "Iteration 3, loss = 0.99084129\n",
      "Iteration 4, loss = 0.72062480\n",
      "Iteration 5, loss = 0.54596386\n",
      "Iteration 6, loss = 0.47845222\n",
      "Iteration 7, loss = 0.44905925\n",
      "Iteration 8, loss = 0.42295178\n",
      "Iteration 9, loss = 0.39769136\n",
      "Iteration 10, loss = 0.37457578\n",
      "Iteration 11, loss = 0.35254432\n",
      "Iteration 12, loss = 0.34378318\n",
      "Iteration 13, loss = 0.33904592\n",
      "Iteration 14, loss = 0.33454488\n",
      "Iteration 15, loss = 0.33501829\n",
      "Iteration 16, loss = 0.32626327\n",
      "Iteration 17, loss = 0.31155233\n",
      "Iteration 18, loss = 0.29618472\n",
      "Iteration 19, loss = 0.30389299\n",
      "Iteration 20, loss = 0.31327295\n",
      "Iteration 21, loss = 0.29156874\n",
      "Iteration 22, loss = 0.30061401\n",
      "Iteration 23, loss = 0.31513721\n",
      "Iteration 24, loss = 0.29613622\n",
      "Iteration 25, loss = 0.28764118\n",
      "Iteration 26, loss = 0.29363632\n",
      "Iteration 27, loss = 0.30464903\n",
      "Iteration 28, loss = 0.29302116\n",
      "Iteration 29, loss = 0.28576086\n",
      "Iteration 30, loss = 0.28761614\n",
      "Iteration 31, loss = 0.29007468\n",
      "Iteration 32, loss = 0.29116318\n",
      "Iteration 33, loss = 0.29495901\n",
      "Iteration 34, loss = 0.29608470\n",
      "Iteration 35, loss = 0.30435672\n",
      "Iteration 36, loss = 0.30051035\n",
      "Iteration 37, loss = 0.28127950\n",
      "Iteration 38, loss = 0.28215055\n",
      "Iteration 39, loss = 0.29180463\n",
      "Iteration 40, loss = 0.28655733\n",
      "Iteration 41, loss = 0.27653571\n",
      "Iteration 42, loss = 0.28993190\n",
      "Iteration 43, loss = 0.30347389\n",
      "Iteration 44, loss = 0.29375946\n",
      "Iteration 45, loss = 0.28679162\n",
      "Iteration 46, loss = 0.27874120\n",
      "Iteration 47, loss = 0.28596979\n",
      "Iteration 48, loss = 0.30276025\n",
      "Iteration 49, loss = 0.29900193\n",
      "Iteration 50, loss = 0.29496475\n",
      "Iteration 51, loss = 0.28975963\n",
      "Iteration 52, loss = 0.28351744\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.30637057\n",
      "Iteration 2, loss = 1.40280429\n",
      "Iteration 3, loss = 0.96977495\n",
      "Iteration 4, loss = 0.70471508\n",
      "Iteration 5, loss = 0.56767023\n",
      "Iteration 6, loss = 0.48712126\n",
      "Iteration 7, loss = 0.44238500\n",
      "Iteration 8, loss = 0.41992440\n",
      "Iteration 9, loss = 0.39870768\n",
      "Iteration 10, loss = 0.37479690\n",
      "Iteration 11, loss = 0.35042954\n",
      "Iteration 12, loss = 0.33715495\n",
      "Iteration 13, loss = 0.32975015\n",
      "Iteration 14, loss = 0.32376280\n",
      "Iteration 15, loss = 0.32177902\n",
      "Iteration 16, loss = 0.31798378\n",
      "Iteration 17, loss = 0.31244190\n",
      "Iteration 18, loss = 0.29425741\n",
      "Iteration 19, loss = 0.30076511\n",
      "Iteration 20, loss = 0.30742512\n",
      "Iteration 21, loss = 0.29408307\n",
      "Iteration 22, loss = 0.29061536\n",
      "Iteration 23, loss = 0.29646784\n",
      "Iteration 24, loss = 0.29246582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 25, loss = 0.28213813\n",
      "Iteration 26, loss = 0.27758992\n",
      "Iteration 27, loss = 0.28483185\n",
      "Iteration 28, loss = 0.28939030\n",
      "Iteration 29, loss = 0.28975049\n",
      "Iteration 30, loss = 0.28102443\n",
      "Iteration 31, loss = 0.28138285\n",
      "Iteration 32, loss = 0.28645738\n",
      "Iteration 33, loss = 0.28075417\n",
      "Iteration 34, loss = 0.27830503\n",
      "Iteration 35, loss = 0.30297444\n",
      "Iteration 36, loss = 0.31171272\n",
      "Iteration 37, loss = 0.26947287\n",
      "Iteration 38, loss = 0.26052180\n",
      "Iteration 39, loss = 0.29958887\n",
      "Iteration 40, loss = 0.30509202\n",
      "Iteration 41, loss = 0.27564811\n",
      "Iteration 42, loss = 0.28575820\n",
      "Iteration 43, loss = 0.29735629\n",
      "Iteration 44, loss = 0.28323219\n",
      "Iteration 45, loss = 0.27463980\n",
      "Iteration 46, loss = 0.27336906\n",
      "Iteration 47, loss = 0.28881525\n",
      "Iteration 48, loss = 0.30209857\n",
      "Iteration 49, loss = 0.29492849\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.32939855\n",
      "Iteration 2, loss = 1.38862455\n",
      "Iteration 3, loss = 0.99722759\n",
      "Iteration 4, loss = 0.71934007\n",
      "Iteration 5, loss = 0.55438421\n",
      "Iteration 6, loss = 0.48932651\n",
      "Iteration 7, loss = 0.45689215\n",
      "Iteration 8, loss = 0.42874426\n",
      "Iteration 9, loss = 0.40045869\n",
      "Iteration 10, loss = 0.37221403\n",
      "Iteration 11, loss = 0.34411870\n",
      "Iteration 12, loss = 0.33918225\n",
      "Iteration 13, loss = 0.33676702\n",
      "Iteration 14, loss = 0.32852199\n",
      "Iteration 15, loss = 0.31885709\n",
      "Iteration 16, loss = 0.30998284\n",
      "Iteration 17, loss = 0.30819646\n",
      "Iteration 18, loss = 0.30102034\n",
      "Iteration 19, loss = 0.31528470\n",
      "Iteration 20, loss = 0.31324067\n",
      "Iteration 21, loss = 0.28931153\n",
      "Iteration 22, loss = 0.29279023\n",
      "Iteration 23, loss = 0.31310214\n",
      "Iteration 24, loss = 0.30620599\n",
      "Iteration 25, loss = 0.29224540\n",
      "Iteration 26, loss = 0.28639692\n",
      "Iteration 27, loss = 0.29084539\n",
      "Iteration 28, loss = 0.28975644\n",
      "Iteration 29, loss = 0.28654636\n",
      "Iteration 30, loss = 0.28651394\n",
      "Iteration 31, loss = 0.29510718\n",
      "Iteration 32, loss = 0.29420656\n",
      "Iteration 33, loss = 0.28680505\n",
      "Iteration 34, loss = 0.30575294\n",
      "Iteration 35, loss = 0.32856587\n",
      "Iteration 36, loss = 0.30766479\n",
      "Iteration 37, loss = 0.27662202\n",
      "Iteration 38, loss = 0.29319020\n",
      "Iteration 39, loss = 0.31417794\n",
      "Iteration 40, loss = 0.28703855\n",
      "Iteration 41, loss = 0.27445825\n",
      "Iteration 42, loss = 0.30973508\n",
      "Iteration 43, loss = 0.29312424\n",
      "Iteration 44, loss = 0.27588268\n",
      "Iteration 45, loss = 0.30235542\n",
      "Iteration 46, loss = 0.29990764\n",
      "Iteration 47, loss = 0.29228298\n",
      "Iteration 48, loss = 0.30284738\n",
      "Iteration 49, loss = 0.30277299\n",
      "Iteration 50, loss = 0.30336253\n",
      "Iteration 51, loss = 0.30092300\n",
      "Iteration 52, loss = 0.29407385\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.33256317\n",
      "Iteration 2, loss = 1.38499642\n",
      "Iteration 3, loss = 0.99751371\n",
      "Iteration 4, loss = 0.71138654\n",
      "Iteration 5, loss = 0.54498950\n",
      "Iteration 6, loss = 0.48317889\n",
      "Iteration 7, loss = 0.45398104\n",
      "Iteration 8, loss = 0.42845949\n",
      "Iteration 9, loss = 0.39799102\n",
      "Iteration 10, loss = 0.36620301\n",
      "Iteration 11, loss = 0.33746579\n",
      "Iteration 12, loss = 0.33399296\n",
      "Iteration 13, loss = 0.32968182\n",
      "Iteration 14, loss = 0.31836523\n",
      "Iteration 15, loss = 0.30806679\n",
      "Iteration 16, loss = 0.30105341\n",
      "Iteration 17, loss = 0.29902934\n",
      "Iteration 18, loss = 0.29747367\n",
      "Iteration 19, loss = 0.30084353\n",
      "Iteration 20, loss = 0.29655114\n",
      "Iteration 21, loss = 0.27697248\n",
      "Iteration 22, loss = 0.28066991\n",
      "Iteration 23, loss = 0.30292661\n",
      "Iteration 24, loss = 0.31067418\n",
      "Iteration 25, loss = 0.29813128\n",
      "Iteration 26, loss = 0.28519926\n",
      "Iteration 27, loss = 0.28521297\n",
      "Iteration 28, loss = 0.28873765\n",
      "Iteration 29, loss = 0.28760649\n",
      "Iteration 30, loss = 0.29112185\n",
      "Iteration 31, loss = 0.30285215\n",
      "Iteration 32, loss = 0.29508802\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.34229849\n",
      "Iteration 2, loss = 1.40333147\n",
      "Iteration 3, loss = 1.07128706\n",
      "Iteration 4, loss = 0.76427849\n",
      "Iteration 5, loss = 0.57020776\n",
      "Iteration 6, loss = 0.50679085\n",
      "Iteration 7, loss = 0.48711556\n",
      "Iteration 8, loss = 0.46487288\n",
      "Iteration 9, loss = 0.42929518\n",
      "Iteration 10, loss = 0.39409222\n",
      "Iteration 11, loss = 0.35984826\n",
      "Iteration 12, loss = 0.35101459\n",
      "Iteration 13, loss = 0.34612432\n",
      "Iteration 14, loss = 0.33779604\n",
      "Iteration 15, loss = 0.32433943\n",
      "Iteration 16, loss = 0.31300624\n",
      "Iteration 17, loss = 0.31135831\n",
      "Iteration 18, loss = 0.31377616\n",
      "Iteration 19, loss = 0.31545793\n",
      "Iteration 20, loss = 0.31071921\n",
      "Iteration 21, loss = 0.29766690\n",
      "Iteration 22, loss = 0.29008358\n",
      "Iteration 23, loss = 0.30079967\n",
      "Iteration 24, loss = 0.31607706\n",
      "Iteration 25, loss = 0.31002030\n",
      "Iteration 26, loss = 0.29734296\n",
      "Iteration 27, loss = 0.28990133\n",
      "Iteration 28, loss = 0.29038430\n",
      "Iteration 29, loss = 0.29235886\n",
      "Iteration 30, loss = 0.29581425\n",
      "Iteration 31, loss = 0.30527394\n",
      "Iteration 32, loss = 0.30414866\n",
      "Iteration 33, loss = 0.29627038\n",
      "Iteration 34, loss = 0.29469790\n",
      "Iteration 35, loss = 0.29640814\n",
      "Iteration 36, loss = 0.29200344\n",
      "Iteration 37, loss = 0.28430891\n",
      "Iteration 38, loss = 0.28887400\n",
      "Iteration 39, loss = 0.28408342\n",
      "Iteration 40, loss = 0.27803109\n",
      "Iteration 41, loss = 0.27551138\n",
      "Iteration 42, loss = 0.27546273\n",
      "Iteration 43, loss = 0.26284045\n",
      "Iteration 44, loss = 0.26294831\n",
      "Iteration 45, loss = 0.27039161\n",
      "Iteration 46, loss = 0.27368121\n",
      "Iteration 47, loss = 0.28749368\n",
      "Iteration 48, loss = 0.29490278\n",
      "Iteration 49, loss = 0.27673456\n",
      "Iteration 50, loss = 0.26901787\n",
      "Iteration 51, loss = 0.27510154\n",
      "Iteration 52, loss = 0.28104952\n",
      "Iteration 53, loss = 0.27327278\n",
      "Iteration 54, loss = 0.27548690\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.34457838\n",
      "Iteration 2, loss = 1.40966918\n",
      "Iteration 3, loss = 1.09958700\n",
      "Iteration 4, loss = 0.79675745\n",
      "Iteration 5, loss = 0.59299243\n",
      "Iteration 6, loss = 0.52256501\n",
      "Iteration 7, loss = 0.50120302\n",
      "Iteration 8, loss = 0.48318985\n",
      "Iteration 9, loss = 0.45200539\n",
      "Iteration 10, loss = 0.41382908\n",
      "Iteration 11, loss = 0.37255713\n",
      "Iteration 12, loss = 0.36614307\n",
      "Iteration 13, loss = 0.36328346\n",
      "Iteration 14, loss = 0.35345130\n",
      "Iteration 15, loss = 0.34333603\n",
      "Iteration 16, loss = 0.33435932\n",
      "Iteration 17, loss = 0.32541050\n",
      "Iteration 18, loss = 0.32531739\n",
      "Iteration 19, loss = 0.32596867\n",
      "Iteration 20, loss = 0.32836094\n",
      "Iteration 21, loss = 0.31957160\n",
      "Iteration 22, loss = 0.30645995\n",
      "Iteration 23, loss = 0.31473464\n",
      "Iteration 24, loss = 0.32838552\n",
      "Iteration 25, loss = 0.32281243\n",
      "Iteration 26, loss = 0.30713977\n",
      "Iteration 27, loss = 0.29908167\n",
      "Iteration 28, loss = 0.29810853\n",
      "Iteration 29, loss = 0.29574990\n",
      "Iteration 30, loss = 0.31038075\n",
      "Iteration 31, loss = 0.33971584\n",
      "Iteration 32, loss = 0.34585882\n",
      "Iteration 33, loss = 0.32140520\n",
      "Iteration 34, loss = 0.30942793\n",
      "Iteration 35, loss = 0.31812705\n",
      "Iteration 36, loss = 0.30343965\n",
      "Iteration 37, loss = 0.29340034\n",
      "Iteration 38, loss = 0.30836684\n",
      "Iteration 39, loss = 0.30358029\n",
      "Iteration 40, loss = 0.29187933\n",
      "Iteration 41, loss = 0.28578472\n",
      "Iteration 42, loss = 0.28470375\n",
      "Iteration 43, loss = 0.27909307\n",
      "Iteration 44, loss = 0.27971901\n",
      "Iteration 45, loss = 0.28754452\n",
      "Iteration 46, loss = 0.28456452\n",
      "Iteration 47, loss = 0.28661131\n",
      "Iteration 48, loss = 0.29867552\n",
      "Iteration 49, loss = 0.29347905\n",
      "Iteration 50, loss = 0.28999840\n",
      "Iteration 51, loss = 0.28920911\n",
      "Iteration 52, loss = 0.29023717\n",
      "Iteration 53, loss = 0.28601077\n",
      "Iteration 54, loss = 0.29438306\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.34091171\n",
      "Iteration 2, loss = 1.40330636\n",
      "Iteration 3, loss = 1.07937111\n",
      "Iteration 4, loss = 0.77426182\n",
      "Iteration 5, loss = 0.57647216\n",
      "Iteration 6, loss = 0.50984303\n",
      "Iteration 7, loss = 0.48811714\n",
      "Iteration 8, loss = 0.46916326\n",
      "Iteration 9, loss = 0.43931438\n",
      "Iteration 10, loss = 0.40665411\n",
      "Iteration 11, loss = 0.36928498\n",
      "Iteration 12, loss = 0.35761110\n",
      "Iteration 13, loss = 0.35332668\n",
      "Iteration 14, loss = 0.34627773\n",
      "Iteration 15, loss = 0.34026199\n",
      "Iteration 16, loss = 0.33287430\n",
      "Iteration 17, loss = 0.32462829\n",
      "Iteration 18, loss = 0.32362599\n",
      "Iteration 19, loss = 0.31884058\n",
      "Iteration 20, loss = 0.32051331\n",
      "Iteration 21, loss = 0.31999130\n",
      "Iteration 22, loss = 0.30842493\n",
      "Iteration 23, loss = 0.30406078\n",
      "Iteration 24, loss = 0.32105431\n",
      "Iteration 25, loss = 0.32183988\n",
      "Iteration 26, loss = 0.30133123\n",
      "Iteration 27, loss = 0.28327875\n",
      "Iteration 28, loss = 0.28777204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 29, loss = 0.29208273\n",
      "Iteration 30, loss = 0.30013976\n",
      "Iteration 31, loss = 0.32248584\n",
      "Iteration 32, loss = 0.33408326\n",
      "Iteration 33, loss = 0.31222448\n",
      "Iteration 34, loss = 0.30007130\n",
      "Iteration 35, loss = 0.30801655\n",
      "Iteration 36, loss = 0.29848780\n",
      "Iteration 37, loss = 0.29126124\n",
      "Iteration 38, loss = 0.30477654\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.34673671\n",
      "Iteration 2, loss = 1.41182533\n",
      "Iteration 3, loss = 1.10578761\n",
      "Iteration 4, loss = 0.80200610\n",
      "Iteration 5, loss = 0.58939699\n",
      "Iteration 6, loss = 0.50495893\n",
      "Iteration 7, loss = 0.48431591\n",
      "Iteration 8, loss = 0.47167800\n",
      "Iteration 9, loss = 0.44591886\n",
      "Iteration 10, loss = 0.41223447\n",
      "Iteration 11, loss = 0.36603382\n",
      "Iteration 12, loss = 0.34730613\n",
      "Iteration 13, loss = 0.34414399\n",
      "Iteration 14, loss = 0.34672170\n",
      "Iteration 15, loss = 0.34005961\n",
      "Iteration 16, loss = 0.32727291\n",
      "Iteration 17, loss = 0.31457619\n",
      "Iteration 18, loss = 0.31109352\n",
      "Iteration 19, loss = 0.31194848\n",
      "Iteration 20, loss = 0.29986664\n",
      "Iteration 21, loss = 0.29278931\n",
      "Iteration 22, loss = 0.29849155\n",
      "Iteration 23, loss = 0.30166702\n",
      "Iteration 24, loss = 0.30258398\n",
      "Iteration 25, loss = 0.30116889\n",
      "Iteration 26, loss = 0.29523292\n",
      "Iteration 27, loss = 0.27684310\n",
      "Iteration 28, loss = 0.27597818\n",
      "Iteration 29, loss = 0.28400104\n",
      "Iteration 30, loss = 0.29629817\n",
      "Iteration 31, loss = 0.30908172\n",
      "Iteration 32, loss = 0.30621345\n",
      "Iteration 33, loss = 0.28911832\n",
      "Iteration 34, loss = 0.28365774\n",
      "Iteration 35, loss = 0.29810462\n",
      "Iteration 36, loss = 0.29000190\n",
      "Iteration 37, loss = 0.28230235\n",
      "Iteration 38, loss = 0.29915778\n",
      "Iteration 39, loss = 0.29925320\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.30836449\n",
      "Iteration 2, loss = 1.41870211\n",
      "Iteration 3, loss = 0.96291023\n",
      "Iteration 4, loss = 0.69333431\n",
      "Iteration 5, loss = 0.59430551\n",
      "Iteration 6, loss = 0.52504532\n",
      "Iteration 7, loss = 0.45879919\n",
      "Iteration 8, loss = 0.41860251\n",
      "Iteration 9, loss = 0.40034152\n",
      "Iteration 10, loss = 0.38289141\n",
      "Iteration 11, loss = 0.36166090\n",
      "Iteration 12, loss = 0.34345544\n",
      "Iteration 13, loss = 0.33162783\n",
      "Iteration 14, loss = 0.32240777\n",
      "Iteration 15, loss = 0.31838646\n",
      "Iteration 16, loss = 0.30668064\n",
      "Iteration 17, loss = 0.30004807\n",
      "Iteration 18, loss = 0.30656408\n",
      "Iteration 19, loss = 0.30465759\n",
      "Iteration 20, loss = 0.29363545\n",
      "Iteration 21, loss = 0.29220122\n",
      "Iteration 22, loss = 0.29073463\n",
      "Iteration 23, loss = 0.30398543\n",
      "Iteration 24, loss = 0.31581039\n",
      "Iteration 25, loss = 0.30250994\n",
      "Iteration 26, loss = 0.29472145\n",
      "Iteration 27, loss = 0.30495219\n",
      "Iteration 28, loss = 0.31467572\n",
      "Iteration 29, loss = 0.29719449\n",
      "Iteration 30, loss = 0.29814322\n",
      "Iteration 31, loss = 0.30376005\n",
      "Iteration 32, loss = 0.31080603\n",
      "Iteration 33, loss = 0.30721388\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.29627881\n",
      "Iteration 2, loss = 1.37501350\n",
      "Iteration 3, loss = 0.92100187\n",
      "Iteration 4, loss = 0.65378718\n",
      "Iteration 5, loss = 0.55850136\n",
      "Iteration 6, loss = 0.50090513\n",
      "Iteration 7, loss = 0.44092198\n",
      "Iteration 8, loss = 0.39692381\n",
      "Iteration 9, loss = 0.37143302\n",
      "Iteration 10, loss = 0.35785924\n",
      "Iteration 11, loss = 0.34672204\n",
      "Iteration 12, loss = 0.33622913\n",
      "Iteration 13, loss = 0.33129513\n",
      "Iteration 14, loss = 0.31963017\n",
      "Iteration 15, loss = 0.31288572\n",
      "Iteration 16, loss = 0.30327238\n",
      "Iteration 17, loss = 0.30810448\n",
      "Iteration 18, loss = 0.30701307\n",
      "Iteration 19, loss = 0.29745954\n",
      "Iteration 20, loss = 0.29395151\n",
      "Iteration 21, loss = 0.28094173\n",
      "Iteration 22, loss = 0.30481470\n",
      "Iteration 23, loss = 0.30275855\n",
      "Iteration 24, loss = 0.29653780\n",
      "Iteration 25, loss = 0.29163000\n",
      "Iteration 26, loss = 0.29538888\n",
      "Iteration 27, loss = 0.30359510\n",
      "Iteration 28, loss = 0.30389866\n",
      "Iteration 29, loss = 0.29119552\n",
      "Iteration 30, loss = 0.29485144\n",
      "Iteration 31, loss = 0.29935535\n",
      "Iteration 32, loss = 0.29290676\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.28198742\n",
      "Iteration 2, loss = 1.39584279\n",
      "Iteration 3, loss = 0.93643715\n",
      "Iteration 4, loss = 0.65355888\n",
      "Iteration 5, loss = 0.54208989\n",
      "Iteration 6, loss = 0.49692967\n",
      "Iteration 7, loss = 0.45275002\n",
      "Iteration 8, loss = 0.41182606\n",
      "Iteration 9, loss = 0.37678204\n",
      "Iteration 10, loss = 0.35350930\n",
      "Iteration 11, loss = 0.34701358\n",
      "Iteration 12, loss = 0.33572209\n",
      "Iteration 13, loss = 0.32773130\n",
      "Iteration 14, loss = 0.32243118\n",
      "Iteration 15, loss = 0.32034970\n",
      "Iteration 16, loss = 0.31324544\n",
      "Iteration 17, loss = 0.30340368\n",
      "Iteration 18, loss = 0.30091988\n",
      "Iteration 19, loss = 0.30639852\n",
      "Iteration 20, loss = 0.29423490\n",
      "Iteration 21, loss = 0.27607136\n",
      "Iteration 22, loss = 0.30505254\n",
      "Iteration 23, loss = 0.30863332\n",
      "Iteration 24, loss = 0.30225603\n",
      "Iteration 25, loss = 0.29373324\n",
      "Iteration 26, loss = 0.28746778\n",
      "Iteration 27, loss = 0.29569201\n",
      "Iteration 28, loss = 0.30744402\n",
      "Iteration 29, loss = 0.29438279\n",
      "Iteration 30, loss = 0.29614527\n",
      "Iteration 31, loss = 0.30394620\n",
      "Iteration 32, loss = 0.30409985\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.29057874\n",
      "Iteration 2, loss = 1.44148293\n",
      "Iteration 3, loss = 0.96223191\n",
      "Iteration 4, loss = 0.64980144\n",
      "Iteration 5, loss = 0.55591325\n",
      "Iteration 6, loss = 0.52240988\n",
      "Iteration 7, loss = 0.47028090\n",
      "Iteration 8, loss = 0.41699153\n",
      "Iteration 9, loss = 0.37480599\n",
      "Iteration 10, loss = 0.35124796\n",
      "Iteration 11, loss = 0.34831647\n",
      "Iteration 12, loss = 0.33785110\n",
      "Iteration 13, loss = 0.32458835\n",
      "Iteration 14, loss = 0.32193855\n",
      "Iteration 15, loss = 0.32363358\n",
      "Iteration 16, loss = 0.31308430\n",
      "Iteration 17, loss = 0.29590707\n",
      "Iteration 18, loss = 0.28694680\n",
      "Iteration 19, loss = 0.30091137\n",
      "Iteration 20, loss = 0.30168003\n",
      "Iteration 21, loss = 0.28251354\n",
      "Iteration 22, loss = 0.28891171\n",
      "Iteration 23, loss = 0.30418191\n",
      "Iteration 24, loss = 0.30798249\n",
      "Iteration 25, loss = 0.28976306\n",
      "Iteration 26, loss = 0.27326219\n",
      "Iteration 27, loss = 0.27972973\n",
      "Iteration 28, loss = 0.29343413\n",
      "Iteration 29, loss = 0.28581766\n",
      "Iteration 30, loss = 0.28264194\n",
      "Iteration 31, loss = 0.28458749\n",
      "Iteration 32, loss = 0.28051068\n",
      "Iteration 33, loss = 0.28371502\n",
      "Iteration 34, loss = 0.27562953\n",
      "Iteration 35, loss = 0.28490510\n",
      "Iteration 36, loss = 0.29035934\n",
      "Iteration 37, loss = 0.28398356\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.28418806\n",
      "Iteration 2, loss = 1.46098141\n",
      "Iteration 3, loss = 0.99656112\n",
      "Iteration 4, loss = 0.68168579\n",
      "Iteration 5, loss = 0.57184590\n",
      "Iteration 6, loss = 0.54385138\n",
      "Iteration 7, loss = 0.50296595\n",
      "Iteration 8, loss = 0.45224198\n",
      "Iteration 9, loss = 0.40199678\n",
      "Iteration 10, loss = 0.37042064\n",
      "Iteration 11, loss = 0.35859121\n",
      "Iteration 12, loss = 0.35127032\n",
      "Iteration 13, loss = 0.34956510\n",
      "Iteration 14, loss = 0.35090078\n",
      "Iteration 15, loss = 0.34261003\n",
      "Iteration 16, loss = 0.31749964\n",
      "Iteration 17, loss = 0.29611109\n",
      "Iteration 18, loss = 0.29820474\n",
      "Iteration 19, loss = 0.31311833\n",
      "Iteration 20, loss = 0.30886295\n",
      "Iteration 21, loss = 0.29147353\n",
      "Iteration 22, loss = 0.30344617\n",
      "Iteration 23, loss = 0.31879234\n",
      "Iteration 24, loss = 0.32174049\n",
      "Iteration 25, loss = 0.29925806\n",
      "Iteration 26, loss = 0.28433966\n",
      "Iteration 27, loss = 0.29189974\n",
      "Iteration 28, loss = 0.30663282\n",
      "Iteration 29, loss = 0.29754723\n",
      "Iteration 30, loss = 0.29282273\n",
      "Iteration 31, loss = 0.30068386\n",
      "Iteration 32, loss = 0.29889558\n",
      "Iteration 33, loss = 0.28207821\n",
      "Iteration 34, loss = 0.27817717\n",
      "Iteration 35, loss = 0.28832131\n",
      "Iteration 36, loss = 0.29503258\n",
      "Iteration 37, loss = 0.29108335\n",
      "Iteration 38, loss = 0.28564520\n",
      "Iteration 39, loss = 0.29296337\n",
      "Iteration 40, loss = 0.29598412\n",
      "Iteration 41, loss = 0.28633039\n",
      "Iteration 42, loss = 0.27874834\n",
      "Iteration 43, loss = 0.28098325\n",
      "Iteration 44, loss = 0.27828403\n",
      "Iteration 45, loss = 0.28268961\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.28469987\n",
      "Iteration 2, loss = 1.42421753\n",
      "Iteration 3, loss = 0.95915732\n",
      "Iteration 4, loss = 0.65499749\n",
      "Iteration 5, loss = 0.55455576\n",
      "Iteration 6, loss = 0.52336310\n",
      "Iteration 7, loss = 0.48287814\n",
      "Iteration 8, loss = 0.43278668\n",
      "Iteration 9, loss = 0.38761331\n",
      "Iteration 10, loss = 0.35628621\n",
      "Iteration 11, loss = 0.34883885\n",
      "Iteration 12, loss = 0.34335570\n",
      "Iteration 13, loss = 0.33379994\n",
      "Iteration 14, loss = 0.32819728\n",
      "Iteration 15, loss = 0.32163805\n",
      "Iteration 16, loss = 0.30120642\n",
      "Iteration 17, loss = 0.28415976\n",
      "Iteration 18, loss = 0.29527265\n",
      "Iteration 19, loss = 0.31479483\n",
      "Iteration 20, loss = 0.30476875\n",
      "Iteration 21, loss = 0.28763722\n",
      "Iteration 22, loss = 0.28937105\n",
      "Iteration 23, loss = 0.29536283\n",
      "Iteration 24, loss = 0.30005939\n",
      "Iteration 25, loss = 0.28715523\n",
      "Iteration 26, loss = 0.29107244\n",
      "Iteration 27, loss = 0.30683163\n",
      "Iteration 28, loss = 0.31416625\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.27581640\n",
      "Iteration 2, loss = 1.47130881\n",
      "Iteration 3, loss = 0.98339100\n",
      "Iteration 4, loss = 0.66511148\n",
      "Iteration 5, loss = 0.56977757\n",
      "Iteration 6, loss = 0.54782998\n",
      "Iteration 7, loss = 0.49860897\n",
      "Iteration 8, loss = 0.43644277\n",
      "Iteration 9, loss = 0.38798623\n",
      "Iteration 10, loss = 0.35985840\n",
      "Iteration 11, loss = 0.36148986\n",
      "Iteration 12, loss = 0.34721500\n",
      "Iteration 13, loss = 0.33413872\n",
      "Iteration 14, loss = 0.33479135\n",
      "Iteration 15, loss = 0.33385939\n",
      "Iteration 16, loss = 0.31892553\n",
      "Iteration 17, loss = 0.29835771\n",
      "Iteration 18, loss = 0.29582376\n",
      "Iteration 19, loss = 0.31076170\n",
      "Iteration 20, loss = 0.31137963\n",
      "Iteration 21, loss = 0.29844961\n",
      "Iteration 22, loss = 0.28730685\n",
      "Iteration 23, loss = 0.28582432\n",
      "Iteration 24, loss = 0.29481983\n",
      "Iteration 25, loss = 0.29588127\n",
      "Iteration 26, loss = 0.28654753\n",
      "Iteration 27, loss = 0.29436330\n",
      "Iteration 28, loss = 0.31164147\n",
      "Iteration 29, loss = 0.31014405\n",
      "Iteration 30, loss = 0.29218863\n",
      "Iteration 31, loss = 0.28748688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 32, loss = 0.28844416\n",
      "Iteration 33, loss = 0.27574330\n",
      "Iteration 34, loss = 0.27198807\n",
      "Iteration 35, loss = 0.28848524\n",
      "Iteration 36, loss = 0.30318954\n",
      "Iteration 37, loss = 0.29954648\n",
      "Iteration 38, loss = 0.29293279\n",
      "Iteration 39, loss = 0.28458287\n",
      "Iteration 40, loss = 0.27663267\n",
      "Iteration 41, loss = 0.27518892\n",
      "Iteration 42, loss = 0.28330195\n",
      "Iteration 43, loss = 0.29302765\n",
      "Iteration 44, loss = 0.28968458\n",
      "Iteration 45, loss = 0.28471847\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.27933125\n",
      "Iteration 2, loss = 1.47265846\n",
      "Iteration 3, loss = 0.99560008\n",
      "Iteration 4, loss = 0.67734599\n",
      "Iteration 5, loss = 0.57755190\n",
      "Iteration 6, loss = 0.55215332\n",
      "Iteration 7, loss = 0.50437267\n",
      "Iteration 8, loss = 0.44828580\n",
      "Iteration 9, loss = 0.40303535\n",
      "Iteration 10, loss = 0.37933109\n",
      "Iteration 11, loss = 0.36966074\n",
      "Iteration 12, loss = 0.36189593\n",
      "Iteration 13, loss = 0.35577111\n",
      "Iteration 14, loss = 0.35576100\n",
      "Iteration 15, loss = 0.35129776\n",
      "Iteration 16, loss = 0.33027408\n",
      "Iteration 17, loss = 0.30967681\n",
      "Iteration 18, loss = 0.31301497\n",
      "Iteration 19, loss = 0.33294010\n",
      "Iteration 20, loss = 0.32217673\n",
      "Iteration 21, loss = 0.31718059\n",
      "Iteration 22, loss = 0.32554192\n",
      "Iteration 23, loss = 0.33109711\n",
      "Iteration 24, loss = 0.32342537\n",
      "Iteration 25, loss = 0.30644837\n",
      "Iteration 26, loss = 0.29716337\n",
      "Iteration 27, loss = 0.30374948\n",
      "Iteration 28, loss = 0.32541921\n",
      "Iteration 29, loss = 0.32428213\n",
      "Iteration 30, loss = 0.30635443\n",
      "Iteration 31, loss = 0.30398826\n",
      "Iteration 32, loss = 0.30668344\n",
      "Iteration 33, loss = 0.28950873\n",
      "Iteration 34, loss = 0.28909202\n",
      "Iteration 35, loss = 0.31522299\n",
      "Iteration 36, loss = 0.32571475\n",
      "Iteration 37, loss = 0.31653923\n",
      "Iteration 38, loss = 0.30883318\n",
      "Iteration 39, loss = 0.30122714\n",
      "Iteration 40, loss = 0.29223794\n",
      "Iteration 41, loss = 0.29625172\n",
      "Iteration 42, loss = 0.31016437\n",
      "Iteration 43, loss = 0.31479612\n",
      "Iteration 44, loss = 0.30430781\n",
      "Iteration 45, loss = 0.29539450\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.28842842\n",
      "Iteration 2, loss = 1.44029512\n",
      "Iteration 3, loss = 0.95419717\n",
      "Iteration 4, loss = 0.65436035\n",
      "Iteration 5, loss = 0.56911470\n",
      "Iteration 6, loss = 0.53886318\n",
      "Iteration 7, loss = 0.48384021\n",
      "Iteration 8, loss = 0.43047209\n",
      "Iteration 9, loss = 0.39188032\n",
      "Iteration 10, loss = 0.37486312\n",
      "Iteration 11, loss = 0.36930997\n",
      "Iteration 12, loss = 0.35562137\n",
      "Iteration 13, loss = 0.34486106\n",
      "Iteration 14, loss = 0.34639613\n",
      "Iteration 15, loss = 0.34508042\n",
      "Iteration 16, loss = 0.32600498\n",
      "Iteration 17, loss = 0.30476880\n",
      "Iteration 18, loss = 0.30319216\n",
      "Iteration 19, loss = 0.32030596\n",
      "Iteration 20, loss = 0.31687189\n",
      "Iteration 21, loss = 0.31622002\n",
      "Iteration 22, loss = 0.31466521\n",
      "Iteration 23, loss = 0.31292896\n",
      "Iteration 24, loss = 0.31318900\n",
      "Iteration 25, loss = 0.30942287\n",
      "Iteration 26, loss = 0.29695271\n",
      "Iteration 27, loss = 0.29628979\n",
      "Iteration 28, loss = 0.31149311\n",
      "Iteration 29, loss = 0.31449838\n",
      "Iteration 30, loss = 0.30656167\n",
      "Iteration 31, loss = 0.30750903\n",
      "Iteration 32, loss = 0.30547116\n",
      "Iteration 33, loss = 0.28775303\n",
      "Iteration 34, loss = 0.28925891\n",
      "Iteration 35, loss = 0.32376988\n",
      "Iteration 36, loss = 0.33885669\n",
      "Iteration 37, loss = 0.32078635\n",
      "Iteration 38, loss = 0.30336824\n",
      "Iteration 39, loss = 0.29107891\n",
      "Iteration 40, loss = 0.28162825\n",
      "Iteration 41, loss = 0.29142082\n",
      "Iteration 42, loss = 0.30390726\n",
      "Iteration 43, loss = 0.30708199\n",
      "Iteration 44, loss = 0.29465810\n",
      "Iteration 45, loss = 0.28506075\n",
      "Iteration 46, loss = 0.28705838\n",
      "Iteration 47, loss = 0.30166679\n",
      "Iteration 48, loss = 0.30785653\n",
      "Iteration 49, loss = 0.30714524\n",
      "Iteration 50, loss = 0.31891992\n",
      "Iteration 51, loss = 0.32440443\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.27094483\n",
      "Iteration 2, loss = 1.47810851\n",
      "Iteration 3, loss = 0.99898250\n",
      "Iteration 4, loss = 0.66881510\n",
      "Iteration 5, loss = 0.57050989\n",
      "Iteration 6, loss = 0.54059491\n",
      "Iteration 7, loss = 0.48366657\n",
      "Iteration 8, loss = 0.42741110\n",
      "Iteration 9, loss = 0.38729651\n",
      "Iteration 10, loss = 0.36318008\n",
      "Iteration 11, loss = 0.35046344\n",
      "Iteration 12, loss = 0.34162793\n",
      "Iteration 13, loss = 0.33590729\n",
      "Iteration 14, loss = 0.33182571\n",
      "Iteration 15, loss = 0.32385024\n",
      "Iteration 16, loss = 0.30818176\n",
      "Iteration 17, loss = 0.29374028\n",
      "Iteration 18, loss = 0.30051258\n",
      "Iteration 19, loss = 0.31567883\n",
      "Iteration 20, loss = 0.30760804\n",
      "Iteration 21, loss = 0.31069381\n",
      "Iteration 22, loss = 0.31386777\n",
      "Iteration 23, loss = 0.31452028\n",
      "Iteration 24, loss = 0.30060988\n",
      "Iteration 25, loss = 0.29268324\n",
      "Iteration 26, loss = 0.29231104\n",
      "Iteration 27, loss = 0.29655286\n",
      "Iteration 28, loss = 0.30034460\n",
      "Iteration 29, loss = 0.29118422\n",
      "Iteration 30, loss = 0.28254128\n",
      "Iteration 31, loss = 0.28771117\n",
      "Iteration 32, loss = 0.29377752\n",
      "Iteration 33, loss = 0.28022038\n",
      "Iteration 34, loss = 0.28097923\n",
      "Iteration 35, loss = 0.29602669\n",
      "Iteration 36, loss = 0.30526449\n",
      "Iteration 37, loss = 0.29705984\n",
      "Iteration 38, loss = 0.28640895\n",
      "Iteration 39, loss = 0.27939650\n",
      "Iteration 40, loss = 0.27298515\n",
      "Iteration 41, loss = 0.28126460\n",
      "Iteration 42, loss = 0.28907014\n",
      "Iteration 43, loss = 0.29573393\n",
      "Iteration 44, loss = 0.28856781\n",
      "Iteration 45, loss = 0.27750024\n",
      "Iteration 46, loss = 0.28341420\n",
      "Iteration 47, loss = 0.30024083\n",
      "Iteration 48, loss = 0.30211436\n",
      "Iteration 49, loss = 0.29916989\n",
      "Iteration 50, loss = 0.30262905\n",
      "Iteration 51, loss = 0.30419824\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.33569994\n",
      "Iteration 2, loss = 1.33932603\n",
      "Iteration 3, loss = 0.90720938\n",
      "Iteration 4, loss = 0.64953747\n",
      "Iteration 5, loss = 0.52190301\n",
      "Iteration 6, loss = 0.48401534\n",
      "Iteration 7, loss = 0.44755408\n",
      "Iteration 8, loss = 0.40749951\n",
      "Iteration 9, loss = 0.37392684\n",
      "Iteration 10, loss = 0.35397703\n",
      "Iteration 11, loss = 0.34489491\n",
      "Iteration 12, loss = 0.34385150\n",
      "Iteration 13, loss = 0.33562286\n",
      "Iteration 14, loss = 0.31789492\n",
      "Iteration 15, loss = 0.30831659\n",
      "Iteration 16, loss = 0.30895407\n",
      "Iteration 17, loss = 0.30397868\n",
      "Iteration 18, loss = 0.30555589\n",
      "Iteration 19, loss = 0.31257772\n",
      "Iteration 20, loss = 0.29924386\n",
      "Iteration 21, loss = 0.29154287\n",
      "Iteration 22, loss = 0.29881033\n",
      "Iteration 23, loss = 0.31182372\n",
      "Iteration 24, loss = 0.30362360\n",
      "Iteration 25, loss = 0.28981542\n",
      "Iteration 26, loss = 0.29138578\n",
      "Iteration 27, loss = 0.31335267\n",
      "Iteration 28, loss = 0.31819572\n",
      "Iteration 29, loss = 0.30784611\n",
      "Iteration 30, loss = 0.30275444\n",
      "Iteration 31, loss = 0.31014479\n",
      "Iteration 32, loss = 0.31202389\n",
      "Iteration 33, loss = 0.31265066\n",
      "Iteration 34, loss = 0.31528936\n",
      "Iteration 35, loss = 0.29272567\n",
      "Iteration 36, loss = 0.30774190\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.33715540\n",
      "Iteration 2, loss = 1.30992493\n",
      "Iteration 3, loss = 0.85961444\n",
      "Iteration 4, loss = 0.62353615\n",
      "Iteration 5, loss = 0.51980914\n",
      "Iteration 6, loss = 0.47712801\n",
      "Iteration 7, loss = 0.43893877\n",
      "Iteration 8, loss = 0.39989186\n",
      "Iteration 9, loss = 0.38268941\n",
      "Iteration 10, loss = 0.36731490\n",
      "Iteration 11, loss = 0.35185564\n",
      "Iteration 12, loss = 0.34102427\n",
      "Iteration 13, loss = 0.32545060\n",
      "Iteration 14, loss = 0.31200264\n",
      "Iteration 15, loss = 0.30993577\n",
      "Iteration 16, loss = 0.31204153\n",
      "Iteration 17, loss = 0.30690418\n",
      "Iteration 18, loss = 0.30558111\n",
      "Iteration 19, loss = 0.29797507\n",
      "Iteration 20, loss = 0.29098753\n",
      "Iteration 21, loss = 0.29280700\n",
      "Iteration 22, loss = 0.29837176\n",
      "Iteration 23, loss = 0.29510416\n",
      "Iteration 24, loss = 0.29762083\n",
      "Iteration 25, loss = 0.31413209\n",
      "Iteration 26, loss = 0.30177803\n",
      "Iteration 27, loss = 0.29049043\n",
      "Iteration 28, loss = 0.29674344\n",
      "Iteration 29, loss = 0.29294544\n",
      "Iteration 30, loss = 0.27603966\n",
      "Iteration 31, loss = 0.28637926\n",
      "Iteration 32, loss = 0.29193841\n",
      "Iteration 33, loss = 0.28825290\n",
      "Iteration 34, loss = 0.30599642\n",
      "Iteration 35, loss = 0.30882440\n",
      "Iteration 36, loss = 0.30028495\n",
      "Iteration 37, loss = 0.29151407\n",
      "Iteration 38, loss = 0.29658331\n",
      "Iteration 39, loss = 0.29526002\n",
      "Iteration 40, loss = 0.29163931\n",
      "Iteration 41, loss = 0.27976424\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.34845432\n",
      "Iteration 2, loss = 1.32830141\n",
      "Iteration 3, loss = 0.87088392\n",
      "Iteration 4, loss = 0.62220312\n",
      "Iteration 5, loss = 0.51374846\n",
      "Iteration 6, loss = 0.46521075\n",
      "Iteration 7, loss = 0.43802460\n",
      "Iteration 8, loss = 0.40873439\n",
      "Iteration 9, loss = 0.38485970\n",
      "Iteration 10, loss = 0.36211025\n",
      "Iteration 11, loss = 0.34319297\n",
      "Iteration 12, loss = 0.33150225\n",
      "Iteration 13, loss = 0.32561821\n",
      "Iteration 14, loss = 0.31753462\n",
      "Iteration 15, loss = 0.31644670\n",
      "Iteration 16, loss = 0.30855187\n",
      "Iteration 17, loss = 0.29407602\n",
      "Iteration 18, loss = 0.29275530\n",
      "Iteration 19, loss = 0.29442833\n",
      "Iteration 20, loss = 0.29232637\n",
      "Iteration 21, loss = 0.29013860\n",
      "Iteration 22, loss = 0.29113456\n",
      "Iteration 23, loss = 0.28974301\n",
      "Iteration 24, loss = 0.29726134\n",
      "Iteration 25, loss = 0.32073321\n",
      "Iteration 26, loss = 0.30875664\n",
      "Iteration 27, loss = 0.29665577\n",
      "Iteration 28, loss = 0.30309321\n",
      "Iteration 29, loss = 0.28474635\n",
      "Iteration 30, loss = 0.28322062\n",
      "Iteration 31, loss = 0.30898473\n",
      "Iteration 32, loss = 0.30596826\n",
      "Iteration 33, loss = 0.28815184\n",
      "Iteration 34, loss = 0.29606485\n",
      "Iteration 35, loss = 0.30538227\n",
      "Iteration 36, loss = 0.29839999\n",
      "Iteration 37, loss = 0.28922396\n",
      "Iteration 38, loss = 0.29643693\n",
      "Iteration 39, loss = 0.28795606\n",
      "Iteration 40, loss = 0.28228081\n",
      "Iteration 41, loss = 0.27830203\n",
      "Iteration 42, loss = 0.27860378\n",
      "Iteration 43, loss = 0.28502312\n",
      "Iteration 44, loss = 0.29220522\n",
      "Iteration 45, loss = 0.29061445\n",
      "Iteration 46, loss = 0.28593035\n",
      "Iteration 47, loss = 0.29679237\n",
      "Iteration 48, loss = 0.28808677\n",
      "Iteration 49, loss = 0.29234113\n",
      "Iteration 50, loss = 0.29616577\n",
      "Iteration 51, loss = 0.29710134\n",
      "Iteration 52, loss = 0.29527005\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.29163710\n",
      "Iteration 2, loss = 1.25510227\n",
      "Iteration 3, loss = 0.81607327\n",
      "Iteration 4, loss = 0.59052215\n",
      "Iteration 5, loss = 0.49756336\n",
      "Iteration 6, loss = 0.45579025\n",
      "Iteration 7, loss = 0.43332384\n",
      "Iteration 8, loss = 0.40775377\n",
      "Iteration 9, loss = 0.37961233\n",
      "Iteration 10, loss = 0.36067545\n",
      "Iteration 11, loss = 0.34470086\n",
      "Iteration 12, loss = 0.32759337\n",
      "Iteration 13, loss = 0.32656596\n",
      "Iteration 14, loss = 0.34106776\n",
      "Iteration 15, loss = 0.33129902\n",
      "Iteration 16, loss = 0.30835101\n",
      "Iteration 17, loss = 0.29804689\n",
      "Iteration 18, loss = 0.29581041\n",
      "Iteration 19, loss = 0.28580244\n",
      "Iteration 20, loss = 0.28525184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 21, loss = 0.28610144\n",
      "Iteration 22, loss = 0.28265807\n",
      "Iteration 23, loss = 0.28237389\n",
      "Iteration 24, loss = 0.28165258\n",
      "Iteration 25, loss = 0.29749333\n",
      "Iteration 26, loss = 0.29796405\n",
      "Iteration 27, loss = 0.28251343\n",
      "Iteration 28, loss = 0.27574638\n",
      "Iteration 29, loss = 0.27396736\n",
      "Iteration 30, loss = 0.26934524\n",
      "Iteration 31, loss = 0.27513911\n",
      "Iteration 32, loss = 0.27157792\n",
      "Iteration 33, loss = 0.26499909\n",
      "Iteration 34, loss = 0.27777644\n",
      "Iteration 35, loss = 0.27778517\n",
      "Iteration 36, loss = 0.28480241\n",
      "Iteration 37, loss = 0.29243771\n",
      "Iteration 38, loss = 0.28679221\n",
      "Iteration 39, loss = 0.26624917\n",
      "Iteration 40, loss = 0.26339599\n",
      "Iteration 41, loss = 0.27377335\n",
      "Iteration 42, loss = 0.28606544\n",
      "Iteration 43, loss = 0.29096401\n",
      "Iteration 44, loss = 0.29131794\n",
      "Iteration 45, loss = 0.28348184\n",
      "Iteration 46, loss = 0.27140273\n",
      "Iteration 47, loss = 0.28900334\n",
      "Iteration 48, loss = 0.28352666\n",
      "Iteration 49, loss = 0.28591251\n",
      "Iteration 50, loss = 0.28668485\n",
      "Iteration 51, loss = 0.28926001\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.32255899\n",
      "Iteration 2, loss = 1.32861338\n",
      "Iteration 3, loss = 0.86725370\n",
      "Iteration 4, loss = 0.63095381\n",
      "Iteration 5, loss = 0.51051419\n",
      "Iteration 6, loss = 0.45847635\n",
      "Iteration 7, loss = 0.44595682\n",
      "Iteration 8, loss = 0.42390851\n",
      "Iteration 9, loss = 0.39603081\n",
      "Iteration 10, loss = 0.37273148\n",
      "Iteration 11, loss = 0.35081172\n",
      "Iteration 12, loss = 0.33873143\n",
      "Iteration 13, loss = 0.34026853\n",
      "Iteration 14, loss = 0.32920735\n",
      "Iteration 15, loss = 0.31786663\n",
      "Iteration 16, loss = 0.30730036\n",
      "Iteration 17, loss = 0.30104964\n",
      "Iteration 18, loss = 0.29769874\n",
      "Iteration 19, loss = 0.29833835\n",
      "Iteration 20, loss = 0.30727587\n",
      "Iteration 21, loss = 0.30142531\n",
      "Iteration 22, loss = 0.28638715\n",
      "Iteration 23, loss = 0.28299170\n",
      "Iteration 24, loss = 0.28623801\n",
      "Iteration 25, loss = 0.29614977\n",
      "Iteration 26, loss = 0.28996502\n",
      "Iteration 27, loss = 0.28299483\n",
      "Iteration 28, loss = 0.28745428\n",
      "Iteration 29, loss = 0.28532987\n",
      "Iteration 30, loss = 0.27298411\n",
      "Iteration 31, loss = 0.27601328\n",
      "Iteration 32, loss = 0.28136157\n",
      "Iteration 33, loss = 0.28512538\n",
      "Iteration 34, loss = 0.29292032\n",
      "Iteration 35, loss = 0.28105323\n",
      "Iteration 36, loss = 0.29835830\n",
      "Iteration 37, loss = 0.31071627\n",
      "Iteration 38, loss = 0.29989885\n",
      "Iteration 39, loss = 0.28250109\n",
      "Iteration 40, loss = 0.28871984\n",
      "Iteration 41, loss = 0.29706990\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.31851845\n",
      "Iteration 2, loss = 1.26822502\n",
      "Iteration 3, loss = 0.83318739\n",
      "Iteration 4, loss = 0.60797543\n",
      "Iteration 5, loss = 0.47847507\n",
      "Iteration 6, loss = 0.43816479\n",
      "Iteration 7, loss = 0.43940574\n",
      "Iteration 8, loss = 0.41927857\n",
      "Iteration 9, loss = 0.38593179\n",
      "Iteration 10, loss = 0.36018724\n",
      "Iteration 11, loss = 0.34101617\n",
      "Iteration 12, loss = 0.33031654\n",
      "Iteration 13, loss = 0.31865553\n",
      "Iteration 14, loss = 0.31165191\n",
      "Iteration 15, loss = 0.30786943\n",
      "Iteration 16, loss = 0.30117653\n",
      "Iteration 17, loss = 0.28846784\n",
      "Iteration 18, loss = 0.28158213\n",
      "Iteration 19, loss = 0.28629410\n",
      "Iteration 20, loss = 0.30066745\n",
      "Iteration 21, loss = 0.29565708\n",
      "Iteration 22, loss = 0.28103440\n",
      "Iteration 23, loss = 0.27945009\n",
      "Iteration 24, loss = 0.28501676\n",
      "Iteration 25, loss = 0.28421822\n",
      "Iteration 26, loss = 0.27415255\n",
      "Iteration 27, loss = 0.28826757\n",
      "Iteration 28, loss = 0.29589623\n",
      "Iteration 29, loss = 0.28005278\n",
      "Iteration 30, loss = 0.26596787\n",
      "Iteration 31, loss = 0.27724264\n",
      "Iteration 32, loss = 0.28940954\n",
      "Iteration 33, loss = 0.28221740\n",
      "Iteration 34, loss = 0.28432995\n",
      "Iteration 35, loss = 0.28634796\n",
      "Iteration 36, loss = 0.31077884\n",
      "Iteration 37, loss = 0.32224347\n",
      "Iteration 38, loss = 0.30023574\n",
      "Iteration 39, loss = 0.28506774\n",
      "Iteration 40, loss = 0.31105530\n",
      "Iteration 41, loss = 0.32184049\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.32219712\n",
      "Iteration 2, loss = 1.28443181\n",
      "Iteration 3, loss = 0.85711341\n",
      "Iteration 4, loss = 0.60918221\n",
      "Iteration 5, loss = 0.47874143\n",
      "Iteration 6, loss = 0.44671638\n",
      "Iteration 7, loss = 0.44617376\n",
      "Iteration 8, loss = 0.42349254\n",
      "Iteration 9, loss = 0.39146877\n",
      "Iteration 10, loss = 0.36861967\n",
      "Iteration 11, loss = 0.34976787\n",
      "Iteration 12, loss = 0.34209487\n",
      "Iteration 13, loss = 0.33190682\n",
      "Iteration 14, loss = 0.31620403\n",
      "Iteration 15, loss = 0.30892279\n",
      "Iteration 16, loss = 0.30413450\n",
      "Iteration 17, loss = 0.29087450\n",
      "Iteration 18, loss = 0.28791380\n",
      "Iteration 19, loss = 0.30191879\n",
      "Iteration 20, loss = 0.31263801\n",
      "Iteration 21, loss = 0.29812342\n",
      "Iteration 22, loss = 0.28984140\n",
      "Iteration 23, loss = 0.28393127\n",
      "Iteration 24, loss = 0.28004394\n",
      "Iteration 25, loss = 0.27686064\n",
      "Iteration 26, loss = 0.27925837\n",
      "Iteration 27, loss = 0.29432452\n",
      "Iteration 28, loss = 0.28293948\n",
      "Iteration 29, loss = 0.26812027\n",
      "Iteration 30, loss = 0.25652513\n",
      "Iteration 31, loss = 0.26590682\n",
      "Iteration 32, loss = 0.28108713\n",
      "Iteration 33, loss = 0.28326772\n",
      "Iteration 34, loss = 0.28513166\n",
      "Iteration 35, loss = 0.27349082\n",
      "Iteration 36, loss = 0.28914059\n",
      "Iteration 37, loss = 0.31309393\n",
      "Iteration 38, loss = 0.30630422\n",
      "Iteration 39, loss = 0.29676913\n",
      "Iteration 40, loss = 0.31087892\n",
      "Iteration 41, loss = 0.31375642\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.32553897\n",
      "Iteration 2, loss = 1.30283155\n",
      "Iteration 3, loss = 0.87048524\n",
      "Iteration 4, loss = 0.62219580\n",
      "Iteration 5, loss = 0.49993071\n",
      "Iteration 6, loss = 0.46372073\n",
      "Iteration 7, loss = 0.45399965\n",
      "Iteration 8, loss = 0.43429886\n",
      "Iteration 9, loss = 0.40598220\n",
      "Iteration 10, loss = 0.37925812\n",
      "Iteration 11, loss = 0.35409470\n",
      "Iteration 12, loss = 0.34801744\n",
      "Iteration 13, loss = 0.33781508\n",
      "Iteration 14, loss = 0.32514475\n",
      "Iteration 15, loss = 0.31987632\n",
      "Iteration 16, loss = 0.31066131\n",
      "Iteration 17, loss = 0.29356066\n",
      "Iteration 18, loss = 0.29149404\n",
      "Iteration 19, loss = 0.30586778\n",
      "Iteration 20, loss = 0.32076876\n",
      "Iteration 21, loss = 0.31855795\n",
      "Iteration 22, loss = 0.30161744\n",
      "Iteration 23, loss = 0.28875294\n",
      "Iteration 24, loss = 0.29534815\n",
      "Iteration 25, loss = 0.29052012\n",
      "Iteration 26, loss = 0.28156894\n",
      "Iteration 27, loss = 0.28789199\n",
      "Iteration 28, loss = 0.28742241\n",
      "Iteration 29, loss = 0.28123723\n",
      "Iteration 30, loss = 0.27315688\n",
      "Iteration 31, loss = 0.28049019\n",
      "Iteration 32, loss = 0.28842880\n",
      "Iteration 33, loss = 0.29112969\n",
      "Iteration 34, loss = 0.29377549\n",
      "Iteration 35, loss = 0.28440634\n",
      "Iteration 36, loss = 0.29592951\n",
      "Iteration 37, loss = 0.31592612\n",
      "Iteration 38, loss = 0.30269502\n",
      "Iteration 39, loss = 0.30047798\n",
      "Iteration 40, loss = 0.33092729\n",
      "Iteration 41, loss = 0.31505177\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.30390015\n",
      "Iteration 2, loss = 1.26057905\n",
      "Iteration 3, loss = 0.84374680\n",
      "Iteration 4, loss = 0.60246875\n",
      "Iteration 5, loss = 0.48302890\n",
      "Iteration 6, loss = 0.45242697\n",
      "Iteration 7, loss = 0.45084296\n",
      "Iteration 8, loss = 0.43504437\n",
      "Iteration 9, loss = 0.40235385\n",
      "Iteration 10, loss = 0.37018454\n",
      "Iteration 11, loss = 0.34188791\n",
      "Iteration 12, loss = 0.33681248\n",
      "Iteration 13, loss = 0.32959591\n",
      "Iteration 14, loss = 0.31692291\n",
      "Iteration 15, loss = 0.31466140\n",
      "Iteration 16, loss = 0.31154570\n",
      "Iteration 17, loss = 0.30015732\n",
      "Iteration 18, loss = 0.28893191\n",
      "Iteration 19, loss = 0.29340521\n",
      "Iteration 20, loss = 0.30821100\n",
      "Iteration 21, loss = 0.30644220\n",
      "Iteration 22, loss = 0.29510491\n",
      "Iteration 23, loss = 0.28632246\n",
      "Iteration 24, loss = 0.29508603\n",
      "Iteration 25, loss = 0.28365025\n",
      "Iteration 26, loss = 0.27594363\n",
      "Iteration 27, loss = 0.29264440\n",
      "Iteration 28, loss = 0.29670858\n",
      "Iteration 29, loss = 0.28620283\n",
      "Iteration 30, loss = 0.26971742\n",
      "Iteration 31, loss = 0.27953126\n",
      "Iteration 32, loss = 0.28707374\n",
      "Iteration 33, loss = 0.28188148\n",
      "Iteration 34, loss = 0.28702110\n",
      "Iteration 35, loss = 0.28699615\n",
      "Iteration 36, loss = 0.30028419\n",
      "Iteration 37, loss = 0.30773304\n",
      "Iteration 38, loss = 0.29324969\n",
      "Iteration 39, loss = 0.29225172\n",
      "Iteration 40, loss = 0.30594771\n",
      "Iteration 41, loss = 0.30383190\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.32818089\n",
      "Iteration 2, loss = 1.27034448\n",
      "Iteration 3, loss = 0.88280907\n",
      "Iteration 4, loss = 0.62694610\n",
      "Iteration 5, loss = 0.48446502\n",
      "Iteration 6, loss = 0.44219048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7, loss = 0.43983900\n",
      "Iteration 8, loss = 0.42708931\n",
      "Iteration 9, loss = 0.39664242\n",
      "Iteration 10, loss = 0.36760878\n",
      "Iteration 11, loss = 0.33860646\n",
      "Iteration 12, loss = 0.33826134\n",
      "Iteration 13, loss = 0.33093987\n",
      "Iteration 14, loss = 0.31380155\n",
      "Iteration 15, loss = 0.30497674\n",
      "Iteration 16, loss = 0.29356249\n",
      "Iteration 17, loss = 0.28930431\n",
      "Iteration 18, loss = 0.28381905\n",
      "Iteration 19, loss = 0.29072915\n",
      "Iteration 20, loss = 0.31036206\n",
      "Iteration 21, loss = 0.30776788\n",
      "Iteration 22, loss = 0.28437881\n",
      "Iteration 23, loss = 0.27006802\n",
      "Iteration 24, loss = 0.28931086\n",
      "Iteration 25, loss = 0.29812095\n",
      "Iteration 26, loss = 0.27682550\n",
      "Iteration 27, loss = 0.28020353\n",
      "Iteration 28, loss = 0.28867732\n",
      "Iteration 29, loss = 0.28406000\n",
      "Iteration 30, loss = 0.26888388\n",
      "Iteration 31, loss = 0.27080334\n",
      "Iteration 32, loss = 0.27838452\n",
      "Iteration 33, loss = 0.27507404\n",
      "Iteration 34, loss = 0.27257245\n",
      "Iteration 35, loss = 0.26929850\n",
      "Iteration 36, loss = 0.28389044\n",
      "Iteration 37, loss = 0.29785968\n",
      "Iteration 38, loss = 0.28912122\n",
      "Iteration 39, loss = 0.28171226\n",
      "Iteration 40, loss = 0.29604294\n",
      "Iteration 41, loss = 0.28404165\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.22118134\n",
      "Iteration 2, loss = 1.54323525\n",
      "Iteration 3, loss = 1.01504674\n",
      "Iteration 4, loss = 0.71150432\n",
      "Iteration 5, loss = 0.61448417\n",
      "Iteration 6, loss = 0.57208178\n",
      "Iteration 7, loss = 0.49711217\n",
      "Iteration 8, loss = 0.43613120\n",
      "Iteration 9, loss = 0.41429459\n",
      "Iteration 10, loss = 0.41136361\n",
      "Iteration 11, loss = 0.39942201\n",
      "Iteration 12, loss = 0.38467922\n",
      "Iteration 13, loss = 0.36812745\n",
      "Iteration 14, loss = 0.36051268\n",
      "Iteration 15, loss = 0.35437269\n",
      "Iteration 16, loss = 0.34042721\n",
      "Iteration 17, loss = 0.31642215\n",
      "Iteration 18, loss = 0.31284984\n",
      "Iteration 19, loss = 0.30732569\n",
      "Iteration 20, loss = 0.30225465\n",
      "Iteration 21, loss = 0.30188150\n",
      "Iteration 22, loss = 0.29296892\n",
      "Iteration 23, loss = 0.30257413\n",
      "Iteration 24, loss = 0.31690893\n",
      "Iteration 25, loss = 0.32942286\n",
      "Iteration 26, loss = 0.31603856\n",
      "Iteration 27, loss = 0.30525535\n",
      "Iteration 28, loss = 0.30224573\n",
      "Iteration 29, loss = 0.30233818\n",
      "Iteration 30, loss = 0.30613088\n",
      "Iteration 31, loss = 0.30702269\n",
      "Iteration 32, loss = 0.30737662\n",
      "Iteration 33, loss = 0.30072400\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.20591881\n",
      "Iteration 2, loss = 1.37982748\n",
      "Iteration 3, loss = 0.86243873\n",
      "Iteration 4, loss = 0.67605375\n",
      "Iteration 5, loss = 0.57150548\n",
      "Iteration 6, loss = 0.49363841\n",
      "Iteration 7, loss = 0.44605736\n",
      "Iteration 8, loss = 0.41770387\n",
      "Iteration 9, loss = 0.39547218\n",
      "Iteration 10, loss = 0.36955340\n",
      "Iteration 11, loss = 0.35215906\n",
      "Iteration 12, loss = 0.33891362\n",
      "Iteration 13, loss = 0.33450277\n",
      "Iteration 14, loss = 0.32395619\n",
      "Iteration 15, loss = 0.30607319\n",
      "Iteration 16, loss = 0.30104606\n",
      "Iteration 17, loss = 0.30279126\n",
      "Iteration 18, loss = 0.30550504\n",
      "Iteration 19, loss = 0.30063086\n",
      "Iteration 20, loss = 0.30653418\n",
      "Iteration 21, loss = 0.31337552\n",
      "Iteration 22, loss = 0.30062034\n",
      "Iteration 23, loss = 0.29535994\n",
      "Iteration 24, loss = 0.29232498\n",
      "Iteration 25, loss = 0.28968532\n",
      "Iteration 26, loss = 0.29370932\n",
      "Iteration 27, loss = 0.28875968\n",
      "Iteration 28, loss = 0.28518370\n",
      "Iteration 29, loss = 0.29831652\n",
      "Iteration 30, loss = 0.33661420\n",
      "Iteration 31, loss = 0.31386634\n",
      "Iteration 32, loss = 0.31311550\n",
      "Iteration 33, loss = 0.32540379\n",
      "Iteration 34, loss = 0.33245689\n",
      "Iteration 35, loss = 0.33037395\n",
      "Iteration 36, loss = 0.31346514\n",
      "Iteration 37, loss = 0.30305683\n",
      "Iteration 38, loss = 0.29935790\n",
      "Iteration 39, loss = 0.28859560\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.21143668\n",
      "Iteration 2, loss = 1.40477340\n",
      "Iteration 3, loss = 0.87398057\n",
      "Iteration 4, loss = 0.67353885\n",
      "Iteration 5, loss = 0.57268161\n",
      "Iteration 6, loss = 0.48764481\n",
      "Iteration 7, loss = 0.43258155\n",
      "Iteration 8, loss = 0.40985439\n",
      "Iteration 9, loss = 0.38363235\n",
      "Iteration 10, loss = 0.35627000\n",
      "Iteration 11, loss = 0.34407182\n",
      "Iteration 12, loss = 0.33388890\n",
      "Iteration 13, loss = 0.32965078\n",
      "Iteration 14, loss = 0.31825287\n",
      "Iteration 15, loss = 0.30040184\n",
      "Iteration 16, loss = 0.29176994\n",
      "Iteration 17, loss = 0.29400289\n",
      "Iteration 18, loss = 0.30322127\n",
      "Iteration 19, loss = 0.29843657\n",
      "Iteration 20, loss = 0.29926121\n",
      "Iteration 21, loss = 0.30040882\n",
      "Iteration 22, loss = 0.29574939\n",
      "Iteration 23, loss = 0.29738991\n",
      "Iteration 24, loss = 0.29628317\n",
      "Iteration 25, loss = 0.29331330\n",
      "Iteration 26, loss = 0.28890031\n",
      "Iteration 27, loss = 0.28239272\n",
      "Iteration 28, loss = 0.27962365\n",
      "Iteration 29, loss = 0.28589825\n",
      "Iteration 30, loss = 0.30451859\n",
      "Iteration 31, loss = 0.30400357\n",
      "Iteration 32, loss = 0.29166839\n",
      "Iteration 33, loss = 0.27755165\n",
      "Iteration 34, loss = 0.28456436\n",
      "Iteration 35, loss = 0.30527121\n",
      "Iteration 36, loss = 0.30428053\n",
      "Iteration 37, loss = 0.29248888\n",
      "Iteration 38, loss = 0.28346988\n",
      "Iteration 39, loss = 0.28078760\n",
      "Iteration 40, loss = 0.28255372\n",
      "Iteration 41, loss = 0.27333181\n",
      "Iteration 42, loss = 0.27295752\n",
      "Iteration 43, loss = 0.28382634\n",
      "Iteration 44, loss = 0.30360783\n",
      "Iteration 45, loss = 0.28897942\n",
      "Iteration 46, loss = 0.30386521\n",
      "Iteration 47, loss = 0.29544361\n",
      "Iteration 48, loss = 0.28531177\n",
      "Iteration 49, loss = 0.28854538\n",
      "Iteration 50, loss = 0.28493273\n",
      "Iteration 51, loss = 0.27813279\n",
      "Iteration 52, loss = 0.28675422\n",
      "Iteration 53, loss = 0.28860045\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.19033899\n",
      "Iteration 2, loss = 1.34982237\n",
      "Iteration 3, loss = 0.82548040\n",
      "Iteration 4, loss = 0.64672561\n",
      "Iteration 5, loss = 0.53611046\n",
      "Iteration 6, loss = 0.45625057\n",
      "Iteration 7, loss = 0.41639313\n",
      "Iteration 8, loss = 0.39735294\n",
      "Iteration 9, loss = 0.36863709\n",
      "Iteration 10, loss = 0.34109552\n",
      "Iteration 11, loss = 0.32611003\n",
      "Iteration 12, loss = 0.31650078\n",
      "Iteration 13, loss = 0.31750674\n",
      "Iteration 14, loss = 0.29854638\n",
      "Iteration 15, loss = 0.28534232\n",
      "Iteration 16, loss = 0.28171726\n",
      "Iteration 17, loss = 0.28326924\n",
      "Iteration 18, loss = 0.28958518\n",
      "Iteration 19, loss = 0.28711099\n",
      "Iteration 20, loss = 0.28667439\n",
      "Iteration 21, loss = 0.29263782\n",
      "Iteration 22, loss = 0.28260757\n",
      "Iteration 23, loss = 0.27194524\n",
      "Iteration 24, loss = 0.29152121\n",
      "Iteration 25, loss = 0.30037330\n",
      "Iteration 26, loss = 0.28127103\n",
      "Iteration 27, loss = 0.26063103\n",
      "Iteration 28, loss = 0.26090729\n",
      "Iteration 29, loss = 0.27308784\n",
      "Iteration 30, loss = 0.28664138\n",
      "Iteration 31, loss = 0.28097462\n",
      "Iteration 32, loss = 0.28129417\n",
      "Iteration 33, loss = 0.27486661\n",
      "Iteration 34, loss = 0.28342802\n",
      "Iteration 35, loss = 0.29811650\n",
      "Iteration 36, loss = 0.29965540\n",
      "Iteration 37, loss = 0.29151716\n",
      "Iteration 38, loss = 0.27443265\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.21556855\n",
      "Iteration 2, loss = 1.38762122\n",
      "Iteration 3, loss = 0.86325698\n",
      "Iteration 4, loss = 0.67901476\n",
      "Iteration 5, loss = 0.56651139\n",
      "Iteration 6, loss = 0.47486404\n",
      "Iteration 7, loss = 0.42687361\n",
      "Iteration 8, loss = 0.40585997\n",
      "Iteration 9, loss = 0.38010388\n",
      "Iteration 10, loss = 0.35168644\n",
      "Iteration 11, loss = 0.33895710\n",
      "Iteration 12, loss = 0.33007718\n",
      "Iteration 13, loss = 0.33471585\n",
      "Iteration 14, loss = 0.31329931\n",
      "Iteration 15, loss = 0.28748745\n",
      "Iteration 16, loss = 0.28623394\n",
      "Iteration 17, loss = 0.29363293\n",
      "Iteration 18, loss = 0.29851160\n",
      "Iteration 19, loss = 0.28937159\n",
      "Iteration 20, loss = 0.28997741\n",
      "Iteration 21, loss = 0.30439460\n",
      "Iteration 22, loss = 0.29063669\n",
      "Iteration 23, loss = 0.28548706\n",
      "Iteration 24, loss = 0.30831157\n",
      "Iteration 25, loss = 0.31038557\n",
      "Iteration 26, loss = 0.28598287\n",
      "Iteration 27, loss = 0.26463699\n",
      "Iteration 28, loss = 0.26719294\n",
      "Iteration 29, loss = 0.27952546\n",
      "Iteration 30, loss = 0.28449091\n",
      "Iteration 31, loss = 0.27898172\n",
      "Iteration 32, loss = 0.28537049\n",
      "Iteration 33, loss = 0.28871685\n",
      "Iteration 34, loss = 0.29253935\n",
      "Iteration 35, loss = 0.29139210\n",
      "Iteration 36, loss = 0.29555900\n",
      "Iteration 37, loss = 0.30141776\n",
      "Iteration 38, loss = 0.28916560\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.20115406\n",
      "Iteration 2, loss = 1.35619329\n",
      "Iteration 3, loss = 0.83830558\n",
      "Iteration 4, loss = 0.65253343\n",
      "Iteration 5, loss = 0.53884574\n",
      "Iteration 6, loss = 0.45457821\n",
      "Iteration 7, loss = 0.41057387\n",
      "Iteration 8, loss = 0.38864537\n",
      "Iteration 9, loss = 0.36167526\n",
      "Iteration 10, loss = 0.33514550\n",
      "Iteration 11, loss = 0.32118324\n",
      "Iteration 12, loss = 0.31429081\n",
      "Iteration 13, loss = 0.31948880\n",
      "Iteration 14, loss = 0.29802351\n",
      "Iteration 15, loss = 0.28128855\n",
      "Iteration 16, loss = 0.29258310\n",
      "Iteration 17, loss = 0.30211120\n",
      "Iteration 18, loss = 0.29788029\n",
      "Iteration 19, loss = 0.27889688\n",
      "Iteration 20, loss = 0.27874174\n",
      "Iteration 21, loss = 0.28297579\n",
      "Iteration 22, loss = 0.27708265\n",
      "Iteration 23, loss = 0.28590166\n",
      "Iteration 24, loss = 0.31125813\n",
      "Iteration 25, loss = 0.30838766\n",
      "Iteration 26, loss = 0.29196836\n",
      "Iteration 27, loss = 0.27503465\n",
      "Iteration 28, loss = 0.27519754\n",
      "Iteration 29, loss = 0.27864519\n",
      "Iteration 30, loss = 0.27926036\n",
      "Iteration 31, loss = 0.27546140\n",
      "Iteration 32, loss = 0.28597775\n",
      "Iteration 33, loss = 0.28816263\n",
      "Iteration 34, loss = 0.29140865\n",
      "Iteration 35, loss = 0.28668146\n",
      "Iteration 36, loss = 0.28955157\n",
      "Iteration 37, loss = 0.29896977\n",
      "Iteration 38, loss = 0.29813077\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.22776393\n",
      "Iteration 2, loss = 1.37301997\n",
      "Iteration 3, loss = 0.86648132\n",
      "Iteration 4, loss = 0.66499423\n",
      "Iteration 5, loss = 0.53859349\n",
      "Iteration 6, loss = 0.45848654\n",
      "Iteration 7, loss = 0.41691467\n",
      "Iteration 8, loss = 0.39248224\n",
      "Iteration 9, loss = 0.36067163\n",
      "Iteration 10, loss = 0.33599232\n",
      "Iteration 11, loss = 0.32375742\n",
      "Iteration 12, loss = 0.31643042\n",
      "Iteration 13, loss = 0.32662404\n",
      "Iteration 14, loss = 0.30937655\n",
      "Iteration 15, loss = 0.29090883\n",
      "Iteration 16, loss = 0.29920675\n",
      "Iteration 17, loss = 0.29787282\n",
      "Iteration 18, loss = 0.28924874\n",
      "Iteration 19, loss = 0.27949231\n",
      "Iteration 20, loss = 0.28294370\n",
      "Iteration 21, loss = 0.28434307\n",
      "Iteration 22, loss = 0.27464507\n",
      "Iteration 23, loss = 0.28594027\n",
      "Iteration 24, loss = 0.30097678\n",
      "Iteration 25, loss = 0.29264635\n",
      "Iteration 26, loss = 0.29449498\n",
      "Iteration 27, loss = 0.28734659\n",
      "Iteration 28, loss = 0.28650281\n",
      "Iteration 29, loss = 0.27970170\n",
      "Iteration 30, loss = 0.28019320\n",
      "Iteration 31, loss = 0.28818571\n",
      "Iteration 32, loss = 0.29574724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 33, loss = 0.29109573\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.22436378\n",
      "Iteration 2, loss = 1.41419230\n",
      "Iteration 3, loss = 0.90742462\n",
      "Iteration 4, loss = 0.69841496\n",
      "Iteration 5, loss = 0.57221329\n",
      "Iteration 6, loss = 0.48868524\n",
      "Iteration 7, loss = 0.44874780\n",
      "Iteration 8, loss = 0.42551664\n",
      "Iteration 9, loss = 0.38569812\n",
      "Iteration 10, loss = 0.35490539\n",
      "Iteration 11, loss = 0.34114907\n",
      "Iteration 12, loss = 0.33575499\n",
      "Iteration 13, loss = 0.33586936\n",
      "Iteration 14, loss = 0.30858078\n",
      "Iteration 15, loss = 0.29787507\n",
      "Iteration 16, loss = 0.31331613\n",
      "Iteration 17, loss = 0.30768083\n",
      "Iteration 18, loss = 0.29638616\n",
      "Iteration 19, loss = 0.29087060\n",
      "Iteration 20, loss = 0.29590658\n",
      "Iteration 21, loss = 0.29300387\n",
      "Iteration 22, loss = 0.28287467\n",
      "Iteration 23, loss = 0.29677301\n",
      "Iteration 24, loss = 0.29937880\n",
      "Iteration 25, loss = 0.28860526\n",
      "Iteration 26, loss = 0.29189058\n",
      "Iteration 27, loss = 0.28745028\n",
      "Iteration 28, loss = 0.28201130\n",
      "Iteration 29, loss = 0.27384934\n",
      "Iteration 30, loss = 0.27670965\n",
      "Iteration 31, loss = 0.29828963\n",
      "Iteration 32, loss = 0.30398526\n",
      "Iteration 33, loss = 0.29894941\n",
      "Iteration 34, loss = 0.29313304\n",
      "Iteration 35, loss = 0.30951395\n",
      "Iteration 36, loss = 0.32093380\n",
      "Iteration 37, loss = 0.31090344\n",
      "Iteration 38, loss = 0.28895742\n",
      "Iteration 39, loss = 0.28558151\n",
      "Iteration 40, loss = 0.28551063\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.21428704\n",
      "Iteration 2, loss = 1.37915212\n",
      "Iteration 3, loss = 0.87086208\n",
      "Iteration 4, loss = 0.68012086\n",
      "Iteration 5, loss = 0.55551245\n",
      "Iteration 6, loss = 0.47345554\n",
      "Iteration 7, loss = 0.43780882\n",
      "Iteration 8, loss = 0.42229272\n",
      "Iteration 9, loss = 0.38580735\n",
      "Iteration 10, loss = 0.35185995\n",
      "Iteration 11, loss = 0.33427562\n",
      "Iteration 12, loss = 0.32712848\n",
      "Iteration 13, loss = 0.32892812\n",
      "Iteration 14, loss = 0.30731884\n",
      "Iteration 15, loss = 0.29495783\n",
      "Iteration 16, loss = 0.29962914\n",
      "Iteration 17, loss = 0.29478778\n",
      "Iteration 18, loss = 0.29230851\n",
      "Iteration 19, loss = 0.28888059\n",
      "Iteration 20, loss = 0.28945311\n",
      "Iteration 21, loss = 0.28263075\n",
      "Iteration 22, loss = 0.27907699\n",
      "Iteration 23, loss = 0.28500979\n",
      "Iteration 24, loss = 0.28052150\n",
      "Iteration 25, loss = 0.27529152\n",
      "Iteration 26, loss = 0.28478674\n",
      "Iteration 27, loss = 0.28568433\n",
      "Iteration 28, loss = 0.28482127\n",
      "Iteration 29, loss = 0.27593303\n",
      "Iteration 30, loss = 0.27068736\n",
      "Iteration 31, loss = 0.29177318\n",
      "Iteration 32, loss = 0.29241388\n",
      "Iteration 33, loss = 0.29026867\n",
      "Iteration 34, loss = 0.28346227\n",
      "Iteration 35, loss = 0.29319695\n",
      "Iteration 36, loss = 0.30619433\n",
      "Iteration 37, loss = 0.30296795\n",
      "Iteration 38, loss = 0.29180755\n",
      "Iteration 39, loss = 0.28628602\n",
      "Iteration 40, loss = 0.28882696\n",
      "Iteration 41, loss = 0.28247860\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.22465215\n",
      "Iteration 2, loss = 1.38185472\n",
      "Iteration 3, loss = 0.87233141\n",
      "Iteration 4, loss = 0.67606194\n",
      "Iteration 5, loss = 0.53850355\n",
      "Iteration 6, loss = 0.45569680\n",
      "Iteration 7, loss = 0.42766721\n",
      "Iteration 8, loss = 0.41164145\n",
      "Iteration 9, loss = 0.37399683\n",
      "Iteration 10, loss = 0.34210188\n",
      "Iteration 11, loss = 0.32741896\n",
      "Iteration 12, loss = 0.31768377\n",
      "Iteration 13, loss = 0.30804673\n",
      "Iteration 14, loss = 0.29748213\n",
      "Iteration 15, loss = 0.29385965\n",
      "Iteration 16, loss = 0.29415306\n",
      "Iteration 17, loss = 0.28895178\n",
      "Iteration 18, loss = 0.28387223\n",
      "Iteration 19, loss = 0.28033911\n",
      "Iteration 20, loss = 0.27990231\n",
      "Iteration 21, loss = 0.27427605\n",
      "Iteration 22, loss = 0.27182610\n",
      "Iteration 23, loss = 0.27444218\n",
      "Iteration 24, loss = 0.26973146\n",
      "Iteration 25, loss = 0.26832753\n",
      "Iteration 26, loss = 0.28512720\n",
      "Iteration 27, loss = 0.28550206\n",
      "Iteration 28, loss = 0.27831128\n",
      "Iteration 29, loss = 0.27838117\n",
      "Iteration 30, loss = 0.27862939\n",
      "Iteration 31, loss = 0.29292060\n",
      "Iteration 32, loss = 0.29040671\n",
      "Iteration 33, loss = 0.29219515\n",
      "Iteration 34, loss = 0.28834903\n",
      "Iteration 35, loss = 0.28786566\n",
      "Iteration 36, loss = 0.27970318\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.39621877\n",
      "Iteration 2, loss = 1.50870703\n",
      "Iteration 3, loss = 0.99133417\n",
      "Iteration 4, loss = 0.71562797\n",
      "Iteration 5, loss = 0.61283109\n",
      "Iteration 6, loss = 0.53320601\n",
      "Iteration 7, loss = 0.46934170\n",
      "Iteration 8, loss = 0.43059603\n",
      "Iteration 9, loss = 0.40949528\n",
      "Iteration 10, loss = 0.39152180\n",
      "Iteration 11, loss = 0.36930161\n",
      "Iteration 12, loss = 0.34995136\n",
      "Iteration 13, loss = 0.34408075\n",
      "Iteration 14, loss = 0.33275658\n",
      "Iteration 15, loss = 0.33537300\n",
      "Iteration 16, loss = 0.33862688\n",
      "Iteration 17, loss = 0.33469825\n",
      "Iteration 18, loss = 0.31332821\n",
      "Iteration 19, loss = 0.30185947\n",
      "Iteration 20, loss = 0.29945722\n",
      "Iteration 21, loss = 0.31077116\n",
      "Iteration 22, loss = 0.31220333\n",
      "Iteration 23, loss = 0.30935504\n",
      "Iteration 24, loss = 0.30720595\n",
      "Iteration 25, loss = 0.29970613\n",
      "Iteration 26, loss = 0.28700272\n",
      "Iteration 27, loss = 0.28746707\n",
      "Iteration 28, loss = 0.29756950\n",
      "Iteration 29, loss = 0.30433179\n",
      "Iteration 30, loss = 0.29732711\n",
      "Iteration 31, loss = 0.29886652\n",
      "Iteration 32, loss = 0.30844602\n",
      "Iteration 33, loss = 0.30447684\n",
      "Iteration 34, loss = 0.30368643\n",
      "Iteration 35, loss = 0.30074419\n",
      "Iteration 36, loss = 0.29137270\n",
      "Iteration 37, loss = 0.30060468\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.38642636\n",
      "Iteration 2, loss = 1.48262318\n",
      "Iteration 3, loss = 0.97327229\n",
      "Iteration 4, loss = 0.69014321\n",
      "Iteration 5, loss = 0.62492178\n",
      "Iteration 6, loss = 0.57242945\n",
      "Iteration 7, loss = 0.49035078\n",
      "Iteration 8, loss = 0.43749978\n",
      "Iteration 9, loss = 0.42723944\n",
      "Iteration 10, loss = 0.41537193\n",
      "Iteration 11, loss = 0.38759752\n",
      "Iteration 12, loss = 0.35439970\n",
      "Iteration 13, loss = 0.33628305\n",
      "Iteration 14, loss = 0.33064686\n",
      "Iteration 15, loss = 0.32888760\n",
      "Iteration 16, loss = 0.33683045\n",
      "Iteration 17, loss = 0.32904460\n",
      "Iteration 18, loss = 0.31261858\n",
      "Iteration 19, loss = 0.31401926\n",
      "Iteration 20, loss = 0.32190111\n",
      "Iteration 21, loss = 0.31344602\n",
      "Iteration 22, loss = 0.29635180\n",
      "Iteration 23, loss = 0.27522376\n",
      "Iteration 24, loss = 0.26541274\n",
      "Iteration 25, loss = 0.27191842\n",
      "Iteration 26, loss = 0.27885509\n",
      "Iteration 27, loss = 0.29531234\n",
      "Iteration 28, loss = 0.30758019\n",
      "Iteration 29, loss = 0.29270134\n",
      "Iteration 30, loss = 0.29016776\n",
      "Iteration 31, loss = 0.30474274\n",
      "Iteration 32, loss = 0.31188299\n",
      "Iteration 33, loss = 0.30932751\n",
      "Iteration 34, loss = 0.31323210\n",
      "Iteration 35, loss = 0.29047720\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.38654942\n",
      "Iteration 2, loss = 1.50302104\n",
      "Iteration 3, loss = 0.96539806\n",
      "Iteration 4, loss = 0.69206791\n",
      "Iteration 5, loss = 0.60676838\n",
      "Iteration 6, loss = 0.54121464\n",
      "Iteration 7, loss = 0.46971845\n",
      "Iteration 8, loss = 0.41972754\n",
      "Iteration 9, loss = 0.40098661\n",
      "Iteration 10, loss = 0.38499633\n",
      "Iteration 11, loss = 0.36992129\n",
      "Iteration 12, loss = 0.35457050\n",
      "Iteration 13, loss = 0.34189888\n",
      "Iteration 14, loss = 0.32847750\n",
      "Iteration 15, loss = 0.31724779\n",
      "Iteration 16, loss = 0.32869875\n",
      "Iteration 17, loss = 0.32861412\n",
      "Iteration 18, loss = 0.31310634\n",
      "Iteration 19, loss = 0.30610258\n",
      "Iteration 20, loss = 0.29940817\n",
      "Iteration 21, loss = 0.30172462\n",
      "Iteration 22, loss = 0.29633667\n",
      "Iteration 23, loss = 0.28596634\n",
      "Iteration 24, loss = 0.27788597\n",
      "Iteration 25, loss = 0.27732839\n",
      "Iteration 26, loss = 0.28610012\n",
      "Iteration 27, loss = 0.30634659\n",
      "Iteration 28, loss = 0.32183743\n",
      "Iteration 29, loss = 0.30591565\n",
      "Iteration 30, loss = 0.28883809\n",
      "Iteration 31, loss = 0.30298991\n",
      "Iteration 32, loss = 0.31546483\n",
      "Iteration 33, loss = 0.31130375\n",
      "Iteration 34, loss = 0.30495878\n",
      "Iteration 35, loss = 0.28456188\n",
      "Iteration 36, loss = 0.27627732\n",
      "Iteration 37, loss = 0.28174384\n",
      "Iteration 38, loss = 0.29555359\n",
      "Iteration 39, loss = 0.29749374\n",
      "Iteration 40, loss = 0.29749980\n",
      "Iteration 41, loss = 0.29695915\n",
      "Iteration 42, loss = 0.30299059\n",
      "Iteration 43, loss = 0.29172768\n",
      "Iteration 44, loss = 0.28639774\n",
      "Iteration 45, loss = 0.28933712\n",
      "Iteration 46, loss = 0.30036831\n",
      "Iteration 47, loss = 0.29149600\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.37530389\n",
      "Iteration 2, loss = 1.44564523\n",
      "Iteration 3, loss = 0.95153250\n",
      "Iteration 4, loss = 0.67521757\n",
      "Iteration 5, loss = 0.58825867\n",
      "Iteration 6, loss = 0.53828081\n",
      "Iteration 7, loss = 0.46969707\n",
      "Iteration 8, loss = 0.41487139\n",
      "Iteration 9, loss = 0.39658358\n",
      "Iteration 10, loss = 0.38649573\n",
      "Iteration 11, loss = 0.37421270\n",
      "Iteration 12, loss = 0.35453508\n",
      "Iteration 13, loss = 0.34623582\n",
      "Iteration 14, loss = 0.33551968\n",
      "Iteration 15, loss = 0.31532994\n",
      "Iteration 16, loss = 0.31318016\n",
      "Iteration 17, loss = 0.31222313\n",
      "Iteration 18, loss = 0.30093984\n",
      "Iteration 19, loss = 0.29597769\n",
      "Iteration 20, loss = 0.29075109\n",
      "Iteration 21, loss = 0.28791767\n",
      "Iteration 22, loss = 0.28195633\n",
      "Iteration 23, loss = 0.27137557\n",
      "Iteration 24, loss = 0.26876429\n",
      "Iteration 25, loss = 0.26893920\n",
      "Iteration 26, loss = 0.27252737\n",
      "Iteration 27, loss = 0.29079778\n",
      "Iteration 28, loss = 0.29664724\n",
      "Iteration 29, loss = 0.28022797\n",
      "Iteration 30, loss = 0.27435790\n",
      "Iteration 31, loss = 0.29620325\n",
      "Iteration 32, loss = 0.31080603\n",
      "Iteration 33, loss = 0.30453453\n",
      "Iteration 34, loss = 0.28792857\n",
      "Iteration 35, loss = 0.27004650\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.38782290\n",
      "Iteration 2, loss = 1.47643063\n",
      "Iteration 3, loss = 0.94951049\n",
      "Iteration 4, loss = 0.68236956\n",
      "Iteration 5, loss = 0.59289817\n",
      "Iteration 6, loss = 0.52461551\n",
      "Iteration 7, loss = 0.45728098\n",
      "Iteration 8, loss = 0.41271462\n",
      "Iteration 9, loss = 0.39512608\n",
      "Iteration 10, loss = 0.37677174\n",
      "Iteration 11, loss = 0.35927154\n",
      "Iteration 12, loss = 0.34266414\n",
      "Iteration 13, loss = 0.33347469\n",
      "Iteration 14, loss = 0.32423373\n",
      "Iteration 15, loss = 0.31472560\n",
      "Iteration 16, loss = 0.31290067\n",
      "Iteration 17, loss = 0.30503797\n",
      "Iteration 18, loss = 0.29582812\n",
      "Iteration 19, loss = 0.29841772\n",
      "Iteration 20, loss = 0.29921551\n",
      "Iteration 21, loss = 0.29540873\n",
      "Iteration 22, loss = 0.29004025\n",
      "Iteration 23, loss = 0.27973202\n",
      "Iteration 24, loss = 0.27893234\n",
      "Iteration 25, loss = 0.27830796\n",
      "Iteration 26, loss = 0.29003030\n",
      "Iteration 27, loss = 0.30397168\n",
      "Iteration 28, loss = 0.30274781\n",
      "Iteration 29, loss = 0.28583081\n",
      "Iteration 30, loss = 0.28877968\n",
      "Iteration 31, loss = 0.30360016\n",
      "Iteration 32, loss = 0.31389708\n",
      "Iteration 33, loss = 0.31080104\n",
      "Iteration 34, loss = 0.29910268\n",
      "Iteration 35, loss = 0.28306018\n",
      "Iteration 36, loss = 0.28322593\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.38640001\n",
      "Iteration 2, loss = 1.48351216\n",
      "Iteration 3, loss = 0.94831829\n",
      "Iteration 4, loss = 0.68285737\n",
      "Iteration 5, loss = 0.58533647\n",
      "Iteration 6, loss = 0.50247171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7, loss = 0.43084049\n",
      "Iteration 8, loss = 0.38928707\n",
      "Iteration 9, loss = 0.37178902\n",
      "Iteration 10, loss = 0.35771883\n",
      "Iteration 11, loss = 0.34537995\n",
      "Iteration 12, loss = 0.32988181\n",
      "Iteration 13, loss = 0.32537231\n",
      "Iteration 14, loss = 0.31601229\n",
      "Iteration 15, loss = 0.30137223\n",
      "Iteration 16, loss = 0.29826289\n",
      "Iteration 17, loss = 0.30927667\n",
      "Iteration 18, loss = 0.30027287\n",
      "Iteration 19, loss = 0.28726268\n",
      "Iteration 20, loss = 0.28438192\n",
      "Iteration 21, loss = 0.28464982\n",
      "Iteration 22, loss = 0.28054603\n",
      "Iteration 23, loss = 0.27413670\n",
      "Iteration 24, loss = 0.27328530\n",
      "Iteration 25, loss = 0.27136492\n",
      "Iteration 26, loss = 0.27683841\n",
      "Iteration 27, loss = 0.28559598\n",
      "Iteration 28, loss = 0.29021908\n",
      "Iteration 29, loss = 0.27616387\n",
      "Iteration 30, loss = 0.28589343\n",
      "Iteration 31, loss = 0.30034716\n",
      "Iteration 32, loss = 0.30965083\n",
      "Iteration 33, loss = 0.31092136\n",
      "Iteration 34, loss = 0.29659539\n",
      "Iteration 35, loss = 0.27868263\n",
      "Iteration 36, loss = 0.27373519\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.39049672\n",
      "Iteration 2, loss = 1.50439217\n",
      "Iteration 3, loss = 0.94062784\n",
      "Iteration 4, loss = 0.70002090\n",
      "Iteration 5, loss = 0.60545841\n",
      "Iteration 6, loss = 0.50845566\n",
      "Iteration 7, loss = 0.43576867\n",
      "Iteration 8, loss = 0.39562942\n",
      "Iteration 9, loss = 0.37880425\n",
      "Iteration 10, loss = 0.36273016\n",
      "Iteration 11, loss = 0.34691632\n",
      "Iteration 12, loss = 0.33429262\n",
      "Iteration 13, loss = 0.33417299\n",
      "Iteration 14, loss = 0.32800477\n",
      "Iteration 15, loss = 0.31137859\n",
      "Iteration 16, loss = 0.30312976\n",
      "Iteration 17, loss = 0.32030203\n",
      "Iteration 18, loss = 0.31624824\n",
      "Iteration 19, loss = 0.30669998\n",
      "Iteration 20, loss = 0.29181194\n",
      "Iteration 21, loss = 0.28213598\n",
      "Iteration 22, loss = 0.27897785\n",
      "Iteration 23, loss = 0.27773250\n",
      "Iteration 24, loss = 0.27324823\n",
      "Iteration 25, loss = 0.27536783\n",
      "Iteration 26, loss = 0.29200634\n",
      "Iteration 27, loss = 0.30265600\n",
      "Iteration 28, loss = 0.30466828\n",
      "Iteration 29, loss = 0.28985243\n",
      "Iteration 30, loss = 0.28941030\n",
      "Iteration 31, loss = 0.29460142\n",
      "Iteration 32, loss = 0.28982084\n",
      "Iteration 33, loss = 0.28296886\n",
      "Iteration 34, loss = 0.27563670\n",
      "Iteration 35, loss = 0.27449195\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.38631195\n",
      "Iteration 2, loss = 1.51396832\n",
      "Iteration 3, loss = 0.96005069\n",
      "Iteration 4, loss = 0.72581087\n",
      "Iteration 5, loss = 0.62792814\n",
      "Iteration 6, loss = 0.52585690\n",
      "Iteration 7, loss = 0.45211120\n",
      "Iteration 8, loss = 0.41101301\n",
      "Iteration 9, loss = 0.38792638\n",
      "Iteration 10, loss = 0.37109222\n",
      "Iteration 11, loss = 0.35901919\n",
      "Iteration 12, loss = 0.34856874\n",
      "Iteration 13, loss = 0.34108443\n",
      "Iteration 14, loss = 0.33026340\n",
      "Iteration 15, loss = 0.31604345\n",
      "Iteration 16, loss = 0.32719613\n",
      "Iteration 17, loss = 0.34191120\n",
      "Iteration 18, loss = 0.32334820\n",
      "Iteration 19, loss = 0.31117468\n",
      "Iteration 20, loss = 0.30296441\n",
      "Iteration 21, loss = 0.29589017\n",
      "Iteration 22, loss = 0.29120022\n",
      "Iteration 23, loss = 0.28887709\n",
      "Iteration 24, loss = 0.28827790\n",
      "Iteration 25, loss = 0.30348329\n",
      "Iteration 26, loss = 0.31602812\n",
      "Iteration 27, loss = 0.31179874\n",
      "Iteration 28, loss = 0.31132923\n",
      "Iteration 29, loss = 0.29846361\n",
      "Iteration 30, loss = 0.29389469\n",
      "Iteration 31, loss = 0.30104727\n",
      "Iteration 32, loss = 0.30717672\n",
      "Iteration 33, loss = 0.29861897\n",
      "Iteration 34, loss = 0.28854849\n",
      "Iteration 35, loss = 0.29410892\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.38283291\n",
      "Iteration 2, loss = 1.49420704\n",
      "Iteration 3, loss = 0.94909568\n",
      "Iteration 4, loss = 0.70974652\n",
      "Iteration 5, loss = 0.61541505\n",
      "Iteration 6, loss = 0.52029188\n",
      "Iteration 7, loss = 0.44915120\n",
      "Iteration 8, loss = 0.40765426\n",
      "Iteration 9, loss = 0.38205014\n",
      "Iteration 10, loss = 0.36139794\n",
      "Iteration 11, loss = 0.35161498\n",
      "Iteration 12, loss = 0.34762995\n",
      "Iteration 13, loss = 0.34106510\n",
      "Iteration 14, loss = 0.32672334\n",
      "Iteration 15, loss = 0.30837167\n",
      "Iteration 16, loss = 0.32317404\n",
      "Iteration 17, loss = 0.33311022\n",
      "Iteration 18, loss = 0.30683589\n",
      "Iteration 19, loss = 0.29956314\n",
      "Iteration 20, loss = 0.30182015\n",
      "Iteration 21, loss = 0.29562326\n",
      "Iteration 22, loss = 0.28858341\n",
      "Iteration 23, loss = 0.28624615\n",
      "Iteration 24, loss = 0.28806921\n",
      "Iteration 25, loss = 0.30614862\n",
      "Iteration 26, loss = 0.31853443\n",
      "Iteration 27, loss = 0.31186619\n",
      "Iteration 28, loss = 0.30918651\n",
      "Iteration 29, loss = 0.29504352\n",
      "Iteration 30, loss = 0.28309313\n",
      "Iteration 31, loss = 0.28630147\n",
      "Iteration 32, loss = 0.29406300\n",
      "Iteration 33, loss = 0.29028910\n",
      "Iteration 34, loss = 0.28653064\n",
      "Iteration 35, loss = 0.29112148\n",
      "Iteration 36, loss = 0.29487217\n",
      "Iteration 37, loss = 0.29658625\n",
      "Iteration 38, loss = 0.30073743\n",
      "Iteration 39, loss = 0.30127621\n",
      "Iteration 40, loss = 0.29969801\n",
      "Iteration 41, loss = 0.31296942\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.38508054\n",
      "Iteration 2, loss = 1.51466827\n",
      "Iteration 3, loss = 0.93907042\n",
      "Iteration 4, loss = 0.69307580\n",
      "Iteration 5, loss = 0.59441250\n",
      "Iteration 6, loss = 0.50368632\n",
      "Iteration 7, loss = 0.44121402\n",
      "Iteration 8, loss = 0.40014336\n",
      "Iteration 9, loss = 0.37155531\n",
      "Iteration 10, loss = 0.35435036\n",
      "Iteration 11, loss = 0.34219748\n",
      "Iteration 12, loss = 0.33236146\n",
      "Iteration 13, loss = 0.32998398\n",
      "Iteration 14, loss = 0.32191219\n",
      "Iteration 15, loss = 0.30421126\n",
      "Iteration 16, loss = 0.31275102\n",
      "Iteration 17, loss = 0.32519680\n",
      "Iteration 18, loss = 0.30310590\n",
      "Iteration 19, loss = 0.29327290\n",
      "Iteration 20, loss = 0.28812528\n",
      "Iteration 21, loss = 0.27843943\n",
      "Iteration 22, loss = 0.27843717\n",
      "Iteration 23, loss = 0.28529798\n",
      "Iteration 24, loss = 0.28819374\n",
      "Iteration 25, loss = 0.29559912\n",
      "Iteration 26, loss = 0.30899156\n",
      "Iteration 27, loss = 0.30778060\n",
      "Iteration 28, loss = 0.29140570\n",
      "Iteration 29, loss = 0.28209491\n",
      "Iteration 30, loss = 0.27749067\n",
      "Iteration 31, loss = 0.26884400\n",
      "Iteration 32, loss = 0.27067160\n",
      "Iteration 33, loss = 0.27831299\n",
      "Iteration 34, loss = 0.28840843\n",
      "Iteration 35, loss = 0.29006772\n",
      "Iteration 36, loss = 0.29445038\n",
      "Iteration 37, loss = 0.29138375\n",
      "Iteration 38, loss = 0.29276870\n",
      "Iteration 39, loss = 0.29408678\n",
      "Iteration 40, loss = 0.30149849\n",
      "Iteration 41, loss = 0.31565614\n",
      "Iteration 42, loss = 0.30299353\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.18179344\n",
      "Iteration 2, loss = 1.46879358\n",
      "Iteration 3, loss = 0.95124735\n",
      "Iteration 4, loss = 0.66760825\n",
      "Iteration 5, loss = 0.56200044\n",
      "Iteration 6, loss = 0.51356421\n",
      "Iteration 7, loss = 0.46509115\n",
      "Iteration 8, loss = 0.41825076\n",
      "Iteration 9, loss = 0.38714884\n",
      "Iteration 10, loss = 0.36962964\n",
      "Iteration 11, loss = 0.35676614\n",
      "Iteration 12, loss = 0.35527610\n",
      "Iteration 13, loss = 0.35079790\n",
      "Iteration 14, loss = 0.32765189\n",
      "Iteration 15, loss = 0.32363055\n",
      "Iteration 16, loss = 0.32195861\n",
      "Iteration 17, loss = 0.31367150\n",
      "Iteration 18, loss = 0.31477705\n",
      "Iteration 19, loss = 0.32011183\n",
      "Iteration 20, loss = 0.31141240\n",
      "Iteration 21, loss = 0.30087185\n",
      "Iteration 22, loss = 0.30149561\n",
      "Iteration 23, loss = 0.31613420\n",
      "Iteration 24, loss = 0.32302119\n",
      "Iteration 25, loss = 0.31430044\n",
      "Iteration 26, loss = 0.29800864\n",
      "Iteration 27, loss = 0.30808399\n",
      "Iteration 28, loss = 0.30998028\n",
      "Iteration 29, loss = 0.29236167\n",
      "Iteration 30, loss = 0.29400443\n",
      "Iteration 31, loss = 0.30447006\n",
      "Iteration 32, loss = 0.29924151\n",
      "Iteration 33, loss = 0.29875507\n",
      "Iteration 34, loss = 0.30697379\n",
      "Iteration 35, loss = 0.30307878\n",
      "Iteration 36, loss = 0.29458737\n",
      "Iteration 37, loss = 0.28777342\n",
      "Iteration 38, loss = 0.30242516\n",
      "Iteration 39, loss = 0.33197452\n",
      "Iteration 40, loss = 0.31138401\n",
      "Iteration 41, loss = 0.29754345\n",
      "Iteration 42, loss = 0.30648341\n",
      "Iteration 43, loss = 0.31071912\n",
      "Iteration 44, loss = 0.31180863\n",
      "Iteration 45, loss = 0.29744962\n",
      "Iteration 46, loss = 0.28393889\n",
      "Iteration 47, loss = 0.27485659\n",
      "Iteration 48, loss = 0.29650951\n",
      "Iteration 49, loss = 0.32731383\n",
      "Iteration 50, loss = 0.32993171\n",
      "Iteration 51, loss = 0.31464419\n",
      "Iteration 52, loss = 0.30448054\n",
      "Iteration 53, loss = 0.31662684\n",
      "Iteration 54, loss = 0.32132561\n",
      "Iteration 55, loss = 0.30953708\n",
      "Iteration 56, loss = 0.30346264\n",
      "Iteration 57, loss = 0.31234489\n",
      "Iteration 58, loss = 0.29789085\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.16925186\n",
      "Iteration 2, loss = 1.32610512\n",
      "Iteration 3, loss = 0.83718681\n",
      "Iteration 4, loss = 0.63288160\n",
      "Iteration 5, loss = 0.57219077\n",
      "Iteration 6, loss = 0.54843632\n",
      "Iteration 7, loss = 0.47962792\n",
      "Iteration 8, loss = 0.40635468\n",
      "Iteration 9, loss = 0.39165286\n",
      "Iteration 10, loss = 0.40618633\n",
      "Iteration 11, loss = 0.38811798\n",
      "Iteration 12, loss = 0.34635561\n",
      "Iteration 13, loss = 0.33086676\n",
      "Iteration 14, loss = 0.33322897\n",
      "Iteration 15, loss = 0.33087534\n",
      "Iteration 16, loss = 0.32192330\n",
      "Iteration 17, loss = 0.31116732\n",
      "Iteration 18, loss = 0.30542403\n",
      "Iteration 19, loss = 0.30533336\n",
      "Iteration 20, loss = 0.30136290\n",
      "Iteration 21, loss = 0.30515968\n",
      "Iteration 22, loss = 0.31492681\n",
      "Iteration 23, loss = 0.33651218\n",
      "Iteration 24, loss = 0.32380678\n",
      "Iteration 25, loss = 0.29648484\n",
      "Iteration 26, loss = 0.29069900\n",
      "Iteration 27, loss = 0.30288100\n",
      "Iteration 28, loss = 0.30435981\n",
      "Iteration 29, loss = 0.30656248\n",
      "Iteration 30, loss = 0.30152347\n",
      "Iteration 31, loss = 0.30035837\n",
      "Iteration 32, loss = 0.29403075\n",
      "Iteration 33, loss = 0.29441673\n",
      "Iteration 34, loss = 0.29784228\n",
      "Iteration 35, loss = 0.28697389\n",
      "Iteration 36, loss = 0.28018139\n",
      "Iteration 37, loss = 0.28145882\n",
      "Iteration 38, loss = 0.27963246\n",
      "Iteration 39, loss = 0.28747136\n",
      "Iteration 40, loss = 0.30696134\n",
      "Iteration 41, loss = 0.30365631\n",
      "Iteration 42, loss = 0.29083003\n",
      "Iteration 43, loss = 0.30358094\n",
      "Iteration 44, loss = 0.32480479\n",
      "Iteration 45, loss = 0.31306571\n",
      "Iteration 46, loss = 0.30324905\n",
      "Iteration 47, loss = 0.30850715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 48, loss = 0.28971074\n",
      "Iteration 49, loss = 0.27185438\n",
      "Iteration 50, loss = 0.28363045\n",
      "Iteration 51, loss = 0.29722575\n",
      "Iteration 52, loss = 0.29450654\n",
      "Iteration 53, loss = 0.29593469\n",
      "Iteration 54, loss = 0.29214625\n",
      "Iteration 55, loss = 0.28509850\n",
      "Iteration 56, loss = 0.28415990\n",
      "Iteration 57, loss = 0.28868423\n",
      "Iteration 58, loss = 0.29899157\n",
      "Iteration 59, loss = 0.27989184\n",
      "Iteration 60, loss = 0.28161321\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.18411779\n",
      "Iteration 2, loss = 1.35211242\n",
      "Iteration 3, loss = 0.85642922\n",
      "Iteration 4, loss = 0.62667404\n",
      "Iteration 5, loss = 0.55096070\n",
      "Iteration 6, loss = 0.51406980\n",
      "Iteration 7, loss = 0.44285931\n",
      "Iteration 8, loss = 0.39035724\n",
      "Iteration 9, loss = 0.39039628\n",
      "Iteration 10, loss = 0.40272268\n",
      "Iteration 11, loss = 0.37315429\n",
      "Iteration 12, loss = 0.33157326\n",
      "Iteration 13, loss = 0.32939012\n",
      "Iteration 14, loss = 0.33995602\n",
      "Iteration 15, loss = 0.32909173\n",
      "Iteration 16, loss = 0.32350176\n",
      "Iteration 17, loss = 0.31598161\n",
      "Iteration 18, loss = 0.30261174\n",
      "Iteration 19, loss = 0.29532126\n",
      "Iteration 20, loss = 0.29522480\n",
      "Iteration 21, loss = 0.31048005\n",
      "Iteration 22, loss = 0.32422005\n",
      "Iteration 23, loss = 0.33523492\n",
      "Iteration 24, loss = 0.30702992\n",
      "Iteration 25, loss = 0.28171523\n",
      "Iteration 26, loss = 0.30261096\n",
      "Iteration 27, loss = 0.32863044\n",
      "Iteration 28, loss = 0.31679059\n",
      "Iteration 29, loss = 0.30172750\n",
      "Iteration 30, loss = 0.29109476\n",
      "Iteration 31, loss = 0.30076985\n",
      "Iteration 32, loss = 0.29972627\n",
      "Iteration 33, loss = 0.29492218\n",
      "Iteration 34, loss = 0.29939116\n",
      "Iteration 35, loss = 0.29463996\n",
      "Iteration 36, loss = 0.28271799\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.15366514\n",
      "Iteration 2, loss = 1.34584876\n",
      "Iteration 3, loss = 0.81922440\n",
      "Iteration 4, loss = 0.61077906\n",
      "Iteration 5, loss = 0.56896025\n",
      "Iteration 6, loss = 0.51380596\n",
      "Iteration 7, loss = 0.43655625\n",
      "Iteration 8, loss = 0.38309507\n",
      "Iteration 9, loss = 0.37741847\n",
      "Iteration 10, loss = 0.39091275\n",
      "Iteration 11, loss = 0.36685132\n",
      "Iteration 12, loss = 0.32418222\n",
      "Iteration 13, loss = 0.31920241\n",
      "Iteration 14, loss = 0.32867319\n",
      "Iteration 15, loss = 0.31750323\n",
      "Iteration 16, loss = 0.30587810\n",
      "Iteration 17, loss = 0.29249903\n",
      "Iteration 18, loss = 0.28655041\n",
      "Iteration 19, loss = 0.28841275\n",
      "Iteration 20, loss = 0.29024105\n",
      "Iteration 21, loss = 0.29152808\n",
      "Iteration 22, loss = 0.29168722\n",
      "Iteration 23, loss = 0.30088916\n",
      "Iteration 24, loss = 0.29249289\n",
      "Iteration 25, loss = 0.28752286\n",
      "Iteration 26, loss = 0.28523502\n",
      "Iteration 27, loss = 0.28976837\n",
      "Iteration 28, loss = 0.29223163\n",
      "Iteration 29, loss = 0.30083441\n",
      "Iteration 30, loss = 0.29602262\n",
      "Iteration 31, loss = 0.28583232\n",
      "Iteration 32, loss = 0.27169696\n",
      "Iteration 33, loss = 0.28127990\n",
      "Iteration 34, loss = 0.29795048\n",
      "Iteration 35, loss = 0.29641167\n",
      "Iteration 36, loss = 0.28297908\n",
      "Iteration 37, loss = 0.27497747\n",
      "Iteration 38, loss = 0.27539008\n",
      "Iteration 39, loss = 0.28773388\n",
      "Iteration 40, loss = 0.29342791\n",
      "Iteration 41, loss = 0.29970123\n",
      "Iteration 42, loss = 0.28701162\n",
      "Iteration 43, loss = 0.27621187\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.17487117\n",
      "Iteration 2, loss = 1.37761414\n",
      "Iteration 3, loss = 0.82817540\n",
      "Iteration 4, loss = 0.63808311\n",
      "Iteration 5, loss = 0.58410201\n",
      "Iteration 6, loss = 0.51344414\n",
      "Iteration 7, loss = 0.44090282\n",
      "Iteration 8, loss = 0.39668112\n",
      "Iteration 9, loss = 0.38215336\n",
      "Iteration 10, loss = 0.38229956\n",
      "Iteration 11, loss = 0.36226635\n",
      "Iteration 12, loss = 0.32884629\n",
      "Iteration 13, loss = 0.32449280\n",
      "Iteration 14, loss = 0.32715413\n",
      "Iteration 15, loss = 0.31650010\n",
      "Iteration 16, loss = 0.30863289\n",
      "Iteration 17, loss = 0.29204995\n",
      "Iteration 18, loss = 0.29044728\n",
      "Iteration 19, loss = 0.30135288\n",
      "Iteration 20, loss = 0.30672190\n",
      "Iteration 21, loss = 0.30918809\n",
      "Iteration 22, loss = 0.30423735\n",
      "Iteration 23, loss = 0.30213451\n",
      "Iteration 24, loss = 0.29831407\n",
      "Iteration 25, loss = 0.29509000\n",
      "Iteration 26, loss = 0.29257779\n",
      "Iteration 27, loss = 0.29954300\n",
      "Iteration 28, loss = 0.30192559\n",
      "Iteration 29, loss = 0.31055440\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.17359745\n",
      "Iteration 2, loss = 1.36267070\n",
      "Iteration 3, loss = 0.81259895\n",
      "Iteration 4, loss = 0.61778311\n",
      "Iteration 5, loss = 0.55481419\n",
      "Iteration 6, loss = 0.47899929\n",
      "Iteration 7, loss = 0.41413597\n",
      "Iteration 8, loss = 0.37907339\n",
      "Iteration 9, loss = 0.36948390\n",
      "Iteration 10, loss = 0.36078881\n",
      "Iteration 11, loss = 0.33981564\n",
      "Iteration 12, loss = 0.32703346\n",
      "Iteration 13, loss = 0.32262835\n",
      "Iteration 14, loss = 0.30985927\n",
      "Iteration 15, loss = 0.30612872\n",
      "Iteration 16, loss = 0.30728157\n",
      "Iteration 17, loss = 0.29635716\n",
      "Iteration 18, loss = 0.28864653\n",
      "Iteration 19, loss = 0.28890446\n",
      "Iteration 20, loss = 0.29495587\n",
      "Iteration 21, loss = 0.30553657\n",
      "Iteration 22, loss = 0.30891023\n",
      "Iteration 23, loss = 0.30405947\n",
      "Iteration 24, loss = 0.29873919\n",
      "Iteration 25, loss = 0.29309404\n",
      "Iteration 26, loss = 0.28090627\n",
      "Iteration 27, loss = 0.28570018\n",
      "Iteration 28, loss = 0.30224261\n",
      "Iteration 29, loss = 0.31223480\n",
      "Iteration 30, loss = 0.29101583\n",
      "Iteration 31, loss = 0.28489392\n",
      "Iteration 32, loss = 0.27971817\n",
      "Iteration 33, loss = 0.29017805\n",
      "Iteration 34, loss = 0.29724621\n",
      "Iteration 35, loss = 0.29883670\n",
      "Iteration 36, loss = 0.29399799\n",
      "Iteration 37, loss = 0.28214938\n",
      "Iteration 38, loss = 0.27557712\n",
      "Iteration 39, loss = 0.26711138\n",
      "Iteration 40, loss = 0.28172440\n",
      "Iteration 41, loss = 0.30622391\n",
      "Iteration 42, loss = 0.30251237\n",
      "Iteration 43, loss = 0.29799478\n",
      "Iteration 44, loss = 0.29533207\n",
      "Iteration 45, loss = 0.28601494\n",
      "Iteration 46, loss = 0.27873579\n",
      "Iteration 47, loss = 0.29024838\n",
      "Iteration 48, loss = 0.30274350\n",
      "Iteration 49, loss = 0.28269426\n",
      "Iteration 50, loss = 0.28593992\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.17833785\n",
      "Iteration 2, loss = 1.36181274\n",
      "Iteration 3, loss = 0.80957970\n",
      "Iteration 4, loss = 0.62818909\n",
      "Iteration 5, loss = 0.56275212\n",
      "Iteration 6, loss = 0.48288967\n",
      "Iteration 7, loss = 0.41345594\n",
      "Iteration 8, loss = 0.38729441\n",
      "Iteration 9, loss = 0.37837142\n",
      "Iteration 10, loss = 0.36011943\n",
      "Iteration 11, loss = 0.33317733\n",
      "Iteration 12, loss = 0.32555435\n",
      "Iteration 13, loss = 0.32706255\n",
      "Iteration 14, loss = 0.31188923\n",
      "Iteration 15, loss = 0.30151998\n",
      "Iteration 16, loss = 0.29849088\n",
      "Iteration 17, loss = 0.28858002\n",
      "Iteration 18, loss = 0.28496882\n",
      "Iteration 19, loss = 0.29558900\n",
      "Iteration 20, loss = 0.30490917\n",
      "Iteration 21, loss = 0.30218791\n",
      "Iteration 22, loss = 0.28885396\n",
      "Iteration 23, loss = 0.27971854\n",
      "Iteration 24, loss = 0.28549107\n",
      "Iteration 25, loss = 0.29584142\n",
      "Iteration 26, loss = 0.28704577\n",
      "Iteration 27, loss = 0.28431036\n",
      "Iteration 28, loss = 0.29523451\n",
      "Iteration 29, loss = 0.29556167\n",
      "Iteration 30, loss = 0.28584488\n",
      "Iteration 31, loss = 0.28497656\n",
      "Iteration 32, loss = 0.27988520\n",
      "Iteration 33, loss = 0.28599137\n",
      "Iteration 34, loss = 0.29276085\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.18124648\n",
      "Iteration 2, loss = 1.39555350\n",
      "Iteration 3, loss = 0.83934009\n",
      "Iteration 4, loss = 0.64360306\n",
      "Iteration 5, loss = 0.57257457\n",
      "Iteration 6, loss = 0.49034845\n",
      "Iteration 7, loss = 0.42416281\n",
      "Iteration 8, loss = 0.40013527\n",
      "Iteration 9, loss = 0.38942536\n",
      "Iteration 10, loss = 0.37236386\n",
      "Iteration 11, loss = 0.34653874\n",
      "Iteration 12, loss = 0.33813186\n",
      "Iteration 13, loss = 0.33972678\n",
      "Iteration 14, loss = 0.31950553\n",
      "Iteration 15, loss = 0.31490341\n",
      "Iteration 16, loss = 0.31400024\n",
      "Iteration 17, loss = 0.30002278\n",
      "Iteration 18, loss = 0.29396280\n",
      "Iteration 19, loss = 0.30105155\n",
      "Iteration 20, loss = 0.31100285\n",
      "Iteration 21, loss = 0.31249556\n",
      "Iteration 22, loss = 0.30749361\n",
      "Iteration 23, loss = 0.30020712\n",
      "Iteration 24, loss = 0.30080750\n",
      "Iteration 25, loss = 0.30337313\n",
      "Iteration 26, loss = 0.29171472\n",
      "Iteration 27, loss = 0.29156262\n",
      "Iteration 28, loss = 0.31095092\n",
      "Iteration 29, loss = 0.30627826\n",
      "Iteration 30, loss = 0.30118896\n",
      "Iteration 31, loss = 0.30315058\n",
      "Iteration 32, loss = 0.29121863\n",
      "Iteration 33, loss = 0.30019758\n",
      "Iteration 34, loss = 0.31679534\n",
      "Iteration 35, loss = 0.32317702\n",
      "Iteration 36, loss = 0.31283104\n",
      "Iteration 37, loss = 0.29784201\n",
      "Iteration 38, loss = 0.28660397\n",
      "Iteration 39, loss = 0.27566395\n",
      "Iteration 40, loss = 0.29495833\n",
      "Iteration 41, loss = 0.32937153\n",
      "Iteration 42, loss = 0.32905218\n",
      "Iteration 43, loss = 0.31043581\n",
      "Iteration 44, loss = 0.32244738\n",
      "Iteration 45, loss = 0.33541852\n",
      "Iteration 46, loss = 0.31868757\n",
      "Iteration 47, loss = 0.31253078\n",
      "Iteration 48, loss = 0.31942759\n",
      "Iteration 49, loss = 0.29437246\n",
      "Iteration 50, loss = 0.30347962\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.17704668\n",
      "Iteration 2, loss = 1.39587619\n",
      "Iteration 3, loss = 0.82959337\n",
      "Iteration 4, loss = 0.64078977\n",
      "Iteration 5, loss = 0.56797298\n",
      "Iteration 6, loss = 0.48295188\n",
      "Iteration 7, loss = 0.42794775\n",
      "Iteration 8, loss = 0.40734570\n",
      "Iteration 9, loss = 0.38956515\n",
      "Iteration 10, loss = 0.37215198\n",
      "Iteration 11, loss = 0.35011428\n",
      "Iteration 12, loss = 0.33749500\n",
      "Iteration 13, loss = 0.33300196\n",
      "Iteration 14, loss = 0.31952794\n",
      "Iteration 15, loss = 0.31697001\n",
      "Iteration 16, loss = 0.31214843\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 17, loss = 0.29547506\n",
      "Iteration 18, loss = 0.28885058\n",
      "Iteration 19, loss = 0.29662558\n",
      "Iteration 20, loss = 0.30467426\n",
      "Iteration 21, loss = 0.30421426\n",
      "Iteration 22, loss = 0.29845729\n",
      "Iteration 23, loss = 0.29304385\n",
      "Iteration 24, loss = 0.29258587\n",
      "Iteration 25, loss = 0.29786318\n",
      "Iteration 26, loss = 0.28915935\n",
      "Iteration 27, loss = 0.28917379\n",
      "Iteration 28, loss = 0.30863671\n",
      "Iteration 29, loss = 0.29879220\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.17949787\n",
      "Iteration 2, loss = 1.38780415\n",
      "Iteration 3, loss = 0.81484850\n",
      "Iteration 4, loss = 0.63582367\n",
      "Iteration 5, loss = 0.55212900\n",
      "Iteration 6, loss = 0.46512959\n",
      "Iteration 7, loss = 0.41528540\n",
      "Iteration 8, loss = 0.39451795\n",
      "Iteration 9, loss = 0.37451435\n",
      "Iteration 10, loss = 0.35426008\n",
      "Iteration 11, loss = 0.33480258\n",
      "Iteration 12, loss = 0.32760146\n",
      "Iteration 13, loss = 0.32316403\n",
      "Iteration 14, loss = 0.31079447\n",
      "Iteration 15, loss = 0.31251182\n",
      "Iteration 16, loss = 0.30251191\n",
      "Iteration 17, loss = 0.29438239\n",
      "Iteration 18, loss = 0.29622976\n",
      "Iteration 19, loss = 0.29387265\n",
      "Iteration 20, loss = 0.29015084\n",
      "Iteration 21, loss = 0.28718231\n",
      "Iteration 22, loss = 0.28475519\n",
      "Iteration 23, loss = 0.28634432\n",
      "Iteration 24, loss = 0.28998836\n",
      "Iteration 25, loss = 0.28999406\n",
      "Iteration 26, loss = 0.27660669\n",
      "Iteration 27, loss = 0.28993782\n",
      "Iteration 28, loss = 0.30735595\n",
      "Iteration 29, loss = 0.29455820\n",
      "Iteration 30, loss = 0.28665873\n",
      "Iteration 31, loss = 0.29070146\n",
      "Iteration 32, loss = 0.28252458\n",
      "Iteration 33, loss = 0.27241642\n",
      "Iteration 34, loss = 0.27476850\n",
      "Iteration 35, loss = 0.28745502\n",
      "Iteration 36, loss = 0.28371627\n",
      "Iteration 37, loss = 0.27865168\n",
      "Iteration 38, loss = 0.27403509\n",
      "Iteration 39, loss = 0.27222350\n",
      "Iteration 40, loss = 0.29281211\n",
      "Iteration 41, loss = 0.30167280\n",
      "Iteration 42, loss = 0.29386517\n",
      "Iteration 43, loss = 0.28351588\n",
      "Iteration 44, loss = 0.29344412\n",
      "Iteration 45, loss = 0.29604589\n",
      "Iteration 46, loss = 0.28101674\n",
      "Iteration 47, loss = 0.28926127\n",
      "Iteration 48, loss = 0.29847084\n",
      "Iteration 49, loss = 0.27833164\n",
      "Iteration 50, loss = 0.28987396\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.47451345\n",
      "Iteration 2, loss = 1.42586643\n",
      "Iteration 3, loss = 0.97617174\n",
      "Iteration 4, loss = 0.67191874\n",
      "Iteration 5, loss = 0.56317926\n",
      "Iteration 6, loss = 0.52662731\n",
      "Iteration 7, loss = 0.47846392\n",
      "Iteration 8, loss = 0.42252686\n",
      "Iteration 9, loss = 0.38543762\n",
      "Iteration 10, loss = 0.37466288\n",
      "Iteration 11, loss = 0.38468842\n",
      "Iteration 12, loss = 0.38190332\n",
      "Iteration 13, loss = 0.36190336\n",
      "Iteration 14, loss = 0.34960453\n",
      "Iteration 15, loss = 0.33122731\n",
      "Iteration 16, loss = 0.32089434\n",
      "Iteration 17, loss = 0.32231706\n",
      "Iteration 18, loss = 0.31761176\n",
      "Iteration 19, loss = 0.31336226\n",
      "Iteration 20, loss = 0.30659689\n",
      "Iteration 21, loss = 0.29101838\n",
      "Iteration 22, loss = 0.28252452\n",
      "Iteration 23, loss = 0.30711416\n",
      "Iteration 24, loss = 0.31017582\n",
      "Iteration 25, loss = 0.29795022\n",
      "Iteration 26, loss = 0.29010680\n",
      "Iteration 27, loss = 0.29053599\n",
      "Iteration 28, loss = 0.29042344\n",
      "Iteration 29, loss = 0.29434982\n",
      "Iteration 30, loss = 0.29764122\n",
      "Iteration 31, loss = 0.30263213\n",
      "Iteration 32, loss = 0.30197195\n",
      "Iteration 33, loss = 0.30425849\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.45734619\n",
      "Iteration 2, loss = 1.46670181\n",
      "Iteration 3, loss = 1.13931675\n",
      "Iteration 4, loss = 0.82908510\n",
      "Iteration 5, loss = 0.63873098\n",
      "Iteration 6, loss = 0.53932495\n",
      "Iteration 7, loss = 0.50293740\n",
      "Iteration 8, loss = 0.49534114\n",
      "Iteration 9, loss = 0.44409737\n",
      "Iteration 10, loss = 0.39059130\n",
      "Iteration 11, loss = 0.38549922\n",
      "Iteration 12, loss = 0.39863837\n",
      "Iteration 13, loss = 0.39513621\n",
      "Iteration 14, loss = 0.36866656\n",
      "Iteration 15, loss = 0.33552621\n",
      "Iteration 16, loss = 0.31786521\n",
      "Iteration 17, loss = 0.30895218\n",
      "Iteration 18, loss = 0.30314242\n",
      "Iteration 19, loss = 0.29491189\n",
      "Iteration 20, loss = 0.29084116\n",
      "Iteration 21, loss = 0.28853409\n",
      "Iteration 22, loss = 0.28774538\n",
      "Iteration 23, loss = 0.28489498\n",
      "Iteration 24, loss = 0.28420287\n",
      "Iteration 25, loss = 0.29380515\n",
      "Iteration 26, loss = 0.29278909\n",
      "Iteration 27, loss = 0.28626689\n",
      "Iteration 28, loss = 0.29156427\n",
      "Iteration 29, loss = 0.29013994\n",
      "Iteration 30, loss = 0.29169706\n",
      "Iteration 31, loss = 0.30711081\n",
      "Iteration 32, loss = 0.30398114\n",
      "Iteration 33, loss = 0.29490287\n",
      "Iteration 34, loss = 0.29004431\n",
      "Iteration 35, loss = 0.29514768\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.45172533\n",
      "Iteration 2, loss = 1.54186674\n",
      "Iteration 3, loss = 1.16445292\n",
      "Iteration 4, loss = 0.79583347\n",
      "Iteration 5, loss = 0.61024351\n",
      "Iteration 6, loss = 0.54185678\n",
      "Iteration 7, loss = 0.50604600\n",
      "Iteration 8, loss = 0.46794732\n",
      "Iteration 9, loss = 0.41442454\n",
      "Iteration 10, loss = 0.38015127\n",
      "Iteration 11, loss = 0.38093118\n",
      "Iteration 12, loss = 0.38493051\n",
      "Iteration 13, loss = 0.37708816\n",
      "Iteration 14, loss = 0.35043484\n",
      "Iteration 15, loss = 0.32652636\n",
      "Iteration 16, loss = 0.31328618\n",
      "Iteration 17, loss = 0.30696354\n",
      "Iteration 18, loss = 0.30419326\n",
      "Iteration 19, loss = 0.30066961\n",
      "Iteration 20, loss = 0.29149178\n",
      "Iteration 21, loss = 0.28038070\n",
      "Iteration 22, loss = 0.27971791\n",
      "Iteration 23, loss = 0.27990135\n",
      "Iteration 24, loss = 0.28016556\n",
      "Iteration 25, loss = 0.28518999\n",
      "Iteration 26, loss = 0.28936354\n",
      "Iteration 27, loss = 0.28696377\n",
      "Iteration 28, loss = 0.28207943\n",
      "Iteration 29, loss = 0.28352248\n",
      "Iteration 30, loss = 0.28541192\n",
      "Iteration 31, loss = 0.29629264\n",
      "Iteration 32, loss = 0.29799582\n",
      "Iteration 33, loss = 0.30360811\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.47362351\n",
      "Iteration 2, loss = 1.50403450\n",
      "Iteration 3, loss = 1.12475250\n",
      "Iteration 4, loss = 0.79324787\n",
      "Iteration 5, loss = 0.61282354\n",
      "Iteration 6, loss = 0.53314275\n",
      "Iteration 7, loss = 0.49271524\n",
      "Iteration 8, loss = 0.45577828\n",
      "Iteration 9, loss = 0.41168538\n",
      "Iteration 10, loss = 0.37972323\n",
      "Iteration 11, loss = 0.37304559\n",
      "Iteration 12, loss = 0.37566579\n",
      "Iteration 13, loss = 0.37179303\n",
      "Iteration 14, loss = 0.34975117\n",
      "Iteration 15, loss = 0.32480988\n",
      "Iteration 16, loss = 0.31148063\n",
      "Iteration 17, loss = 0.30236669\n",
      "Iteration 18, loss = 0.29390939\n",
      "Iteration 19, loss = 0.29289978\n",
      "Iteration 20, loss = 0.28488386\n",
      "Iteration 21, loss = 0.27844698\n",
      "Iteration 22, loss = 0.27407129\n",
      "Iteration 23, loss = 0.27368228\n",
      "Iteration 24, loss = 0.27386578\n",
      "Iteration 25, loss = 0.27657481\n",
      "Iteration 26, loss = 0.28148624\n",
      "Iteration 27, loss = 0.28985295\n",
      "Iteration 28, loss = 0.28154183\n",
      "Iteration 29, loss = 0.27543421\n",
      "Iteration 30, loss = 0.27901316\n",
      "Iteration 31, loss = 0.28485377\n",
      "Iteration 32, loss = 0.28126965\n",
      "Iteration 33, loss = 0.27998336\n",
      "Iteration 34, loss = 0.26826700\n",
      "Iteration 35, loss = 0.26591875\n",
      "Iteration 36, loss = 0.28124986\n",
      "Iteration 37, loss = 0.28367673\n",
      "Iteration 38, loss = 0.28895834\n",
      "Iteration 39, loss = 0.29591186\n",
      "Iteration 40, loss = 0.29270531\n",
      "Iteration 41, loss = 0.29266317\n",
      "Iteration 42, loss = 0.30144913\n",
      "Iteration 43, loss = 0.28619570\n",
      "Iteration 44, loss = 0.26966750\n",
      "Iteration 45, loss = 0.29324015\n",
      "Iteration 46, loss = 0.29367281\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.46809738\n",
      "Iteration 2, loss = 1.48602878\n",
      "Iteration 3, loss = 1.07171944\n",
      "Iteration 4, loss = 0.74878732\n",
      "Iteration 5, loss = 0.59345307\n",
      "Iteration 6, loss = 0.53883838\n",
      "Iteration 7, loss = 0.49475386\n",
      "Iteration 8, loss = 0.44359966\n",
      "Iteration 9, loss = 0.40200675\n",
      "Iteration 10, loss = 0.38615166\n",
      "Iteration 11, loss = 0.38788608\n",
      "Iteration 12, loss = 0.38317366\n",
      "Iteration 13, loss = 0.36595120\n",
      "Iteration 14, loss = 0.34046730\n",
      "Iteration 15, loss = 0.33050879\n",
      "Iteration 16, loss = 0.32738757\n",
      "Iteration 17, loss = 0.32373118\n",
      "Iteration 18, loss = 0.31509208\n",
      "Iteration 19, loss = 0.30728822\n",
      "Iteration 20, loss = 0.29456859\n",
      "Iteration 21, loss = 0.28631845\n",
      "Iteration 22, loss = 0.27473790\n",
      "Iteration 23, loss = 0.26939150\n",
      "Iteration 24, loss = 0.27604926\n",
      "Iteration 25, loss = 0.27952278\n",
      "Iteration 26, loss = 0.28569989\n",
      "Iteration 27, loss = 0.29942590\n",
      "Iteration 28, loss = 0.29744406\n",
      "Iteration 29, loss = 0.28641035\n",
      "Iteration 30, loss = 0.28145129\n",
      "Iteration 31, loss = 0.28397416\n",
      "Iteration 32, loss = 0.28557092\n",
      "Iteration 33, loss = 0.28861561\n",
      "Iteration 34, loss = 0.28051578\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.46786080\n",
      "Iteration 2, loss = 1.46038169\n",
      "Iteration 3, loss = 1.05095320\n",
      "Iteration 4, loss = 0.72882255\n",
      "Iteration 5, loss = 0.56897074\n",
      "Iteration 6, loss = 0.51661525\n",
      "Iteration 7, loss = 0.47965848\n",
      "Iteration 8, loss = 0.43099684\n",
      "Iteration 9, loss = 0.38656833\n",
      "Iteration 10, loss = 0.35979082\n",
      "Iteration 11, loss = 0.35013521\n",
      "Iteration 12, loss = 0.34778991\n",
      "Iteration 13, loss = 0.34206244\n",
      "Iteration 14, loss = 0.32363502\n",
      "Iteration 15, loss = 0.31568084\n",
      "Iteration 16, loss = 0.30978778\n",
      "Iteration 17, loss = 0.30815576\n",
      "Iteration 18, loss = 0.30397998\n",
      "Iteration 19, loss = 0.29310224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 20, loss = 0.28429845\n",
      "Iteration 21, loss = 0.28006724\n",
      "Iteration 22, loss = 0.27909103\n",
      "Iteration 23, loss = 0.27580684\n",
      "Iteration 24, loss = 0.27694587\n",
      "Iteration 25, loss = 0.28186849\n",
      "Iteration 26, loss = 0.28703099\n",
      "Iteration 27, loss = 0.28650005\n",
      "Iteration 28, loss = 0.28250277\n",
      "Iteration 29, loss = 0.27789014\n",
      "Iteration 30, loss = 0.27836498\n",
      "Iteration 31, loss = 0.27059734\n",
      "Iteration 32, loss = 0.27733511\n",
      "Iteration 33, loss = 0.28445000\n",
      "Iteration 34, loss = 0.27914828\n",
      "Iteration 35, loss = 0.27068696\n",
      "Iteration 36, loss = 0.26822461\n",
      "Iteration 37, loss = 0.27167088\n",
      "Iteration 38, loss = 0.29427361\n",
      "Iteration 39, loss = 0.30866078\n",
      "Iteration 40, loss = 0.30441708\n",
      "Iteration 41, loss = 0.27853899\n",
      "Iteration 42, loss = 0.26656750\n",
      "Iteration 43, loss = 0.27115887\n",
      "Iteration 44, loss = 0.27752805\n",
      "Iteration 45, loss = 0.27577518\n",
      "Iteration 46, loss = 0.27681087\n",
      "Iteration 47, loss = 0.27637019\n",
      "Iteration 48, loss = 0.29872453\n",
      "Iteration 49, loss = 0.32164009\n",
      "Iteration 50, loss = 0.30477902\n",
      "Iteration 51, loss = 0.28234744\n",
      "Iteration 52, loss = 0.28126437\n",
      "Iteration 53, loss = 0.28424894\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.47437165\n",
      "Iteration 2, loss = 1.45058460\n",
      "Iteration 3, loss = 1.03261370\n",
      "Iteration 4, loss = 0.75033960\n",
      "Iteration 5, loss = 0.58011548\n",
      "Iteration 6, loss = 0.51540045\n",
      "Iteration 7, loss = 0.47132837\n",
      "Iteration 8, loss = 0.41981422\n",
      "Iteration 9, loss = 0.38154383\n",
      "Iteration 10, loss = 0.36384335\n",
      "Iteration 11, loss = 0.35728151\n",
      "Iteration 12, loss = 0.35219502\n",
      "Iteration 13, loss = 0.34048834\n",
      "Iteration 14, loss = 0.32665661\n",
      "Iteration 15, loss = 0.32211607\n",
      "Iteration 16, loss = 0.32475383\n",
      "Iteration 17, loss = 0.31288592\n",
      "Iteration 18, loss = 0.29950394\n",
      "Iteration 19, loss = 0.29859249\n",
      "Iteration 20, loss = 0.29948148\n",
      "Iteration 21, loss = 0.28815503\n",
      "Iteration 22, loss = 0.28432981\n",
      "Iteration 23, loss = 0.27885264\n",
      "Iteration 24, loss = 0.27825253\n",
      "Iteration 25, loss = 0.28666909\n",
      "Iteration 26, loss = 0.29352965\n",
      "Iteration 27, loss = 0.28959786\n",
      "Iteration 28, loss = 0.27713422\n",
      "Iteration 29, loss = 0.27338472\n",
      "Iteration 30, loss = 0.27758861\n",
      "Iteration 31, loss = 0.27025574\n",
      "Iteration 32, loss = 0.28596082\n",
      "Iteration 33, loss = 0.29634261\n",
      "Iteration 34, loss = 0.27296238\n",
      "Iteration 35, loss = 0.27447824\n",
      "Iteration 36, loss = 0.28929649\n",
      "Iteration 37, loss = 0.29802901\n",
      "Iteration 38, loss = 0.31040398\n",
      "Iteration 39, loss = 0.32260601\n",
      "Iteration 40, loss = 0.31968998\n",
      "Iteration 41, loss = 0.29051274\n",
      "Iteration 42, loss = 0.28257358\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.47224552\n",
      "Iteration 2, loss = 1.42774733\n",
      "Iteration 3, loss = 1.00319218\n",
      "Iteration 4, loss = 0.75789069\n",
      "Iteration 5, loss = 0.59874028\n",
      "Iteration 6, loss = 0.52451399\n",
      "Iteration 7, loss = 0.47717780\n",
      "Iteration 8, loss = 0.42830145\n",
      "Iteration 9, loss = 0.39881471\n",
      "Iteration 10, loss = 0.38152341\n",
      "Iteration 11, loss = 0.37024605\n",
      "Iteration 12, loss = 0.36162457\n",
      "Iteration 13, loss = 0.35157339\n",
      "Iteration 14, loss = 0.33781851\n",
      "Iteration 15, loss = 0.33151215\n",
      "Iteration 16, loss = 0.33047166\n",
      "Iteration 17, loss = 0.31946558\n",
      "Iteration 18, loss = 0.30939094\n",
      "Iteration 19, loss = 0.30998030\n",
      "Iteration 20, loss = 0.30806855\n",
      "Iteration 21, loss = 0.29697696\n",
      "Iteration 22, loss = 0.29412227\n",
      "Iteration 23, loss = 0.28464519\n",
      "Iteration 24, loss = 0.28754004\n",
      "Iteration 25, loss = 0.30527090\n",
      "Iteration 26, loss = 0.30891872\n",
      "Iteration 27, loss = 0.29808596\n",
      "Iteration 28, loss = 0.28991817\n",
      "Iteration 29, loss = 0.29537361\n",
      "Iteration 30, loss = 0.29695117\n",
      "Iteration 31, loss = 0.28589637\n",
      "Iteration 32, loss = 0.30319670\n",
      "Iteration 33, loss = 0.30639328\n",
      "Iteration 34, loss = 0.28095763\n",
      "Iteration 35, loss = 0.28933237\n",
      "Iteration 36, loss = 0.31101103\n",
      "Iteration 37, loss = 0.31360075\n",
      "Iteration 38, loss = 0.31825958\n",
      "Iteration 39, loss = 0.32523585\n",
      "Iteration 40, loss = 0.32689366\n",
      "Iteration 41, loss = 0.30162253\n",
      "Iteration 42, loss = 0.30097316\n",
      "Iteration 43, loss = 0.31065421\n",
      "Iteration 44, loss = 0.30559681\n",
      "Iteration 45, loss = 0.28336153\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.47745886\n",
      "Iteration 2, loss = 1.44712763\n",
      "Iteration 3, loss = 1.02891580\n",
      "Iteration 4, loss = 0.74744495\n",
      "Iteration 5, loss = 0.58359530\n",
      "Iteration 6, loss = 0.52750338\n",
      "Iteration 7, loss = 0.49284908\n",
      "Iteration 8, loss = 0.43551357\n",
      "Iteration 9, loss = 0.39632055\n",
      "Iteration 10, loss = 0.38111182\n",
      "Iteration 11, loss = 0.37856137\n",
      "Iteration 12, loss = 0.37276096\n",
      "Iteration 13, loss = 0.35964692\n",
      "Iteration 14, loss = 0.33846040\n",
      "Iteration 15, loss = 0.32759849\n",
      "Iteration 16, loss = 0.32527074\n",
      "Iteration 17, loss = 0.31731253\n",
      "Iteration 18, loss = 0.30647905\n",
      "Iteration 19, loss = 0.30600230\n",
      "Iteration 20, loss = 0.30443824\n",
      "Iteration 21, loss = 0.29199713\n",
      "Iteration 22, loss = 0.29486841\n",
      "Iteration 23, loss = 0.29062461\n",
      "Iteration 24, loss = 0.29931192\n",
      "Iteration 25, loss = 0.31864840\n",
      "Iteration 26, loss = 0.30959111\n",
      "Iteration 27, loss = 0.29091775\n",
      "Iteration 28, loss = 0.28297278\n",
      "Iteration 29, loss = 0.29169247\n",
      "Iteration 30, loss = 0.29951430\n",
      "Iteration 31, loss = 0.28510543\n",
      "Iteration 32, loss = 0.30044166\n",
      "Iteration 33, loss = 0.31136419\n",
      "Iteration 34, loss = 0.28243343\n",
      "Iteration 35, loss = 0.28041673\n",
      "Iteration 36, loss = 0.30190619\n",
      "Iteration 37, loss = 0.30584333\n",
      "Iteration 38, loss = 0.30779276\n",
      "Iteration 39, loss = 0.31749015\n",
      "Iteration 40, loss = 0.32918758\n",
      "Iteration 41, loss = 0.29807519\n",
      "Iteration 42, loss = 0.28889634\n",
      "Iteration 43, loss = 0.31079714\n",
      "Iteration 44, loss = 0.32024490\n",
      "Iteration 45, loss = 0.29403247\n",
      "Iteration 46, loss = 0.27518014\n",
      "Iteration 47, loss = 0.27866061\n",
      "Iteration 48, loss = 0.30063320\n",
      "Iteration 49, loss = 0.31164596\n",
      "Iteration 50, loss = 0.30729029\n",
      "Iteration 51, loss = 0.30213836\n",
      "Iteration 52, loss = 0.31416439\n",
      "Iteration 53, loss = 0.30958624\n",
      "Iteration 54, loss = 0.30249404\n",
      "Iteration 55, loss = 0.30431237\n",
      "Iteration 56, loss = 0.29520682\n",
      "Iteration 57, loss = 0.29366635\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.47890527\n",
      "Iteration 2, loss = 1.44573982\n",
      "Iteration 3, loss = 0.99186534\n",
      "Iteration 4, loss = 0.72587323\n",
      "Iteration 5, loss = 0.57517749\n",
      "Iteration 6, loss = 0.50899494\n",
      "Iteration 7, loss = 0.46616874\n",
      "Iteration 8, loss = 0.41440155\n",
      "Iteration 9, loss = 0.38132063\n",
      "Iteration 10, loss = 0.36905018\n",
      "Iteration 11, loss = 0.36585386\n",
      "Iteration 12, loss = 0.35694217\n",
      "Iteration 13, loss = 0.34076610\n",
      "Iteration 14, loss = 0.32003972\n",
      "Iteration 15, loss = 0.31318865\n",
      "Iteration 16, loss = 0.31459129\n",
      "Iteration 17, loss = 0.31001149\n",
      "Iteration 18, loss = 0.29463332\n",
      "Iteration 19, loss = 0.29075756\n",
      "Iteration 20, loss = 0.29342958\n",
      "Iteration 21, loss = 0.28376608\n",
      "Iteration 22, loss = 0.28440590\n",
      "Iteration 23, loss = 0.27946678\n",
      "Iteration 24, loss = 0.28164124\n",
      "Iteration 25, loss = 0.29848691\n",
      "Iteration 26, loss = 0.30273137\n",
      "Iteration 27, loss = 0.28223424\n",
      "Iteration 28, loss = 0.27210223\n",
      "Iteration 29, loss = 0.28272962\n",
      "Iteration 30, loss = 0.29678817\n",
      "Iteration 31, loss = 0.28175705\n",
      "Iteration 32, loss = 0.29990366\n",
      "Iteration 33, loss = 0.31632128\n",
      "Iteration 34, loss = 0.28040242\n",
      "Iteration 35, loss = 0.27264464\n",
      "Iteration 36, loss = 0.29157880\n",
      "Iteration 37, loss = 0.29996831\n",
      "Iteration 38, loss = 0.29866681\n",
      "Iteration 39, loss = 0.29271086\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.29000391\n",
      "Iteration 2, loss = 1.30276640\n",
      "Iteration 3, loss = 0.85376673\n",
      "Iteration 4, loss = 0.62091484\n",
      "Iteration 5, loss = 0.50812155\n",
      "Iteration 6, loss = 0.47732117\n",
      "Iteration 7, loss = 0.45207567\n",
      "Iteration 8, loss = 0.41698311\n",
      "Iteration 9, loss = 0.38908610\n",
      "Iteration 10, loss = 0.37482673\n",
      "Iteration 11, loss = 0.39472410\n",
      "Iteration 12, loss = 0.38461550\n",
      "Iteration 13, loss = 0.34427030\n",
      "Iteration 14, loss = 0.32886296\n",
      "Iteration 15, loss = 0.32479809\n",
      "Iteration 16, loss = 0.32341957\n",
      "Iteration 17, loss = 0.31653065\n",
      "Iteration 18, loss = 0.31759628\n",
      "Iteration 19, loss = 0.31660042\n",
      "Iteration 20, loss = 0.31778285\n",
      "Iteration 21, loss = 0.30658159\n",
      "Iteration 22, loss = 0.31182503\n",
      "Iteration 23, loss = 0.31509436\n",
      "Iteration 24, loss = 0.29552689\n",
      "Iteration 25, loss = 0.28408782\n",
      "Iteration 26, loss = 0.28632991\n",
      "Iteration 27, loss = 0.29486999\n",
      "Iteration 28, loss = 0.30197837\n",
      "Iteration 29, loss = 0.29941711\n",
      "Iteration 30, loss = 0.32216522\n",
      "Iteration 31, loss = 0.31105283\n",
      "Iteration 32, loss = 0.29421272\n",
      "Iteration 33, loss = 0.29134333\n",
      "Iteration 34, loss = 0.27987180\n",
      "Iteration 35, loss = 0.28481095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 36, loss = 0.30642077\n",
      "Iteration 37, loss = 0.31686188\n",
      "Iteration 38, loss = 0.31591024\n",
      "Iteration 39, loss = 0.31868616\n",
      "Iteration 40, loss = 0.30146376\n",
      "Iteration 41, loss = 0.30932254\n",
      "Iteration 42, loss = 0.32247596\n",
      "Iteration 43, loss = 0.32282920\n",
      "Iteration 44, loss = 0.31469513\n",
      "Iteration 45, loss = 0.29552976\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.32432912\n",
      "Iteration 2, loss = 1.28909421\n",
      "Iteration 3, loss = 1.07523569\n",
      "Iteration 4, loss = 0.77810511\n",
      "Iteration 5, loss = 0.56877846\n",
      "Iteration 6, loss = 0.52931021\n",
      "Iteration 7, loss = 0.51994083\n",
      "Iteration 8, loss = 0.47493001\n",
      "Iteration 9, loss = 0.42870833\n",
      "Iteration 10, loss = 0.38667316\n",
      "Iteration 11, loss = 0.36771607\n",
      "Iteration 12, loss = 0.36711202\n",
      "Iteration 13, loss = 0.37048125\n",
      "Iteration 14, loss = 0.35841022\n",
      "Iteration 15, loss = 0.33410109\n",
      "Iteration 16, loss = 0.32405022\n",
      "Iteration 17, loss = 0.33517332\n",
      "Iteration 18, loss = 0.33431094\n",
      "Iteration 19, loss = 0.32068364\n",
      "Iteration 20, loss = 0.31513739\n",
      "Iteration 21, loss = 0.32630435\n",
      "Iteration 22, loss = 0.31910207\n",
      "Iteration 23, loss = 0.29997295\n",
      "Iteration 24, loss = 0.30132987\n",
      "Iteration 25, loss = 0.30724986\n",
      "Iteration 26, loss = 0.30547641\n",
      "Iteration 27, loss = 0.31059803\n",
      "Iteration 28, loss = 0.30582125\n",
      "Iteration 29, loss = 0.29390145\n",
      "Iteration 30, loss = 0.29577662\n",
      "Iteration 31, loss = 0.29168065\n",
      "Iteration 32, loss = 0.31375186\n",
      "Iteration 33, loss = 0.29488102\n",
      "Iteration 34, loss = 0.30696584\n",
      "Iteration 35, loss = 0.32220278\n",
      "Iteration 36, loss = 0.31190156\n",
      "Iteration 37, loss = 0.32775097\n",
      "Iteration 38, loss = 0.33695960\n",
      "Iteration 39, loss = 0.32087452\n",
      "Iteration 40, loss = 0.31599302\n",
      "Iteration 41, loss = 0.32268179\n",
      "Iteration 42, loss = 0.32373744\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.29610155\n",
      "Iteration 2, loss = 1.31212073\n",
      "Iteration 3, loss = 1.01111266\n",
      "Iteration 4, loss = 0.69280355\n",
      "Iteration 5, loss = 0.53974349\n",
      "Iteration 6, loss = 0.51584300\n",
      "Iteration 7, loss = 0.49570964\n",
      "Iteration 8, loss = 0.44506597\n",
      "Iteration 9, loss = 0.40249444\n",
      "Iteration 10, loss = 0.36550761\n",
      "Iteration 11, loss = 0.35214176\n",
      "Iteration 12, loss = 0.35228488\n",
      "Iteration 13, loss = 0.35525532\n",
      "Iteration 14, loss = 0.34446191\n",
      "Iteration 15, loss = 0.32636827\n",
      "Iteration 16, loss = 0.32079898\n",
      "Iteration 17, loss = 0.32497531\n",
      "Iteration 18, loss = 0.31507231\n",
      "Iteration 19, loss = 0.30498863\n",
      "Iteration 20, loss = 0.30871823\n",
      "Iteration 21, loss = 0.31687614\n",
      "Iteration 22, loss = 0.30219738\n",
      "Iteration 23, loss = 0.28382730\n",
      "Iteration 24, loss = 0.29389957\n",
      "Iteration 25, loss = 0.30227763\n",
      "Iteration 26, loss = 0.30333109\n",
      "Iteration 27, loss = 0.31882364\n",
      "Iteration 28, loss = 0.30989199\n",
      "Iteration 29, loss = 0.29322771\n",
      "Iteration 30, loss = 0.29339857\n",
      "Iteration 31, loss = 0.29028117\n",
      "Iteration 32, loss = 0.30378947\n",
      "Iteration 33, loss = 0.29345534\n",
      "Iteration 34, loss = 0.30492899\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.34035155\n",
      "Iteration 2, loss = 1.21713160\n",
      "Iteration 3, loss = 0.96708269\n",
      "Iteration 4, loss = 0.67628143\n",
      "Iteration 5, loss = 0.50747617\n",
      "Iteration 6, loss = 0.48043986\n",
      "Iteration 7, loss = 0.47453911\n",
      "Iteration 8, loss = 0.43616725\n",
      "Iteration 9, loss = 0.39271854\n",
      "Iteration 10, loss = 0.35196165\n",
      "Iteration 11, loss = 0.34879815\n",
      "Iteration 12, loss = 0.34550841\n",
      "Iteration 13, loss = 0.33744575\n",
      "Iteration 14, loss = 0.33457571\n",
      "Iteration 15, loss = 0.32598855\n",
      "Iteration 16, loss = 0.31677208\n",
      "Iteration 17, loss = 0.30849346\n",
      "Iteration 18, loss = 0.30111696\n",
      "Iteration 19, loss = 0.29152124\n",
      "Iteration 20, loss = 0.29692650\n",
      "Iteration 21, loss = 0.29949804\n",
      "Iteration 22, loss = 0.28757942\n",
      "Iteration 23, loss = 0.27716793\n",
      "Iteration 24, loss = 0.29071995\n",
      "Iteration 25, loss = 0.29930107\n",
      "Iteration 26, loss = 0.28782891\n",
      "Iteration 27, loss = 0.28975203\n",
      "Iteration 28, loss = 0.29398933\n",
      "Iteration 29, loss = 0.29351085\n",
      "Iteration 30, loss = 0.28919362\n",
      "Iteration 31, loss = 0.28215777\n",
      "Iteration 32, loss = 0.29378287\n",
      "Iteration 33, loss = 0.28275175\n",
      "Iteration 34, loss = 0.28635360\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.31920933\n",
      "Iteration 2, loss = 1.26047591\n",
      "Iteration 3, loss = 0.95172012\n",
      "Iteration 4, loss = 0.65979519\n",
      "Iteration 5, loss = 0.52373575\n",
      "Iteration 6, loss = 0.50865755\n",
      "Iteration 7, loss = 0.49207742\n",
      "Iteration 8, loss = 0.44419023\n",
      "Iteration 9, loss = 0.39569144\n",
      "Iteration 10, loss = 0.35670856\n",
      "Iteration 11, loss = 0.35312120\n",
      "Iteration 12, loss = 0.35422084\n",
      "Iteration 13, loss = 0.34920624\n",
      "Iteration 14, loss = 0.34134696\n",
      "Iteration 15, loss = 0.32488880\n",
      "Iteration 16, loss = 0.31236062\n",
      "Iteration 17, loss = 0.31000565\n",
      "Iteration 18, loss = 0.30876418\n",
      "Iteration 19, loss = 0.30301684\n",
      "Iteration 20, loss = 0.29893465\n",
      "Iteration 21, loss = 0.28921682\n",
      "Iteration 22, loss = 0.27754546\n",
      "Iteration 23, loss = 0.27693663\n",
      "Iteration 24, loss = 0.29307768\n",
      "Iteration 25, loss = 0.30124918\n",
      "Iteration 26, loss = 0.28914862\n",
      "Iteration 27, loss = 0.29536655\n",
      "Iteration 28, loss = 0.31402242\n",
      "Iteration 29, loss = 0.31448570\n",
      "Iteration 30, loss = 0.30018722\n",
      "Iteration 31, loss = 0.29512784\n",
      "Iteration 32, loss = 0.29964485\n",
      "Iteration 33, loss = 0.27935423\n",
      "Iteration 34, loss = 0.27999265\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.32814196\n",
      "Iteration 2, loss = 1.26575106\n",
      "Iteration 3, loss = 0.92153093\n",
      "Iteration 4, loss = 0.62725250\n",
      "Iteration 5, loss = 0.51557763\n",
      "Iteration 6, loss = 0.49493841\n",
      "Iteration 7, loss = 0.46612702\n",
      "Iteration 8, loss = 0.41994223\n",
      "Iteration 9, loss = 0.37448801\n",
      "Iteration 10, loss = 0.34669235\n",
      "Iteration 11, loss = 0.35314118\n",
      "Iteration 12, loss = 0.35636472\n",
      "Iteration 13, loss = 0.34380632\n",
      "Iteration 14, loss = 0.33169471\n",
      "Iteration 15, loss = 0.31393794\n",
      "Iteration 16, loss = 0.30560149\n",
      "Iteration 17, loss = 0.30904029\n",
      "Iteration 18, loss = 0.30814753\n",
      "Iteration 19, loss = 0.30293097\n",
      "Iteration 20, loss = 0.29942223\n",
      "Iteration 21, loss = 0.30152702\n",
      "Iteration 22, loss = 0.28569645\n",
      "Iteration 23, loss = 0.28028178\n",
      "Iteration 24, loss = 0.29785865\n",
      "Iteration 25, loss = 0.30161362\n",
      "Iteration 26, loss = 0.28617909\n",
      "Iteration 27, loss = 0.27911323\n",
      "Iteration 28, loss = 0.28416740\n",
      "Iteration 29, loss = 0.28335248\n",
      "Iteration 30, loss = 0.29138711\n",
      "Iteration 31, loss = 0.28889367\n",
      "Iteration 32, loss = 0.28398980\n",
      "Iteration 33, loss = 0.27631215\n",
      "Iteration 34, loss = 0.26737244\n",
      "Iteration 35, loss = 0.27571584\n",
      "Iteration 36, loss = 0.29049192\n",
      "Iteration 37, loss = 0.28671230\n",
      "Iteration 38, loss = 0.27925405\n",
      "Iteration 39, loss = 0.27522260\n",
      "Iteration 40, loss = 0.27745168\n",
      "Iteration 41, loss = 0.27776861\n",
      "Iteration 42, loss = 0.28440213\n",
      "Iteration 43, loss = 0.28378559\n",
      "Iteration 44, loss = 0.28743731\n",
      "Iteration 45, loss = 0.27943099\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.30537770\n",
      "Iteration 2, loss = 1.18321144\n",
      "Iteration 3, loss = 0.79468624\n",
      "Iteration 4, loss = 0.56482358\n",
      "Iteration 5, loss = 0.49328588\n",
      "Iteration 6, loss = 0.47142194\n",
      "Iteration 7, loss = 0.41872581\n",
      "Iteration 8, loss = 0.36551665\n",
      "Iteration 9, loss = 0.35036744\n",
      "Iteration 10, loss = 0.34499347\n",
      "Iteration 11, loss = 0.34180496\n",
      "Iteration 12, loss = 0.33738671\n",
      "Iteration 13, loss = 0.33427737\n",
      "Iteration 14, loss = 0.33156461\n",
      "Iteration 15, loss = 0.30575783\n",
      "Iteration 16, loss = 0.29402234\n",
      "Iteration 17, loss = 0.31115273\n",
      "Iteration 18, loss = 0.31848012\n",
      "Iteration 19, loss = 0.30201535\n",
      "Iteration 20, loss = 0.28539914\n",
      "Iteration 21, loss = 0.29207128\n",
      "Iteration 22, loss = 0.28693694\n",
      "Iteration 23, loss = 0.29202541\n",
      "Iteration 24, loss = 0.32447195\n",
      "Iteration 25, loss = 0.30825868\n",
      "Iteration 26, loss = 0.27654819\n",
      "Iteration 27, loss = 0.27671007\n",
      "Iteration 28, loss = 0.29174710\n",
      "Iteration 29, loss = 0.29900550\n",
      "Iteration 30, loss = 0.30998232\n",
      "Iteration 31, loss = 0.30350282\n",
      "Iteration 32, loss = 0.29044288\n",
      "Iteration 33, loss = 0.28884169\n",
      "Iteration 34, loss = 0.28183409\n",
      "Iteration 35, loss = 0.28671695\n",
      "Iteration 36, loss = 0.29911633\n",
      "Iteration 37, loss = 0.30261864\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.30345630\n",
      "Iteration 2, loss = 1.20896134\n",
      "Iteration 3, loss = 0.77899698\n",
      "Iteration 4, loss = 0.56903134\n",
      "Iteration 5, loss = 0.50684764\n",
      "Iteration 6, loss = 0.47577038\n",
      "Iteration 7, loss = 0.42451119\n",
      "Iteration 8, loss = 0.37700194\n",
      "Iteration 9, loss = 0.36462062\n",
      "Iteration 10, loss = 0.34924179\n",
      "Iteration 11, loss = 0.34695010\n",
      "Iteration 12, loss = 0.34973723\n",
      "Iteration 13, loss = 0.34583536\n",
      "Iteration 14, loss = 0.33507375\n",
      "Iteration 15, loss = 0.30925136\n",
      "Iteration 16, loss = 0.31412306\n",
      "Iteration 17, loss = 0.34192680\n",
      "Iteration 18, loss = 0.33587044\n",
      "Iteration 19, loss = 0.31586487\n",
      "Iteration 20, loss = 0.29972776\n",
      "Iteration 21, loss = 0.30140166\n",
      "Iteration 22, loss = 0.29787014\n",
      "Iteration 23, loss = 0.30265935\n",
      "Iteration 24, loss = 0.32369989\n",
      "Iteration 25, loss = 0.30742915\n",
      "Iteration 26, loss = 0.28934637\n",
      "Iteration 27, loss = 0.28771650\n",
      "Iteration 28, loss = 0.29930646\n",
      "Iteration 29, loss = 0.31158673\n",
      "Iteration 30, loss = 0.30390821\n",
      "Iteration 31, loss = 0.30404067\n",
      "Iteration 32, loss = 0.30566979\n",
      "Iteration 33, loss = 0.29299067\n",
      "Iteration 34, loss = 0.28151586\n",
      "Iteration 35, loss = 0.28753142\n",
      "Iteration 36, loss = 0.30705936\n",
      "Iteration 37, loss = 0.31572734\n",
      "Iteration 38, loss = 0.31106506\n",
      "Iteration 39, loss = 0.29675455\n",
      "Iteration 40, loss = 0.29644252\n",
      "Iteration 41, loss = 0.29448908\n",
      "Iteration 42, loss = 0.29065130\n",
      "Iteration 43, loss = 0.29993022\n",
      "Iteration 44, loss = 0.30952922\n",
      "Iteration 45, loss = 0.30015606\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.30567580\n",
      "Iteration 2, loss = 1.23803097\n",
      "Iteration 3, loss = 0.77662457\n",
      "Iteration 4, loss = 0.57168675\n",
      "Iteration 5, loss = 0.50642547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6, loss = 0.46911519\n",
      "Iteration 7, loss = 0.42524889\n",
      "Iteration 8, loss = 0.39002874\n",
      "Iteration 9, loss = 0.37129971\n",
      "Iteration 10, loss = 0.35249958\n",
      "Iteration 11, loss = 0.34766050\n",
      "Iteration 12, loss = 0.34973832\n",
      "Iteration 13, loss = 0.34503527\n",
      "Iteration 14, loss = 0.33657013\n",
      "Iteration 15, loss = 0.31076846\n",
      "Iteration 16, loss = 0.31243457\n",
      "Iteration 17, loss = 0.33928011\n",
      "Iteration 18, loss = 0.33045625\n",
      "Iteration 19, loss = 0.31036599\n",
      "Iteration 20, loss = 0.29600693\n",
      "Iteration 21, loss = 0.30002245\n",
      "Iteration 22, loss = 0.29822073\n",
      "Iteration 23, loss = 0.30364535\n",
      "Iteration 24, loss = 0.32915568\n",
      "Iteration 25, loss = 0.31910686\n",
      "Iteration 26, loss = 0.29106763\n",
      "Iteration 27, loss = 0.28352262\n",
      "Iteration 28, loss = 0.30131528\n",
      "Iteration 29, loss = 0.30530737\n",
      "Iteration 30, loss = 0.29018428\n",
      "Iteration 31, loss = 0.30423711\n",
      "Iteration 32, loss = 0.31248822\n",
      "Iteration 33, loss = 0.29840735\n",
      "Iteration 34, loss = 0.27791799\n",
      "Iteration 35, loss = 0.27940492\n",
      "Iteration 36, loss = 0.30380388\n",
      "Iteration 37, loss = 0.30846871\n",
      "Iteration 38, loss = 0.31598155\n",
      "Iteration 39, loss = 0.31083282\n",
      "Iteration 40, loss = 0.29836968\n",
      "Iteration 41, loss = 0.29302935\n",
      "Iteration 42, loss = 0.28407206\n",
      "Iteration 43, loss = 0.29539715\n",
      "Iteration 44, loss = 0.31959463\n",
      "Iteration 45, loss = 0.31135603\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.29549246\n",
      "Iteration 2, loss = 1.25502270\n",
      "Iteration 3, loss = 0.78874150\n",
      "Iteration 4, loss = 0.59620238\n",
      "Iteration 5, loss = 0.51096063\n",
      "Iteration 6, loss = 0.46628533\n",
      "Iteration 7, loss = 0.41982961\n",
      "Iteration 8, loss = 0.38155272\n",
      "Iteration 9, loss = 0.35988127\n",
      "Iteration 10, loss = 0.34439672\n",
      "Iteration 11, loss = 0.34027616\n",
      "Iteration 12, loss = 0.33239303\n",
      "Iteration 13, loss = 0.32062209\n",
      "Iteration 14, loss = 0.31573270\n",
      "Iteration 15, loss = 0.30069604\n",
      "Iteration 16, loss = 0.31228378\n",
      "Iteration 17, loss = 0.33027872\n",
      "Iteration 18, loss = 0.31031529\n",
      "Iteration 19, loss = 0.28819629\n",
      "Iteration 20, loss = 0.28718636\n",
      "Iteration 21, loss = 0.29400596\n",
      "Iteration 22, loss = 0.28396505\n",
      "Iteration 23, loss = 0.29622643\n",
      "Iteration 24, loss = 0.32118551\n",
      "Iteration 25, loss = 0.30110886\n",
      "Iteration 26, loss = 0.28110697\n",
      "Iteration 27, loss = 0.28734177\n",
      "Iteration 28, loss = 0.30915630\n",
      "Iteration 29, loss = 0.30099907\n",
      "Iteration 30, loss = 0.28778532\n",
      "Iteration 31, loss = 0.31681036\n",
      "Iteration 32, loss = 0.31308774\n",
      "Iteration 33, loss = 0.28567292\n",
      "Iteration 34, loss = 0.28266380\n",
      "Iteration 35, loss = 0.30956747\n",
      "Iteration 36, loss = 0.32650260\n",
      "Iteration 37, loss = 0.29776356\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.10287711\n",
      "Iteration 2, loss = 1.27180693\n",
      "Iteration 3, loss = 0.80111064\n",
      "Iteration 4, loss = 0.63713944\n",
      "Iteration 5, loss = 0.53415731\n",
      "Iteration 6, loss = 0.44932934\n",
      "Iteration 7, loss = 0.41364958\n",
      "Iteration 8, loss = 0.39367091\n",
      "Iteration 9, loss = 0.37447313\n",
      "Iteration 10, loss = 0.36322900\n",
      "Iteration 11, loss = 0.35295645\n",
      "Iteration 12, loss = 0.33849814\n",
      "Iteration 13, loss = 0.32854986\n",
      "Iteration 14, loss = 0.32195965\n",
      "Iteration 15, loss = 0.31249576\n",
      "Iteration 16, loss = 0.30598286\n",
      "Iteration 17, loss = 0.30651486\n",
      "Iteration 18, loss = 0.29868838\n",
      "Iteration 19, loss = 0.31074354\n",
      "Iteration 20, loss = 0.30918437\n",
      "Iteration 21, loss = 0.28750081\n",
      "Iteration 22, loss = 0.29557814\n",
      "Iteration 23, loss = 0.30030906\n",
      "Iteration 24, loss = 0.29980586\n",
      "Iteration 25, loss = 0.28818746\n",
      "Iteration 26, loss = 0.28872889\n",
      "Iteration 27, loss = 0.30538418\n",
      "Iteration 28, loss = 0.33132692\n",
      "Iteration 29, loss = 0.33630739\n",
      "Iteration 30, loss = 0.33011624\n",
      "Iteration 31, loss = 0.36336904\n",
      "Iteration 32, loss = 0.32626918\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.09868583\n",
      "Iteration 2, loss = 1.26910157\n",
      "Iteration 3, loss = 0.83306915\n",
      "Iteration 4, loss = 0.65941751\n",
      "Iteration 5, loss = 0.53724442\n",
      "Iteration 6, loss = 0.48447189\n",
      "Iteration 7, loss = 0.45158848\n",
      "Iteration 8, loss = 0.41957480\n",
      "Iteration 9, loss = 0.38986108\n",
      "Iteration 10, loss = 0.37395636\n",
      "Iteration 11, loss = 0.36036335\n",
      "Iteration 12, loss = 0.34430139\n",
      "Iteration 13, loss = 0.33256248\n",
      "Iteration 14, loss = 0.32884178\n",
      "Iteration 15, loss = 0.32295645\n",
      "Iteration 16, loss = 0.30784264\n",
      "Iteration 17, loss = 0.30052844\n",
      "Iteration 18, loss = 0.29487903\n",
      "Iteration 19, loss = 0.28888784\n",
      "Iteration 20, loss = 0.28622005\n",
      "Iteration 21, loss = 0.28500600\n",
      "Iteration 22, loss = 0.28529427\n",
      "Iteration 23, loss = 0.30562839\n",
      "Iteration 24, loss = 0.30719415\n",
      "Iteration 25, loss = 0.29436804\n",
      "Iteration 26, loss = 0.30074860\n",
      "Iteration 27, loss = 0.30830050\n",
      "Iteration 28, loss = 0.30340847\n",
      "Iteration 29, loss = 0.28588018\n",
      "Iteration 30, loss = 0.28324390\n",
      "Iteration 31, loss = 0.29556917\n",
      "Iteration 32, loss = 0.30726780\n",
      "Iteration 33, loss = 0.30303268\n",
      "Iteration 34, loss = 0.29246371\n",
      "Iteration 35, loss = 0.29714683\n",
      "Iteration 36, loss = 0.29432715\n",
      "Iteration 37, loss = 0.29951034\n",
      "Iteration 38, loss = 0.30661202\n",
      "Iteration 39, loss = 0.30155786\n",
      "Iteration 40, loss = 0.29382091\n",
      "Iteration 41, loss = 0.31161721\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.11321844\n",
      "Iteration 2, loss = 1.31094301\n",
      "Iteration 3, loss = 0.86778192\n",
      "Iteration 4, loss = 0.67337111\n",
      "Iteration 5, loss = 0.53510904\n",
      "Iteration 6, loss = 0.47179109\n",
      "Iteration 7, loss = 0.44229133\n",
      "Iteration 8, loss = 0.41431624\n",
      "Iteration 9, loss = 0.38128606\n",
      "Iteration 10, loss = 0.36052512\n",
      "Iteration 11, loss = 0.35743967\n",
      "Iteration 12, loss = 0.35296166\n",
      "Iteration 13, loss = 0.33705811\n",
      "Iteration 14, loss = 0.32647374\n",
      "Iteration 15, loss = 0.32049589\n",
      "Iteration 16, loss = 0.30828069\n",
      "Iteration 17, loss = 0.30985969\n",
      "Iteration 18, loss = 0.30953871\n",
      "Iteration 19, loss = 0.29745147\n",
      "Iteration 20, loss = 0.30088023\n",
      "Iteration 21, loss = 0.29972777\n",
      "Iteration 22, loss = 0.28090145\n",
      "Iteration 23, loss = 0.30586559\n",
      "Iteration 24, loss = 0.32549221\n",
      "Iteration 25, loss = 0.30698109\n",
      "Iteration 26, loss = 0.30295330\n",
      "Iteration 27, loss = 0.30816792\n",
      "Iteration 28, loss = 0.30089680\n",
      "Iteration 29, loss = 0.29682789\n",
      "Iteration 30, loss = 0.29807700\n",
      "Iteration 31, loss = 0.29372953\n",
      "Iteration 32, loss = 0.30902427\n",
      "Iteration 33, loss = 0.31433689\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.09844689\n",
      "Iteration 2, loss = 1.26251974\n",
      "Iteration 3, loss = 0.83826921\n",
      "Iteration 4, loss = 0.68943427\n",
      "Iteration 5, loss = 0.54566032\n",
      "Iteration 6, loss = 0.46174562\n",
      "Iteration 7, loss = 0.43383931\n",
      "Iteration 8, loss = 0.41747661\n",
      "Iteration 9, loss = 0.39564491\n",
      "Iteration 10, loss = 0.36895679\n",
      "Iteration 11, loss = 0.35369002\n",
      "Iteration 12, loss = 0.35156571\n",
      "Iteration 13, loss = 0.33959105\n",
      "Iteration 14, loss = 0.32308853\n",
      "Iteration 15, loss = 0.30857377\n",
      "Iteration 16, loss = 0.30300408\n",
      "Iteration 17, loss = 0.31805871\n",
      "Iteration 18, loss = 0.31508008\n",
      "Iteration 19, loss = 0.28819925\n",
      "Iteration 20, loss = 0.28389271\n",
      "Iteration 21, loss = 0.29584621\n",
      "Iteration 22, loss = 0.28630359\n",
      "Iteration 23, loss = 0.29173421\n",
      "Iteration 24, loss = 0.30536464\n",
      "Iteration 25, loss = 0.29287458\n",
      "Iteration 26, loss = 0.28457742\n",
      "Iteration 27, loss = 0.29810268\n",
      "Iteration 28, loss = 0.29658047\n",
      "Iteration 29, loss = 0.28215285\n",
      "Iteration 30, loss = 0.28512374\n",
      "Iteration 31, loss = 0.28924934\n",
      "Iteration 32, loss = 0.30379235\n",
      "Iteration 33, loss = 0.30299586\n",
      "Iteration 34, loss = 0.28829727\n",
      "Iteration 35, loss = 0.27973894\n",
      "Iteration 36, loss = 0.28082343\n",
      "Iteration 37, loss = 0.28950071\n",
      "Iteration 38, loss = 0.29821843\n",
      "Iteration 39, loss = 0.29903740\n",
      "Iteration 40, loss = 0.29563197\n",
      "Iteration 41, loss = 0.30275624\n",
      "Iteration 42, loss = 0.30665646\n",
      "Iteration 43, loss = 0.29818627\n",
      "Iteration 44, loss = 0.29332095\n",
      "Iteration 45, loss = 0.28432146\n",
      "Iteration 46, loss = 0.28116102\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.10699525\n",
      "Iteration 2, loss = 1.30427676\n",
      "Iteration 3, loss = 0.87693869\n",
      "Iteration 4, loss = 0.70022693\n",
      "Iteration 5, loss = 0.55015378\n",
      "Iteration 6, loss = 0.47372655\n",
      "Iteration 7, loss = 0.44165234\n",
      "Iteration 8, loss = 0.41250595\n",
      "Iteration 9, loss = 0.38241782\n",
      "Iteration 10, loss = 0.35978244\n",
      "Iteration 11, loss = 0.34860509\n",
      "Iteration 12, loss = 0.34040218\n",
      "Iteration 13, loss = 0.32519920\n",
      "Iteration 14, loss = 0.31685747\n",
      "Iteration 15, loss = 0.31270814\n",
      "Iteration 16, loss = 0.30071509\n",
      "Iteration 17, loss = 0.30725921\n",
      "Iteration 18, loss = 0.30688363\n",
      "Iteration 19, loss = 0.29188503\n",
      "Iteration 20, loss = 0.29611664\n",
      "Iteration 21, loss = 0.30838813\n",
      "Iteration 22, loss = 0.29182308\n",
      "Iteration 23, loss = 0.28743517\n",
      "Iteration 24, loss = 0.29129896\n",
      "Iteration 25, loss = 0.28728066\n",
      "Iteration 26, loss = 0.28802733\n",
      "Iteration 27, loss = 0.29441918\n",
      "Iteration 28, loss = 0.28311074\n",
      "Iteration 29, loss = 0.27619061\n",
      "Iteration 30, loss = 0.29721282\n",
      "Iteration 31, loss = 0.29842274\n",
      "Iteration 32, loss = 0.30011959\n",
      "Iteration 33, loss = 0.30701696\n",
      "Iteration 34, loss = 0.30146905\n",
      "Iteration 35, loss = 0.28432640\n",
      "Iteration 36, loss = 0.28565969\n",
      "Iteration 37, loss = 0.29782230\n",
      "Iteration 38, loss = 0.30441505\n",
      "Iteration 39, loss = 0.31544438\n",
      "Iteration 40, loss = 0.31583397\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.11088982\n",
      "Iteration 2, loss = 1.30476626\n",
      "Iteration 3, loss = 0.87161073\n",
      "Iteration 4, loss = 0.68930749\n",
      "Iteration 5, loss = 0.54240330\n",
      "Iteration 6, loss = 0.45791853\n",
      "Iteration 7, loss = 0.42170965\n",
      "Iteration 8, loss = 0.39204107\n",
      "Iteration 9, loss = 0.36085865\n",
      "Iteration 10, loss = 0.33879991\n",
      "Iteration 11, loss = 0.33059761\n",
      "Iteration 12, loss = 0.32959143\n",
      "Iteration 13, loss = 0.31674938\n",
      "Iteration 14, loss = 0.30934530\n",
      "Iteration 15, loss = 0.30139704\n",
      "Iteration 16, loss = 0.29902359\n",
      "Iteration 17, loss = 0.31644831\n",
      "Iteration 18, loss = 0.30942208\n",
      "Iteration 19, loss = 0.28585244\n",
      "Iteration 20, loss = 0.28821688\n",
      "Iteration 21, loss = 0.30750793\n",
      "Iteration 22, loss = 0.30765744\n",
      "Iteration 23, loss = 0.29360277\n",
      "Iteration 24, loss = 0.28568687\n",
      "Iteration 25, loss = 0.28432670\n",
      "Iteration 26, loss = 0.28024332\n",
      "Iteration 27, loss = 0.27662282\n",
      "Iteration 28, loss = 0.27315407\n",
      "Iteration 29, loss = 0.26917957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 30, loss = 0.28681121\n",
      "Iteration 31, loss = 0.30449145\n",
      "Iteration 32, loss = 0.29822727\n",
      "Iteration 33, loss = 0.28921189\n",
      "Iteration 34, loss = 0.28272403\n",
      "Iteration 35, loss = 0.27089004\n",
      "Iteration 36, loss = 0.28857192\n",
      "Iteration 37, loss = 0.29749126\n",
      "Iteration 38, loss = 0.28266841\n",
      "Iteration 39, loss = 0.29377628\n",
      "Iteration 40, loss = 0.30579837\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.11555168\n",
      "Iteration 2, loss = 1.35303088\n",
      "Iteration 3, loss = 0.89634857\n",
      "Iteration 4, loss = 0.71263420\n",
      "Iteration 5, loss = 0.55475436\n",
      "Iteration 6, loss = 0.46790800\n",
      "Iteration 7, loss = 0.43175845\n",
      "Iteration 8, loss = 0.40002713\n",
      "Iteration 9, loss = 0.36782639\n",
      "Iteration 10, loss = 0.34699598\n",
      "Iteration 11, loss = 0.34170606\n",
      "Iteration 12, loss = 0.34068966\n",
      "Iteration 13, loss = 0.32270044\n",
      "Iteration 14, loss = 0.31320010\n",
      "Iteration 15, loss = 0.30929990\n",
      "Iteration 16, loss = 0.30270160\n",
      "Iteration 17, loss = 0.31086607\n",
      "Iteration 18, loss = 0.31036523\n",
      "Iteration 19, loss = 0.29493868\n",
      "Iteration 20, loss = 0.29739917\n",
      "Iteration 21, loss = 0.31608094\n",
      "Iteration 22, loss = 0.31364974\n",
      "Iteration 23, loss = 0.30000331\n",
      "Iteration 24, loss = 0.29219186\n",
      "Iteration 25, loss = 0.28417189\n",
      "Iteration 26, loss = 0.28425344\n",
      "Iteration 27, loss = 0.29434764\n",
      "Iteration 28, loss = 0.28954873\n",
      "Iteration 29, loss = 0.27365832\n",
      "Iteration 30, loss = 0.28506120\n",
      "Iteration 31, loss = 0.31101059\n",
      "Iteration 32, loss = 0.31464850\n",
      "Iteration 33, loss = 0.30324548\n",
      "Iteration 34, loss = 0.28851622\n",
      "Iteration 35, loss = 0.27621156\n",
      "Iteration 36, loss = 0.28876747\n",
      "Iteration 37, loss = 0.29810816\n",
      "Iteration 38, loss = 0.28707809\n",
      "Iteration 39, loss = 0.28630591\n",
      "Iteration 40, loss = 0.30077000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.11452269\n",
      "Iteration 2, loss = 1.36384866\n",
      "Iteration 3, loss = 0.92276091\n",
      "Iteration 4, loss = 0.73369135\n",
      "Iteration 5, loss = 0.57093938\n",
      "Iteration 6, loss = 0.48841078\n",
      "Iteration 7, loss = 0.45085485\n",
      "Iteration 8, loss = 0.41402986\n",
      "Iteration 9, loss = 0.37999697\n",
      "Iteration 10, loss = 0.36305001\n",
      "Iteration 11, loss = 0.36135950\n",
      "Iteration 12, loss = 0.36133672\n",
      "Iteration 13, loss = 0.33599651\n",
      "Iteration 14, loss = 0.31962594\n",
      "Iteration 15, loss = 0.32130539\n",
      "Iteration 16, loss = 0.31780607\n",
      "Iteration 17, loss = 0.31773242\n",
      "Iteration 18, loss = 0.31383384\n",
      "Iteration 19, loss = 0.30510821\n",
      "Iteration 20, loss = 0.30735288\n",
      "Iteration 21, loss = 0.32141459\n",
      "Iteration 22, loss = 0.32317218\n",
      "Iteration 23, loss = 0.31626130\n",
      "Iteration 24, loss = 0.30665646\n",
      "Iteration 25, loss = 0.29537594\n",
      "Iteration 26, loss = 0.29002483\n",
      "Iteration 27, loss = 0.30059624\n",
      "Iteration 28, loss = 0.30691721\n",
      "Iteration 29, loss = 0.29030439\n",
      "Iteration 30, loss = 0.28660788\n",
      "Iteration 31, loss = 0.30062185\n",
      "Iteration 32, loss = 0.30883336\n",
      "Iteration 33, loss = 0.31062825\n",
      "Iteration 34, loss = 0.29697314\n",
      "Iteration 35, loss = 0.28941603\n",
      "Iteration 36, loss = 0.30322478\n",
      "Iteration 37, loss = 0.30612284\n",
      "Iteration 38, loss = 0.30533080\n",
      "Iteration 39, loss = 0.30687333\n",
      "Iteration 40, loss = 0.31611846\n",
      "Iteration 41, loss = 0.31791242\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.11082087\n",
      "Iteration 2, loss = 1.33780117\n",
      "Iteration 3, loss = 0.87959705\n",
      "Iteration 4, loss = 0.68671599\n",
      "Iteration 5, loss = 0.55463201\n",
      "Iteration 6, loss = 0.47706118\n",
      "Iteration 7, loss = 0.43539923\n",
      "Iteration 8, loss = 0.40130875\n",
      "Iteration 9, loss = 0.37286490\n",
      "Iteration 10, loss = 0.35733832\n",
      "Iteration 11, loss = 0.34958381\n",
      "Iteration 12, loss = 0.34820316\n",
      "Iteration 13, loss = 0.32998661\n",
      "Iteration 14, loss = 0.31397908\n",
      "Iteration 15, loss = 0.31296249\n",
      "Iteration 16, loss = 0.30907848\n",
      "Iteration 17, loss = 0.31018597\n",
      "Iteration 18, loss = 0.30666212\n",
      "Iteration 19, loss = 0.29666127\n",
      "Iteration 20, loss = 0.29794371\n",
      "Iteration 21, loss = 0.30894005\n",
      "Iteration 22, loss = 0.30289303\n",
      "Iteration 23, loss = 0.29615187\n",
      "Iteration 24, loss = 0.28750668\n",
      "Iteration 25, loss = 0.28265231\n",
      "Iteration 26, loss = 0.28200292\n",
      "Iteration 27, loss = 0.29300140\n",
      "Iteration 28, loss = 0.29374006\n",
      "Iteration 29, loss = 0.28336051\n",
      "Iteration 30, loss = 0.27536177\n",
      "Iteration 31, loss = 0.28732905\n",
      "Iteration 32, loss = 0.29411135\n",
      "Iteration 33, loss = 0.29399260\n",
      "Iteration 34, loss = 0.28590129\n",
      "Iteration 35, loss = 0.28903647\n",
      "Iteration 36, loss = 0.30898555\n",
      "Iteration 37, loss = 0.30363489\n",
      "Iteration 38, loss = 0.29321955\n",
      "Iteration 39, loss = 0.29826296\n",
      "Iteration 40, loss = 0.31021618\n",
      "Iteration 41, loss = 0.30792131\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.12227378\n",
      "Iteration 2, loss = 1.37979219\n",
      "Iteration 3, loss = 0.92560202\n",
      "Iteration 4, loss = 0.70554973\n",
      "Iteration 5, loss = 0.55872214\n",
      "Iteration 6, loss = 0.47191852\n",
      "Iteration 7, loss = 0.43046679\n",
      "Iteration 8, loss = 0.39865451\n",
      "Iteration 9, loss = 0.36784982\n",
      "Iteration 10, loss = 0.34543267\n",
      "Iteration 11, loss = 0.33833828\n",
      "Iteration 12, loss = 0.33719006\n",
      "Iteration 13, loss = 0.32142680\n",
      "Iteration 14, loss = 0.30884865\n",
      "Iteration 15, loss = 0.30843126\n",
      "Iteration 16, loss = 0.30641504\n",
      "Iteration 17, loss = 0.30615633\n",
      "Iteration 18, loss = 0.30345706\n",
      "Iteration 19, loss = 0.29674545\n",
      "Iteration 20, loss = 0.28892214\n",
      "Iteration 21, loss = 0.28465069\n",
      "Iteration 22, loss = 0.28099680\n",
      "Iteration 23, loss = 0.28850426\n",
      "Iteration 24, loss = 0.28628976\n",
      "Iteration 25, loss = 0.27438344\n",
      "Iteration 26, loss = 0.26083960\n",
      "Iteration 27, loss = 0.26673632\n",
      "Iteration 28, loss = 0.27115472\n",
      "Iteration 29, loss = 0.27187719\n",
      "Iteration 30, loss = 0.26459079\n",
      "Iteration 31, loss = 0.26954095\n",
      "Iteration 32, loss = 0.27584470\n",
      "Iteration 33, loss = 0.27987817\n",
      "Iteration 34, loss = 0.28137509\n",
      "Iteration 35, loss = 0.28576821\n",
      "Iteration 36, loss = 0.29627252\n",
      "Iteration 37, loss = 0.29328378\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.19891971\n",
      "Iteration 2, loss = 1.48600298\n",
      "Iteration 3, loss = 0.89865406\n",
      "Iteration 4, loss = 0.69192994\n",
      "Iteration 5, loss = 0.62542880\n",
      "Iteration 6, loss = 0.51999658\n",
      "Iteration 7, loss = 0.43930585\n",
      "Iteration 8, loss = 0.43936365\n",
      "Iteration 9, loss = 0.44841221\n",
      "Iteration 10, loss = 0.41619224\n",
      "Iteration 11, loss = 0.36988356\n",
      "Iteration 12, loss = 0.34980334\n",
      "Iteration 13, loss = 0.34399789\n",
      "Iteration 14, loss = 0.33461397\n",
      "Iteration 15, loss = 0.33609789\n",
      "Iteration 16, loss = 0.33310113\n",
      "Iteration 17, loss = 0.31713033\n",
      "Iteration 18, loss = 0.29953575\n",
      "Iteration 19, loss = 0.29855525\n",
      "Iteration 20, loss = 0.29699469\n",
      "Iteration 21, loss = 0.29311093\n",
      "Iteration 22, loss = 0.29317287\n",
      "Iteration 23, loss = 0.30329869\n",
      "Iteration 24, loss = 0.30795858\n",
      "Iteration 25, loss = 0.30411949\n",
      "Iteration 26, loss = 0.29920717\n",
      "Iteration 27, loss = 0.30947364\n",
      "Iteration 28, loss = 0.31366585\n",
      "Iteration 29, loss = 0.30120271\n",
      "Iteration 30, loss = 0.30378620\n",
      "Iteration 31, loss = 0.32811109\n",
      "Iteration 32, loss = 0.33468192\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.18097677\n",
      "Iteration 2, loss = 1.41954538\n",
      "Iteration 3, loss = 0.83400380\n",
      "Iteration 4, loss = 0.67239820\n",
      "Iteration 5, loss = 0.57281752\n",
      "Iteration 6, loss = 0.49344603\n",
      "Iteration 7, loss = 0.45341286\n",
      "Iteration 8, loss = 0.42189965\n",
      "Iteration 9, loss = 0.39316484\n",
      "Iteration 10, loss = 0.37613359\n",
      "Iteration 11, loss = 0.36428978\n",
      "Iteration 12, loss = 0.34510285\n",
      "Iteration 13, loss = 0.34420350\n",
      "Iteration 14, loss = 0.32966066\n",
      "Iteration 15, loss = 0.32561767\n",
      "Iteration 16, loss = 0.32470490\n",
      "Iteration 17, loss = 0.31367288\n",
      "Iteration 18, loss = 0.29474311\n",
      "Iteration 19, loss = 0.31611033\n",
      "Iteration 20, loss = 0.32832306\n",
      "Iteration 21, loss = 0.30099168\n",
      "Iteration 22, loss = 0.28897479\n",
      "Iteration 23, loss = 0.32020631\n",
      "Iteration 24, loss = 0.31794861\n",
      "Iteration 25, loss = 0.29342232\n",
      "Iteration 26, loss = 0.28911398\n",
      "Iteration 27, loss = 0.30201523\n",
      "Iteration 28, loss = 0.30545486\n",
      "Iteration 29, loss = 0.29205580\n",
      "Iteration 30, loss = 0.29495452\n",
      "Iteration 31, loss = 0.29003621\n",
      "Iteration 32, loss = 0.29365444\n",
      "Iteration 33, loss = 0.29580404\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.18799819\n",
      "Iteration 2, loss = 1.47287051\n",
      "Iteration 3, loss = 0.83920065\n",
      "Iteration 4, loss = 0.68091821\n",
      "Iteration 5, loss = 0.58400338\n",
      "Iteration 6, loss = 0.49861941\n",
      "Iteration 7, loss = 0.44920805\n",
      "Iteration 8, loss = 0.41495884\n",
      "Iteration 9, loss = 0.38762020\n",
      "Iteration 10, loss = 0.37190459\n",
      "Iteration 11, loss = 0.36713988\n",
      "Iteration 12, loss = 0.36195872\n",
      "Iteration 13, loss = 0.36138172\n",
      "Iteration 14, loss = 0.33905936\n",
      "Iteration 15, loss = 0.33637019\n",
      "Iteration 16, loss = 0.32172480\n",
      "Iteration 17, loss = 0.31093060\n",
      "Iteration 18, loss = 0.30570148\n",
      "Iteration 19, loss = 0.32101718\n",
      "Iteration 20, loss = 0.31562918\n",
      "Iteration 21, loss = 0.29319089\n",
      "Iteration 22, loss = 0.29631290\n",
      "Iteration 23, loss = 0.31362487\n",
      "Iteration 24, loss = 0.30451714\n",
      "Iteration 25, loss = 0.28968231\n",
      "Iteration 26, loss = 0.28494006\n",
      "Iteration 27, loss = 0.29118901\n",
      "Iteration 28, loss = 0.31789457\n",
      "Iteration 29, loss = 0.31046795\n",
      "Iteration 30, loss = 0.30483443\n",
      "Iteration 31, loss = 0.29500462\n",
      "Iteration 32, loss = 0.29583359\n",
      "Iteration 33, loss = 0.28406472\n",
      "Iteration 34, loss = 0.28061361\n",
      "Iteration 35, loss = 0.29413450\n",
      "Iteration 36, loss = 0.29314713\n",
      "Iteration 37, loss = 0.28303440\n",
      "Iteration 38, loss = 0.28292749\n",
      "Iteration 39, loss = 0.28015874\n",
      "Iteration 40, loss = 0.28135429\n",
      "Iteration 41, loss = 0.28497737\n",
      "Iteration 42, loss = 0.29349349\n",
      "Iteration 43, loss = 0.29660801\n",
      "Iteration 44, loss = 0.28577366\n",
      "Iteration 45, loss = 0.29436240\n",
      "Iteration 46, loss = 0.29359324\n",
      "Iteration 47, loss = 0.29648364\n",
      "Iteration 48, loss = 0.30284929\n",
      "Iteration 49, loss = 0.32033526\n",
      "Iteration 50, loss = 0.30242711\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.18783581\n",
      "Iteration 2, loss = 1.37459556\n",
      "Iteration 3, loss = 0.82238912\n",
      "Iteration 4, loss = 0.63507373\n",
      "Iteration 5, loss = 0.56939634\n",
      "Iteration 6, loss = 0.51366412\n",
      "Iteration 7, loss = 0.45274180\n",
      "Iteration 8, loss = 0.40623767\n",
      "Iteration 9, loss = 0.37781225\n",
      "Iteration 10, loss = 0.36530405\n",
      "Iteration 11, loss = 0.35407421\n",
      "Iteration 12, loss = 0.33704128\n",
      "Iteration 13, loss = 0.32551648\n",
      "Iteration 14, loss = 0.33328900\n",
      "Iteration 15, loss = 0.34333783\n",
      "Iteration 16, loss = 0.31667979\n",
      "Iteration 17, loss = 0.31270972\n",
      "Iteration 18, loss = 0.31174032\n",
      "Iteration 19, loss = 0.32096489\n",
      "Iteration 20, loss = 0.32043723\n",
      "Iteration 21, loss = 0.28635814\n",
      "Iteration 22, loss = 0.28871751\n",
      "Iteration 23, loss = 0.30696848\n",
      "Iteration 24, loss = 0.31357115\n",
      "Iteration 25, loss = 0.32415449\n",
      "Iteration 26, loss = 0.28999263\n",
      "Iteration 27, loss = 0.28689886\n",
      "Iteration 28, loss = 0.32276916\n",
      "Iteration 29, loss = 0.30810671\n",
      "Iteration 30, loss = 0.29346984\n",
      "Iteration 31, loss = 0.29913686\n",
      "Iteration 32, loss = 0.30953480\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.19836103\n",
      "Iteration 2, loss = 1.45852476\n",
      "Iteration 3, loss = 0.86058425\n",
      "Iteration 4, loss = 0.68125922\n",
      "Iteration 5, loss = 0.59764727\n",
      "Iteration 6, loss = 0.51808824\n",
      "Iteration 7, loss = 0.46153132\n",
      "Iteration 8, loss = 0.42369691\n",
      "Iteration 9, loss = 0.38847861\n",
      "Iteration 10, loss = 0.37177854\n",
      "Iteration 11, loss = 0.36150173\n",
      "Iteration 12, loss = 0.34936923\n",
      "Iteration 13, loss = 0.34599992\n",
      "Iteration 14, loss = 0.33159915\n",
      "Iteration 15, loss = 0.32903447\n",
      "Iteration 16, loss = 0.31996642\n",
      "Iteration 17, loss = 0.33303059\n",
      "Iteration 18, loss = 0.31374704\n",
      "Iteration 19, loss = 0.30143931\n",
      "Iteration 20, loss = 0.30289730\n",
      "Iteration 21, loss = 0.28915904\n",
      "Iteration 22, loss = 0.30092182\n",
      "Iteration 23, loss = 0.31280051\n",
      "Iteration 24, loss = 0.30697762\n",
      "Iteration 25, loss = 0.28811264\n",
      "Iteration 26, loss = 0.27418928\n",
      "Iteration 27, loss = 0.29973735\n",
      "Iteration 28, loss = 0.31344307\n",
      "Iteration 29, loss = 0.29493405\n",
      "Iteration 30, loss = 0.30560728\n",
      "Iteration 31, loss = 0.31609987\n",
      "Iteration 32, loss = 0.29819896\n",
      "Iteration 33, loss = 0.27418338\n",
      "Iteration 34, loss = 0.27364775\n",
      "Iteration 35, loss = 0.28853226\n",
      "Iteration 36, loss = 0.29361395\n",
      "Iteration 37, loss = 0.28477843\n",
      "Iteration 38, loss = 0.27368530\n",
      "Iteration 39, loss = 0.26984857\n",
      "Iteration 40, loss = 0.28473792\n",
      "Iteration 41, loss = 0.30141940\n",
      "Iteration 42, loss = 0.29514323\n",
      "Iteration 43, loss = 0.29281860\n",
      "Iteration 44, loss = 0.29472380\n",
      "Iteration 45, loss = 0.29030271\n",
      "Iteration 46, loss = 0.27737505\n",
      "Iteration 47, loss = 0.29433047\n",
      "Iteration 48, loss = 0.29684346\n",
      "Iteration 49, loss = 0.29457059\n",
      "Iteration 50, loss = 0.29087527\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.19257317\n",
      "Iteration 2, loss = 1.40551069\n",
      "Iteration 3, loss = 0.84584790\n",
      "Iteration 4, loss = 0.65878716\n",
      "Iteration 5, loss = 0.57511400\n",
      "Iteration 6, loss = 0.50879859\n",
      "Iteration 7, loss = 0.44536201\n",
      "Iteration 8, loss = 0.40532683\n",
      "Iteration 9, loss = 0.37440719\n",
      "Iteration 10, loss = 0.35804030\n",
      "Iteration 11, loss = 0.34073648\n",
      "Iteration 12, loss = 0.32758516\n",
      "Iteration 13, loss = 0.32349456\n",
      "Iteration 14, loss = 0.31636261\n",
      "Iteration 15, loss = 0.31582688\n",
      "Iteration 16, loss = 0.29950278\n",
      "Iteration 17, loss = 0.31750961\n",
      "Iteration 18, loss = 0.30942425\n",
      "Iteration 19, loss = 0.29919636\n",
      "Iteration 20, loss = 0.29944180\n",
      "Iteration 21, loss = 0.29363319\n",
      "Iteration 22, loss = 0.27457646\n",
      "Iteration 23, loss = 0.26303458\n",
      "Iteration 24, loss = 0.27059171\n",
      "Iteration 25, loss = 0.29113918\n",
      "Iteration 26, loss = 0.28032183\n",
      "Iteration 27, loss = 0.30603090\n",
      "Iteration 28, loss = 0.31766490\n",
      "Iteration 29, loss = 0.29096402\n",
      "Iteration 30, loss = 0.31038064\n",
      "Iteration 31, loss = 0.33177467\n",
      "Iteration 32, loss = 0.29899566\n",
      "Iteration 33, loss = 0.28066004\n",
      "Iteration 34, loss = 0.27698097\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.20233743\n",
      "Iteration 2, loss = 1.49390666\n",
      "Iteration 3, loss = 0.88871008\n",
      "Iteration 4, loss = 0.69144072\n",
      "Iteration 5, loss = 0.60415582\n",
      "Iteration 6, loss = 0.51767858\n",
      "Iteration 7, loss = 0.45509334\n",
      "Iteration 8, loss = 0.42245075\n",
      "Iteration 9, loss = 0.39307494\n",
      "Iteration 10, loss = 0.36198911\n",
      "Iteration 11, loss = 0.34388444\n",
      "Iteration 12, loss = 0.34445373\n",
      "Iteration 13, loss = 0.34919294\n",
      "Iteration 14, loss = 0.33507456\n",
      "Iteration 15, loss = 0.32018310\n",
      "Iteration 16, loss = 0.30328551\n",
      "Iteration 17, loss = 0.31499273\n",
      "Iteration 18, loss = 0.30687650\n",
      "Iteration 19, loss = 0.30261725\n",
      "Iteration 20, loss = 0.30686124\n",
      "Iteration 21, loss = 0.30001911\n",
      "Iteration 22, loss = 0.28663313\n",
      "Iteration 23, loss = 0.28153050\n",
      "Iteration 24, loss = 0.28327418\n",
      "Iteration 25, loss = 0.28394203\n",
      "Iteration 26, loss = 0.27804197\n",
      "Iteration 27, loss = 0.30914293\n",
      "Iteration 28, loss = 0.31418960\n",
      "Iteration 29, loss = 0.29470684\n",
      "Iteration 30, loss = 0.31379908\n",
      "Iteration 31, loss = 0.32001692\n",
      "Iteration 32, loss = 0.29514485\n",
      "Iteration 33, loss = 0.28347942\n",
      "Iteration 34, loss = 0.28236649\n",
      "Iteration 35, loss = 0.27281108\n",
      "Iteration 36, loss = 0.27009036\n",
      "Iteration 37, loss = 0.28869172\n",
      "Iteration 38, loss = 0.28569631\n",
      "Iteration 39, loss = 0.26709990\n",
      "Iteration 40, loss = 0.27921560\n",
      "Iteration 41, loss = 0.30801072\n",
      "Iteration 42, loss = 0.30592209\n",
      "Iteration 43, loss = 0.29358230\n",
      "Iteration 44, loss = 0.28475272\n",
      "Iteration 45, loss = 0.29486998\n",
      "Iteration 46, loss = 0.30761279\n",
      "Iteration 47, loss = 0.29102846\n",
      "Iteration 48, loss = 0.28590936\n",
      "Iteration 49, loss = 0.32501842\n",
      "Iteration 50, loss = 0.32516565\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.20466265\n",
      "Iteration 2, loss = 1.51306890\n",
      "Iteration 3, loss = 0.90705160\n",
      "Iteration 4, loss = 0.71651463\n",
      "Iteration 5, loss = 0.63740622\n",
      "Iteration 6, loss = 0.54351351\n",
      "Iteration 7, loss = 0.46854620\n",
      "Iteration 8, loss = 0.43556467\n",
      "Iteration 9, loss = 0.41343307\n",
      "Iteration 10, loss = 0.37928914\n",
      "Iteration 11, loss = 0.35601600\n",
      "Iteration 12, loss = 0.35811149\n",
      "Iteration 13, loss = 0.35958391\n",
      "Iteration 14, loss = 0.34338592\n",
      "Iteration 15, loss = 0.32985213\n",
      "Iteration 16, loss = 0.31866056\n",
      "Iteration 17, loss = 0.32907890\n",
      "Iteration 18, loss = 0.32543061\n",
      "Iteration 19, loss = 0.32627848\n",
      "Iteration 20, loss = 0.32379032\n",
      "Iteration 21, loss = 0.31069511\n",
      "Iteration 22, loss = 0.29555098\n",
      "Iteration 23, loss = 0.29471332\n",
      "Iteration 24, loss = 0.30152033\n",
      "Iteration 25, loss = 0.29608282\n",
      "Iteration 26, loss = 0.29392987\n",
      "Iteration 27, loss = 0.33839772\n",
      "Iteration 28, loss = 0.35276209\n",
      "Iteration 29, loss = 0.31040465\n",
      "Iteration 30, loss = 0.31456018\n",
      "Iteration 31, loss = 0.33298312\n",
      "Iteration 32, loss = 0.31744466\n",
      "Iteration 33, loss = 0.30151672\n",
      "Iteration 34, loss = 0.30448586\n",
      "Iteration 35, loss = 0.29439353\n",
      "Iteration 36, loss = 0.28648574\n",
      "Iteration 37, loss = 0.30113000\n",
      "Iteration 38, loss = 0.29615583\n",
      "Iteration 39, loss = 0.28174863\n",
      "Iteration 40, loss = 0.28451300\n",
      "Iteration 41, loss = 0.32550479\n",
      "Iteration 42, loss = 0.32715852\n",
      "Iteration 43, loss = 0.29475402\n",
      "Iteration 44, loss = 0.28957632\n",
      "Iteration 45, loss = 0.31640201\n",
      "Iteration 46, loss = 0.32778948\n",
      "Iteration 47, loss = 0.30431231\n",
      "Iteration 48, loss = 0.31776573\n",
      "Iteration 49, loss = 0.35582473\n",
      "Iteration 50, loss = 0.33591722\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.19987477\n",
      "Iteration 2, loss = 1.49212489\n",
      "Iteration 3, loss = 0.88767705\n",
      "Iteration 4, loss = 0.70174369\n",
      "Iteration 5, loss = 0.62057175\n",
      "Iteration 6, loss = 0.53362650\n",
      "Iteration 7, loss = 0.46445228\n",
      "Iteration 8, loss = 0.43063708\n",
      "Iteration 9, loss = 0.41243760\n",
      "Iteration 10, loss = 0.38499918\n",
      "Iteration 11, loss = 0.36001908\n",
      "Iteration 12, loss = 0.35740831\n",
      "Iteration 13, loss = 0.35653664\n",
      "Iteration 14, loss = 0.33442182\n",
      "Iteration 15, loss = 0.32116286\n",
      "Iteration 16, loss = 0.31501638\n",
      "Iteration 17, loss = 0.33280207\n",
      "Iteration 18, loss = 0.31937707\n",
      "Iteration 19, loss = 0.30907934\n",
      "Iteration 20, loss = 0.30654140\n",
      "Iteration 21, loss = 0.29989520\n",
      "Iteration 22, loss = 0.29099499\n",
      "Iteration 23, loss = 0.30092199\n",
      "Iteration 24, loss = 0.30511062\n",
      "Iteration 25, loss = 0.29588098\n",
      "Iteration 26, loss = 0.29386932\n",
      "Iteration 27, loss = 0.35123880\n",
      "Iteration 28, loss = 0.36597745\n",
      "Iteration 29, loss = 0.31115545\n",
      "Iteration 30, loss = 0.31382312\n",
      "Iteration 31, loss = 0.34130432\n",
      "Iteration 32, loss = 0.32577192\n",
      "Iteration 33, loss = 0.30693375\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.20068607\n",
      "Iteration 2, loss = 1.46119162\n",
      "Iteration 3, loss = 0.85325847\n",
      "Iteration 4, loss = 0.68375160\n",
      "Iteration 5, loss = 0.59184597\n",
      "Iteration 6, loss = 0.50233716\n",
      "Iteration 7, loss = 0.43875845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8, loss = 0.41924414\n",
      "Iteration 9, loss = 0.40808038\n",
      "Iteration 10, loss = 0.36944184\n",
      "Iteration 11, loss = 0.34042312\n",
      "Iteration 12, loss = 0.35060003\n",
      "Iteration 13, loss = 0.34937906\n",
      "Iteration 14, loss = 0.32996339\n",
      "Iteration 15, loss = 0.31542907\n",
      "Iteration 16, loss = 0.30785275\n",
      "Iteration 17, loss = 0.32011768\n",
      "Iteration 18, loss = 0.29786795\n",
      "Iteration 19, loss = 0.29192633\n",
      "Iteration 20, loss = 0.29945350\n",
      "Iteration 21, loss = 0.29785682\n",
      "Iteration 22, loss = 0.28395766\n",
      "Iteration 23, loss = 0.28721796\n",
      "Iteration 24, loss = 0.29821617\n",
      "Iteration 25, loss = 0.29374801\n",
      "Iteration 26, loss = 0.28712764\n",
      "Iteration 27, loss = 0.31479797\n",
      "Iteration 28, loss = 0.32971959\n",
      "Iteration 29, loss = 0.29508913\n",
      "Iteration 30, loss = 0.29443874\n",
      "Iteration 31, loss = 0.32700342\n",
      "Iteration 32, loss = 0.31553933\n",
      "Iteration 33, loss = 0.29735366\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.52717057\n",
      "Iteration 2, loss = 1.41772976\n",
      "Iteration 3, loss = 0.95720937\n",
      "Iteration 4, loss = 0.69413798\n",
      "Iteration 5, loss = 0.59369188\n",
      "Iteration 6, loss = 0.54287805\n",
      "Iteration 7, loss = 0.48540255\n",
      "Iteration 8, loss = 0.43482648\n",
      "Iteration 9, loss = 0.41977440\n",
      "Iteration 10, loss = 0.41376793\n",
      "Iteration 11, loss = 0.39376901\n",
      "Iteration 12, loss = 0.37984335\n",
      "Iteration 13, loss = 0.36945438\n",
      "Iteration 14, loss = 0.36552138\n",
      "Iteration 15, loss = 0.34614694\n",
      "Iteration 16, loss = 0.32536530\n",
      "Iteration 17, loss = 0.32478896\n",
      "Iteration 18, loss = 0.32342177\n",
      "Iteration 19, loss = 0.30573434\n",
      "Iteration 20, loss = 0.29406642\n",
      "Iteration 21, loss = 0.29983557\n",
      "Iteration 22, loss = 0.30271012\n",
      "Iteration 23, loss = 0.30589247\n",
      "Iteration 24, loss = 0.30575758\n",
      "Iteration 25, loss = 0.30514003\n",
      "Iteration 26, loss = 0.29331473\n",
      "Iteration 27, loss = 0.27830204\n",
      "Iteration 28, loss = 0.27296508\n",
      "Iteration 29, loss = 0.28447125\n",
      "Iteration 30, loss = 0.29765972\n",
      "Iteration 31, loss = 0.29447227\n",
      "Iteration 32, loss = 0.28478127\n",
      "Iteration 33, loss = 0.28001060\n",
      "Iteration 34, loss = 0.29152559\n",
      "Iteration 35, loss = 0.30318813\n",
      "Iteration 36, loss = 0.30019240\n",
      "Iteration 37, loss = 0.29495996\n",
      "Iteration 38, loss = 0.29658055\n",
      "Iteration 39, loss = 0.28715782\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.49511012\n",
      "Iteration 2, loss = 1.51543829\n",
      "Iteration 3, loss = 1.00864481\n",
      "Iteration 4, loss = 0.65899758\n",
      "Iteration 5, loss = 0.61964379\n",
      "Iteration 6, loss = 0.59946360\n",
      "Iteration 7, loss = 0.51564550\n",
      "Iteration 8, loss = 0.43977336\n",
      "Iteration 9, loss = 0.42307660\n",
      "Iteration 10, loss = 0.43590885\n",
      "Iteration 11, loss = 0.42446972\n",
      "Iteration 12, loss = 0.38586230\n",
      "Iteration 13, loss = 0.36303807\n",
      "Iteration 14, loss = 0.34792095\n",
      "Iteration 15, loss = 0.34200680\n",
      "Iteration 16, loss = 0.33291130\n",
      "Iteration 17, loss = 0.32130823\n",
      "Iteration 18, loss = 0.32746302\n",
      "Iteration 19, loss = 0.31745694\n",
      "Iteration 20, loss = 0.30372168\n",
      "Iteration 21, loss = 0.30712052\n",
      "Iteration 22, loss = 0.31664610\n",
      "Iteration 23, loss = 0.29709083\n",
      "Iteration 24, loss = 0.28652019\n",
      "Iteration 25, loss = 0.30580838\n",
      "Iteration 26, loss = 0.31941500\n",
      "Iteration 27, loss = 0.30920935\n",
      "Iteration 28, loss = 0.28981278\n",
      "Iteration 29, loss = 0.28925522\n",
      "Iteration 30, loss = 0.29996378\n",
      "Iteration 31, loss = 0.30667378\n",
      "Iteration 32, loss = 0.30211854\n",
      "Iteration 33, loss = 0.31400456\n",
      "Iteration 34, loss = 0.31904225\n",
      "Iteration 35, loss = 0.29788555\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.45651607\n",
      "Iteration 2, loss = 1.52154931\n",
      "Iteration 3, loss = 0.98512824\n",
      "Iteration 4, loss = 0.65843064\n",
      "Iteration 5, loss = 0.58568349\n",
      "Iteration 6, loss = 0.55781696\n",
      "Iteration 7, loss = 0.50502531\n",
      "Iteration 8, loss = 0.44366668\n",
      "Iteration 9, loss = 0.41364342\n",
      "Iteration 10, loss = 0.41758505\n",
      "Iteration 11, loss = 0.41563972\n",
      "Iteration 12, loss = 0.38541258\n",
      "Iteration 13, loss = 0.36625104\n",
      "Iteration 14, loss = 0.35015455\n",
      "Iteration 15, loss = 0.34789583\n",
      "Iteration 16, loss = 0.34256180\n",
      "Iteration 17, loss = 0.32628014\n",
      "Iteration 18, loss = 0.32110474\n",
      "Iteration 19, loss = 0.31609633\n",
      "Iteration 20, loss = 0.30717321\n",
      "Iteration 21, loss = 0.30226926\n",
      "Iteration 22, loss = 0.30731738\n",
      "Iteration 23, loss = 0.29856185\n",
      "Iteration 24, loss = 0.29855877\n",
      "Iteration 25, loss = 0.31318180\n",
      "Iteration 26, loss = 0.30780214\n",
      "Iteration 27, loss = 0.31125264\n",
      "Iteration 28, loss = 0.30322905\n",
      "Iteration 29, loss = 0.29160757\n",
      "Iteration 30, loss = 0.29388718\n",
      "Iteration 31, loss = 0.30570978\n",
      "Iteration 32, loss = 0.29708425\n",
      "Iteration 33, loss = 0.30149141\n",
      "Iteration 34, loss = 0.31153016\n",
      "Iteration 35, loss = 0.31159242\n",
      "Iteration 36, loss = 0.29905181\n",
      "Iteration 37, loss = 0.31368965\n",
      "Iteration 38, loss = 0.32077690\n",
      "Iteration 39, loss = 0.31346129\n",
      "Iteration 40, loss = 0.29983442\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.43307768\n",
      "Iteration 2, loss = 1.60375492\n",
      "Iteration 3, loss = 1.05660357\n",
      "Iteration 4, loss = 0.65086561\n",
      "Iteration 5, loss = 0.60387617\n",
      "Iteration 6, loss = 0.59218520\n",
      "Iteration 7, loss = 0.51539506\n",
      "Iteration 8, loss = 0.44441537\n",
      "Iteration 9, loss = 0.41381620\n",
      "Iteration 10, loss = 0.41251199\n",
      "Iteration 11, loss = 0.41199812\n",
      "Iteration 12, loss = 0.38424543\n",
      "Iteration 13, loss = 0.35795998\n",
      "Iteration 14, loss = 0.33637094\n",
      "Iteration 15, loss = 0.32869553\n",
      "Iteration 16, loss = 0.32198255\n",
      "Iteration 17, loss = 0.31709916\n",
      "Iteration 18, loss = 0.31429880\n",
      "Iteration 19, loss = 0.30231512\n",
      "Iteration 20, loss = 0.29600546\n",
      "Iteration 21, loss = 0.28721161\n",
      "Iteration 22, loss = 0.28954601\n",
      "Iteration 23, loss = 0.29093910\n",
      "Iteration 24, loss = 0.29073462\n",
      "Iteration 25, loss = 0.29497927\n",
      "Iteration 26, loss = 0.28936420\n",
      "Iteration 27, loss = 0.29224846\n",
      "Iteration 28, loss = 0.27839305\n",
      "Iteration 29, loss = 0.27649816\n",
      "Iteration 30, loss = 0.29840516\n",
      "Iteration 31, loss = 0.30021749\n",
      "Iteration 32, loss = 0.28273279\n",
      "Iteration 33, loss = 0.29162440\n",
      "Iteration 34, loss = 0.29475576\n",
      "Iteration 35, loss = 0.29350377\n",
      "Iteration 36, loss = 0.28510906\n",
      "Iteration 37, loss = 0.28622807\n",
      "Iteration 38, loss = 0.28827615\n",
      "Iteration 39, loss = 0.28853877\n",
      "Iteration 40, loss = 0.28223689\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.42385505\n",
      "Iteration 2, loss = 1.61390323\n",
      "Iteration 3, loss = 1.04875864\n",
      "Iteration 4, loss = 0.67891390\n",
      "Iteration 5, loss = 0.62339519\n",
      "Iteration 6, loss = 0.58830504\n",
      "Iteration 7, loss = 0.52335734\n",
      "Iteration 8, loss = 0.46423556\n",
      "Iteration 9, loss = 0.43344379\n",
      "Iteration 10, loss = 0.42551072\n",
      "Iteration 11, loss = 0.42391362\n",
      "Iteration 12, loss = 0.40012241\n",
      "Iteration 13, loss = 0.37651450\n",
      "Iteration 14, loss = 0.34639396\n",
      "Iteration 15, loss = 0.33820069\n",
      "Iteration 16, loss = 0.33310981\n",
      "Iteration 17, loss = 0.32606556\n",
      "Iteration 18, loss = 0.32164719\n",
      "Iteration 19, loss = 0.30869666\n",
      "Iteration 20, loss = 0.30187912\n",
      "Iteration 21, loss = 0.29572700\n",
      "Iteration 22, loss = 0.29348924\n",
      "Iteration 23, loss = 0.29805963\n",
      "Iteration 24, loss = 0.29805005\n",
      "Iteration 25, loss = 0.30233605\n",
      "Iteration 26, loss = 0.30541214\n",
      "Iteration 27, loss = 0.30365357\n",
      "Iteration 28, loss = 0.29240660\n",
      "Iteration 29, loss = 0.28566635\n",
      "Iteration 30, loss = 0.29747738\n",
      "Iteration 31, loss = 0.31161612\n",
      "Iteration 32, loss = 0.29668715\n",
      "Iteration 33, loss = 0.29444445\n",
      "Iteration 34, loss = 0.29278805\n",
      "Iteration 35, loss = 0.29596849\n",
      "Iteration 36, loss = 0.29655828\n",
      "Iteration 37, loss = 0.29768610\n",
      "Iteration 38, loss = 0.29894542\n",
      "Iteration 39, loss = 0.29636132\n",
      "Iteration 40, loss = 0.28888598\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.42449955\n",
      "Iteration 2, loss = 1.68767006\n",
      "Iteration 3, loss = 1.07225110\n",
      "Iteration 4, loss = 0.69274769\n",
      "Iteration 5, loss = 0.64808618\n",
      "Iteration 6, loss = 0.60038329\n",
      "Iteration 7, loss = 0.51921366\n",
      "Iteration 8, loss = 0.45464037\n",
      "Iteration 9, loss = 0.42189204\n",
      "Iteration 10, loss = 0.40659046\n",
      "Iteration 11, loss = 0.39885743\n",
      "Iteration 12, loss = 0.37833007\n",
      "Iteration 13, loss = 0.35760660\n",
      "Iteration 14, loss = 0.33433408\n",
      "Iteration 15, loss = 0.32528867\n",
      "Iteration 16, loss = 0.30920902\n",
      "Iteration 17, loss = 0.30405398\n",
      "Iteration 18, loss = 0.31122579\n",
      "Iteration 19, loss = 0.29726270\n",
      "Iteration 20, loss = 0.28885021\n",
      "Iteration 21, loss = 0.28160664\n",
      "Iteration 22, loss = 0.28209847\n",
      "Iteration 23, loss = 0.28787095\n",
      "Iteration 24, loss = 0.29307956\n",
      "Iteration 25, loss = 0.29919714\n",
      "Iteration 26, loss = 0.29324122\n",
      "Iteration 27, loss = 0.28213100\n",
      "Iteration 28, loss = 0.28101228\n",
      "Iteration 29, loss = 0.27905368\n",
      "Iteration 30, loss = 0.28620248\n",
      "Iteration 31, loss = 0.29646190\n",
      "Iteration 32, loss = 0.29143279\n",
      "Iteration 33, loss = 0.28331070\n",
      "Iteration 34, loss = 0.27933852\n",
      "Iteration 35, loss = 0.28639187\n",
      "Iteration 36, loss = 0.30656963\n",
      "Iteration 37, loss = 0.29901856\n",
      "Iteration 38, loss = 0.27998695\n",
      "Iteration 39, loss = 0.27685757\n",
      "Iteration 40, loss = 0.28008086\n",
      "Iteration 41, loss = 0.28894798\n",
      "Iteration 42, loss = 0.29235449\n",
      "Iteration 43, loss = 0.27065790\n",
      "Iteration 44, loss = 0.27514067\n",
      "Iteration 45, loss = 0.30411798\n",
      "Iteration 46, loss = 0.30749705\n",
      "Iteration 47, loss = 0.28395722\n",
      "Iteration 48, loss = 0.28290969\n",
      "Iteration 49, loss = 0.28981187\n",
      "Iteration 50, loss = 0.29775056\n",
      "Iteration 51, loss = 0.30306897\n",
      "Iteration 52, loss = 0.29153542\n",
      "Iteration 53, loss = 0.29151945\n",
      "Iteration 54, loss = 0.29690944\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.42105581\n",
      "Iteration 2, loss = 1.56475876\n",
      "Iteration 3, loss = 0.97184648\n",
      "Iteration 4, loss = 0.65399514\n",
      "Iteration 5, loss = 0.63365771\n",
      "Iteration 6, loss = 0.56549358\n",
      "Iteration 7, loss = 0.47906606\n",
      "Iteration 8, loss = 0.43241965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9, loss = 0.41632560\n",
      "Iteration 10, loss = 0.40530378\n",
      "Iteration 11, loss = 0.39296692\n",
      "Iteration 12, loss = 0.36833194\n",
      "Iteration 13, loss = 0.35100314\n",
      "Iteration 14, loss = 0.33343723\n",
      "Iteration 15, loss = 0.32912057\n",
      "Iteration 16, loss = 0.31457984\n",
      "Iteration 17, loss = 0.31606236\n",
      "Iteration 18, loss = 0.32510444\n",
      "Iteration 19, loss = 0.30129507\n",
      "Iteration 20, loss = 0.29158267\n",
      "Iteration 21, loss = 0.28889450\n",
      "Iteration 22, loss = 0.29692976\n",
      "Iteration 23, loss = 0.30878819\n",
      "Iteration 24, loss = 0.30364032\n",
      "Iteration 25, loss = 0.29334529\n",
      "Iteration 26, loss = 0.28265565\n",
      "Iteration 27, loss = 0.28290358\n",
      "Iteration 28, loss = 0.29245156\n",
      "Iteration 29, loss = 0.29035321\n",
      "Iteration 30, loss = 0.29699744\n",
      "Iteration 31, loss = 0.30289500\n",
      "Iteration 32, loss = 0.30074882\n",
      "Iteration 33, loss = 0.29265372\n",
      "Iteration 34, loss = 0.28796292\n",
      "Iteration 35, loss = 0.29646591\n",
      "Iteration 36, loss = 0.31277691\n",
      "Iteration 37, loss = 0.29534915\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.43549476\n",
      "Iteration 2, loss = 1.49364459\n",
      "Iteration 3, loss = 0.93450184\n",
      "Iteration 4, loss = 0.66455315\n",
      "Iteration 5, loss = 0.62892308\n",
      "Iteration 6, loss = 0.55011036\n",
      "Iteration 7, loss = 0.49052683\n",
      "Iteration 8, loss = 0.46804997\n",
      "Iteration 9, loss = 0.45215176\n",
      "Iteration 10, loss = 0.42017145\n",
      "Iteration 11, loss = 0.39587857\n",
      "Iteration 12, loss = 0.37564274\n",
      "Iteration 13, loss = 0.36045744\n",
      "Iteration 14, loss = 0.34178335\n",
      "Iteration 15, loss = 0.33879231\n",
      "Iteration 16, loss = 0.32810162\n",
      "Iteration 17, loss = 0.31535802\n",
      "Iteration 18, loss = 0.33652716\n",
      "Iteration 19, loss = 0.32996583\n",
      "Iteration 20, loss = 0.31108467\n",
      "Iteration 21, loss = 0.29284109\n",
      "Iteration 22, loss = 0.30480622\n",
      "Iteration 23, loss = 0.32329414\n",
      "Iteration 24, loss = 0.31266840\n",
      "Iteration 25, loss = 0.29989707\n",
      "Iteration 26, loss = 0.29676282\n",
      "Iteration 27, loss = 0.30139989\n",
      "Iteration 28, loss = 0.30776754\n",
      "Iteration 29, loss = 0.30986845\n",
      "Iteration 30, loss = 0.31175271\n",
      "Iteration 31, loss = 0.30263212\n",
      "Iteration 32, loss = 0.29992061\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.46864151\n",
      "Iteration 2, loss = 1.41233584\n",
      "Iteration 3, loss = 0.91358281\n",
      "Iteration 4, loss = 0.63480907\n",
      "Iteration 5, loss = 0.57028266\n",
      "Iteration 6, loss = 0.51313486\n",
      "Iteration 7, loss = 0.47504176\n",
      "Iteration 8, loss = 0.45341375\n",
      "Iteration 9, loss = 0.43064212\n",
      "Iteration 10, loss = 0.39805354\n",
      "Iteration 11, loss = 0.37762707\n",
      "Iteration 12, loss = 0.36464385\n",
      "Iteration 13, loss = 0.35526990\n",
      "Iteration 14, loss = 0.33836132\n",
      "Iteration 15, loss = 0.32873580\n",
      "Iteration 16, loss = 0.31642164\n",
      "Iteration 17, loss = 0.30994673\n",
      "Iteration 18, loss = 0.32815029\n",
      "Iteration 19, loss = 0.31951592\n",
      "Iteration 20, loss = 0.30088757\n",
      "Iteration 21, loss = 0.28620144\n",
      "Iteration 22, loss = 0.29861961\n",
      "Iteration 23, loss = 0.31423320\n",
      "Iteration 24, loss = 0.31155470\n",
      "Iteration 25, loss = 0.30931860\n",
      "Iteration 26, loss = 0.30512241\n",
      "Iteration 27, loss = 0.30243950\n",
      "Iteration 28, loss = 0.30202576\n",
      "Iteration 29, loss = 0.29549290\n",
      "Iteration 30, loss = 0.29976226\n",
      "Iteration 31, loss = 0.29989482\n",
      "Iteration 32, loss = 0.30051485\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.44456611\n",
      "Iteration 2, loss = 1.41675735\n",
      "Iteration 3, loss = 0.93577556\n",
      "Iteration 4, loss = 0.63945940\n",
      "Iteration 5, loss = 0.54339442\n",
      "Iteration 6, loss = 0.49673784\n",
      "Iteration 7, loss = 0.47232357\n",
      "Iteration 8, loss = 0.44337635\n",
      "Iteration 9, loss = 0.41169534\n",
      "Iteration 10, loss = 0.37928428\n",
      "Iteration 11, loss = 0.36767500\n",
      "Iteration 12, loss = 0.36415865\n",
      "Iteration 13, loss = 0.35279639\n",
      "Iteration 14, loss = 0.33169868\n",
      "Iteration 15, loss = 0.31470379\n",
      "Iteration 16, loss = 0.30614266\n",
      "Iteration 17, loss = 0.30597531\n",
      "Iteration 18, loss = 0.32264384\n",
      "Iteration 19, loss = 0.31607643\n",
      "Iteration 20, loss = 0.29828622\n",
      "Iteration 21, loss = 0.27784735\n",
      "Iteration 22, loss = 0.29130653\n",
      "Iteration 23, loss = 0.31420142\n",
      "Iteration 24, loss = 0.30515295\n",
      "Iteration 25, loss = 0.28603332\n",
      "Iteration 26, loss = 0.29397018\n",
      "Iteration 27, loss = 0.32015813\n",
      "Iteration 28, loss = 0.32986557\n",
      "Iteration 29, loss = 0.31105578\n",
      "Iteration 30, loss = 0.31457844\n",
      "Iteration 31, loss = 0.31472320\n",
      "Iteration 32, loss = 0.30909205\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.99090682\n",
      "Iteration 2, loss = 1.21574344\n",
      "Iteration 3, loss = 0.74167217\n",
      "Iteration 4, loss = 0.63367294\n",
      "Iteration 5, loss = 0.49512911\n",
      "Iteration 6, loss = 0.44484272\n",
      "Iteration 7, loss = 0.43159146\n",
      "Iteration 8, loss = 0.40372649\n",
      "Iteration 9, loss = 0.37483647\n",
      "Iteration 10, loss = 0.36223938\n",
      "Iteration 11, loss = 0.35180610\n",
      "Iteration 12, loss = 0.33670303\n",
      "Iteration 13, loss = 0.32839774\n",
      "Iteration 14, loss = 0.32335453\n",
      "Iteration 15, loss = 0.31128370\n",
      "Iteration 16, loss = 0.31746098\n",
      "Iteration 17, loss = 0.30485396\n",
      "Iteration 18, loss = 0.29296565\n",
      "Iteration 19, loss = 0.30407334\n",
      "Iteration 20, loss = 0.31680283\n",
      "Iteration 21, loss = 0.30261887\n",
      "Iteration 22, loss = 0.29650014\n",
      "Iteration 23, loss = 0.30814333\n",
      "Iteration 24, loss = 0.30675083\n",
      "Iteration 25, loss = 0.29485028\n",
      "Iteration 26, loss = 0.31383531\n",
      "Iteration 27, loss = 0.31715479\n",
      "Iteration 28, loss = 0.29270562\n",
      "Iteration 29, loss = 0.28526405\n",
      "Iteration 30, loss = 0.29288330\n",
      "Iteration 31, loss = 0.30171099\n",
      "Iteration 32, loss = 0.30452470\n",
      "Iteration 33, loss = 0.32154427\n",
      "Iteration 34, loss = 0.30596477\n",
      "Iteration 35, loss = 0.29599274\n",
      "Iteration 36, loss = 0.30051864\n",
      "Iteration 37, loss = 0.32556678\n",
      "Iteration 38, loss = 0.30557217\n",
      "Iteration 39, loss = 0.30092872\n",
      "Iteration 40, loss = 0.31236453\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.99690308\n",
      "Iteration 2, loss = 1.14249114\n",
      "Iteration 3, loss = 0.74850741\n",
      "Iteration 4, loss = 0.62740482\n",
      "Iteration 5, loss = 0.51314766\n",
      "Iteration 6, loss = 0.46460799\n",
      "Iteration 7, loss = 0.45619013\n",
      "Iteration 8, loss = 0.40832302\n",
      "Iteration 9, loss = 0.35542401\n",
      "Iteration 10, loss = 0.35140202\n",
      "Iteration 11, loss = 0.35815965\n",
      "Iteration 12, loss = 0.34713067\n",
      "Iteration 13, loss = 0.31232097\n",
      "Iteration 14, loss = 0.29704619\n",
      "Iteration 15, loss = 0.31536731\n",
      "Iteration 16, loss = 0.34314742\n",
      "Iteration 17, loss = 0.33436467\n",
      "Iteration 18, loss = 0.30159808\n",
      "Iteration 19, loss = 0.28748649\n",
      "Iteration 20, loss = 0.31243144\n",
      "Iteration 21, loss = 0.32727802\n",
      "Iteration 22, loss = 0.32248366\n",
      "Iteration 23, loss = 0.32088234\n",
      "Iteration 24, loss = 0.32008658\n",
      "Iteration 25, loss = 0.30289964\n",
      "Iteration 26, loss = 0.30130024\n",
      "Iteration 27, loss = 0.31371420\n",
      "Iteration 28, loss = 0.31757502\n",
      "Iteration 29, loss = 0.33102013\n",
      "Iteration 30, loss = 0.33259644\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.00504222\n",
      "Iteration 2, loss = 1.17560428\n",
      "Iteration 3, loss = 0.74189967\n",
      "Iteration 4, loss = 0.62595745\n",
      "Iteration 5, loss = 0.51455384\n",
      "Iteration 6, loss = 0.44731750\n",
      "Iteration 7, loss = 0.43587302\n",
      "Iteration 8, loss = 0.40426297\n",
      "Iteration 9, loss = 0.35663434\n",
      "Iteration 10, loss = 0.34625426\n",
      "Iteration 11, loss = 0.35074148\n",
      "Iteration 12, loss = 0.34764974\n",
      "Iteration 13, loss = 0.31660000\n",
      "Iteration 14, loss = 0.30978417\n",
      "Iteration 15, loss = 0.31890795\n",
      "Iteration 16, loss = 0.33811929\n",
      "Iteration 17, loss = 0.34899901\n",
      "Iteration 18, loss = 0.31861596\n",
      "Iteration 19, loss = 0.29416692\n",
      "Iteration 20, loss = 0.31677184\n",
      "Iteration 21, loss = 0.32435013\n",
      "Iteration 22, loss = 0.31587034\n",
      "Iteration 23, loss = 0.31295804\n",
      "Iteration 24, loss = 0.31533992\n",
      "Iteration 25, loss = 0.30929120\n",
      "Iteration 26, loss = 0.30972630\n",
      "Iteration 27, loss = 0.32333126\n",
      "Iteration 28, loss = 0.30972870\n",
      "Iteration 29, loss = 0.30818229\n",
      "Iteration 30, loss = 0.31281913\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.99375764\n",
      "Iteration 2, loss = 1.09397511\n",
      "Iteration 3, loss = 0.71004085\n",
      "Iteration 4, loss = 0.57693058\n",
      "Iteration 5, loss = 0.46672927\n",
      "Iteration 6, loss = 0.43135907\n",
      "Iteration 7, loss = 0.42261386\n",
      "Iteration 8, loss = 0.38022801\n",
      "Iteration 9, loss = 0.34441698\n",
      "Iteration 10, loss = 0.33829364\n",
      "Iteration 11, loss = 0.33106413\n",
      "Iteration 12, loss = 0.31676416\n",
      "Iteration 13, loss = 0.29366672\n",
      "Iteration 14, loss = 0.30491255\n",
      "Iteration 15, loss = 0.30987714\n",
      "Iteration 16, loss = 0.30197827\n",
      "Iteration 17, loss = 0.31166902\n",
      "Iteration 18, loss = 0.30979495\n",
      "Iteration 19, loss = 0.29539084\n",
      "Iteration 20, loss = 0.31315574\n",
      "Iteration 21, loss = 0.31834640\n",
      "Iteration 22, loss = 0.30497728\n",
      "Iteration 23, loss = 0.29929000\n",
      "Iteration 24, loss = 0.30627007\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.01020684\n",
      "Iteration 2, loss = 1.14486016\n",
      "Iteration 3, loss = 0.76083010\n",
      "Iteration 4, loss = 0.61537362\n",
      "Iteration 5, loss = 0.49290117\n",
      "Iteration 6, loss = 0.45902870\n",
      "Iteration 7, loss = 0.43803154\n",
      "Iteration 8, loss = 0.38588074\n",
      "Iteration 9, loss = 0.34551592\n",
      "Iteration 10, loss = 0.33674114\n",
      "Iteration 11, loss = 0.33336471\n",
      "Iteration 12, loss = 0.32532328\n",
      "Iteration 13, loss = 0.30162313\n",
      "Iteration 14, loss = 0.30446194\n",
      "Iteration 15, loss = 0.31441890\n",
      "Iteration 16, loss = 0.31901009\n",
      "Iteration 17, loss = 0.31629651\n",
      "Iteration 18, loss = 0.29906708\n",
      "Iteration 19, loss = 0.28514342\n",
      "Iteration 20, loss = 0.29548411\n",
      "Iteration 21, loss = 0.29615352\n",
      "Iteration 22, loss = 0.28753877\n",
      "Iteration 23, loss = 0.28335524\n",
      "Iteration 24, loss = 0.29271745\n",
      "Iteration 25, loss = 0.29900020\n",
      "Iteration 26, loss = 0.30755709\n",
      "Iteration 27, loss = 0.30485661\n",
      "Iteration 28, loss = 0.29733357\n",
      "Iteration 29, loss = 0.29706764\n",
      "Iteration 30, loss = 0.28650849\n",
      "Iteration 31, loss = 0.28656800\n",
      "Iteration 32, loss = 0.29506576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 33, loss = 0.28902655\n",
      "Iteration 34, loss = 0.29486490\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.00689889\n",
      "Iteration 2, loss = 1.12206060\n",
      "Iteration 3, loss = 0.74459939\n",
      "Iteration 4, loss = 0.56992310\n",
      "Iteration 5, loss = 0.44674749\n",
      "Iteration 6, loss = 0.43169044\n",
      "Iteration 7, loss = 0.41478670\n",
      "Iteration 8, loss = 0.36549343\n",
      "Iteration 9, loss = 0.33564323\n",
      "Iteration 10, loss = 0.33383121\n",
      "Iteration 11, loss = 0.32952025\n",
      "Iteration 12, loss = 0.31447591\n",
      "Iteration 13, loss = 0.29224661\n",
      "Iteration 14, loss = 0.29770359\n",
      "Iteration 15, loss = 0.30674839\n",
      "Iteration 16, loss = 0.31315807\n",
      "Iteration 17, loss = 0.30654404\n",
      "Iteration 18, loss = 0.28731481\n",
      "Iteration 19, loss = 0.30065488\n",
      "Iteration 20, loss = 0.30061815\n",
      "Iteration 21, loss = 0.29722679\n",
      "Iteration 22, loss = 0.29359132\n",
      "Iteration 23, loss = 0.28730532\n",
      "Iteration 24, loss = 0.30243469\n",
      "Iteration 25, loss = 0.30570520\n",
      "Iteration 26, loss = 0.30084287\n",
      "Iteration 27, loss = 0.30595109\n",
      "Iteration 28, loss = 0.30243583\n",
      "Iteration 29, loss = 0.28996574\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.00209907\n",
      "Iteration 2, loss = 1.09062868\n",
      "Iteration 3, loss = 0.76269181\n",
      "Iteration 4, loss = 0.57423007\n",
      "Iteration 5, loss = 0.45145001\n",
      "Iteration 6, loss = 0.44986391\n",
      "Iteration 7, loss = 0.43392075\n",
      "Iteration 8, loss = 0.37837134\n",
      "Iteration 9, loss = 0.33959359\n",
      "Iteration 10, loss = 0.33457138\n",
      "Iteration 11, loss = 0.33841247\n",
      "Iteration 12, loss = 0.32231161\n",
      "Iteration 13, loss = 0.30292743\n",
      "Iteration 14, loss = 0.32662192\n",
      "Iteration 15, loss = 0.31895059\n",
      "Iteration 16, loss = 0.32431186\n",
      "Iteration 17, loss = 0.32462925\n",
      "Iteration 18, loss = 0.31620935\n",
      "Iteration 19, loss = 0.32558032\n",
      "Iteration 20, loss = 0.31859548\n",
      "Iteration 21, loss = 0.30687384\n",
      "Iteration 22, loss = 0.30725399\n",
      "Iteration 23, loss = 0.29817237\n",
      "Iteration 24, loss = 0.30207293\n",
      "Iteration 25, loss = 0.31314747\n",
      "Iteration 26, loss = 0.32823569\n",
      "Iteration 27, loss = 0.33437405\n",
      "Iteration 28, loss = 0.33086804\n",
      "Iteration 29, loss = 0.31386201\n",
      "Iteration 30, loss = 0.28190330\n",
      "Iteration 31, loss = 0.28351175\n",
      "Iteration 32, loss = 0.30607808\n",
      "Iteration 33, loss = 0.30135057\n",
      "Iteration 34, loss = 0.27032999\n",
      "Iteration 35, loss = 0.27562528\n",
      "Iteration 36, loss = 0.29589993\n",
      "Iteration 37, loss = 0.32354503\n",
      "Iteration 38, loss = 0.31337233\n",
      "Iteration 39, loss = 0.28582885\n",
      "Iteration 40, loss = 0.27906296\n",
      "Iteration 41, loss = 0.28900606\n",
      "Iteration 42, loss = 0.30074463\n",
      "Iteration 43, loss = 0.27971657\n",
      "Iteration 44, loss = 0.29058729\n",
      "Iteration 45, loss = 0.29781882\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.01509085\n",
      "Iteration 2, loss = 1.11332749\n",
      "Iteration 3, loss = 0.78415396\n",
      "Iteration 4, loss = 0.60157046\n",
      "Iteration 5, loss = 0.47739466\n",
      "Iteration 6, loss = 0.47245033\n",
      "Iteration 7, loss = 0.45722911\n",
      "Iteration 8, loss = 0.40002848\n",
      "Iteration 9, loss = 0.36079201\n",
      "Iteration 10, loss = 0.35268930\n",
      "Iteration 11, loss = 0.35410267\n",
      "Iteration 12, loss = 0.33755706\n",
      "Iteration 13, loss = 0.32047051\n",
      "Iteration 14, loss = 0.33751447\n",
      "Iteration 15, loss = 0.32825881\n",
      "Iteration 16, loss = 0.33260229\n",
      "Iteration 17, loss = 0.32850889\n",
      "Iteration 18, loss = 0.31398022\n",
      "Iteration 19, loss = 0.32308949\n",
      "Iteration 20, loss = 0.32411304\n",
      "Iteration 21, loss = 0.32210906\n",
      "Iteration 22, loss = 0.31853041\n",
      "Iteration 23, loss = 0.30792907\n",
      "Iteration 24, loss = 0.30428499\n",
      "Iteration 25, loss = 0.31927112\n",
      "Iteration 26, loss = 0.32838415\n",
      "Iteration 27, loss = 0.32886747\n",
      "Iteration 28, loss = 0.32598241\n",
      "Iteration 29, loss = 0.30542063\n",
      "Iteration 30, loss = 0.27922358\n",
      "Iteration 31, loss = 0.30627368\n",
      "Iteration 32, loss = 0.32784288\n",
      "Iteration 33, loss = 0.31482998\n",
      "Iteration 34, loss = 0.29738811\n",
      "Iteration 35, loss = 0.31582101\n",
      "Iteration 36, loss = 0.32787904\n",
      "Iteration 37, loss = 0.33407288\n",
      "Iteration 38, loss = 0.32026390\n",
      "Iteration 39, loss = 0.30306315\n",
      "Iteration 40, loss = 0.30358775\n",
      "Iteration 41, loss = 0.30672061\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.01746351\n",
      "Iteration 2, loss = 1.11811551\n",
      "Iteration 3, loss = 0.77612146\n",
      "Iteration 4, loss = 0.59542061\n",
      "Iteration 5, loss = 0.47694122\n",
      "Iteration 6, loss = 0.46615430\n",
      "Iteration 7, loss = 0.44695555\n",
      "Iteration 8, loss = 0.40008951\n",
      "Iteration 9, loss = 0.36760229\n",
      "Iteration 10, loss = 0.36238538\n",
      "Iteration 11, loss = 0.36004067\n",
      "Iteration 12, loss = 0.33930077\n",
      "Iteration 13, loss = 0.32457753\n",
      "Iteration 14, loss = 0.34395402\n",
      "Iteration 15, loss = 0.32710637\n",
      "Iteration 16, loss = 0.32074622\n",
      "Iteration 17, loss = 0.32282372\n",
      "Iteration 18, loss = 0.31762709\n",
      "Iteration 19, loss = 0.33000133\n",
      "Iteration 20, loss = 0.31852231\n",
      "Iteration 21, loss = 0.31737946\n",
      "Iteration 22, loss = 0.32668356\n",
      "Iteration 23, loss = 0.32843055\n",
      "Iteration 24, loss = 0.30833421\n",
      "Iteration 25, loss = 0.30257165\n",
      "Iteration 26, loss = 0.31567345\n",
      "Iteration 27, loss = 0.32484307\n",
      "Iteration 28, loss = 0.32501058\n",
      "Iteration 29, loss = 0.30563341\n",
      "Iteration 30, loss = 0.28131125\n",
      "Iteration 31, loss = 0.30269046\n",
      "Iteration 32, loss = 0.33464324\n",
      "Iteration 33, loss = 0.33521898\n",
      "Iteration 34, loss = 0.30053838\n",
      "Iteration 35, loss = 0.30761201\n",
      "Iteration 36, loss = 0.33880236\n",
      "Iteration 37, loss = 0.34160003\n",
      "Iteration 38, loss = 0.32035300\n",
      "Iteration 39, loss = 0.30101314\n",
      "Iteration 40, loss = 0.29352457\n",
      "Iteration 41, loss = 0.29268248\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.02024094\n",
      "Iteration 2, loss = 1.11626194\n",
      "Iteration 3, loss = 0.76103637\n",
      "Iteration 4, loss = 0.58001729\n",
      "Iteration 5, loss = 0.45867459\n",
      "Iteration 6, loss = 0.44714935\n",
      "Iteration 7, loss = 0.43293724\n",
      "Iteration 8, loss = 0.39833571\n",
      "Iteration 9, loss = 0.36025359\n",
      "Iteration 10, loss = 0.34586010\n",
      "Iteration 11, loss = 0.34649637\n",
      "Iteration 12, loss = 0.33128724\n",
      "Iteration 13, loss = 0.32056608\n",
      "Iteration 14, loss = 0.33549013\n",
      "Iteration 15, loss = 0.31605542\n",
      "Iteration 16, loss = 0.30700656\n",
      "Iteration 17, loss = 0.31230506\n",
      "Iteration 18, loss = 0.31167171\n",
      "Iteration 19, loss = 0.31902328\n",
      "Iteration 20, loss = 0.31356776\n",
      "Iteration 21, loss = 0.31454665\n",
      "Iteration 22, loss = 0.32291906\n",
      "Iteration 23, loss = 0.32648506\n",
      "Iteration 24, loss = 0.30207002\n",
      "Iteration 25, loss = 0.28432101\n",
      "Iteration 26, loss = 0.30528605\n",
      "Iteration 27, loss = 0.33215798\n",
      "Iteration 28, loss = 0.33427081\n",
      "Iteration 29, loss = 0.30641158\n",
      "Iteration 30, loss = 0.29017554\n",
      "Iteration 31, loss = 0.28299894\n",
      "Iteration 32, loss = 0.31103551\n",
      "Iteration 33, loss = 0.35752684\n",
      "Iteration 34, loss = 0.32422155\n",
      "Iteration 35, loss = 0.30826070\n",
      "Iteration 36, loss = 0.31806566\n",
      "Iteration 37, loss = 0.31536521\n",
      "Iteration 38, loss = 0.30918172\n",
      "Iteration 39, loss = 0.31641285\n",
      "Iteration 40, loss = 0.32051962\n",
      "Iteration 41, loss = 0.29433633\n",
      "Iteration 42, loss = 0.28792871\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.67427785\n",
      "Iteration 2, loss = 1.68965225\n",
      "Iteration 3, loss = 1.08725893\n",
      "Iteration 4, loss = 0.67060758\n",
      "Iteration 5, loss = 0.62137092\n",
      "Iteration 6, loss = 0.57970050\n",
      "Iteration 7, loss = 0.49235579\n",
      "Iteration 8, loss = 0.43465992\n",
      "Iteration 9, loss = 0.41150143\n",
      "Iteration 10, loss = 0.40319105\n",
      "Iteration 11, loss = 0.40044877\n",
      "Iteration 12, loss = 0.38508916\n",
      "Iteration 13, loss = 0.35305815\n",
      "Iteration 14, loss = 0.33758879\n",
      "Iteration 15, loss = 0.34089456\n",
      "Iteration 16, loss = 0.34546322\n",
      "Iteration 17, loss = 0.34158453\n",
      "Iteration 18, loss = 0.31703072\n",
      "Iteration 19, loss = 0.31335426\n",
      "Iteration 20, loss = 0.34688175\n",
      "Iteration 21, loss = 0.33844778\n",
      "Iteration 22, loss = 0.30594566\n",
      "Iteration 23, loss = 0.29762701\n",
      "Iteration 24, loss = 0.30194549\n",
      "Iteration 25, loss = 0.30233328\n",
      "Iteration 26, loss = 0.30542271\n",
      "Iteration 27, loss = 0.29869567\n",
      "Iteration 28, loss = 0.29559124\n",
      "Iteration 29, loss = 0.30378677\n",
      "Iteration 30, loss = 0.31505816\n",
      "Iteration 31, loss = 0.30339355\n",
      "Iteration 32, loss = 0.29323793\n",
      "Iteration 33, loss = 0.30754899\n",
      "Iteration 34, loss = 0.30895283\n",
      "Iteration 35, loss = 0.29793128\n",
      "Iteration 36, loss = 0.28866249\n",
      "Iteration 37, loss = 0.28840117\n",
      "Iteration 38, loss = 0.30162592\n",
      "Iteration 39, loss = 0.31641162\n",
      "Iteration 40, loss = 0.32127065\n",
      "Iteration 41, loss = 0.29968352\n",
      "Iteration 42, loss = 0.31407009\n",
      "Iteration 43, loss = 0.34808809\n",
      "Iteration 44, loss = 0.34842648\n",
      "Iteration 45, loss = 0.31424580\n",
      "Iteration 46, loss = 0.29718146\n",
      "Iteration 47, loss = 0.32086012\n",
      "Iteration 48, loss = 0.33125055\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.68014509\n",
      "Iteration 2, loss = 1.46513932\n",
      "Iteration 3, loss = 1.02996528\n",
      "Iteration 4, loss = 0.67482770\n",
      "Iteration 5, loss = 0.55501984\n",
      "Iteration 6, loss = 0.54801732\n",
      "Iteration 7, loss = 0.49502555\n",
      "Iteration 8, loss = 0.42565144\n",
      "Iteration 9, loss = 0.39646447\n",
      "Iteration 10, loss = 0.39803266\n",
      "Iteration 11, loss = 0.39934985\n",
      "Iteration 12, loss = 0.37966939\n",
      "Iteration 13, loss = 0.34963745\n",
      "Iteration 14, loss = 0.32955222\n",
      "Iteration 15, loss = 0.31957405\n",
      "Iteration 16, loss = 0.31427932\n",
      "Iteration 17, loss = 0.31948689\n",
      "Iteration 18, loss = 0.31124449\n",
      "Iteration 19, loss = 0.30926325\n",
      "Iteration 20, loss = 0.31271468\n",
      "Iteration 21, loss = 0.30455785\n",
      "Iteration 22, loss = 0.31445634\n",
      "Iteration 23, loss = 0.31275618\n",
      "Iteration 24, loss = 0.30545454\n",
      "Iteration 25, loss = 0.31813424\n",
      "Iteration 26, loss = 0.32074549\n",
      "Iteration 27, loss = 0.31341871\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 28, loss = 0.30188029\n",
      "Iteration 29, loss = 0.29859468\n",
      "Iteration 30, loss = 0.29592325\n",
      "Iteration 31, loss = 0.30980807\n",
      "Iteration 32, loss = 0.29431351\n",
      "Iteration 33, loss = 0.29965025\n",
      "Iteration 34, loss = 0.31917641\n",
      "Iteration 35, loss = 0.31951782\n",
      "Iteration 36, loss = 0.29586446\n",
      "Iteration 37, loss = 0.31460693\n",
      "Iteration 38, loss = 0.31933138\n",
      "Iteration 39, loss = 0.32387928\n",
      "Iteration 40, loss = 0.33229064\n",
      "Iteration 41, loss = 0.33202321\n",
      "Iteration 42, loss = 0.32533458\n",
      "Iteration 43, loss = 0.30252356\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.67043257\n",
      "Iteration 2, loss = 1.53477538\n",
      "Iteration 3, loss = 0.99849255\n",
      "Iteration 4, loss = 0.64619908\n",
      "Iteration 5, loss = 0.57679485\n",
      "Iteration 6, loss = 0.53359554\n",
      "Iteration 7, loss = 0.45875905\n",
      "Iteration 8, loss = 0.40656922\n",
      "Iteration 9, loss = 0.39431317\n",
      "Iteration 10, loss = 0.40143968\n",
      "Iteration 11, loss = 0.40079403\n",
      "Iteration 12, loss = 0.37873901\n",
      "Iteration 13, loss = 0.35195635\n",
      "Iteration 14, loss = 0.33314795\n",
      "Iteration 15, loss = 0.31601446\n",
      "Iteration 16, loss = 0.31294872\n",
      "Iteration 17, loss = 0.32495403\n",
      "Iteration 18, loss = 0.31637054\n",
      "Iteration 19, loss = 0.30634663\n",
      "Iteration 20, loss = 0.30712633\n",
      "Iteration 21, loss = 0.30058261\n",
      "Iteration 22, loss = 0.31410806\n",
      "Iteration 23, loss = 0.33803550\n",
      "Iteration 24, loss = 0.33022766\n",
      "Iteration 25, loss = 0.32648387\n",
      "Iteration 26, loss = 0.31992734\n",
      "Iteration 27, loss = 0.30987829\n",
      "Iteration 28, loss = 0.31085013\n",
      "Iteration 29, loss = 0.30150529\n",
      "Iteration 30, loss = 0.28956522\n",
      "Iteration 31, loss = 0.29488704\n",
      "Iteration 32, loss = 0.28763660\n",
      "Iteration 33, loss = 0.30591868\n",
      "Iteration 34, loss = 0.32784394\n",
      "Iteration 35, loss = 0.33530956\n",
      "Iteration 36, loss = 0.30506333\n",
      "Iteration 37, loss = 0.30705485\n",
      "Iteration 38, loss = 0.33340438\n",
      "Iteration 39, loss = 0.33762065\n",
      "Iteration 40, loss = 0.32816466\n",
      "Iteration 41, loss = 0.33059418\n",
      "Iteration 42, loss = 0.31912390\n",
      "Iteration 43, loss = 0.29993768\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.69276369\n",
      "Iteration 2, loss = 1.49344985\n",
      "Iteration 3, loss = 1.00325889\n",
      "Iteration 4, loss = 0.64295980\n",
      "Iteration 5, loss = 0.54157434\n",
      "Iteration 6, loss = 0.51810747\n",
      "Iteration 7, loss = 0.47029570\n",
      "Iteration 8, loss = 0.40887146\n",
      "Iteration 9, loss = 0.38205222\n",
      "Iteration 10, loss = 0.39136661\n",
      "Iteration 11, loss = 0.40088329\n",
      "Iteration 12, loss = 0.38301968\n",
      "Iteration 13, loss = 0.35377685\n",
      "Iteration 14, loss = 0.33164871\n",
      "Iteration 15, loss = 0.31217788\n",
      "Iteration 16, loss = 0.31505629\n",
      "Iteration 17, loss = 0.32679759\n",
      "Iteration 18, loss = 0.31456864\n",
      "Iteration 19, loss = 0.29978094\n",
      "Iteration 20, loss = 0.30072854\n",
      "Iteration 21, loss = 0.29585493\n",
      "Iteration 22, loss = 0.30583262\n",
      "Iteration 23, loss = 0.33198124\n",
      "Iteration 24, loss = 0.32114548\n",
      "Iteration 25, loss = 0.31723470\n",
      "Iteration 26, loss = 0.31196729\n",
      "Iteration 27, loss = 0.30497406\n",
      "Iteration 28, loss = 0.30703205\n",
      "Iteration 29, loss = 0.30556757\n",
      "Iteration 30, loss = 0.28149548\n",
      "Iteration 31, loss = 0.29281553\n",
      "Iteration 32, loss = 0.28402525\n",
      "Iteration 33, loss = 0.29100900\n",
      "Iteration 34, loss = 0.30046728\n",
      "Iteration 35, loss = 0.30895463\n",
      "Iteration 36, loss = 0.29534495\n",
      "Iteration 37, loss = 0.30464169\n",
      "Iteration 38, loss = 0.30796049\n",
      "Iteration 39, loss = 0.30424139\n",
      "Iteration 40, loss = 0.31218647\n",
      "Iteration 41, loss = 0.32384334\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.67614134\n",
      "Iteration 2, loss = 1.52106406\n",
      "Iteration 3, loss = 1.01735895\n",
      "Iteration 4, loss = 0.64124903\n",
      "Iteration 5, loss = 0.56335861\n",
      "Iteration 6, loss = 0.54414925\n",
      "Iteration 7, loss = 0.48447324\n",
      "Iteration 8, loss = 0.41972711\n",
      "Iteration 9, loss = 0.39366785\n",
      "Iteration 10, loss = 0.40657118\n",
      "Iteration 11, loss = 0.41922075\n",
      "Iteration 12, loss = 0.39700112\n",
      "Iteration 13, loss = 0.35426619\n",
      "Iteration 14, loss = 0.33637455\n",
      "Iteration 15, loss = 0.33407140\n",
      "Iteration 16, loss = 0.32333415\n",
      "Iteration 17, loss = 0.32140809\n",
      "Iteration 18, loss = 0.32222859\n",
      "Iteration 19, loss = 0.32844262\n",
      "Iteration 20, loss = 0.32415363\n",
      "Iteration 21, loss = 0.31045122\n",
      "Iteration 22, loss = 0.32682378\n",
      "Iteration 23, loss = 0.33923690\n",
      "Iteration 24, loss = 0.31663437\n",
      "Iteration 25, loss = 0.31343531\n",
      "Iteration 26, loss = 0.31211088\n",
      "Iteration 27, loss = 0.30840156\n",
      "Iteration 28, loss = 0.31065570\n",
      "Iteration 29, loss = 0.30159475\n",
      "Iteration 30, loss = 0.28899612\n",
      "Iteration 31, loss = 0.30177284\n",
      "Iteration 32, loss = 0.30120744\n",
      "Iteration 33, loss = 0.30623885\n",
      "Iteration 34, loss = 0.32197121\n",
      "Iteration 35, loss = 0.31954529\n",
      "Iteration 36, loss = 0.30263275\n",
      "Iteration 37, loss = 0.30359854\n",
      "Iteration 38, loss = 0.31071041\n",
      "Iteration 39, loss = 0.32877659\n",
      "Iteration 40, loss = 0.33334853\n",
      "Iteration 41, loss = 0.32611353\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.67821208\n",
      "Iteration 2, loss = 1.54229088\n",
      "Iteration 3, loss = 1.07669591\n",
      "Iteration 4, loss = 0.67486761\n",
      "Iteration 5, loss = 0.57541570\n",
      "Iteration 6, loss = 0.55326956\n",
      "Iteration 7, loss = 0.50039710\n",
      "Iteration 8, loss = 0.43045507\n",
      "Iteration 9, loss = 0.39296854\n",
      "Iteration 10, loss = 0.39539714\n",
      "Iteration 11, loss = 0.39906281\n",
      "Iteration 12, loss = 0.38011610\n",
      "Iteration 13, loss = 0.35200020\n",
      "Iteration 14, loss = 0.33252041\n",
      "Iteration 15, loss = 0.32042996\n",
      "Iteration 16, loss = 0.30806978\n",
      "Iteration 17, loss = 0.30448294\n",
      "Iteration 18, loss = 0.31030490\n",
      "Iteration 19, loss = 0.31830549\n",
      "Iteration 20, loss = 0.31280065\n",
      "Iteration 21, loss = 0.29749065\n",
      "Iteration 22, loss = 0.31335085\n",
      "Iteration 23, loss = 0.32927330\n",
      "Iteration 24, loss = 0.31793812\n",
      "Iteration 25, loss = 0.30338736\n",
      "Iteration 26, loss = 0.29546604\n",
      "Iteration 27, loss = 0.28702053\n",
      "Iteration 28, loss = 0.28100246\n",
      "Iteration 29, loss = 0.28706575\n",
      "Iteration 30, loss = 0.28372123\n",
      "Iteration 31, loss = 0.28807475\n",
      "Iteration 32, loss = 0.29334353\n",
      "Iteration 33, loss = 0.30876495\n",
      "Iteration 34, loss = 0.31880701\n",
      "Iteration 35, loss = 0.28315736\n",
      "Iteration 36, loss = 0.28932769\n",
      "Iteration 37, loss = 0.34426421\n",
      "Iteration 38, loss = 0.34125406\n",
      "Iteration 39, loss = 0.31509781\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.67991435\n",
      "Iteration 2, loss = 1.57936304\n",
      "Iteration 3, loss = 1.09079912\n",
      "Iteration 4, loss = 0.70405208\n",
      "Iteration 5, loss = 0.60177996\n",
      "Iteration 6, loss = 0.57625499\n",
      "Iteration 7, loss = 0.51806803\n",
      "Iteration 8, loss = 0.44727962\n",
      "Iteration 9, loss = 0.40790894\n",
      "Iteration 10, loss = 0.40689095\n",
      "Iteration 11, loss = 0.40561065\n",
      "Iteration 12, loss = 0.38097335\n",
      "Iteration 13, loss = 0.34902648\n",
      "Iteration 14, loss = 0.33474451\n",
      "Iteration 15, loss = 0.32077657\n",
      "Iteration 16, loss = 0.31060780\n",
      "Iteration 17, loss = 0.31091207\n",
      "Iteration 18, loss = 0.31793375\n",
      "Iteration 19, loss = 0.31914166\n",
      "Iteration 20, loss = 0.31449158\n",
      "Iteration 21, loss = 0.29471941\n",
      "Iteration 22, loss = 0.30551400\n",
      "Iteration 23, loss = 0.33090070\n",
      "Iteration 24, loss = 0.32523780\n",
      "Iteration 25, loss = 0.30937581\n",
      "Iteration 26, loss = 0.29651938\n",
      "Iteration 27, loss = 0.29133063\n",
      "Iteration 28, loss = 0.30013862\n",
      "Iteration 29, loss = 0.29328707\n",
      "Iteration 30, loss = 0.28075589\n",
      "Iteration 31, loss = 0.27886370\n",
      "Iteration 32, loss = 0.28086218\n",
      "Iteration 33, loss = 0.30285474\n",
      "Iteration 34, loss = 0.32517224\n",
      "Iteration 35, loss = 0.28738165\n",
      "Iteration 36, loss = 0.28453459\n",
      "Iteration 37, loss = 0.34337056\n",
      "Iteration 38, loss = 0.33480913\n",
      "Iteration 39, loss = 0.30596794\n",
      "Iteration 40, loss = 0.28681086\n",
      "Iteration 41, loss = 0.27330005\n",
      "Iteration 42, loss = 0.28165455\n",
      "Iteration 43, loss = 0.28701690\n",
      "Iteration 44, loss = 0.28278350\n",
      "Iteration 45, loss = 0.29272774\n",
      "Iteration 46, loss = 0.29361601\n",
      "Iteration 47, loss = 0.29864491\n",
      "Iteration 48, loss = 0.29465906\n",
      "Iteration 49, loss = 0.29019930\n",
      "Iteration 50, loss = 0.28970944\n",
      "Iteration 51, loss = 0.27951792\n",
      "Iteration 52, loss = 0.27737375\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.67846494\n",
      "Iteration 2, loss = 1.57642941\n",
      "Iteration 3, loss = 1.10384381\n",
      "Iteration 4, loss = 0.71822252\n",
      "Iteration 5, loss = 0.61474911\n",
      "Iteration 6, loss = 0.60526282\n",
      "Iteration 7, loss = 0.55162726\n",
      "Iteration 8, loss = 0.47658474\n",
      "Iteration 9, loss = 0.43546678\n",
      "Iteration 10, loss = 0.43608314\n",
      "Iteration 11, loss = 0.43726924\n",
      "Iteration 12, loss = 0.40846142\n",
      "Iteration 13, loss = 0.36672839\n",
      "Iteration 14, loss = 0.35059821\n",
      "Iteration 15, loss = 0.34859522\n",
      "Iteration 16, loss = 0.33283585\n",
      "Iteration 17, loss = 0.32734167\n",
      "Iteration 18, loss = 0.33164102\n",
      "Iteration 19, loss = 0.33504962\n",
      "Iteration 20, loss = 0.33016213\n",
      "Iteration 21, loss = 0.31189450\n",
      "Iteration 22, loss = 0.33874574\n",
      "Iteration 23, loss = 0.37116513\n",
      "Iteration 24, loss = 0.35372854\n",
      "Iteration 25, loss = 0.32802442\n",
      "Iteration 26, loss = 0.32669004\n",
      "Iteration 27, loss = 0.33962536\n",
      "Iteration 28, loss = 0.34225030\n",
      "Iteration 29, loss = 0.31637511\n",
      "Iteration 30, loss = 0.29013751\n",
      "Iteration 31, loss = 0.30405992\n",
      "Iteration 32, loss = 0.32429999\n",
      "Iteration 33, loss = 0.35758675\n",
      "Iteration 34, loss = 0.33299847\n",
      "Iteration 35, loss = 0.29551181\n",
      "Iteration 36, loss = 0.31768020\n",
      "Iteration 37, loss = 0.37370480\n",
      "Iteration 38, loss = 0.37485040\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 39, loss = 0.33889734\n",
      "Iteration 40, loss = 0.30602015\n",
      "Iteration 41, loss = 0.29155776\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.68381672\n",
      "Iteration 2, loss = 1.51502634\n",
      "Iteration 3, loss = 1.12247640\n",
      "Iteration 4, loss = 0.74246585\n",
      "Iteration 5, loss = 0.58782976\n",
      "Iteration 6, loss = 0.58219958\n",
      "Iteration 7, loss = 0.56528808\n",
      "Iteration 8, loss = 0.50097447\n",
      "Iteration 9, loss = 0.44221458\n",
      "Iteration 10, loss = 0.42453570\n",
      "Iteration 11, loss = 0.42356660\n",
      "Iteration 12, loss = 0.41142103\n",
      "Iteration 13, loss = 0.37851184\n",
      "Iteration 14, loss = 0.35833423\n",
      "Iteration 15, loss = 0.34538159\n",
      "Iteration 16, loss = 0.32887228\n",
      "Iteration 17, loss = 0.32173575\n",
      "Iteration 18, loss = 0.32379573\n",
      "Iteration 19, loss = 0.32785243\n",
      "Iteration 20, loss = 0.33111597\n",
      "Iteration 21, loss = 0.31906112\n",
      "Iteration 22, loss = 0.33541390\n",
      "Iteration 23, loss = 0.35592722\n",
      "Iteration 24, loss = 0.33955030\n",
      "Iteration 25, loss = 0.32457297\n",
      "Iteration 26, loss = 0.32708922\n",
      "Iteration 27, loss = 0.33321388\n",
      "Iteration 28, loss = 0.33485326\n",
      "Iteration 29, loss = 0.31439453\n",
      "Iteration 30, loss = 0.29561659\n",
      "Iteration 31, loss = 0.30954974\n",
      "Iteration 32, loss = 0.31901320\n",
      "Iteration 33, loss = 0.34259909\n",
      "Iteration 34, loss = 0.32948716\n",
      "Iteration 35, loss = 0.28944585\n",
      "Iteration 36, loss = 0.30854248\n",
      "Iteration 37, loss = 0.34822826\n",
      "Iteration 38, loss = 0.35310416\n",
      "Iteration 39, loss = 0.34267347\n",
      "Iteration 40, loss = 0.31648723\n",
      "Iteration 41, loss = 0.29493414\n",
      "Iteration 42, loss = 0.30779433\n",
      "Iteration 43, loss = 0.30036471\n",
      "Iteration 44, loss = 0.30178246\n",
      "Iteration 45, loss = 0.31006933\n",
      "Iteration 46, loss = 0.30365343\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.67470247\n",
      "Iteration 2, loss = 1.53152611\n",
      "Iteration 3, loss = 1.05071071\n",
      "Iteration 4, loss = 0.70512186\n",
      "Iteration 5, loss = 0.58368419\n",
      "Iteration 6, loss = 0.56149531\n",
      "Iteration 7, loss = 0.51764082\n",
      "Iteration 8, loss = 0.45698863\n",
      "Iteration 9, loss = 0.41396534\n",
      "Iteration 10, loss = 0.40739519\n",
      "Iteration 11, loss = 0.41426798\n",
      "Iteration 12, loss = 0.40036701\n",
      "Iteration 13, loss = 0.35725483\n",
      "Iteration 14, loss = 0.34068053\n",
      "Iteration 15, loss = 0.33003679\n",
      "Iteration 16, loss = 0.32047413\n",
      "Iteration 17, loss = 0.31961175\n",
      "Iteration 18, loss = 0.31602844\n",
      "Iteration 19, loss = 0.31155507\n",
      "Iteration 20, loss = 0.30648067\n",
      "Iteration 21, loss = 0.29783308\n",
      "Iteration 22, loss = 0.32366436\n",
      "Iteration 23, loss = 0.35195280\n",
      "Iteration 24, loss = 0.33899169\n",
      "Iteration 25, loss = 0.31371697\n",
      "Iteration 26, loss = 0.31580526\n",
      "Iteration 27, loss = 0.33011214\n",
      "Iteration 28, loss = 0.33161760\n",
      "Iteration 29, loss = 0.30259665\n",
      "Iteration 30, loss = 0.27873876\n",
      "Iteration 31, loss = 0.29298399\n",
      "Iteration 32, loss = 0.29930810\n",
      "Iteration 33, loss = 0.31953662\n",
      "Iteration 34, loss = 0.29929337\n",
      "Iteration 35, loss = 0.29084457\n",
      "Iteration 36, loss = 0.31424088\n",
      "Iteration 37, loss = 0.33886834\n",
      "Iteration 38, loss = 0.33265611\n",
      "Iteration 39, loss = 0.33146566\n",
      "Iteration 40, loss = 0.31710771\n",
      "Iteration 41, loss = 0.30322771\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.09438151\n",
      "Iteration 2, loss = 1.23471972\n",
      "Iteration 3, loss = 0.67979573\n",
      "Iteration 4, loss = 0.57092123\n",
      "Iteration 5, loss = 0.52721490\n",
      "Iteration 6, loss = 0.44014993\n",
      "Iteration 7, loss = 0.39258639\n",
      "Iteration 8, loss = 0.38876447\n",
      "Iteration 9, loss = 0.37626091\n",
      "Iteration 10, loss = 0.35400079\n",
      "Iteration 11, loss = 0.35561843\n",
      "Iteration 12, loss = 0.33050329\n",
      "Iteration 13, loss = 0.32843528\n",
      "Iteration 14, loss = 0.35722498\n",
      "Iteration 15, loss = 0.33494911\n",
      "Iteration 16, loss = 0.31902587\n",
      "Iteration 17, loss = 0.31269372\n",
      "Iteration 18, loss = 0.31024112\n",
      "Iteration 19, loss = 0.32396562\n",
      "Iteration 20, loss = 0.33674768\n",
      "Iteration 21, loss = 0.30314380\n",
      "Iteration 22, loss = 0.30166465\n",
      "Iteration 23, loss = 0.34874283\n",
      "Iteration 24, loss = 0.32037894\n",
      "Iteration 25, loss = 0.31155332\n",
      "Iteration 26, loss = 0.34772344\n",
      "Iteration 27, loss = 0.33385199\n",
      "Iteration 28, loss = 0.29602167\n",
      "Iteration 29, loss = 0.29890881\n",
      "Iteration 30, loss = 0.33854259\n",
      "Iteration 31, loss = 0.30915391\n",
      "Iteration 32, loss = 0.30935269\n",
      "Iteration 33, loss = 0.32187840\n",
      "Iteration 34, loss = 0.30207953\n",
      "Iteration 35, loss = 0.29384455\n",
      "Iteration 36, loss = 0.31104490\n",
      "Iteration 37, loss = 0.30853370\n",
      "Iteration 38, loss = 0.30359837\n",
      "Iteration 39, loss = 0.31349653\n",
      "Iteration 40, loss = 0.29537093\n",
      "Iteration 41, loss = 0.32784449\n",
      "Iteration 42, loss = 0.33213581\n",
      "Iteration 43, loss = 0.30575373\n",
      "Iteration 44, loss = 0.30700817\n",
      "Iteration 45, loss = 0.32767192\n",
      "Iteration 46, loss = 0.30810438\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.07787310\n",
      "Iteration 2, loss = 1.18832448\n",
      "Iteration 3, loss = 0.67640622\n",
      "Iteration 4, loss = 0.61048733\n",
      "Iteration 5, loss = 0.52383855\n",
      "Iteration 6, loss = 0.43866923\n",
      "Iteration 7, loss = 0.41424571\n",
      "Iteration 8, loss = 0.38984011\n",
      "Iteration 9, loss = 0.39073639\n",
      "Iteration 10, loss = 0.38047019\n",
      "Iteration 11, loss = 0.35728608\n",
      "Iteration 12, loss = 0.34339292\n",
      "Iteration 13, loss = 0.34306377\n",
      "Iteration 14, loss = 0.34194749\n",
      "Iteration 15, loss = 0.30124250\n",
      "Iteration 16, loss = 0.29426142\n",
      "Iteration 17, loss = 0.31978805\n",
      "Iteration 18, loss = 0.33337544\n",
      "Iteration 19, loss = 0.31158193\n",
      "Iteration 20, loss = 0.32672028\n",
      "Iteration 21, loss = 0.32676447\n",
      "Iteration 22, loss = 0.30209884\n",
      "Iteration 23, loss = 0.30786573\n",
      "Iteration 24, loss = 0.31287455\n",
      "Iteration 25, loss = 0.29070087\n",
      "Iteration 26, loss = 0.28343946\n",
      "Iteration 27, loss = 0.28876968\n",
      "Iteration 28, loss = 0.29191741\n",
      "Iteration 29, loss = 0.28692139\n",
      "Iteration 30, loss = 0.28720675\n",
      "Iteration 31, loss = 0.29145489\n",
      "Iteration 32, loss = 0.30860700\n",
      "Iteration 33, loss = 0.29004630\n",
      "Iteration 34, loss = 0.31368480\n",
      "Iteration 35, loss = 0.31456356\n",
      "Iteration 36, loss = 0.30369007\n",
      "Iteration 37, loss = 0.31624597\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.09102321\n",
      "Iteration 2, loss = 1.24887139\n",
      "Iteration 3, loss = 0.69101028\n",
      "Iteration 4, loss = 0.60748882\n",
      "Iteration 5, loss = 0.53573278\n",
      "Iteration 6, loss = 0.45344531\n",
      "Iteration 7, loss = 0.42737881\n",
      "Iteration 8, loss = 0.39446978\n",
      "Iteration 9, loss = 0.38496659\n",
      "Iteration 10, loss = 0.40021501\n",
      "Iteration 11, loss = 0.39663409\n",
      "Iteration 12, loss = 0.36697634\n",
      "Iteration 13, loss = 0.34089312\n",
      "Iteration 14, loss = 0.35186854\n",
      "Iteration 15, loss = 0.33312516\n",
      "Iteration 16, loss = 0.30635966\n",
      "Iteration 17, loss = 0.31363524\n",
      "Iteration 18, loss = 0.34848488\n",
      "Iteration 19, loss = 0.31855653\n",
      "Iteration 20, loss = 0.30823161\n",
      "Iteration 21, loss = 0.30783152\n",
      "Iteration 22, loss = 0.29681423\n",
      "Iteration 23, loss = 0.30292081\n",
      "Iteration 24, loss = 0.30090592\n",
      "Iteration 25, loss = 0.28666877\n",
      "Iteration 26, loss = 0.27470250\n",
      "Iteration 27, loss = 0.27160475\n",
      "Iteration 28, loss = 0.28309569\n",
      "Iteration 29, loss = 0.29137803\n",
      "Iteration 30, loss = 0.29241814\n",
      "Iteration 31, loss = 0.28319560\n",
      "Iteration 32, loss = 0.29448156\n",
      "Iteration 33, loss = 0.29416179\n",
      "Iteration 34, loss = 0.29327367\n",
      "Iteration 35, loss = 0.30433399\n",
      "Iteration 36, loss = 0.29471203\n",
      "Iteration 37, loss = 0.28372676\n",
      "Iteration 38, loss = 0.28288093\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.07045831\n",
      "Iteration 2, loss = 1.14785828\n",
      "Iteration 3, loss = 0.67523879\n",
      "Iteration 4, loss = 0.61829742\n",
      "Iteration 5, loss = 0.52588142\n",
      "Iteration 6, loss = 0.41384885\n",
      "Iteration 7, loss = 0.43401486\n",
      "Iteration 8, loss = 0.44641274\n",
      "Iteration 9, loss = 0.39367694\n",
      "Iteration 10, loss = 0.36160261\n",
      "Iteration 11, loss = 0.37606542\n",
      "Iteration 12, loss = 0.38560444\n",
      "Iteration 13, loss = 0.35543519\n",
      "Iteration 14, loss = 0.33642074\n",
      "Iteration 15, loss = 0.35287450\n",
      "Iteration 16, loss = 0.33026311\n",
      "Iteration 17, loss = 0.30988267\n",
      "Iteration 18, loss = 0.32109029\n",
      "Iteration 19, loss = 0.33209382\n",
      "Iteration 20, loss = 0.33617414\n",
      "Iteration 21, loss = 0.32188748\n",
      "Iteration 22, loss = 0.31783878\n",
      "Iteration 23, loss = 0.29756553\n",
      "Iteration 24, loss = 0.29449109\n",
      "Iteration 25, loss = 0.30553249\n",
      "Iteration 26, loss = 0.31291608\n",
      "Iteration 27, loss = 0.28837032\n",
      "Iteration 28, loss = 0.28121657\n",
      "Iteration 29, loss = 0.28556196\n",
      "Iteration 30, loss = 0.29439423\n",
      "Iteration 31, loss = 0.28652417\n",
      "Iteration 32, loss = 0.30552337\n",
      "Iteration 33, loss = 0.30596548\n",
      "Iteration 34, loss = 0.27404710\n",
      "Iteration 35, loss = 0.26901550\n",
      "Iteration 36, loss = 0.28657384\n",
      "Iteration 37, loss = 0.29632345\n",
      "Iteration 38, loss = 0.28154763\n",
      "Iteration 39, loss = 0.27634385\n",
      "Iteration 40, loss = 0.27470670\n",
      "Iteration 41, loss = 0.27160665\n",
      "Iteration 42, loss = 0.28001216\n",
      "Iteration 43, loss = 0.26335286\n",
      "Iteration 44, loss = 0.27128286\n",
      "Iteration 45, loss = 0.28394944\n",
      "Iteration 46, loss = 0.28900857\n",
      "Iteration 47, loss = 0.28978780\n",
      "Iteration 48, loss = 0.27053792\n",
      "Iteration 49, loss = 0.26924816\n",
      "Iteration 50, loss = 0.27985638\n",
      "Iteration 51, loss = 0.28927341\n",
      "Iteration 52, loss = 0.28976827\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 53, loss = 0.28699347\n",
      "Iteration 54, loss = 0.28657399\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.08659574\n",
      "Iteration 2, loss = 1.21645745\n",
      "Iteration 3, loss = 0.70033344\n",
      "Iteration 4, loss = 0.60210818\n",
      "Iteration 5, loss = 0.53442006\n",
      "Iteration 6, loss = 0.42221583\n",
      "Iteration 7, loss = 0.40963405\n",
      "Iteration 8, loss = 0.42585833\n",
      "Iteration 9, loss = 0.40178928\n",
      "Iteration 10, loss = 0.36526484\n",
      "Iteration 11, loss = 0.35554912\n",
      "Iteration 12, loss = 0.36473547\n",
      "Iteration 13, loss = 0.34313000\n",
      "Iteration 14, loss = 0.34248906\n",
      "Iteration 15, loss = 0.34826404\n",
      "Iteration 16, loss = 0.31931801\n",
      "Iteration 17, loss = 0.30830417\n",
      "Iteration 18, loss = 0.32866620\n",
      "Iteration 19, loss = 0.31848446\n",
      "Iteration 20, loss = 0.31054081\n",
      "Iteration 21, loss = 0.31214606\n",
      "Iteration 22, loss = 0.29277872\n",
      "Iteration 23, loss = 0.28721240\n",
      "Iteration 24, loss = 0.30466972\n",
      "Iteration 25, loss = 0.30059739\n",
      "Iteration 26, loss = 0.28578528\n",
      "Iteration 27, loss = 0.27720535\n",
      "Iteration 28, loss = 0.28247655\n",
      "Iteration 29, loss = 0.28442217\n",
      "Iteration 30, loss = 0.28502704\n",
      "Iteration 31, loss = 0.28426320\n",
      "Iteration 32, loss = 0.30087371\n",
      "Iteration 33, loss = 0.29438063\n",
      "Iteration 34, loss = 0.28426592\n",
      "Iteration 35, loss = 0.30021787\n",
      "Iteration 36, loss = 0.30078715\n",
      "Iteration 37, loss = 0.29247848\n",
      "Iteration 38, loss = 0.31339913\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.07522867\n",
      "Iteration 2, loss = 1.30442191\n",
      "Iteration 3, loss = 0.70883119\n",
      "Iteration 4, loss = 0.59522126\n",
      "Iteration 5, loss = 0.54752202\n",
      "Iteration 6, loss = 0.44174953\n",
      "Iteration 7, loss = 0.40095529\n",
      "Iteration 8, loss = 0.38595472\n",
      "Iteration 9, loss = 0.37591083\n",
      "Iteration 10, loss = 0.37027486\n",
      "Iteration 11, loss = 0.36415125\n",
      "Iteration 12, loss = 0.35261728\n",
      "Iteration 13, loss = 0.33177357\n",
      "Iteration 14, loss = 0.31836342\n",
      "Iteration 15, loss = 0.31273520\n",
      "Iteration 16, loss = 0.29309403\n",
      "Iteration 17, loss = 0.29841664\n",
      "Iteration 18, loss = 0.32977702\n",
      "Iteration 19, loss = 0.29925685\n",
      "Iteration 20, loss = 0.29753525\n",
      "Iteration 21, loss = 0.31461038\n",
      "Iteration 22, loss = 0.31529991\n",
      "Iteration 23, loss = 0.29592421\n",
      "Iteration 24, loss = 0.30323893\n",
      "Iteration 25, loss = 0.29660296\n",
      "Iteration 26, loss = 0.27700380\n",
      "Iteration 27, loss = 0.27595601\n",
      "Iteration 28, loss = 0.29202822\n",
      "Iteration 29, loss = 0.29361714\n",
      "Iteration 30, loss = 0.27988205\n",
      "Iteration 31, loss = 0.27157097\n",
      "Iteration 32, loss = 0.28202425\n",
      "Iteration 33, loss = 0.29509543\n",
      "Iteration 34, loss = 0.29300459\n",
      "Iteration 35, loss = 0.28997926\n",
      "Iteration 36, loss = 0.28969851\n",
      "Iteration 37, loss = 0.28589687\n",
      "Iteration 38, loss = 0.27528166\n",
      "Iteration 39, loss = 0.28985820\n",
      "Iteration 40, loss = 0.28083667\n",
      "Iteration 41, loss = 0.26633187\n",
      "Iteration 42, loss = 0.26934226\n",
      "Iteration 43, loss = 0.26121314\n",
      "Iteration 44, loss = 0.26096102\n",
      "Iteration 45, loss = 0.27885202\n",
      "Iteration 46, loss = 0.28517791\n",
      "Iteration 47, loss = 0.27172713\n",
      "Iteration 48, loss = 0.26210326\n",
      "Iteration 49, loss = 0.26234627\n",
      "Iteration 50, loss = 0.27719960\n",
      "Iteration 51, loss = 0.28763326\n",
      "Iteration 52, loss = 0.27875810\n",
      "Iteration 53, loss = 0.28041811\n",
      "Iteration 54, loss = 0.29829644\n",
      "Iteration 55, loss = 0.30292304\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.09555353\n",
      "Iteration 2, loss = 1.19888114\n",
      "Iteration 3, loss = 0.66352133\n",
      "Iteration 4, loss = 0.58132839\n",
      "Iteration 5, loss = 0.51472765\n",
      "Iteration 6, loss = 0.41605829\n",
      "Iteration 7, loss = 0.38484773\n",
      "Iteration 8, loss = 0.38669535\n",
      "Iteration 9, loss = 0.37573453\n",
      "Iteration 10, loss = 0.36257764\n",
      "Iteration 11, loss = 0.33981542\n",
      "Iteration 12, loss = 0.32909035\n",
      "Iteration 13, loss = 0.33089274\n",
      "Iteration 14, loss = 0.31843636\n",
      "Iteration 15, loss = 0.30233541\n",
      "Iteration 16, loss = 0.28407597\n",
      "Iteration 17, loss = 0.29129705\n",
      "Iteration 18, loss = 0.32190425\n",
      "Iteration 19, loss = 0.30976926\n",
      "Iteration 20, loss = 0.29667003\n",
      "Iteration 21, loss = 0.29712256\n",
      "Iteration 22, loss = 0.29042291\n",
      "Iteration 23, loss = 0.29712268\n",
      "Iteration 24, loss = 0.29157949\n",
      "Iteration 25, loss = 0.26904519\n",
      "Iteration 26, loss = 0.27381521\n",
      "Iteration 27, loss = 0.28302936\n",
      "Iteration 28, loss = 0.28690303\n",
      "Iteration 29, loss = 0.28477007\n",
      "Iteration 30, loss = 0.29670639\n",
      "Iteration 31, loss = 0.29292116\n",
      "Iteration 32, loss = 0.28012093\n",
      "Iteration 33, loss = 0.28636628\n",
      "Iteration 34, loss = 0.28902802\n",
      "Iteration 35, loss = 0.29869085\n",
      "Iteration 36, loss = 0.30799748\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.09510751\n",
      "Iteration 2, loss = 1.24345287\n",
      "Iteration 3, loss = 0.70912469\n",
      "Iteration 4, loss = 0.64631770\n",
      "Iteration 5, loss = 0.57626400\n",
      "Iteration 6, loss = 0.45973789\n",
      "Iteration 7, loss = 0.42805974\n",
      "Iteration 8, loss = 0.42144282\n",
      "Iteration 9, loss = 0.39726257\n",
      "Iteration 10, loss = 0.38186660\n",
      "Iteration 11, loss = 0.36805517\n",
      "Iteration 12, loss = 0.35406396\n",
      "Iteration 13, loss = 0.34487488\n",
      "Iteration 14, loss = 0.33034644\n",
      "Iteration 15, loss = 0.32283126\n",
      "Iteration 16, loss = 0.30612158\n",
      "Iteration 17, loss = 0.30160484\n",
      "Iteration 18, loss = 0.31861418\n",
      "Iteration 19, loss = 0.31479295\n",
      "Iteration 20, loss = 0.29918312\n",
      "Iteration 21, loss = 0.29836793\n",
      "Iteration 22, loss = 0.29988695\n",
      "Iteration 23, loss = 0.30365283\n",
      "Iteration 24, loss = 0.29889191\n",
      "Iteration 25, loss = 0.28427316\n",
      "Iteration 26, loss = 0.27829783\n",
      "Iteration 27, loss = 0.27958877\n",
      "Iteration 28, loss = 0.28891218\n",
      "Iteration 29, loss = 0.28824340\n",
      "Iteration 30, loss = 0.29176732\n",
      "Iteration 31, loss = 0.30484694\n",
      "Iteration 32, loss = 0.29522732\n",
      "Iteration 33, loss = 0.32927756\n",
      "Iteration 34, loss = 0.32473737\n",
      "Iteration 35, loss = 0.31666336\n",
      "Iteration 36, loss = 0.32707416\n",
      "Iteration 37, loss = 0.30900990\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.07832681\n",
      "Iteration 2, loss = 1.26604068\n",
      "Iteration 3, loss = 0.71138999\n",
      "Iteration 4, loss = 0.61019261\n",
      "Iteration 5, loss = 0.58297394\n",
      "Iteration 6, loss = 0.47093224\n",
      "Iteration 7, loss = 0.42287754\n",
      "Iteration 8, loss = 0.41865975\n",
      "Iteration 9, loss = 0.40200208\n",
      "Iteration 10, loss = 0.38252254\n",
      "Iteration 11, loss = 0.36583718\n",
      "Iteration 12, loss = 0.35579863\n",
      "Iteration 13, loss = 0.34155750\n",
      "Iteration 14, loss = 0.33029871\n",
      "Iteration 15, loss = 0.32934999\n",
      "Iteration 16, loss = 0.31537491\n",
      "Iteration 17, loss = 0.30830003\n",
      "Iteration 18, loss = 0.31694274\n",
      "Iteration 19, loss = 0.31060916\n",
      "Iteration 20, loss = 0.29221189\n",
      "Iteration 21, loss = 0.28640840\n",
      "Iteration 22, loss = 0.30401983\n",
      "Iteration 23, loss = 0.30851100\n",
      "Iteration 24, loss = 0.28779030\n",
      "Iteration 25, loss = 0.28035590\n",
      "Iteration 26, loss = 0.29348917\n",
      "Iteration 27, loss = 0.28295381\n",
      "Iteration 28, loss = 0.27940202\n",
      "Iteration 29, loss = 0.29191133\n",
      "Iteration 30, loss = 0.30234466\n",
      "Iteration 31, loss = 0.29991317\n",
      "Iteration 32, loss = 0.27810207\n",
      "Iteration 33, loss = 0.30787983\n",
      "Iteration 34, loss = 0.32213951\n",
      "Iteration 35, loss = 0.29757371\n",
      "Iteration 36, loss = 0.29793338\n",
      "Iteration 37, loss = 0.29726223\n",
      "Iteration 38, loss = 0.28837356\n",
      "Iteration 39, loss = 0.33003356\n",
      "Iteration 40, loss = 0.32710482\n",
      "Iteration 41, loss = 0.27534705\n",
      "Iteration 42, loss = 0.28506499\n",
      "Iteration 43, loss = 0.29780014\n",
      "Iteration 44, loss = 0.29788122\n",
      "Iteration 45, loss = 0.31508401\n",
      "Iteration 46, loss = 0.32241190\n",
      "Iteration 47, loss = 0.30156518\n",
      "Iteration 48, loss = 0.27955238\n",
      "Iteration 49, loss = 0.28663139\n",
      "Iteration 50, loss = 0.28123793\n",
      "Iteration 51, loss = 0.29730204\n",
      "Iteration 52, loss = 0.31904182\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.09070498\n",
      "Iteration 2, loss = 1.29593628\n",
      "Iteration 3, loss = 0.72029663\n",
      "Iteration 4, loss = 0.59239017\n",
      "Iteration 5, loss = 0.57354840\n",
      "Iteration 6, loss = 0.46724871\n",
      "Iteration 7, loss = 0.41237135\n",
      "Iteration 8, loss = 0.40734793\n",
      "Iteration 9, loss = 0.39461172\n",
      "Iteration 10, loss = 0.37280061\n",
      "Iteration 11, loss = 0.35287439\n",
      "Iteration 12, loss = 0.33996183\n",
      "Iteration 13, loss = 0.33035465\n",
      "Iteration 14, loss = 0.32302836\n",
      "Iteration 15, loss = 0.32468713\n",
      "Iteration 16, loss = 0.31279441\n",
      "Iteration 17, loss = 0.30558239\n",
      "Iteration 18, loss = 0.30828478\n",
      "Iteration 19, loss = 0.29893247\n",
      "Iteration 20, loss = 0.28697670\n",
      "Iteration 21, loss = 0.27551159\n",
      "Iteration 22, loss = 0.29382996\n",
      "Iteration 23, loss = 0.30343063\n",
      "Iteration 24, loss = 0.28870715\n",
      "Iteration 25, loss = 0.28729882\n",
      "Iteration 26, loss = 0.29636671\n",
      "Iteration 27, loss = 0.27330927\n",
      "Iteration 28, loss = 0.28118994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 29, loss = 0.30059325\n",
      "Iteration 30, loss = 0.30380619\n",
      "Iteration 31, loss = 0.30308283\n",
      "Iteration 32, loss = 0.27987750\n",
      "Iteration 33, loss = 0.30553272\n",
      "Iteration 34, loss = 0.32014165\n",
      "Iteration 35, loss = 0.29119879\n",
      "Iteration 36, loss = 0.28763722\n",
      "Iteration 37, loss = 0.28865278\n",
      "Iteration 38, loss = 0.27622517\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.38770343\n",
      "Iteration 2, loss = 1.37255175\n",
      "Iteration 3, loss = 0.79756985\n",
      "Iteration 4, loss = 0.66098515\n",
      "Iteration 5, loss = 0.54965095\n",
      "Iteration 6, loss = 0.47998756\n",
      "Iteration 7, loss = 0.45172200\n",
      "Iteration 8, loss = 0.43249251\n",
      "Iteration 9, loss = 0.41284625\n",
      "Iteration 10, loss = 0.39751696\n",
      "Iteration 11, loss = 0.36921331\n",
      "Iteration 12, loss = 0.34053029\n",
      "Iteration 13, loss = 0.33565651\n",
      "Iteration 14, loss = 0.33235881\n",
      "Iteration 15, loss = 0.31543560\n",
      "Iteration 16, loss = 0.31897149\n",
      "Iteration 17, loss = 0.32833090\n",
      "Iteration 18, loss = 0.31392605\n",
      "Iteration 19, loss = 0.29432172\n",
      "Iteration 20, loss = 0.28757646\n",
      "Iteration 21, loss = 0.29481676\n",
      "Iteration 22, loss = 0.32981432\n",
      "Iteration 23, loss = 0.32084312\n",
      "Iteration 24, loss = 0.29580613\n",
      "Iteration 25, loss = 0.30577025\n",
      "Iteration 26, loss = 0.31837500\n",
      "Iteration 27, loss = 0.31055849\n",
      "Iteration 28, loss = 0.30457099\n",
      "Iteration 29, loss = 0.30751937\n",
      "Iteration 30, loss = 0.30408666\n",
      "Iteration 31, loss = 0.30142687\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.37895827\n",
      "Iteration 2, loss = 1.39415544\n",
      "Iteration 3, loss = 0.85844260\n",
      "Iteration 4, loss = 0.62716181\n",
      "Iteration 5, loss = 0.57184786\n",
      "Iteration 6, loss = 0.51445009\n",
      "Iteration 7, loss = 0.45818026\n",
      "Iteration 8, loss = 0.42748153\n",
      "Iteration 9, loss = 0.39827976\n",
      "Iteration 10, loss = 0.37209672\n",
      "Iteration 11, loss = 0.37935273\n",
      "Iteration 12, loss = 0.37873309\n",
      "Iteration 13, loss = 0.36176148\n",
      "Iteration 14, loss = 0.33749779\n",
      "Iteration 15, loss = 0.32724306\n",
      "Iteration 16, loss = 0.32017910\n",
      "Iteration 17, loss = 0.31850349\n",
      "Iteration 18, loss = 0.31443227\n",
      "Iteration 19, loss = 0.30267780\n",
      "Iteration 20, loss = 0.30374326\n",
      "Iteration 21, loss = 0.30752405\n",
      "Iteration 22, loss = 0.30452104\n",
      "Iteration 23, loss = 0.29415569\n",
      "Iteration 24, loss = 0.29266880\n",
      "Iteration 25, loss = 0.29968277\n",
      "Iteration 26, loss = 0.30200495\n",
      "Iteration 27, loss = 0.28955446\n",
      "Iteration 28, loss = 0.28755446\n",
      "Iteration 29, loss = 0.31292027\n",
      "Iteration 30, loss = 0.32305647\n",
      "Iteration 31, loss = 0.30133838\n",
      "Iteration 32, loss = 0.28772989\n",
      "Iteration 33, loss = 0.28517520\n",
      "Iteration 34, loss = 0.28714332\n",
      "Iteration 35, loss = 0.28788927\n",
      "Iteration 36, loss = 0.28949799\n",
      "Iteration 37, loss = 0.30051073\n",
      "Iteration 38, loss = 0.30492562\n",
      "Iteration 39, loss = 0.30688610\n",
      "Iteration 40, loss = 0.28656569\n",
      "Iteration 41, loss = 0.28726991\n",
      "Iteration 42, loss = 0.30448529\n",
      "Iteration 43, loss = 0.30412933\n",
      "Iteration 44, loss = 0.31183202\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.38211665\n",
      "Iteration 2, loss = 1.50410524\n",
      "Iteration 3, loss = 0.91741920\n",
      "Iteration 4, loss = 0.64991644\n",
      "Iteration 5, loss = 0.59303764\n",
      "Iteration 6, loss = 0.53509852\n",
      "Iteration 7, loss = 0.47123023\n",
      "Iteration 8, loss = 0.42774507\n",
      "Iteration 9, loss = 0.40161830\n",
      "Iteration 10, loss = 0.37502504\n",
      "Iteration 11, loss = 0.37368701\n",
      "Iteration 12, loss = 0.38251750\n",
      "Iteration 13, loss = 0.38307344\n",
      "Iteration 14, loss = 0.36016884\n",
      "Iteration 15, loss = 0.32800206\n",
      "Iteration 16, loss = 0.30960524\n",
      "Iteration 17, loss = 0.31943938\n",
      "Iteration 18, loss = 0.33310211\n",
      "Iteration 19, loss = 0.32428579\n",
      "Iteration 20, loss = 0.31122108\n",
      "Iteration 21, loss = 0.29783208\n",
      "Iteration 22, loss = 0.29273384\n",
      "Iteration 23, loss = 0.29659919\n",
      "Iteration 24, loss = 0.29824318\n",
      "Iteration 25, loss = 0.30401294\n",
      "Iteration 26, loss = 0.30610545\n",
      "Iteration 27, loss = 0.29045194\n",
      "Iteration 28, loss = 0.28733886\n",
      "Iteration 29, loss = 0.32457664\n",
      "Iteration 30, loss = 0.30863649\n",
      "Iteration 31, loss = 0.31303246\n",
      "Iteration 32, loss = 0.31720699\n",
      "Iteration 33, loss = 0.30773051\n",
      "Iteration 34, loss = 0.30515279\n",
      "Iteration 35, loss = 0.28794142\n",
      "Iteration 36, loss = 0.28004836\n",
      "Iteration 37, loss = 0.28992920\n",
      "Iteration 38, loss = 0.31617359\n",
      "Iteration 39, loss = 0.33301767\n",
      "Iteration 40, loss = 0.30746413\n",
      "Iteration 41, loss = 0.28667054\n",
      "Iteration 42, loss = 0.30919168\n",
      "Iteration 43, loss = 0.33048289\n",
      "Iteration 44, loss = 0.31811085\n",
      "Iteration 45, loss = 0.29494893\n",
      "Iteration 46, loss = 0.29219086\n",
      "Iteration 47, loss = 0.28210631\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.39274447\n",
      "Iteration 2, loss = 1.36021300\n",
      "Iteration 3, loss = 0.83436447\n",
      "Iteration 4, loss = 0.58876300\n",
      "Iteration 5, loss = 0.54670843\n",
      "Iteration 6, loss = 0.51375351\n",
      "Iteration 7, loss = 0.45627304\n",
      "Iteration 8, loss = 0.41440407\n",
      "Iteration 9, loss = 0.40033170\n",
      "Iteration 10, loss = 0.37783825\n",
      "Iteration 11, loss = 0.36719316\n",
      "Iteration 12, loss = 0.36653485\n",
      "Iteration 13, loss = 0.36964422\n",
      "Iteration 14, loss = 0.34865514\n",
      "Iteration 15, loss = 0.31718877\n",
      "Iteration 16, loss = 0.29881036\n",
      "Iteration 17, loss = 0.30651013\n",
      "Iteration 18, loss = 0.31796389\n",
      "Iteration 19, loss = 0.31240735\n",
      "Iteration 20, loss = 0.30356133\n",
      "Iteration 21, loss = 0.29580603\n",
      "Iteration 22, loss = 0.27809201\n",
      "Iteration 23, loss = 0.28805618\n",
      "Iteration 24, loss = 0.30778148\n",
      "Iteration 25, loss = 0.31417190\n",
      "Iteration 26, loss = 0.30005452\n",
      "Iteration 27, loss = 0.28734215\n",
      "Iteration 28, loss = 0.28834077\n",
      "Iteration 29, loss = 0.30780000\n",
      "Iteration 30, loss = 0.29709788\n",
      "Iteration 31, loss = 0.30146691\n",
      "Iteration 32, loss = 0.29565228\n",
      "Iteration 33, loss = 0.30970896\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.39106896\n",
      "Iteration 2, loss = 1.38246274\n",
      "Iteration 3, loss = 0.81634581\n",
      "Iteration 4, loss = 0.62518839\n",
      "Iteration 5, loss = 0.56026906\n",
      "Iteration 6, loss = 0.49849146\n",
      "Iteration 7, loss = 0.45466592\n",
      "Iteration 8, loss = 0.42220798\n",
      "Iteration 9, loss = 0.39724039\n",
      "Iteration 10, loss = 0.37082200\n",
      "Iteration 11, loss = 0.37295300\n",
      "Iteration 12, loss = 0.37538018\n",
      "Iteration 13, loss = 0.37027589\n",
      "Iteration 14, loss = 0.34174636\n",
      "Iteration 15, loss = 0.31342584\n",
      "Iteration 16, loss = 0.30159653\n",
      "Iteration 17, loss = 0.30831418\n",
      "Iteration 18, loss = 0.31869291\n",
      "Iteration 19, loss = 0.31384992\n",
      "Iteration 20, loss = 0.31044854\n",
      "Iteration 21, loss = 0.29806896\n",
      "Iteration 22, loss = 0.29521795\n",
      "Iteration 23, loss = 0.30507613\n",
      "Iteration 24, loss = 0.31177686\n",
      "Iteration 25, loss = 0.30876000\n",
      "Iteration 26, loss = 0.29183042\n",
      "Iteration 27, loss = 0.28065875\n",
      "Iteration 28, loss = 0.28002701\n",
      "Iteration 29, loss = 0.30295201\n",
      "Iteration 30, loss = 0.30261907\n",
      "Iteration 31, loss = 0.30956242\n",
      "Iteration 32, loss = 0.30556598\n",
      "Iteration 33, loss = 0.31768159\n",
      "Iteration 34, loss = 0.29824392\n",
      "Iteration 35, loss = 0.28215639\n",
      "Iteration 36, loss = 0.28937873\n",
      "Iteration 37, loss = 0.30893428\n",
      "Iteration 38, loss = 0.31103976\n",
      "Iteration 39, loss = 0.33804748\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.38542494\n",
      "Iteration 2, loss = 1.36946706\n",
      "Iteration 3, loss = 0.80321362\n",
      "Iteration 4, loss = 0.62502017\n",
      "Iteration 5, loss = 0.53864026\n",
      "Iteration 6, loss = 0.47625114\n",
      "Iteration 7, loss = 0.43482591\n",
      "Iteration 8, loss = 0.40730909\n",
      "Iteration 9, loss = 0.39307256\n",
      "Iteration 10, loss = 0.36654922\n",
      "Iteration 11, loss = 0.35580255\n",
      "Iteration 12, loss = 0.35136147\n",
      "Iteration 13, loss = 0.35180087\n",
      "Iteration 14, loss = 0.33236432\n",
      "Iteration 15, loss = 0.31063758\n",
      "Iteration 16, loss = 0.29490766\n",
      "Iteration 17, loss = 0.29287997\n",
      "Iteration 18, loss = 0.29769586\n",
      "Iteration 19, loss = 0.29441479\n",
      "Iteration 20, loss = 0.29660502\n",
      "Iteration 21, loss = 0.29794250\n",
      "Iteration 22, loss = 0.29915128\n",
      "Iteration 23, loss = 0.30231381\n",
      "Iteration 24, loss = 0.30788804\n",
      "Iteration 25, loss = 0.30938123\n",
      "Iteration 26, loss = 0.28503772\n",
      "Iteration 27, loss = 0.28116392\n",
      "Iteration 28, loss = 0.29306787\n",
      "Iteration 29, loss = 0.30061360\n",
      "Iteration 30, loss = 0.28416169\n",
      "Iteration 31, loss = 0.28833886\n",
      "Iteration 32, loss = 0.29321117\n",
      "Iteration 33, loss = 0.29129107\n",
      "Iteration 34, loss = 0.27720173\n",
      "Iteration 35, loss = 0.29436700\n",
      "Iteration 36, loss = 0.29856085\n",
      "Iteration 37, loss = 0.28782103\n",
      "Iteration 38, loss = 0.29149904\n",
      "Iteration 39, loss = 0.30477975\n",
      "Iteration 40, loss = 0.29594457\n",
      "Iteration 41, loss = 0.27480335\n",
      "Iteration 42, loss = 0.30415803\n",
      "Iteration 43, loss = 0.30835807\n",
      "Iteration 44, loss = 0.28131703\n",
      "Iteration 45, loss = 0.28258624\n",
      "Iteration 46, loss = 0.29433572\n",
      "Iteration 47, loss = 0.28209616\n",
      "Iteration 48, loss = 0.29833798\n",
      "Iteration 49, loss = 0.29780248\n",
      "Iteration 50, loss = 0.27370949\n",
      "Iteration 51, loss = 0.28992570\n",
      "Iteration 52, loss = 0.33139780\n",
      "Iteration 53, loss = 0.30744466\n",
      "Iteration 54, loss = 0.27694786\n",
      "Iteration 55, loss = 0.28114099\n",
      "Iteration 56, loss = 0.27676275\n",
      "Iteration 57, loss = 0.27611484\n",
      "Iteration 58, loss = 0.26196019\n",
      "Iteration 59, loss = 0.25352502\n",
      "Iteration 60, loss = 0.26602209\n",
      "Iteration 61, loss = 0.28231822\n",
      "Iteration 62, loss = 0.29200288\n",
      "Iteration 63, loss = 0.28134413\n",
      "Iteration 64, loss = 0.27916801\n",
      "Iteration 65, loss = 0.28341556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 66, loss = 0.27498710\n",
      "Iteration 67, loss = 0.27698805\n",
      "Iteration 68, loss = 0.28453671\n",
      "Iteration 69, loss = 0.29383059\n",
      "Iteration 70, loss = 0.27797411\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.39785428\n",
      "Iteration 2, loss = 1.39125398\n",
      "Iteration 3, loss = 0.80416410\n",
      "Iteration 4, loss = 0.66215993\n",
      "Iteration 5, loss = 0.55392185\n",
      "Iteration 6, loss = 0.47699463\n",
      "Iteration 7, loss = 0.43393740\n",
      "Iteration 8, loss = 0.41456488\n",
      "Iteration 9, loss = 0.40589612\n",
      "Iteration 10, loss = 0.37232265\n",
      "Iteration 11, loss = 0.35943816\n",
      "Iteration 12, loss = 0.36010662\n",
      "Iteration 13, loss = 0.36001289\n",
      "Iteration 14, loss = 0.33781035\n",
      "Iteration 15, loss = 0.30653810\n",
      "Iteration 16, loss = 0.29679720\n",
      "Iteration 17, loss = 0.30482934\n",
      "Iteration 18, loss = 0.30188265\n",
      "Iteration 19, loss = 0.29905036\n",
      "Iteration 20, loss = 0.30107515\n",
      "Iteration 21, loss = 0.30997864\n",
      "Iteration 22, loss = 0.31335107\n",
      "Iteration 23, loss = 0.30935759\n",
      "Iteration 24, loss = 0.31285972\n",
      "Iteration 25, loss = 0.32044573\n",
      "Iteration 26, loss = 0.30025583\n",
      "Iteration 27, loss = 0.29572130\n",
      "Iteration 28, loss = 0.30735164\n",
      "Iteration 29, loss = 0.31007319\n",
      "Iteration 30, loss = 0.29000580\n",
      "Iteration 31, loss = 0.29398589\n",
      "Iteration 32, loss = 0.30227283\n",
      "Iteration 33, loss = 0.29952902\n",
      "Iteration 34, loss = 0.28066237\n",
      "Iteration 35, loss = 0.28963097\n",
      "Iteration 36, loss = 0.30261142\n",
      "Iteration 37, loss = 0.29992830\n",
      "Iteration 38, loss = 0.29539938\n",
      "Iteration 39, loss = 0.28621670\n",
      "Iteration 40, loss = 0.27691610\n",
      "Iteration 41, loss = 0.27206748\n",
      "Iteration 42, loss = 0.28077544\n",
      "Iteration 43, loss = 0.29188162\n",
      "Iteration 44, loss = 0.28766283\n",
      "Iteration 45, loss = 0.28278668\n",
      "Iteration 46, loss = 0.28402211\n",
      "Iteration 47, loss = 0.27996197\n",
      "Iteration 48, loss = 0.29046016\n",
      "Iteration 49, loss = 0.28115855\n",
      "Iteration 50, loss = 0.27937992\n",
      "Iteration 51, loss = 0.30887215\n",
      "Iteration 52, loss = 0.31437058\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.39430512\n",
      "Iteration 2, loss = 1.41455019\n",
      "Iteration 3, loss = 0.82593835\n",
      "Iteration 4, loss = 0.67909579\n",
      "Iteration 5, loss = 0.56848638\n",
      "Iteration 6, loss = 0.50275888\n",
      "Iteration 7, loss = 0.45611710\n",
      "Iteration 8, loss = 0.43658610\n",
      "Iteration 9, loss = 0.42658981\n",
      "Iteration 10, loss = 0.38659424\n",
      "Iteration 11, loss = 0.37744056\n",
      "Iteration 12, loss = 0.38412024\n",
      "Iteration 13, loss = 0.38439295\n",
      "Iteration 14, loss = 0.35499878\n",
      "Iteration 15, loss = 0.31574806\n",
      "Iteration 16, loss = 0.30956489\n",
      "Iteration 17, loss = 0.31553124\n",
      "Iteration 18, loss = 0.31160264\n",
      "Iteration 19, loss = 0.30705197\n",
      "Iteration 20, loss = 0.30515603\n",
      "Iteration 21, loss = 0.31421168\n",
      "Iteration 22, loss = 0.31999666\n",
      "Iteration 23, loss = 0.31941957\n",
      "Iteration 24, loss = 0.31018814\n",
      "Iteration 25, loss = 0.31081482\n",
      "Iteration 26, loss = 0.31176472\n",
      "Iteration 27, loss = 0.32454345\n",
      "Iteration 28, loss = 0.32489476\n",
      "Iteration 29, loss = 0.30798404\n",
      "Iteration 30, loss = 0.29103530\n",
      "Iteration 31, loss = 0.29530470\n",
      "Iteration 32, loss = 0.30929824\n",
      "Iteration 33, loss = 0.31404821\n",
      "Iteration 34, loss = 0.29782365\n",
      "Iteration 35, loss = 0.28290099\n",
      "Iteration 36, loss = 0.28033824\n",
      "Iteration 37, loss = 0.29161055\n",
      "Iteration 38, loss = 0.30397968\n",
      "Iteration 39, loss = 0.30773695\n",
      "Iteration 40, loss = 0.30258203\n",
      "Iteration 41, loss = 0.29328016\n",
      "Iteration 42, loss = 0.30725469\n",
      "Iteration 43, loss = 0.31335105\n",
      "Iteration 44, loss = 0.30465466\n",
      "Iteration 45, loss = 0.29169209\n",
      "Iteration 46, loss = 0.28567041\n",
      "Iteration 47, loss = 0.29047595\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.39426981\n",
      "Iteration 2, loss = 1.36868043\n",
      "Iteration 3, loss = 0.81512067\n",
      "Iteration 4, loss = 0.63918573\n",
      "Iteration 5, loss = 0.55553091\n",
      "Iteration 6, loss = 0.50569687\n",
      "Iteration 7, loss = 0.44296807\n",
      "Iteration 8, loss = 0.41866449\n",
      "Iteration 9, loss = 0.42100319\n",
      "Iteration 10, loss = 0.39383167\n",
      "Iteration 11, loss = 0.37593851\n",
      "Iteration 12, loss = 0.37105635\n",
      "Iteration 13, loss = 0.37362519\n",
      "Iteration 14, loss = 0.35271861\n",
      "Iteration 15, loss = 0.30884288\n",
      "Iteration 16, loss = 0.30856896\n",
      "Iteration 17, loss = 0.32154705\n",
      "Iteration 18, loss = 0.31943145\n",
      "Iteration 19, loss = 0.31376225\n",
      "Iteration 20, loss = 0.30971304\n",
      "Iteration 21, loss = 0.30833228\n",
      "Iteration 22, loss = 0.30887083\n",
      "Iteration 23, loss = 0.31723196\n",
      "Iteration 24, loss = 0.31612950\n",
      "Iteration 25, loss = 0.31529514\n",
      "Iteration 26, loss = 0.31253281\n",
      "Iteration 27, loss = 0.32467570\n",
      "Iteration 28, loss = 0.31376056\n",
      "Iteration 29, loss = 0.30180468\n",
      "Iteration 30, loss = 0.29760619\n",
      "Iteration 31, loss = 0.29117395\n",
      "Iteration 32, loss = 0.28632828\n",
      "Iteration 33, loss = 0.28961194\n",
      "Iteration 34, loss = 0.29436374\n",
      "Iteration 35, loss = 0.28889213\n",
      "Iteration 36, loss = 0.26828958\n",
      "Iteration 37, loss = 0.27978592\n",
      "Iteration 38, loss = 0.29365509\n",
      "Iteration 39, loss = 0.30579874\n",
      "Iteration 40, loss = 0.30231010\n",
      "Iteration 41, loss = 0.28859964\n",
      "Iteration 42, loss = 0.28402522\n",
      "Iteration 43, loss = 0.29178696\n",
      "Iteration 44, loss = 0.29123141\n",
      "Iteration 45, loss = 0.28588220\n",
      "Iteration 46, loss = 0.28269610\n",
      "Iteration 47, loss = 0.28521860\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.39058988\n",
      "Iteration 2, loss = 1.38133810\n",
      "Iteration 3, loss = 0.83187345\n",
      "Iteration 4, loss = 0.65171679\n",
      "Iteration 5, loss = 0.54834463\n",
      "Iteration 6, loss = 0.50523914\n",
      "Iteration 7, loss = 0.45165776\n",
      "Iteration 8, loss = 0.41252256\n",
      "Iteration 9, loss = 0.39770374\n",
      "Iteration 10, loss = 0.38669755\n",
      "Iteration 11, loss = 0.37737943\n",
      "Iteration 12, loss = 0.34841587\n",
      "Iteration 13, loss = 0.33393043\n",
      "Iteration 14, loss = 0.33894799\n",
      "Iteration 15, loss = 0.31519716\n",
      "Iteration 16, loss = 0.30078740\n",
      "Iteration 17, loss = 0.30692375\n",
      "Iteration 18, loss = 0.30566556\n",
      "Iteration 19, loss = 0.29512883\n",
      "Iteration 20, loss = 0.29101504\n",
      "Iteration 21, loss = 0.29686448\n",
      "Iteration 22, loss = 0.29641586\n",
      "Iteration 23, loss = 0.29125566\n",
      "Iteration 24, loss = 0.28698542\n",
      "Iteration 25, loss = 0.29405576\n",
      "Iteration 26, loss = 0.30048896\n",
      "Iteration 27, loss = 0.30837550\n",
      "Iteration 28, loss = 0.29636354\n",
      "Iteration 29, loss = 0.29260788\n",
      "Iteration 30, loss = 0.29746376\n",
      "Iteration 31, loss = 0.27851092\n",
      "Iteration 32, loss = 0.27317176\n",
      "Iteration 33, loss = 0.28910516\n",
      "Iteration 34, loss = 0.28957625\n",
      "Iteration 35, loss = 0.27991625\n",
      "Iteration 36, loss = 0.26436872\n",
      "Iteration 37, loss = 0.26978048\n",
      "Iteration 38, loss = 0.28742341\n",
      "Iteration 39, loss = 0.31312717\n",
      "Iteration 40, loss = 0.30155716\n",
      "Iteration 41, loss = 0.29439227\n",
      "Iteration 42, loss = 0.29429924\n",
      "Iteration 43, loss = 0.30137342\n",
      "Iteration 44, loss = 0.28780967\n",
      "Iteration 45, loss = 0.27055186\n",
      "Iteration 46, loss = 0.26738973\n",
      "Iteration 47, loss = 0.27276884\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.45776675\n",
      "Iteration 2, loss = 1.41620803\n",
      "Iteration 3, loss = 0.88628693\n",
      "Iteration 4, loss = 0.58847209\n",
      "Iteration 5, loss = 0.54184174\n",
      "Iteration 6, loss = 0.50675027\n",
      "Iteration 7, loss = 0.44552734\n",
      "Iteration 8, loss = 0.39891806\n",
      "Iteration 9, loss = 0.37816527\n",
      "Iteration 10, loss = 0.37079106\n",
      "Iteration 11, loss = 0.36301799\n",
      "Iteration 12, loss = 0.35396050\n",
      "Iteration 13, loss = 0.34653022\n",
      "Iteration 14, loss = 0.33593549\n",
      "Iteration 15, loss = 0.33111334\n",
      "Iteration 16, loss = 0.33292547\n",
      "Iteration 17, loss = 0.31763285\n",
      "Iteration 18, loss = 0.31185221\n",
      "Iteration 19, loss = 0.31511505\n",
      "Iteration 20, loss = 0.29324030\n",
      "Iteration 21, loss = 0.29377801\n",
      "Iteration 22, loss = 0.33861812\n",
      "Iteration 23, loss = 0.36089954\n",
      "Iteration 24, loss = 0.33025125\n",
      "Iteration 25, loss = 0.30798006\n",
      "Iteration 26, loss = 0.32212759\n",
      "Iteration 27, loss = 0.33473458\n",
      "Iteration 28, loss = 0.30350412\n",
      "Iteration 29, loss = 0.30052794\n",
      "Iteration 30, loss = 0.32431661\n",
      "Iteration 31, loss = 0.32703449\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.48645490\n",
      "Iteration 2, loss = 1.31830369\n",
      "Iteration 3, loss = 0.77288844\n",
      "Iteration 4, loss = 0.60260731\n",
      "Iteration 5, loss = 0.55851297\n",
      "Iteration 6, loss = 0.47023431\n",
      "Iteration 7, loss = 0.41901605\n",
      "Iteration 8, loss = 0.41292680\n",
      "Iteration 9, loss = 0.42236681\n",
      "Iteration 10, loss = 0.40047833\n",
      "Iteration 11, loss = 0.36386545\n",
      "Iteration 12, loss = 0.34551487\n",
      "Iteration 13, loss = 0.33235190\n",
      "Iteration 14, loss = 0.33141985\n",
      "Iteration 15, loss = 0.33385506\n",
      "Iteration 16, loss = 0.31938484\n",
      "Iteration 17, loss = 0.29874486\n",
      "Iteration 18, loss = 0.29079527\n",
      "Iteration 19, loss = 0.28647465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 20, loss = 0.29325211\n",
      "Iteration 21, loss = 0.31141015\n",
      "Iteration 22, loss = 0.30974097\n",
      "Iteration 23, loss = 0.30501061\n",
      "Iteration 24, loss = 0.29715867\n",
      "Iteration 25, loss = 0.29776564\n",
      "Iteration 26, loss = 0.30336751\n",
      "Iteration 27, loss = 0.30855038\n",
      "Iteration 28, loss = 0.29881767\n",
      "Iteration 29, loss = 0.29045034\n",
      "Iteration 30, loss = 0.28167242\n",
      "Iteration 31, loss = 0.28226189\n",
      "Iteration 32, loss = 0.29031788\n",
      "Iteration 33, loss = 0.29399889\n",
      "Iteration 34, loss = 0.29460227\n",
      "Iteration 35, loss = 0.28576775\n",
      "Iteration 36, loss = 0.27841495\n",
      "Iteration 37, loss = 0.29174472\n",
      "Iteration 38, loss = 0.30089997\n",
      "Iteration 39, loss = 0.29227374\n",
      "Iteration 40, loss = 0.28265615\n",
      "Iteration 41, loss = 0.27779818\n",
      "Iteration 42, loss = 0.27462110\n",
      "Iteration 43, loss = 0.29568175\n",
      "Iteration 44, loss = 0.30340325\n",
      "Iteration 45, loss = 0.29616121\n",
      "Iteration 46, loss = 0.27543465\n",
      "Iteration 47, loss = 0.27928448\n",
      "Iteration 48, loss = 0.29384282\n",
      "Iteration 49, loss = 0.29248857\n",
      "Iteration 50, loss = 0.30822195\n",
      "Iteration 51, loss = 0.30404358\n",
      "Iteration 52, loss = 0.32449327\n",
      "Iteration 53, loss = 0.35284244\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.46851580\n",
      "Iteration 2, loss = 1.33831276\n",
      "Iteration 3, loss = 0.75353517\n",
      "Iteration 4, loss = 0.59204919\n",
      "Iteration 5, loss = 0.53561594\n",
      "Iteration 6, loss = 0.45933660\n",
      "Iteration 7, loss = 0.42426691\n",
      "Iteration 8, loss = 0.41548204\n",
      "Iteration 9, loss = 0.40972951\n",
      "Iteration 10, loss = 0.38279526\n",
      "Iteration 11, loss = 0.35461060\n",
      "Iteration 12, loss = 0.35050800\n",
      "Iteration 13, loss = 0.33752036\n",
      "Iteration 14, loss = 0.32772605\n",
      "Iteration 15, loss = 0.33622295\n",
      "Iteration 16, loss = 0.32999579\n",
      "Iteration 17, loss = 0.31042643\n",
      "Iteration 18, loss = 0.29488343\n",
      "Iteration 19, loss = 0.28634008\n",
      "Iteration 20, loss = 0.29200472\n",
      "Iteration 21, loss = 0.31171089\n",
      "Iteration 22, loss = 0.31385966\n",
      "Iteration 23, loss = 0.31264065\n",
      "Iteration 24, loss = 0.30251943\n",
      "Iteration 25, loss = 0.30042188\n",
      "Iteration 26, loss = 0.30639273\n",
      "Iteration 27, loss = 0.31196319\n",
      "Iteration 28, loss = 0.29173545\n",
      "Iteration 29, loss = 0.28907282\n",
      "Iteration 30, loss = 0.29723477\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.45974900\n",
      "Iteration 2, loss = 1.32865465\n",
      "Iteration 3, loss = 0.79484517\n",
      "Iteration 4, loss = 0.55139014\n",
      "Iteration 5, loss = 0.55394563\n",
      "Iteration 6, loss = 0.50118052\n",
      "Iteration 7, loss = 0.42587588\n",
      "Iteration 8, loss = 0.39232945\n",
      "Iteration 9, loss = 0.40049651\n",
      "Iteration 10, loss = 0.40206379\n",
      "Iteration 11, loss = 0.36474542\n",
      "Iteration 12, loss = 0.33737288\n",
      "Iteration 13, loss = 0.33631604\n",
      "Iteration 14, loss = 0.34118802\n",
      "Iteration 15, loss = 0.34948337\n",
      "Iteration 16, loss = 0.33859250\n",
      "Iteration 17, loss = 0.31471347\n",
      "Iteration 18, loss = 0.29233462\n",
      "Iteration 19, loss = 0.28151042\n",
      "Iteration 20, loss = 0.28767244\n",
      "Iteration 21, loss = 0.30203628\n",
      "Iteration 22, loss = 0.30657890\n",
      "Iteration 23, loss = 0.29918317\n",
      "Iteration 24, loss = 0.28531023\n",
      "Iteration 25, loss = 0.28416279\n",
      "Iteration 26, loss = 0.29068791\n",
      "Iteration 27, loss = 0.29255614\n",
      "Iteration 28, loss = 0.27620629\n",
      "Iteration 29, loss = 0.26888966\n",
      "Iteration 30, loss = 0.27299685\n",
      "Iteration 31, loss = 0.27251170\n",
      "Iteration 32, loss = 0.28082725\n",
      "Iteration 33, loss = 0.28957762\n",
      "Iteration 34, loss = 0.28365863\n",
      "Iteration 35, loss = 0.27365441\n",
      "Iteration 36, loss = 0.28037934\n",
      "Iteration 37, loss = 0.31245210\n",
      "Iteration 38, loss = 0.32896349\n",
      "Iteration 39, loss = 0.28110290\n",
      "Iteration 40, loss = 0.28672754\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.44874143\n",
      "Iteration 2, loss = 1.35257719\n",
      "Iteration 3, loss = 0.78658606\n",
      "Iteration 4, loss = 0.55466263\n",
      "Iteration 5, loss = 0.57758766\n",
      "Iteration 6, loss = 0.51173627\n",
      "Iteration 7, loss = 0.43803750\n",
      "Iteration 8, loss = 0.40865772\n",
      "Iteration 9, loss = 0.40941486\n",
      "Iteration 10, loss = 0.40138337\n",
      "Iteration 11, loss = 0.36611890\n",
      "Iteration 12, loss = 0.34425452\n",
      "Iteration 13, loss = 0.33774633\n",
      "Iteration 14, loss = 0.33890357\n",
      "Iteration 15, loss = 0.34179233\n",
      "Iteration 16, loss = 0.33617885\n",
      "Iteration 17, loss = 0.31787913\n",
      "Iteration 18, loss = 0.30238871\n",
      "Iteration 19, loss = 0.29346045\n",
      "Iteration 20, loss = 0.29463544\n",
      "Iteration 21, loss = 0.30555484\n",
      "Iteration 22, loss = 0.30839584\n",
      "Iteration 23, loss = 0.30697982\n",
      "Iteration 24, loss = 0.29936340\n",
      "Iteration 25, loss = 0.29857536\n",
      "Iteration 26, loss = 0.29970015\n",
      "Iteration 27, loss = 0.28765195\n",
      "Iteration 28, loss = 0.27740525\n",
      "Iteration 29, loss = 0.27555374\n",
      "Iteration 30, loss = 0.28890816\n",
      "Iteration 31, loss = 0.28243887\n",
      "Iteration 32, loss = 0.27560159\n",
      "Iteration 33, loss = 0.28027336\n",
      "Iteration 34, loss = 0.29722014\n",
      "Iteration 35, loss = 0.30715732\n",
      "Iteration 36, loss = 0.29291129\n",
      "Iteration 37, loss = 0.28050278\n",
      "Iteration 38, loss = 0.31267931\n",
      "Iteration 39, loss = 0.28770343\n",
      "Iteration 40, loss = 0.29777177\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.45143100\n",
      "Iteration 2, loss = 1.33054180\n",
      "Iteration 3, loss = 0.76706974\n",
      "Iteration 4, loss = 0.54830609\n",
      "Iteration 5, loss = 0.56847383\n",
      "Iteration 6, loss = 0.49289294\n",
      "Iteration 7, loss = 0.41404862\n",
      "Iteration 8, loss = 0.38634687\n",
      "Iteration 9, loss = 0.39062409\n",
      "Iteration 10, loss = 0.38807713\n",
      "Iteration 11, loss = 0.35905393\n",
      "Iteration 12, loss = 0.33895086\n",
      "Iteration 13, loss = 0.33579129\n",
      "Iteration 14, loss = 0.34155776\n",
      "Iteration 15, loss = 0.34814233\n",
      "Iteration 16, loss = 0.33693665\n",
      "Iteration 17, loss = 0.31290266\n",
      "Iteration 18, loss = 0.29455936\n",
      "Iteration 19, loss = 0.29605704\n",
      "Iteration 20, loss = 0.29547817\n",
      "Iteration 21, loss = 0.29598983\n",
      "Iteration 22, loss = 0.30379750\n",
      "Iteration 23, loss = 0.30500386\n",
      "Iteration 24, loss = 0.30115611\n",
      "Iteration 25, loss = 0.30440322\n",
      "Iteration 26, loss = 0.29863303\n",
      "Iteration 27, loss = 0.29106269\n",
      "Iteration 28, loss = 0.28490890\n",
      "Iteration 29, loss = 0.29084321\n",
      "Iteration 30, loss = 0.29752636\n",
      "Iteration 31, loss = 0.30189983\n",
      "Iteration 32, loss = 0.28577738\n",
      "Iteration 33, loss = 0.27567806\n",
      "Iteration 34, loss = 0.28789314\n",
      "Iteration 35, loss = 0.29226028\n",
      "Iteration 36, loss = 0.28989696\n",
      "Iteration 37, loss = 0.26802761\n",
      "Iteration 38, loss = 0.29132499\n",
      "Iteration 39, loss = 0.29807182\n",
      "Iteration 40, loss = 0.29743442\n",
      "Iteration 41, loss = 0.29357453\n",
      "Iteration 42, loss = 0.28443764\n",
      "Iteration 43, loss = 0.28528473\n",
      "Iteration 44, loss = 0.28889697\n",
      "Iteration 45, loss = 0.29233294\n",
      "Iteration 46, loss = 0.29317957\n",
      "Iteration 47, loss = 0.28558747\n",
      "Iteration 48, loss = 0.26830818\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.44019604\n",
      "Iteration 2, loss = 1.34853719\n",
      "Iteration 3, loss = 0.76095702\n",
      "Iteration 4, loss = 0.55970473\n",
      "Iteration 5, loss = 0.57306825\n",
      "Iteration 6, loss = 0.48224275\n",
      "Iteration 7, loss = 0.42684147\n",
      "Iteration 8, loss = 0.40721556\n",
      "Iteration 9, loss = 0.40573087\n",
      "Iteration 10, loss = 0.39662368\n",
      "Iteration 11, loss = 0.36152404\n",
      "Iteration 12, loss = 0.34774640\n",
      "Iteration 13, loss = 0.35137886\n",
      "Iteration 14, loss = 0.35464633\n",
      "Iteration 15, loss = 0.35910244\n",
      "Iteration 16, loss = 0.34396610\n",
      "Iteration 17, loss = 0.31901217\n",
      "Iteration 18, loss = 0.30902714\n",
      "Iteration 19, loss = 0.31848281\n",
      "Iteration 20, loss = 0.30366196\n",
      "Iteration 21, loss = 0.28721775\n",
      "Iteration 22, loss = 0.30293525\n",
      "Iteration 23, loss = 0.30797089\n",
      "Iteration 24, loss = 0.30802254\n",
      "Iteration 25, loss = 0.30255619\n",
      "Iteration 26, loss = 0.28439041\n",
      "Iteration 27, loss = 0.29775752\n",
      "Iteration 28, loss = 0.30158369\n",
      "Iteration 29, loss = 0.29582599\n",
      "Iteration 30, loss = 0.29449262\n",
      "Iteration 31, loss = 0.30966513\n",
      "Iteration 32, loss = 0.28880988\n",
      "Iteration 33, loss = 0.28130924\n",
      "Iteration 34, loss = 0.29591659\n",
      "Iteration 35, loss = 0.29334321\n",
      "Iteration 36, loss = 0.29114734\n",
      "Iteration 37, loss = 0.27380312\n",
      "Iteration 38, loss = 0.29855844\n",
      "Iteration 39, loss = 0.29897454\n",
      "Iteration 40, loss = 0.31753279\n",
      "Iteration 41, loss = 0.28366727\n",
      "Iteration 42, loss = 0.31312791\n",
      "Iteration 43, loss = 0.30301415\n",
      "Iteration 44, loss = 0.30400789\n",
      "Iteration 45, loss = 0.31643182\n",
      "Iteration 46, loss = 0.30051417\n",
      "Iteration 47, loss = 0.28545477\n",
      "Iteration 48, loss = 0.27931486\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.44144350\n",
      "Iteration 2, loss = 1.33259537\n",
      "Iteration 3, loss = 0.78815543\n",
      "Iteration 4, loss = 0.58854120\n",
      "Iteration 5, loss = 0.60050751\n",
      "Iteration 6, loss = 0.51031227\n",
      "Iteration 7, loss = 0.44228662\n",
      "Iteration 8, loss = 0.41452363\n",
      "Iteration 9, loss = 0.41191850\n",
      "Iteration 10, loss = 0.41045830\n",
      "Iteration 11, loss = 0.37967217\n",
      "Iteration 12, loss = 0.37599067\n",
      "Iteration 13, loss = 0.37546634\n",
      "Iteration 14, loss = 0.36602848\n",
      "Iteration 15, loss = 0.38849011\n",
      "Iteration 16, loss = 0.39872027\n",
      "Iteration 17, loss = 0.36571953\n",
      "Iteration 18, loss = 0.33889306\n",
      "Iteration 19, loss = 0.34228110\n",
      "Iteration 20, loss = 0.33295841\n",
      "Iteration 21, loss = 0.31487399\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 22, loss = 0.31391149\n",
      "Iteration 23, loss = 0.31933619\n",
      "Iteration 24, loss = 0.32798712\n",
      "Iteration 25, loss = 0.33181353\n",
      "Iteration 26, loss = 0.31829287\n",
      "Iteration 27, loss = 0.31585352\n",
      "Iteration 28, loss = 0.30622867\n",
      "Iteration 29, loss = 0.29773300\n",
      "Iteration 30, loss = 0.30308646\n",
      "Iteration 31, loss = 0.33470343\n",
      "Iteration 32, loss = 0.31964163\n",
      "Iteration 33, loss = 0.30072306\n",
      "Iteration 34, loss = 0.30731395\n",
      "Iteration 35, loss = 0.30318491\n",
      "Iteration 36, loss = 0.30661080\n",
      "Iteration 37, loss = 0.29841577\n",
      "Iteration 38, loss = 0.31786749\n",
      "Iteration 39, loss = 0.30529828\n",
      "Iteration 40, loss = 0.32253210\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.44296221\n",
      "Iteration 2, loss = 1.32423199\n",
      "Iteration 3, loss = 0.76405077\n",
      "Iteration 4, loss = 0.55829133\n",
      "Iteration 5, loss = 0.59209757\n",
      "Iteration 6, loss = 0.51603372\n",
      "Iteration 7, loss = 0.44538061\n",
      "Iteration 8, loss = 0.41141737\n",
      "Iteration 9, loss = 0.40691351\n",
      "Iteration 10, loss = 0.39757270\n",
      "Iteration 11, loss = 0.38219446\n",
      "Iteration 12, loss = 0.37553333\n",
      "Iteration 13, loss = 0.35515106\n",
      "Iteration 14, loss = 0.34289027\n",
      "Iteration 15, loss = 0.36827532\n",
      "Iteration 16, loss = 0.37735735\n",
      "Iteration 17, loss = 0.34736209\n",
      "Iteration 18, loss = 0.31857105\n",
      "Iteration 19, loss = 0.32328750\n",
      "Iteration 20, loss = 0.31279849\n",
      "Iteration 21, loss = 0.30538459\n",
      "Iteration 22, loss = 0.30931204\n",
      "Iteration 23, loss = 0.30436263\n",
      "Iteration 24, loss = 0.30100309\n",
      "Iteration 25, loss = 0.30117603\n",
      "Iteration 26, loss = 0.29161923\n",
      "Iteration 27, loss = 0.29600218\n",
      "Iteration 28, loss = 0.29497147\n",
      "Iteration 29, loss = 0.29436350\n",
      "Iteration 30, loss = 0.29905127\n",
      "Iteration 31, loss = 0.32309624\n",
      "Iteration 32, loss = 0.30482196\n",
      "Iteration 33, loss = 0.28868245\n",
      "Iteration 34, loss = 0.30781557\n",
      "Iteration 35, loss = 0.30737220\n",
      "Iteration 36, loss = 0.30802537\n",
      "Iteration 37, loss = 0.28909098\n",
      "Iteration 38, loss = 0.30072874\n",
      "Iteration 39, loss = 0.29458461\n",
      "Iteration 40, loss = 0.31491319\n",
      "Iteration 41, loss = 0.28866436\n",
      "Iteration 42, loss = 0.30706697\n",
      "Iteration 43, loss = 0.29552417\n",
      "Iteration 44, loss = 0.29504309\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.43864928\n",
      "Iteration 2, loss = 1.39273672\n",
      "Iteration 3, loss = 0.80283683\n",
      "Iteration 4, loss = 0.56813488\n",
      "Iteration 5, loss = 0.60123068\n",
      "Iteration 6, loss = 0.52097140\n",
      "Iteration 7, loss = 0.43585902\n",
      "Iteration 8, loss = 0.39327110\n",
      "Iteration 9, loss = 0.38286339\n",
      "Iteration 10, loss = 0.37606351\n",
      "Iteration 11, loss = 0.36423892\n",
      "Iteration 12, loss = 0.35482178\n",
      "Iteration 13, loss = 0.33859422\n",
      "Iteration 14, loss = 0.32680749\n",
      "Iteration 15, loss = 0.35379368\n",
      "Iteration 16, loss = 0.37293576\n",
      "Iteration 17, loss = 0.34874568\n",
      "Iteration 18, loss = 0.31406095\n",
      "Iteration 19, loss = 0.31263321\n",
      "Iteration 20, loss = 0.31382189\n",
      "Iteration 21, loss = 0.30352744\n",
      "Iteration 22, loss = 0.30433149\n",
      "Iteration 23, loss = 0.29055412\n",
      "Iteration 24, loss = 0.29981752\n",
      "Iteration 25, loss = 0.31045693\n",
      "Iteration 26, loss = 0.29211571\n",
      "Iteration 27, loss = 0.28232838\n",
      "Iteration 28, loss = 0.28713014\n",
      "Iteration 29, loss = 0.29070871\n",
      "Iteration 30, loss = 0.28270041\n",
      "Iteration 31, loss = 0.29440215\n",
      "Iteration 32, loss = 0.30097700\n",
      "Iteration 33, loss = 0.29434867\n",
      "Iteration 34, loss = 0.31408438\n",
      "Iteration 35, loss = 0.29365991\n",
      "Iteration 36, loss = 0.30488102\n",
      "Iteration 37, loss = 0.31686338\n",
      "Iteration 38, loss = 0.34638148\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.29152860\n",
      "Iteration 2, loss = 1.52689734\n",
      "Iteration 3, loss = 0.86367582\n",
      "Iteration 4, loss = 0.67593027\n",
      "Iteration 5, loss = 0.67700220\n",
      "Iteration 6, loss = 0.57282656\n",
      "Iteration 7, loss = 0.48680418\n",
      "Iteration 8, loss = 0.46136588\n",
      "Iteration 9, loss = 0.45549491\n",
      "Iteration 10, loss = 0.44511524\n",
      "Iteration 11, loss = 0.40376878\n",
      "Iteration 12, loss = 0.36571577\n",
      "Iteration 13, loss = 0.35179175\n",
      "Iteration 14, loss = 0.35188362\n",
      "Iteration 15, loss = 0.33974760\n",
      "Iteration 16, loss = 0.31944236\n",
      "Iteration 17, loss = 0.31376876\n",
      "Iteration 18, loss = 0.31267816\n",
      "Iteration 19, loss = 0.32296569\n",
      "Iteration 20, loss = 0.31455216\n",
      "Iteration 21, loss = 0.30727646\n",
      "Iteration 22, loss = 0.30151978\n",
      "Iteration 23, loss = 0.31073745\n",
      "Iteration 24, loss = 0.32214654\n",
      "Iteration 25, loss = 0.30309746\n",
      "Iteration 26, loss = 0.31538354\n",
      "Iteration 27, loss = 0.33882733\n",
      "Iteration 28, loss = 0.32202409\n",
      "Iteration 29, loss = 0.29345720\n",
      "Iteration 30, loss = 0.27572600\n",
      "Iteration 31, loss = 0.28242644\n",
      "Iteration 32, loss = 0.30983126\n",
      "Iteration 33, loss = 0.30455914\n",
      "Iteration 34, loss = 0.32486661\n",
      "Iteration 35, loss = 0.32137036\n",
      "Iteration 36, loss = 0.31061348\n",
      "Iteration 37, loss = 0.31734710\n",
      "Iteration 38, loss = 0.32852585\n",
      "Iteration 39, loss = 0.31244874\n",
      "Iteration 40, loss = 0.31437980\n",
      "Iteration 41, loss = 0.33295248\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.28932256\n",
      "Iteration 2, loss = 1.52103743\n",
      "Iteration 3, loss = 0.87632429\n",
      "Iteration 4, loss = 0.64394600\n",
      "Iteration 5, loss = 0.66734823\n",
      "Iteration 6, loss = 0.57706644\n",
      "Iteration 7, loss = 0.47708928\n",
      "Iteration 8, loss = 0.44440313\n",
      "Iteration 9, loss = 0.43792301\n",
      "Iteration 10, loss = 0.41025743\n",
      "Iteration 11, loss = 0.37516156\n",
      "Iteration 12, loss = 0.35469448\n",
      "Iteration 13, loss = 0.34817522\n",
      "Iteration 14, loss = 0.34663860\n",
      "Iteration 15, loss = 0.33145094\n",
      "Iteration 16, loss = 0.31566040\n",
      "Iteration 17, loss = 0.30980810\n",
      "Iteration 18, loss = 0.30395788\n",
      "Iteration 19, loss = 0.31168909\n",
      "Iteration 20, loss = 0.32121156\n",
      "Iteration 21, loss = 0.31599561\n",
      "Iteration 22, loss = 0.31445906\n",
      "Iteration 23, loss = 0.31571229\n",
      "Iteration 24, loss = 0.33173571\n",
      "Iteration 25, loss = 0.31579930\n",
      "Iteration 26, loss = 0.30143606\n",
      "Iteration 27, loss = 0.30517997\n",
      "Iteration 28, loss = 0.31325240\n",
      "Iteration 29, loss = 0.30527571\n",
      "Iteration 30, loss = 0.31062921\n",
      "Iteration 31, loss = 0.35271692\n",
      "Iteration 32, loss = 0.33117190\n",
      "Iteration 33, loss = 0.31775863\n",
      "Iteration 34, loss = 0.35568983\n",
      "Iteration 35, loss = 0.32986530\n",
      "Iteration 36, loss = 0.29359069\n",
      "Iteration 37, loss = 0.29413295\n",
      "Iteration 38, loss = 0.31888739\n",
      "Iteration 39, loss = 0.31767785\n",
      "Iteration 40, loss = 0.32453696\n",
      "Iteration 41, loss = 0.31587033\n",
      "Iteration 42, loss = 0.33370541\n",
      "Iteration 43, loss = 0.33894396\n",
      "Iteration 44, loss = 0.31847440\n",
      "Iteration 45, loss = 0.31288682\n",
      "Iteration 46, loss = 0.30741771\n",
      "Iteration 47, loss = 0.29789728\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.29006914\n",
      "Iteration 2, loss = 1.52810092\n",
      "Iteration 3, loss = 0.81406471\n",
      "Iteration 4, loss = 0.66147790\n",
      "Iteration 5, loss = 0.65240828\n",
      "Iteration 6, loss = 0.51903594\n",
      "Iteration 7, loss = 0.43617939\n",
      "Iteration 8, loss = 0.42014140\n",
      "Iteration 9, loss = 0.41943326\n",
      "Iteration 10, loss = 0.39528510\n",
      "Iteration 11, loss = 0.36820293\n",
      "Iteration 12, loss = 0.35334252\n",
      "Iteration 13, loss = 0.35090716\n",
      "Iteration 14, loss = 0.34289839\n",
      "Iteration 15, loss = 0.33060753\n",
      "Iteration 16, loss = 0.31113767\n",
      "Iteration 17, loss = 0.30226717\n",
      "Iteration 18, loss = 0.30306436\n",
      "Iteration 19, loss = 0.31857974\n",
      "Iteration 20, loss = 0.33120801\n",
      "Iteration 21, loss = 0.32251280\n",
      "Iteration 22, loss = 0.30769889\n",
      "Iteration 23, loss = 0.31303913\n",
      "Iteration 24, loss = 0.33028389\n",
      "Iteration 25, loss = 0.32612216\n",
      "Iteration 26, loss = 0.32747782\n",
      "Iteration 27, loss = 0.32383377\n",
      "Iteration 28, loss = 0.31474751\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.29018328\n",
      "Iteration 2, loss = 1.52928086\n",
      "Iteration 3, loss = 0.88516567\n",
      "Iteration 4, loss = 0.62940015\n",
      "Iteration 5, loss = 0.62552941\n",
      "Iteration 6, loss = 0.56491713\n",
      "Iteration 7, loss = 0.47968207\n",
      "Iteration 8, loss = 0.43364422\n",
      "Iteration 9, loss = 0.40904244\n",
      "Iteration 10, loss = 0.38973493\n",
      "Iteration 11, loss = 0.37320074\n",
      "Iteration 12, loss = 0.36101509\n",
      "Iteration 13, loss = 0.35952408\n",
      "Iteration 14, loss = 0.34575768\n",
      "Iteration 15, loss = 0.33045887\n",
      "Iteration 16, loss = 0.30840367\n",
      "Iteration 17, loss = 0.29795682\n",
      "Iteration 18, loss = 0.29472621\n",
      "Iteration 19, loss = 0.31055990\n",
      "Iteration 20, loss = 0.32913746\n",
      "Iteration 21, loss = 0.32223919\n",
      "Iteration 22, loss = 0.30348785\n",
      "Iteration 23, loss = 0.29790737\n",
      "Iteration 24, loss = 0.31373533\n",
      "Iteration 25, loss = 0.31270900\n",
      "Iteration 26, loss = 0.32142564\n",
      "Iteration 27, loss = 0.31744196\n",
      "Iteration 28, loss = 0.29794760\n",
      "Iteration 29, loss = 0.28224302\n",
      "Iteration 30, loss = 0.30844673\n",
      "Iteration 31, loss = 0.33861166\n",
      "Iteration 32, loss = 0.29614086\n",
      "Iteration 33, loss = 0.31645914\n",
      "Iteration 34, loss = 0.36190329\n",
      "Iteration 35, loss = 0.31756723\n",
      "Iteration 36, loss = 0.28777450\n",
      "Iteration 37, loss = 0.30162824\n",
      "Iteration 38, loss = 0.33791113\n",
      "Iteration 39, loss = 0.31092703\n",
      "Iteration 40, loss = 0.31884946\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.28787355\n",
      "Iteration 2, loss = 1.54633522\n",
      "Iteration 3, loss = 0.84504186\n",
      "Iteration 4, loss = 0.67068233\n",
      "Iteration 5, loss = 0.66935008\n",
      "Iteration 6, loss = 0.55053112\n",
      "Iteration 7, loss = 0.46408714\n",
      "Iteration 8, loss = 0.44138885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9, loss = 0.42060687\n",
      "Iteration 10, loss = 0.39061748\n",
      "Iteration 11, loss = 0.37578151\n",
      "Iteration 12, loss = 0.36457728\n",
      "Iteration 13, loss = 0.35734577\n",
      "Iteration 14, loss = 0.34304296\n",
      "Iteration 15, loss = 0.33530017\n",
      "Iteration 16, loss = 0.30933214\n",
      "Iteration 17, loss = 0.30261940\n",
      "Iteration 18, loss = 0.30858053\n",
      "Iteration 19, loss = 0.32823997\n",
      "Iteration 20, loss = 0.33634280\n",
      "Iteration 21, loss = 0.31776313\n",
      "Iteration 22, loss = 0.30003536\n",
      "Iteration 23, loss = 0.30918105\n",
      "Iteration 24, loss = 0.32315676\n",
      "Iteration 25, loss = 0.32818041\n",
      "Iteration 26, loss = 0.33892213\n",
      "Iteration 27, loss = 0.32072366\n",
      "Iteration 28, loss = 0.30485485\n",
      "Iteration 29, loss = 0.30564646\n",
      "Iteration 30, loss = 0.30931160\n",
      "Iteration 31, loss = 0.31328380\n",
      "Iteration 32, loss = 0.30185434\n",
      "Iteration 33, loss = 0.32753025\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.28838550\n",
      "Iteration 2, loss = 1.51500796\n",
      "Iteration 3, loss = 0.83811329\n",
      "Iteration 4, loss = 0.64981235\n",
      "Iteration 5, loss = 0.64630838\n",
      "Iteration 6, loss = 0.54533072\n",
      "Iteration 7, loss = 0.46719375\n",
      "Iteration 8, loss = 0.43152688\n",
      "Iteration 9, loss = 0.39480292\n",
      "Iteration 10, loss = 0.36690494\n",
      "Iteration 11, loss = 0.36076424\n",
      "Iteration 12, loss = 0.34681798\n",
      "Iteration 13, loss = 0.33590126\n",
      "Iteration 14, loss = 0.33272481\n",
      "Iteration 15, loss = 0.32413844\n",
      "Iteration 16, loss = 0.30254979\n",
      "Iteration 17, loss = 0.29984846\n",
      "Iteration 18, loss = 0.30996406\n",
      "Iteration 19, loss = 0.32483994\n",
      "Iteration 20, loss = 0.32481900\n",
      "Iteration 21, loss = 0.30751617\n",
      "Iteration 22, loss = 0.29381619\n",
      "Iteration 23, loss = 0.29688355\n",
      "Iteration 24, loss = 0.30440510\n",
      "Iteration 25, loss = 0.31538551\n",
      "Iteration 26, loss = 0.30567591\n",
      "Iteration 27, loss = 0.28576347\n",
      "Iteration 28, loss = 0.28551039\n",
      "Iteration 29, loss = 0.28277455\n",
      "Iteration 30, loss = 0.28995931\n",
      "Iteration 31, loss = 0.30729579\n",
      "Iteration 32, loss = 0.30731766\n",
      "Iteration 33, loss = 0.31333039\n",
      "Iteration 34, loss = 0.32152048\n",
      "Iteration 35, loss = 0.28646729\n",
      "Iteration 36, loss = 0.27905509\n",
      "Iteration 37, loss = 0.30448282\n",
      "Iteration 38, loss = 0.32706078\n",
      "Iteration 39, loss = 0.30248592\n",
      "Iteration 40, loss = 0.27829252\n",
      "Iteration 41, loss = 0.29568717\n",
      "Iteration 42, loss = 0.32634924\n",
      "Iteration 43, loss = 0.32340602\n",
      "Iteration 44, loss = 0.31147102\n",
      "Iteration 45, loss = 0.27890307\n",
      "Iteration 46, loss = 0.31767834\n",
      "Iteration 47, loss = 0.35148573\n",
      "Iteration 48, loss = 0.33027700\n",
      "Iteration 49, loss = 0.31054209\n",
      "Iteration 50, loss = 0.31815367\n",
      "Iteration 51, loss = 0.30874464\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.29476863\n",
      "Iteration 2, loss = 1.52563247\n",
      "Iteration 3, loss = 0.79610111\n",
      "Iteration 4, loss = 0.68962816\n",
      "Iteration 5, loss = 0.62726470\n",
      "Iteration 6, loss = 0.48949076\n",
      "Iteration 7, loss = 0.43913644\n",
      "Iteration 8, loss = 0.43161512\n",
      "Iteration 9, loss = 0.40417550\n",
      "Iteration 10, loss = 0.36587538\n",
      "Iteration 11, loss = 0.35327995\n",
      "Iteration 12, loss = 0.34542224\n",
      "Iteration 13, loss = 0.34582418\n",
      "Iteration 14, loss = 0.33645809\n",
      "Iteration 15, loss = 0.31205888\n",
      "Iteration 16, loss = 0.29514327\n",
      "Iteration 17, loss = 0.29403934\n",
      "Iteration 18, loss = 0.29934569\n",
      "Iteration 19, loss = 0.31016231\n",
      "Iteration 20, loss = 0.30863125\n",
      "Iteration 21, loss = 0.29231159\n",
      "Iteration 22, loss = 0.29684389\n",
      "Iteration 23, loss = 0.30584814\n",
      "Iteration 24, loss = 0.31234714\n",
      "Iteration 25, loss = 0.30752436\n",
      "Iteration 26, loss = 0.28835413\n",
      "Iteration 27, loss = 0.27870124\n",
      "Iteration 28, loss = 0.27930866\n",
      "Iteration 29, loss = 0.26616736\n",
      "Iteration 30, loss = 0.27675981\n",
      "Iteration 31, loss = 0.31361380\n",
      "Iteration 32, loss = 0.30230288\n",
      "Iteration 33, loss = 0.30159004\n",
      "Iteration 34, loss = 0.32132037\n",
      "Iteration 35, loss = 0.29072324\n",
      "Iteration 36, loss = 0.28110772\n",
      "Iteration 37, loss = 0.29677394\n",
      "Iteration 38, loss = 0.30842463\n",
      "Iteration 39, loss = 0.28211651\n",
      "Iteration 40, loss = 0.27739347\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.29292372\n",
      "Iteration 2, loss = 1.55847229\n",
      "Iteration 3, loss = 0.83382013\n",
      "Iteration 4, loss = 0.70605736\n",
      "Iteration 5, loss = 0.65262990\n",
      "Iteration 6, loss = 0.52144177\n",
      "Iteration 7, loss = 0.45159840\n",
      "Iteration 8, loss = 0.43987278\n",
      "Iteration 9, loss = 0.42942445\n",
      "Iteration 10, loss = 0.39153847\n",
      "Iteration 11, loss = 0.36770514\n",
      "Iteration 12, loss = 0.35724240\n",
      "Iteration 13, loss = 0.36425215\n",
      "Iteration 14, loss = 0.35822936\n",
      "Iteration 15, loss = 0.33138570\n",
      "Iteration 16, loss = 0.31011243\n",
      "Iteration 17, loss = 0.30604778\n",
      "Iteration 18, loss = 0.31735129\n",
      "Iteration 19, loss = 0.33287210\n",
      "Iteration 20, loss = 0.32951667\n",
      "Iteration 21, loss = 0.30663649\n",
      "Iteration 22, loss = 0.30956318\n",
      "Iteration 23, loss = 0.32424661\n",
      "Iteration 24, loss = 0.32017000\n",
      "Iteration 25, loss = 0.32088846\n",
      "Iteration 26, loss = 0.31420289\n",
      "Iteration 27, loss = 0.30929099\n",
      "Iteration 28, loss = 0.30032345\n",
      "Iteration 29, loss = 0.27893533\n",
      "Iteration 30, loss = 0.28756315\n",
      "Iteration 31, loss = 0.32719213\n",
      "Iteration 32, loss = 0.33240905\n",
      "Iteration 33, loss = 0.34192268\n",
      "Iteration 34, loss = 0.35102047\n",
      "Iteration 35, loss = 0.30569489\n",
      "Iteration 36, loss = 0.30695607\n",
      "Iteration 37, loss = 0.34625174\n",
      "Iteration 38, loss = 0.35249151\n",
      "Iteration 39, loss = 0.31675927\n",
      "Iteration 40, loss = 0.30233333\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.28739529\n",
      "Iteration 2, loss = 1.50556655\n",
      "Iteration 3, loss = 0.80684441\n",
      "Iteration 4, loss = 0.67185536\n",
      "Iteration 5, loss = 0.62775627\n",
      "Iteration 6, loss = 0.51015092\n",
      "Iteration 7, loss = 0.44017668\n",
      "Iteration 8, loss = 0.42782935\n",
      "Iteration 9, loss = 0.42065215\n",
      "Iteration 10, loss = 0.38452111\n",
      "Iteration 11, loss = 0.36177153\n",
      "Iteration 12, loss = 0.34805627\n",
      "Iteration 13, loss = 0.35048464\n",
      "Iteration 14, loss = 0.34566277\n",
      "Iteration 15, loss = 0.32340614\n",
      "Iteration 16, loss = 0.30412345\n",
      "Iteration 17, loss = 0.29753559\n",
      "Iteration 18, loss = 0.30748935\n",
      "Iteration 19, loss = 0.32261232\n",
      "Iteration 20, loss = 0.31809482\n",
      "Iteration 21, loss = 0.29899157\n",
      "Iteration 22, loss = 0.30198118\n",
      "Iteration 23, loss = 0.30982121\n",
      "Iteration 24, loss = 0.30868984\n",
      "Iteration 25, loss = 0.31157067\n",
      "Iteration 26, loss = 0.30570598\n",
      "Iteration 27, loss = 0.30584336\n",
      "Iteration 28, loss = 0.28962655\n",
      "Iteration 29, loss = 0.27794110\n",
      "Iteration 30, loss = 0.29902179\n",
      "Iteration 31, loss = 0.32180273\n",
      "Iteration 32, loss = 0.32283579\n",
      "Iteration 33, loss = 0.31785120\n",
      "Iteration 34, loss = 0.29939412\n",
      "Iteration 35, loss = 0.28693585\n",
      "Iteration 36, loss = 0.30005107\n",
      "Iteration 37, loss = 0.31165101\n",
      "Iteration 38, loss = 0.32046192\n",
      "Iteration 39, loss = 0.30798243\n",
      "Iteration 40, loss = 0.30158466\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.29305354\n",
      "Iteration 2, loss = 1.50163960\n",
      "Iteration 3, loss = 0.79271996\n",
      "Iteration 4, loss = 0.65972155\n",
      "Iteration 5, loss = 0.61478938\n",
      "Iteration 6, loss = 0.50191201\n",
      "Iteration 7, loss = 0.42832609\n",
      "Iteration 8, loss = 0.40681489\n",
      "Iteration 9, loss = 0.40429717\n",
      "Iteration 10, loss = 0.37561418\n",
      "Iteration 11, loss = 0.34884771\n",
      "Iteration 12, loss = 0.33963816\n",
      "Iteration 13, loss = 0.34782198\n",
      "Iteration 14, loss = 0.33767275\n",
      "Iteration 15, loss = 0.30941205\n",
      "Iteration 16, loss = 0.29254557\n",
      "Iteration 17, loss = 0.29753688\n",
      "Iteration 18, loss = 0.30652433\n",
      "Iteration 19, loss = 0.31431526\n",
      "Iteration 20, loss = 0.30316615\n",
      "Iteration 21, loss = 0.28495540\n",
      "Iteration 22, loss = 0.28534555\n",
      "Iteration 23, loss = 0.28388767\n",
      "Iteration 24, loss = 0.29485335\n",
      "Iteration 25, loss = 0.32501114\n",
      "Iteration 26, loss = 0.31700331\n",
      "Iteration 27, loss = 0.30437437\n",
      "Iteration 28, loss = 0.31911280\n",
      "Iteration 29, loss = 0.31521645\n",
      "Iteration 30, loss = 0.30545870\n",
      "Iteration 31, loss = 0.29848793\n",
      "Iteration 32, loss = 0.32135230\n",
      "Iteration 33, loss = 0.34189157\n",
      "Iteration 34, loss = 0.30955737\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.46823684\n",
      "Iteration 2, loss = 1.52870597\n",
      "Iteration 3, loss = 0.85368704\n",
      "Iteration 4, loss = 0.65500511\n",
      "Iteration 5, loss = 0.65020493\n",
      "Iteration 6, loss = 0.54342442\n",
      "Iteration 7, loss = 0.45108757\n",
      "Iteration 8, loss = 0.43084380\n",
      "Iteration 9, loss = 0.43459907\n",
      "Iteration 10, loss = 0.42445359\n",
      "Iteration 11, loss = 0.39309201\n",
      "Iteration 12, loss = 0.35845485\n",
      "Iteration 13, loss = 0.34941549\n",
      "Iteration 14, loss = 0.35614573\n",
      "Iteration 15, loss = 0.37054202\n",
      "Iteration 16, loss = 0.35273290\n",
      "Iteration 17, loss = 0.33858408\n",
      "Iteration 18, loss = 0.33609771\n",
      "Iteration 19, loss = 0.32353766\n",
      "Iteration 20, loss = 0.30368962\n",
      "Iteration 21, loss = 0.29898634\n",
      "Iteration 22, loss = 0.28849435\n",
      "Iteration 23, loss = 0.29755278\n",
      "Iteration 24, loss = 0.31351141\n",
      "Iteration 25, loss = 0.32320131\n",
      "Iteration 26, loss = 0.32366359\n",
      "Iteration 27, loss = 0.31004052\n",
      "Iteration 28, loss = 0.29814616\n",
      "Iteration 29, loss = 0.31016291\n",
      "Iteration 30, loss = 0.33559424\n",
      "Iteration 31, loss = 0.33153454\n",
      "Iteration 32, loss = 0.31488515\n",
      "Iteration 33, loss = 0.31712288\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.45526536\n",
      "Iteration 2, loss = 1.40969513\n",
      "Iteration 3, loss = 0.79684996\n",
      "Iteration 4, loss = 0.62684813\n",
      "Iteration 5, loss = 0.60007899\n",
      "Iteration 6, loss = 0.49940155\n",
      "Iteration 7, loss = 0.44204414\n",
      "Iteration 8, loss = 0.43126406\n",
      "Iteration 9, loss = 0.43101306\n",
      "Iteration 10, loss = 0.40086333\n",
      "Iteration 11, loss = 0.37478955\n",
      "Iteration 12, loss = 0.35282309\n",
      "Iteration 13, loss = 0.33426973\n",
      "Iteration 14, loss = 0.32816436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15, loss = 0.33095568\n",
      "Iteration 16, loss = 0.32785446\n",
      "Iteration 17, loss = 0.32581360\n",
      "Iteration 18, loss = 0.34716529\n",
      "Iteration 19, loss = 0.34733595\n",
      "Iteration 20, loss = 0.31015067\n",
      "Iteration 21, loss = 0.29516891\n",
      "Iteration 22, loss = 0.30409165\n",
      "Iteration 23, loss = 0.30718394\n",
      "Iteration 24, loss = 0.30751115\n",
      "Iteration 25, loss = 0.30192042\n",
      "Iteration 26, loss = 0.30710143\n",
      "Iteration 27, loss = 0.31729788\n",
      "Iteration 28, loss = 0.31330451\n",
      "Iteration 29, loss = 0.30444617\n",
      "Iteration 30, loss = 0.30856174\n",
      "Iteration 31, loss = 0.29709043\n",
      "Iteration 32, loss = 0.31020416\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.46233255\n",
      "Iteration 2, loss = 1.44151626\n",
      "Iteration 3, loss = 0.80014787\n",
      "Iteration 4, loss = 0.62963662\n",
      "Iteration 5, loss = 0.59527133\n",
      "Iteration 6, loss = 0.51471720\n",
      "Iteration 7, loss = 0.45509296\n",
      "Iteration 8, loss = 0.42850810\n",
      "Iteration 9, loss = 0.41762299\n",
      "Iteration 10, loss = 0.38473695\n",
      "Iteration 11, loss = 0.37904858\n",
      "Iteration 12, loss = 0.36367763\n",
      "Iteration 13, loss = 0.33697030\n",
      "Iteration 14, loss = 0.32491397\n",
      "Iteration 15, loss = 0.33568425\n",
      "Iteration 16, loss = 0.34199894\n",
      "Iteration 17, loss = 0.32478263\n",
      "Iteration 18, loss = 0.32858381\n",
      "Iteration 19, loss = 0.34857511\n",
      "Iteration 20, loss = 0.33334944\n",
      "Iteration 21, loss = 0.30310133\n",
      "Iteration 22, loss = 0.29880891\n",
      "Iteration 23, loss = 0.30421451\n",
      "Iteration 24, loss = 0.32042445\n",
      "Iteration 25, loss = 0.32557098\n",
      "Iteration 26, loss = 0.31044173\n",
      "Iteration 27, loss = 0.30068450\n",
      "Iteration 28, loss = 0.31330924\n",
      "Iteration 29, loss = 0.32983697\n",
      "Iteration 30, loss = 0.31575049\n",
      "Iteration 31, loss = 0.30931388\n",
      "Iteration 32, loss = 0.32914589\n",
      "Iteration 33, loss = 0.33973058\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.46017703\n",
      "Iteration 2, loss = 1.34593327\n",
      "Iteration 3, loss = 0.76498484\n",
      "Iteration 4, loss = 0.62197271\n",
      "Iteration 5, loss = 0.58485553\n",
      "Iteration 6, loss = 0.49385760\n",
      "Iteration 7, loss = 0.43215915\n",
      "Iteration 8, loss = 0.41758175\n",
      "Iteration 9, loss = 0.41276195\n",
      "Iteration 10, loss = 0.37694016\n",
      "Iteration 11, loss = 0.36066589\n",
      "Iteration 12, loss = 0.34967098\n",
      "Iteration 13, loss = 0.32843813\n",
      "Iteration 14, loss = 0.30812847\n",
      "Iteration 15, loss = 0.30931330\n",
      "Iteration 16, loss = 0.32093673\n",
      "Iteration 17, loss = 0.32089036\n",
      "Iteration 18, loss = 0.32978043\n",
      "Iteration 19, loss = 0.33251876\n",
      "Iteration 20, loss = 0.31531768\n",
      "Iteration 21, loss = 0.29281014\n",
      "Iteration 22, loss = 0.30061073\n",
      "Iteration 23, loss = 0.30682916\n",
      "Iteration 24, loss = 0.31760033\n",
      "Iteration 25, loss = 0.30333926\n",
      "Iteration 26, loss = 0.29752433\n",
      "Iteration 27, loss = 0.31282585\n",
      "Iteration 28, loss = 0.31502408\n",
      "Iteration 29, loss = 0.29746592\n",
      "Iteration 30, loss = 0.29837920\n",
      "Iteration 31, loss = 0.30228694\n",
      "Iteration 32, loss = 0.30703835\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.46034448\n",
      "Iteration 2, loss = 1.39263677\n",
      "Iteration 3, loss = 0.78816149\n",
      "Iteration 4, loss = 0.65087113\n",
      "Iteration 5, loss = 0.61567717\n",
      "Iteration 6, loss = 0.51336747\n",
      "Iteration 7, loss = 0.45007111\n",
      "Iteration 8, loss = 0.43632253\n",
      "Iteration 9, loss = 0.42831629\n",
      "Iteration 10, loss = 0.39363309\n",
      "Iteration 11, loss = 0.38118746\n",
      "Iteration 12, loss = 0.35511344\n",
      "Iteration 13, loss = 0.33183795\n",
      "Iteration 14, loss = 0.33101273\n",
      "Iteration 15, loss = 0.34024876\n",
      "Iteration 16, loss = 0.34122697\n",
      "Iteration 17, loss = 0.32498983\n",
      "Iteration 18, loss = 0.32655096\n",
      "Iteration 19, loss = 0.33607558\n",
      "Iteration 20, loss = 0.31605783\n",
      "Iteration 21, loss = 0.29081921\n",
      "Iteration 22, loss = 0.29841587\n",
      "Iteration 23, loss = 0.30652734\n",
      "Iteration 24, loss = 0.31249832\n",
      "Iteration 25, loss = 0.30539282\n",
      "Iteration 26, loss = 0.29815391\n",
      "Iteration 27, loss = 0.31002257\n",
      "Iteration 28, loss = 0.31881543\n",
      "Iteration 29, loss = 0.31629522\n",
      "Iteration 30, loss = 0.31515390\n",
      "Iteration 31, loss = 0.29348244\n",
      "Iteration 32, loss = 0.29148012\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.46311229\n",
      "Iteration 2, loss = 1.38987010\n",
      "Iteration 3, loss = 0.78514634\n",
      "Iteration 4, loss = 0.64390348\n",
      "Iteration 5, loss = 0.64180925\n",
      "Iteration 6, loss = 0.54160544\n",
      "Iteration 7, loss = 0.44626579\n",
      "Iteration 8, loss = 0.41680419\n",
      "Iteration 9, loss = 0.41745586\n",
      "Iteration 10, loss = 0.39996458\n",
      "Iteration 11, loss = 0.36437138\n",
      "Iteration 12, loss = 0.33144008\n",
      "Iteration 13, loss = 0.31773087\n",
      "Iteration 14, loss = 0.32478870\n",
      "Iteration 15, loss = 0.33793956\n",
      "Iteration 16, loss = 0.33533525\n",
      "Iteration 17, loss = 0.30825090\n",
      "Iteration 18, loss = 0.30570565\n",
      "Iteration 19, loss = 0.32215016\n",
      "Iteration 20, loss = 0.31409205\n",
      "Iteration 21, loss = 0.28580715\n",
      "Iteration 22, loss = 0.29480930\n",
      "Iteration 23, loss = 0.30043019\n",
      "Iteration 24, loss = 0.29117772\n",
      "Iteration 25, loss = 0.28559706\n",
      "Iteration 26, loss = 0.28432713\n",
      "Iteration 27, loss = 0.30316073\n",
      "Iteration 28, loss = 0.31071978\n",
      "Iteration 29, loss = 0.29933652\n",
      "Iteration 30, loss = 0.29843009\n",
      "Iteration 31, loss = 0.28225125\n",
      "Iteration 32, loss = 0.28553729\n",
      "Iteration 33, loss = 0.30943807\n",
      "Iteration 34, loss = 0.30662850\n",
      "Iteration 35, loss = 0.29196471\n",
      "Iteration 36, loss = 0.28655765\n",
      "Iteration 37, loss = 0.29217153\n",
      "Iteration 38, loss = 0.28294728\n",
      "Iteration 39, loss = 0.27617100\n",
      "Iteration 40, loss = 0.28681520\n",
      "Iteration 41, loss = 0.29552473\n",
      "Iteration 42, loss = 0.28217607\n",
      "Iteration 43, loss = 0.27789490\n",
      "Iteration 44, loss = 0.28223607\n",
      "Iteration 45, loss = 0.28920244\n",
      "Iteration 46, loss = 0.31510577\n",
      "Iteration 47, loss = 0.30809569\n",
      "Iteration 48, loss = 0.29763294\n",
      "Iteration 49, loss = 0.29310031\n",
      "Iteration 50, loss = 0.28471382\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.46577142\n",
      "Iteration 2, loss = 1.45187618\n",
      "Iteration 3, loss = 0.79631805\n",
      "Iteration 4, loss = 0.67514525\n",
      "Iteration 5, loss = 0.65714394\n",
      "Iteration 6, loss = 0.54392462\n",
      "Iteration 7, loss = 0.46175192\n",
      "Iteration 8, loss = 0.43926921\n",
      "Iteration 9, loss = 0.43211023\n",
      "Iteration 10, loss = 0.41624531\n",
      "Iteration 11, loss = 0.37991470\n",
      "Iteration 12, loss = 0.34343903\n",
      "Iteration 13, loss = 0.33303240\n",
      "Iteration 14, loss = 0.34875526\n",
      "Iteration 15, loss = 0.36182859\n",
      "Iteration 16, loss = 0.34534031\n",
      "Iteration 17, loss = 0.31256259\n",
      "Iteration 18, loss = 0.32030019\n",
      "Iteration 19, loss = 0.33224060\n",
      "Iteration 20, loss = 0.31076533\n",
      "Iteration 21, loss = 0.29034000\n",
      "Iteration 22, loss = 0.30464741\n",
      "Iteration 23, loss = 0.31866088\n",
      "Iteration 24, loss = 0.31474348\n",
      "Iteration 25, loss = 0.29050319\n",
      "Iteration 26, loss = 0.29815773\n",
      "Iteration 27, loss = 0.31792537\n",
      "Iteration 28, loss = 0.31485386\n",
      "Iteration 29, loss = 0.30806867\n",
      "Iteration 30, loss = 0.31189785\n",
      "Iteration 31, loss = 0.30272842\n",
      "Iteration 32, loss = 0.30394445\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.46369545\n",
      "Iteration 2, loss = 1.42533369\n",
      "Iteration 3, loss = 0.80668828\n",
      "Iteration 4, loss = 0.69066300\n",
      "Iteration 5, loss = 0.66242914\n",
      "Iteration 6, loss = 0.55194607\n",
      "Iteration 7, loss = 0.47682710\n",
      "Iteration 8, loss = 0.45762207\n",
      "Iteration 9, loss = 0.45330401\n",
      "Iteration 10, loss = 0.43054093\n",
      "Iteration 11, loss = 0.39187731\n",
      "Iteration 12, loss = 0.35468827\n",
      "Iteration 13, loss = 0.33981392\n",
      "Iteration 14, loss = 0.35161638\n",
      "Iteration 15, loss = 0.36375701\n",
      "Iteration 16, loss = 0.35687506\n",
      "Iteration 17, loss = 0.32708684\n",
      "Iteration 18, loss = 0.32820539\n",
      "Iteration 19, loss = 0.33933852\n",
      "Iteration 20, loss = 0.32434494\n",
      "Iteration 21, loss = 0.30086425\n",
      "Iteration 22, loss = 0.31026802\n",
      "Iteration 23, loss = 0.32093617\n",
      "Iteration 24, loss = 0.31777307\n",
      "Iteration 25, loss = 0.30720814\n",
      "Iteration 26, loss = 0.33067403\n",
      "Iteration 27, loss = 0.33188219\n",
      "Iteration 28, loss = 0.30721762\n",
      "Iteration 29, loss = 0.31492427\n",
      "Iteration 30, loss = 0.32954050\n",
      "Iteration 31, loss = 0.31903736\n",
      "Iteration 32, loss = 0.31424243\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.46247116\n",
      "Iteration 2, loss = 1.43579133\n",
      "Iteration 3, loss = 0.83529372\n",
      "Iteration 4, loss = 0.64902495\n",
      "Iteration 5, loss = 0.65733033\n",
      "Iteration 6, loss = 0.57466097\n",
      "Iteration 7, loss = 0.49352841\n",
      "Iteration 8, loss = 0.45731264\n",
      "Iteration 9, loss = 0.45948786\n",
      "Iteration 10, loss = 0.44999375\n",
      "Iteration 11, loss = 0.41295502\n",
      "Iteration 12, loss = 0.36651990\n",
      "Iteration 13, loss = 0.34034860\n",
      "Iteration 14, loss = 0.34425509\n",
      "Iteration 15, loss = 0.35956139\n",
      "Iteration 16, loss = 0.35332898\n",
      "Iteration 17, loss = 0.32626174\n",
      "Iteration 18, loss = 0.33042001\n",
      "Iteration 19, loss = 0.34114359\n",
      "Iteration 20, loss = 0.31336241\n",
      "Iteration 21, loss = 0.28740782\n",
      "Iteration 22, loss = 0.30845832\n",
      "Iteration 23, loss = 0.32610804\n",
      "Iteration 24, loss = 0.31498633\n",
      "Iteration 25, loss = 0.29008711\n",
      "Iteration 26, loss = 0.31464544\n",
      "Iteration 27, loss = 0.32509642\n",
      "Iteration 28, loss = 0.31034750\n",
      "Iteration 29, loss = 0.31337273\n",
      "Iteration 30, loss = 0.31939229\n",
      "Iteration 31, loss = 0.31349445\n",
      "Iteration 32, loss = 0.30928067\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.46479539\n",
      "Iteration 2, loss = 1.48154454\n",
      "Iteration 3, loss = 0.86415481\n",
      "Iteration 4, loss = 0.63838774\n",
      "Iteration 5, loss = 0.63638345\n",
      "Iteration 6, loss = 0.55941566\n",
      "Iteration 7, loss = 0.49087955\n",
      "Iteration 8, loss = 0.45091458\n",
      "Iteration 9, loss = 0.44875834\n",
      "Iteration 10, loss = 0.42813941\n",
      "Iteration 11, loss = 0.38560755\n",
      "Iteration 12, loss = 0.35271391\n",
      "Iteration 13, loss = 0.33339492\n",
      "Iteration 14, loss = 0.32642058\n",
      "Iteration 15, loss = 0.32667088\n",
      "Iteration 16, loss = 0.32371824\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 17, loss = 0.31427476\n",
      "Iteration 18, loss = 0.31523642\n",
      "Iteration 19, loss = 0.31741799\n",
      "Iteration 20, loss = 0.30348297\n",
      "Iteration 21, loss = 0.28704233\n",
      "Iteration 22, loss = 0.30127841\n",
      "Iteration 23, loss = 0.31107699\n",
      "Iteration 24, loss = 0.29815590\n",
      "Iteration 25, loss = 0.28482000\n",
      "Iteration 26, loss = 0.31866244\n",
      "Iteration 27, loss = 0.31181477\n",
      "Iteration 28, loss = 0.29010287\n",
      "Iteration 29, loss = 0.29180596\n",
      "Iteration 30, loss = 0.29678480\n",
      "Iteration 31, loss = 0.31898421\n",
      "Iteration 32, loss = 0.30065335\n",
      "Iteration 33, loss = 0.29297616\n",
      "Iteration 34, loss = 0.29144091\n",
      "Iteration 35, loss = 0.28977981\n",
      "Iteration 36, loss = 0.29431434\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.12500503\n",
      "Iteration 2, loss = 1.30874955\n",
      "Iteration 3, loss = 0.76032467\n",
      "Iteration 4, loss = 0.62939217\n",
      "Iteration 5, loss = 0.49117272\n",
      "Iteration 6, loss = 0.46111751\n",
      "Iteration 7, loss = 0.45482879\n",
      "Iteration 8, loss = 0.41425960\n",
      "Iteration 9, loss = 0.37759658\n",
      "Iteration 10, loss = 0.37512325\n",
      "Iteration 11, loss = 0.37471705\n",
      "Iteration 12, loss = 0.35491349\n",
      "Iteration 13, loss = 0.33072836\n",
      "Iteration 14, loss = 0.32123968\n",
      "Iteration 15, loss = 0.31722062\n",
      "Iteration 16, loss = 0.32551476\n",
      "Iteration 17, loss = 0.32583957\n",
      "Iteration 18, loss = 0.30847838\n",
      "Iteration 19, loss = 0.29494173\n",
      "Iteration 20, loss = 0.30182722\n",
      "Iteration 21, loss = 0.32343401\n",
      "Iteration 22, loss = 0.31362675\n",
      "Iteration 23, loss = 0.30308229\n",
      "Iteration 24, loss = 0.28850541\n",
      "Iteration 25, loss = 0.28713122\n",
      "Iteration 26, loss = 0.29995054\n",
      "Iteration 27, loss = 0.31498246\n",
      "Iteration 28, loss = 0.30953436\n",
      "Iteration 29, loss = 0.30467035\n",
      "Iteration 30, loss = 0.29462897\n",
      "Iteration 31, loss = 0.30205056\n",
      "Iteration 32, loss = 0.31399735\n",
      "Iteration 33, loss = 0.28858638\n",
      "Iteration 34, loss = 0.28154628\n",
      "Iteration 35, loss = 0.28486633\n",
      "Iteration 36, loss = 0.29909905\n",
      "Iteration 37, loss = 0.29408729\n",
      "Iteration 38, loss = 0.28884777\n",
      "Iteration 39, loss = 0.29401073\n",
      "Iteration 40, loss = 0.31763250\n",
      "Iteration 41, loss = 0.31281397\n",
      "Iteration 42, loss = 0.35616140\n",
      "Iteration 43, loss = 0.34958018\n",
      "Iteration 44, loss = 0.34690199\n",
      "Iteration 45, loss = 0.31963012\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.13295727\n",
      "Iteration 2, loss = 1.37417087\n",
      "Iteration 3, loss = 0.77768267\n",
      "Iteration 4, loss = 0.75746192\n",
      "Iteration 5, loss = 0.56205449\n",
      "Iteration 6, loss = 0.48067605\n",
      "Iteration 7, loss = 0.49919611\n",
      "Iteration 8, loss = 0.45829071\n",
      "Iteration 9, loss = 0.39321877\n",
      "Iteration 10, loss = 0.36954907\n",
      "Iteration 11, loss = 0.37014833\n",
      "Iteration 12, loss = 0.35924067\n",
      "Iteration 13, loss = 0.33563801\n",
      "Iteration 14, loss = 0.32249983\n",
      "Iteration 15, loss = 0.30823310\n",
      "Iteration 16, loss = 0.30904348\n",
      "Iteration 17, loss = 0.31808631\n",
      "Iteration 18, loss = 0.30412337\n",
      "Iteration 19, loss = 0.30869934\n",
      "Iteration 20, loss = 0.29577822\n",
      "Iteration 21, loss = 0.30510946\n",
      "Iteration 22, loss = 0.29961266\n",
      "Iteration 23, loss = 0.30862797\n",
      "Iteration 24, loss = 0.31080127\n",
      "Iteration 25, loss = 0.30961149\n",
      "Iteration 26, loss = 0.32788669\n",
      "Iteration 27, loss = 0.32991836\n",
      "Iteration 28, loss = 0.31773048\n",
      "Iteration 29, loss = 0.31679039\n",
      "Iteration 30, loss = 0.31499983\n",
      "Iteration 31, loss = 0.30889606\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.12620000\n",
      "Iteration 2, loss = 1.30948350\n",
      "Iteration 3, loss = 0.77885133\n",
      "Iteration 4, loss = 0.72577759\n",
      "Iteration 5, loss = 0.50168893\n",
      "Iteration 6, loss = 0.46467260\n",
      "Iteration 7, loss = 0.48679993\n",
      "Iteration 8, loss = 0.44127208\n",
      "Iteration 9, loss = 0.37864513\n",
      "Iteration 10, loss = 0.35289389\n",
      "Iteration 11, loss = 0.35930450\n",
      "Iteration 12, loss = 0.36673632\n",
      "Iteration 13, loss = 0.33882254\n",
      "Iteration 14, loss = 0.31685631\n",
      "Iteration 15, loss = 0.31165169\n",
      "Iteration 16, loss = 0.31265770\n",
      "Iteration 17, loss = 0.31656045\n",
      "Iteration 18, loss = 0.29940887\n",
      "Iteration 19, loss = 0.29198731\n",
      "Iteration 20, loss = 0.29408897\n",
      "Iteration 21, loss = 0.30672792\n",
      "Iteration 22, loss = 0.30452098\n",
      "Iteration 23, loss = 0.31236513\n",
      "Iteration 24, loss = 0.30141864\n",
      "Iteration 25, loss = 0.30500245\n",
      "Iteration 26, loss = 0.32708248\n",
      "Iteration 27, loss = 0.34170552\n",
      "Iteration 28, loss = 0.32720475\n",
      "Iteration 29, loss = 0.31675316\n",
      "Iteration 30, loss = 0.31514584\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.14083206\n",
      "Iteration 2, loss = 1.34614375\n",
      "Iteration 3, loss = 0.78692037\n",
      "Iteration 4, loss = 0.72609684\n",
      "Iteration 5, loss = 0.56649179\n",
      "Iteration 6, loss = 0.46942705\n",
      "Iteration 7, loss = 0.48435075\n",
      "Iteration 8, loss = 0.46734593\n",
      "Iteration 9, loss = 0.41201501\n",
      "Iteration 10, loss = 0.37372968\n",
      "Iteration 11, loss = 0.36270149\n",
      "Iteration 12, loss = 0.36011138\n",
      "Iteration 13, loss = 0.34290337\n",
      "Iteration 14, loss = 0.32751393\n",
      "Iteration 15, loss = 0.30212145\n",
      "Iteration 16, loss = 0.29361211\n",
      "Iteration 17, loss = 0.30868621\n",
      "Iteration 18, loss = 0.30492191\n",
      "Iteration 19, loss = 0.29028385\n",
      "Iteration 20, loss = 0.27695274\n",
      "Iteration 21, loss = 0.28420696\n",
      "Iteration 22, loss = 0.28732472\n",
      "Iteration 23, loss = 0.29047757\n",
      "Iteration 24, loss = 0.26969830\n",
      "Iteration 25, loss = 0.29060237\n",
      "Iteration 26, loss = 0.32028906\n",
      "Iteration 27, loss = 0.32470925\n",
      "Iteration 28, loss = 0.29667966\n",
      "Iteration 29, loss = 0.30761646\n",
      "Iteration 30, loss = 0.31566573\n",
      "Iteration 31, loss = 0.30628479\n",
      "Iteration 32, loss = 0.31136339\n",
      "Iteration 33, loss = 0.29927190\n",
      "Iteration 34, loss = 0.28766528\n",
      "Iteration 35, loss = 0.29170357\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.14604770\n",
      "Iteration 2, loss = 1.30149648\n",
      "Iteration 3, loss = 0.79870492\n",
      "Iteration 4, loss = 0.71927872\n",
      "Iteration 5, loss = 0.51831991\n",
      "Iteration 6, loss = 0.48885877\n",
      "Iteration 7, loss = 0.51121944\n",
      "Iteration 8, loss = 0.46115041\n",
      "Iteration 9, loss = 0.39001875\n",
      "Iteration 10, loss = 0.35588222\n",
      "Iteration 11, loss = 0.34800427\n",
      "Iteration 12, loss = 0.34723640\n",
      "Iteration 13, loss = 0.33988861\n",
      "Iteration 14, loss = 0.33682522\n",
      "Iteration 15, loss = 0.31713042\n",
      "Iteration 16, loss = 0.30304431\n",
      "Iteration 17, loss = 0.30538804\n",
      "Iteration 18, loss = 0.30076315\n",
      "Iteration 19, loss = 0.29588798\n",
      "Iteration 20, loss = 0.28380826\n",
      "Iteration 21, loss = 0.28808041\n",
      "Iteration 22, loss = 0.30114832\n",
      "Iteration 23, loss = 0.29722169\n",
      "Iteration 24, loss = 0.28820672\n",
      "Iteration 25, loss = 0.31209154\n",
      "Iteration 26, loss = 0.33701148\n",
      "Iteration 27, loss = 0.34488843\n",
      "Iteration 28, loss = 0.29128282\n",
      "Iteration 29, loss = 0.29396159\n",
      "Iteration 30, loss = 0.30384173\n",
      "Iteration 31, loss = 0.30694304\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.15751717\n",
      "Iteration 2, loss = 1.27444593\n",
      "Iteration 3, loss = 0.81557289\n",
      "Iteration 4, loss = 0.69305620\n",
      "Iteration 5, loss = 0.50700947\n",
      "Iteration 6, loss = 0.48519217\n",
      "Iteration 7, loss = 0.49198633\n",
      "Iteration 8, loss = 0.44369026\n",
      "Iteration 9, loss = 0.38598253\n",
      "Iteration 10, loss = 0.35998941\n",
      "Iteration 11, loss = 0.34752548\n",
      "Iteration 12, loss = 0.33221775\n",
      "Iteration 13, loss = 0.31421131\n",
      "Iteration 14, loss = 0.31464415\n",
      "Iteration 15, loss = 0.30740504\n",
      "Iteration 16, loss = 0.29872169\n",
      "Iteration 17, loss = 0.30828870\n",
      "Iteration 18, loss = 0.29544666\n",
      "Iteration 19, loss = 0.28037242\n",
      "Iteration 20, loss = 0.29209374\n",
      "Iteration 21, loss = 0.29840259\n",
      "Iteration 22, loss = 0.28260383\n",
      "Iteration 23, loss = 0.28447963\n",
      "Iteration 24, loss = 0.29173670\n",
      "Iteration 25, loss = 0.29166065\n",
      "Iteration 26, loss = 0.30123905\n",
      "Iteration 27, loss = 0.30989807\n",
      "Iteration 28, loss = 0.29994587\n",
      "Iteration 29, loss = 0.30143076\n",
      "Iteration 30, loss = 0.30603082\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.17057795\n",
      "Iteration 2, loss = 1.25758054\n",
      "Iteration 3, loss = 0.88230566\n",
      "Iteration 4, loss = 0.69604073\n",
      "Iteration 5, loss = 0.51414150\n",
      "Iteration 6, loss = 0.50844515\n",
      "Iteration 7, loss = 0.48404780\n",
      "Iteration 8, loss = 0.43763265\n",
      "Iteration 9, loss = 0.40737035\n",
      "Iteration 10, loss = 0.39356826\n",
      "Iteration 11, loss = 0.36634167\n",
      "Iteration 12, loss = 0.33533581\n",
      "Iteration 13, loss = 0.32331793\n",
      "Iteration 14, loss = 0.33755129\n",
      "Iteration 15, loss = 0.33700279\n",
      "Iteration 16, loss = 0.32394231\n",
      "Iteration 17, loss = 0.32244778\n",
      "Iteration 18, loss = 0.30102982\n",
      "Iteration 19, loss = 0.29567315\n",
      "Iteration 20, loss = 0.32854898\n",
      "Iteration 21, loss = 0.30913121\n",
      "Iteration 22, loss = 0.28176719\n",
      "Iteration 23, loss = 0.29574063\n",
      "Iteration 24, loss = 0.32323723\n",
      "Iteration 25, loss = 0.30173527\n",
      "Iteration 26, loss = 0.30530200\n",
      "Iteration 27, loss = 0.31953800\n",
      "Iteration 28, loss = 0.30985974\n",
      "Iteration 29, loss = 0.29717611\n",
      "Iteration 30, loss = 0.29708398\n",
      "Iteration 31, loss = 0.29113144\n",
      "Iteration 32, loss = 0.28867634\n",
      "Iteration 33, loss = 0.30114299\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.16226818\n",
      "Iteration 2, loss = 1.24990447\n",
      "Iteration 3, loss = 0.85902386\n",
      "Iteration 4, loss = 0.68564605\n",
      "Iteration 5, loss = 0.51161156\n",
      "Iteration 6, loss = 0.51489504\n",
      "Iteration 7, loss = 0.49727423\n",
      "Iteration 8, loss = 0.45606336\n",
      "Iteration 9, loss = 0.42632918\n",
      "Iteration 10, loss = 0.40319605\n",
      "Iteration 11, loss = 0.36574633\n",
      "Iteration 12, loss = 0.33595599\n",
      "Iteration 13, loss = 0.33833506\n",
      "Iteration 14, loss = 0.35611844\n",
      "Iteration 15, loss = 0.33030275\n",
      "Iteration 16, loss = 0.30897585\n",
      "Iteration 17, loss = 0.32292278\n",
      "Iteration 18, loss = 0.32330350\n",
      "Iteration 19, loss = 0.30612901\n",
      "Iteration 20, loss = 0.33610878\n",
      "Iteration 21, loss = 0.31221036\n",
      "Iteration 22, loss = 0.29628892\n",
      "Iteration 23, loss = 0.32379183\n",
      "Iteration 24, loss = 0.36023707\n",
      "Iteration 25, loss = 0.33529199\n",
      "Iteration 26, loss = 0.31261499\n",
      "Iteration 27, loss = 0.33264032\n",
      "Iteration 28, loss = 0.34312937\n",
      "Iteration 29, loss = 0.31695069\n",
      "Iteration 30, loss = 0.31464540\n",
      "Iteration 31, loss = 0.31288449\n",
      "Iteration 32, loss = 0.30723482\n",
      "Iteration 33, loss = 0.32521735\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.17608804\n",
      "Iteration 2, loss = 1.24643960\n",
      "Iteration 3, loss = 0.84694840\n",
      "Iteration 4, loss = 0.68106934\n",
      "Iteration 5, loss = 0.53091103\n",
      "Iteration 6, loss = 0.52064289\n",
      "Iteration 7, loss = 0.49431982\n",
      "Iteration 8, loss = 0.45911063\n",
      "Iteration 9, loss = 0.43115319\n",
      "Iteration 10, loss = 0.40984258\n",
      "Iteration 11, loss = 0.37495188\n",
      "Iteration 12, loss = 0.34472975\n",
      "Iteration 13, loss = 0.34232073\n",
      "Iteration 14, loss = 0.34791116\n",
      "Iteration 15, loss = 0.33535226\n",
      "Iteration 16, loss = 0.31161338\n",
      "Iteration 17, loss = 0.31160490\n",
      "Iteration 18, loss = 0.31472807\n",
      "Iteration 19, loss = 0.30528842\n",
      "Iteration 20, loss = 0.34138297\n",
      "Iteration 21, loss = 0.32800854\n",
      "Iteration 22, loss = 0.30013676\n",
      "Iteration 23, loss = 0.31461139\n",
      "Iteration 24, loss = 0.34458083\n",
      "Iteration 25, loss = 0.31879853\n",
      "Iteration 26, loss = 0.31365201\n",
      "Iteration 27, loss = 0.31857337\n",
      "Iteration 28, loss = 0.31779923\n",
      "Iteration 29, loss = 0.31696956\n",
      "Iteration 30, loss = 0.32386057\n",
      "Iteration 31, loss = 0.31035399\n",
      "Iteration 32, loss = 0.29468398\n",
      "Iteration 33, loss = 0.31602924\n",
      "Iteration 34, loss = 0.34082398\n",
      "Iteration 35, loss = 0.33040244\n",
      "Iteration 36, loss = 0.31654685\n",
      "Iteration 37, loss = 0.32397106\n",
      "Iteration 38, loss = 0.30453254\n",
      "Iteration 39, loss = 0.28930949\n",
      "Iteration 40, loss = 0.31811552\n",
      "Iteration 41, loss = 0.37037934\n",
      "Iteration 42, loss = 0.35351771\n",
      "Iteration 43, loss = 0.30645094\n",
      "Iteration 44, loss = 0.30580359\n",
      "Iteration 45, loss = 0.31971214\n",
      "Iteration 46, loss = 0.32315705\n",
      "Iteration 47, loss = 0.30608414\n",
      "Iteration 48, loss = 0.29019433\n",
      "Iteration 49, loss = 0.32725822\n",
      "Iteration 50, loss = 0.34858639\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.17283641\n",
      "Iteration 2, loss = 1.24106904\n",
      "Iteration 3, loss = 0.84361627\n",
      "Iteration 4, loss = 0.66908650\n",
      "Iteration 5, loss = 0.51660557\n",
      "Iteration 6, loss = 0.50627802\n",
      "Iteration 7, loss = 0.47210199\n",
      "Iteration 8, loss = 0.44111742\n",
      "Iteration 9, loss = 0.42188742\n",
      "Iteration 10, loss = 0.39865912\n",
      "Iteration 11, loss = 0.36129968\n",
      "Iteration 12, loss = 0.33377205\n",
      "Iteration 13, loss = 0.34135644\n",
      "Iteration 14, loss = 0.35281797\n",
      "Iteration 15, loss = 0.34407558\n",
      "Iteration 16, loss = 0.32179255\n",
      "Iteration 17, loss = 0.30915520\n",
      "Iteration 18, loss = 0.28950682\n",
      "Iteration 19, loss = 0.29271565\n",
      "Iteration 20, loss = 0.33006910\n",
      "Iteration 21, loss = 0.31404011\n",
      "Iteration 22, loss = 0.29223629\n",
      "Iteration 23, loss = 0.30001519\n",
      "Iteration 24, loss = 0.30689192\n",
      "Iteration 25, loss = 0.30703682\n",
      "Iteration 26, loss = 0.32151571\n",
      "Iteration 27, loss = 0.29878665\n",
      "Iteration 28, loss = 0.28859495\n",
      "Iteration 29, loss = 0.30279653\n",
      "Iteration 30, loss = 0.32186433\n",
      "Iteration 31, loss = 0.30146687\n",
      "Iteration 32, loss = 0.28985148\n",
      "Iteration 33, loss = 0.31453797\n",
      "Iteration 34, loss = 0.32719162\n",
      "Iteration 35, loss = 0.31148679\n",
      "Iteration 36, loss = 0.30608371\n",
      "Iteration 37, loss = 0.31746188\n",
      "Iteration 38, loss = 0.29623635\n",
      "Iteration 39, loss = 0.28972398\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.20944199\n",
      "Iteration 2, loss = 1.30095701\n",
      "Iteration 3, loss = 0.74365367\n",
      "Iteration 4, loss = 0.67220225\n",
      "Iteration 5, loss = 0.54305568\n",
      "Iteration 6, loss = 0.45424786\n",
      "Iteration 7, loss = 0.45761214\n",
      "Iteration 8, loss = 0.47434004\n",
      "Iteration 9, loss = 0.44050475\n",
      "Iteration 10, loss = 0.38637771\n",
      "Iteration 11, loss = 0.37642494\n",
      "Iteration 12, loss = 0.39350892\n",
      "Iteration 13, loss = 0.37660109\n",
      "Iteration 14, loss = 0.33176513\n",
      "Iteration 15, loss = 0.32710912\n",
      "Iteration 16, loss = 0.37610503\n",
      "Iteration 17, loss = 0.39453184\n",
      "Iteration 18, loss = 0.36825112\n",
      "Iteration 19, loss = 0.32263763\n",
      "Iteration 20, loss = 0.29215468\n",
      "Iteration 21, loss = 0.33460967\n",
      "Iteration 22, loss = 0.36651334\n",
      "Iteration 23, loss = 0.33112143\n",
      "Iteration 24, loss = 0.31293312\n",
      "Iteration 25, loss = 0.32236140\n",
      "Iteration 26, loss = 0.33386099\n",
      "Iteration 27, loss = 0.35707056\n",
      "Iteration 28, loss = 0.36425033\n",
      "Iteration 29, loss = 0.33512043\n",
      "Iteration 30, loss = 0.32742141\n",
      "Iteration 31, loss = 0.32769214\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.21340744\n",
      "Iteration 2, loss = 1.24380696\n",
      "Iteration 3, loss = 0.71232970\n",
      "Iteration 4, loss = 0.67046460\n",
      "Iteration 5, loss = 0.55538850\n",
      "Iteration 6, loss = 0.46066005\n",
      "Iteration 7, loss = 0.44011169\n",
      "Iteration 8, loss = 0.42858402\n",
      "Iteration 9, loss = 0.39271369\n",
      "Iteration 10, loss = 0.36439247\n",
      "Iteration 11, loss = 0.35084970\n",
      "Iteration 12, loss = 0.34244136\n",
      "Iteration 13, loss = 0.33251918\n",
      "Iteration 14, loss = 0.32816480\n",
      "Iteration 15, loss = 0.32960162\n",
      "Iteration 16, loss = 0.32031111\n",
      "Iteration 17, loss = 0.30604472\n",
      "Iteration 18, loss = 0.30690779\n",
      "Iteration 19, loss = 0.30039327\n",
      "Iteration 20, loss = 0.29706935\n",
      "Iteration 21, loss = 0.30238327\n",
      "Iteration 22, loss = 0.29112998\n",
      "Iteration 23, loss = 0.30114466\n",
      "Iteration 24, loss = 0.31999808\n",
      "Iteration 25, loss = 0.32023138\n",
      "Iteration 26, loss = 0.29817741\n",
      "Iteration 27, loss = 0.29064323\n",
      "Iteration 28, loss = 0.30171242\n",
      "Iteration 29, loss = 0.30972364\n",
      "Iteration 30, loss = 0.30766066\n",
      "Iteration 31, loss = 0.29911808\n",
      "Iteration 32, loss = 0.28882364\n",
      "Iteration 33, loss = 0.32489194\n",
      "Iteration 34, loss = 0.33769629\n",
      "Iteration 35, loss = 0.32344154\n",
      "Iteration 36, loss = 0.34097283\n",
      "Iteration 37, loss = 0.31276989\n",
      "Iteration 38, loss = 0.29718557\n",
      "Iteration 39, loss = 0.30495569\n",
      "Iteration 40, loss = 0.30472485\n",
      "Iteration 41, loss = 0.29693987\n",
      "Iteration 42, loss = 0.32207803\n",
      "Iteration 43, loss = 0.32104020\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.19885488\n",
      "Iteration 2, loss = 1.25244984\n",
      "Iteration 3, loss = 0.69940753\n",
      "Iteration 4, loss = 0.64119828\n",
      "Iteration 5, loss = 0.51894410\n",
      "Iteration 6, loss = 0.44086564\n",
      "Iteration 7, loss = 0.43040852\n",
      "Iteration 8, loss = 0.42261356\n",
      "Iteration 9, loss = 0.39049442\n",
      "Iteration 10, loss = 0.36304117\n",
      "Iteration 11, loss = 0.35672484\n",
      "Iteration 12, loss = 0.34423293\n",
      "Iteration 13, loss = 0.33094558\n",
      "Iteration 14, loss = 0.33354534\n",
      "Iteration 15, loss = 0.33833699\n",
      "Iteration 16, loss = 0.32133359\n",
      "Iteration 17, loss = 0.29555054\n",
      "Iteration 18, loss = 0.29681900\n",
      "Iteration 19, loss = 0.29542668\n",
      "Iteration 20, loss = 0.29449038\n",
      "Iteration 21, loss = 0.29543752\n",
      "Iteration 22, loss = 0.28509253\n",
      "Iteration 23, loss = 0.28227176\n",
      "Iteration 24, loss = 0.31097550\n",
      "Iteration 25, loss = 0.32601322\n",
      "Iteration 26, loss = 0.30817559\n",
      "Iteration 27, loss = 0.27796122\n",
      "Iteration 28, loss = 0.27419623\n",
      "Iteration 29, loss = 0.28645496\n",
      "Iteration 30, loss = 0.30577402\n",
      "Iteration 31, loss = 0.31207641\n",
      "Iteration 32, loss = 0.31118901\n",
      "Iteration 33, loss = 0.34429536\n",
      "Iteration 34, loss = 0.33748960\n",
      "Iteration 35, loss = 0.35053765\n",
      "Iteration 36, loss = 0.38730178\n",
      "Iteration 37, loss = 0.32805339\n",
      "Iteration 38, loss = 0.28781029\n",
      "Iteration 39, loss = 0.30675178\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.21130152\n",
      "Iteration 2, loss = 1.20401546\n",
      "Iteration 3, loss = 0.69365533\n",
      "Iteration 4, loss = 0.61700131\n",
      "Iteration 5, loss = 0.51158372\n",
      "Iteration 6, loss = 0.43106603\n",
      "Iteration 7, loss = 0.42694121\n",
      "Iteration 8, loss = 0.42022056\n",
      "Iteration 9, loss = 0.37938115\n",
      "Iteration 10, loss = 0.34518179\n",
      "Iteration 11, loss = 0.34072990\n",
      "Iteration 12, loss = 0.33314179\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 13, loss = 0.32023943\n",
      "Iteration 14, loss = 0.31415971\n",
      "Iteration 15, loss = 0.31611099\n",
      "Iteration 16, loss = 0.31231323\n",
      "Iteration 17, loss = 0.30775839\n",
      "Iteration 18, loss = 0.29285956\n",
      "Iteration 19, loss = 0.28055361\n",
      "Iteration 20, loss = 0.27860885\n",
      "Iteration 21, loss = 0.27615105\n",
      "Iteration 22, loss = 0.27756401\n",
      "Iteration 23, loss = 0.27774001\n",
      "Iteration 24, loss = 0.28930434\n",
      "Iteration 25, loss = 0.31210084\n",
      "Iteration 26, loss = 0.29553388\n",
      "Iteration 27, loss = 0.27339611\n",
      "Iteration 28, loss = 0.27095160\n",
      "Iteration 29, loss = 0.27375865\n",
      "Iteration 30, loss = 0.28870846\n",
      "Iteration 31, loss = 0.30297968\n",
      "Iteration 32, loss = 0.29728821\n",
      "Iteration 33, loss = 0.30501951\n",
      "Iteration 34, loss = 0.30051434\n",
      "Iteration 35, loss = 0.32917188\n",
      "Iteration 36, loss = 0.32633527\n",
      "Iteration 37, loss = 0.27949374\n",
      "Iteration 38, loss = 0.26956944\n",
      "Iteration 39, loss = 0.29750997\n",
      "Iteration 40, loss = 0.30468395\n",
      "Iteration 41, loss = 0.28055353\n",
      "Iteration 42, loss = 0.32074649\n",
      "Iteration 43, loss = 0.33490383\n",
      "Iteration 44, loss = 0.30483744\n",
      "Iteration 45, loss = 0.30151484\n",
      "Iteration 46, loss = 0.30238413\n",
      "Iteration 47, loss = 0.28230223\n",
      "Iteration 48, loss = 0.30764510\n",
      "Iteration 49, loss = 0.32756155\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.19955236\n",
      "Iteration 2, loss = 1.22986539\n",
      "Iteration 3, loss = 0.68527508\n",
      "Iteration 4, loss = 0.63383046\n",
      "Iteration 5, loss = 0.52164924\n",
      "Iteration 6, loss = 0.44209075\n",
      "Iteration 7, loss = 0.44424295\n",
      "Iteration 8, loss = 0.43608695\n",
      "Iteration 9, loss = 0.39301676\n",
      "Iteration 10, loss = 0.35367500\n",
      "Iteration 11, loss = 0.34543757\n",
      "Iteration 12, loss = 0.32821459\n",
      "Iteration 13, loss = 0.32402762\n",
      "Iteration 14, loss = 0.32836962\n",
      "Iteration 15, loss = 0.32344257\n",
      "Iteration 16, loss = 0.31562576\n",
      "Iteration 17, loss = 0.29873156\n",
      "Iteration 18, loss = 0.29199013\n",
      "Iteration 19, loss = 0.28715740\n",
      "Iteration 20, loss = 0.30678652\n",
      "Iteration 21, loss = 0.31405225\n",
      "Iteration 22, loss = 0.29378238\n",
      "Iteration 23, loss = 0.29935826\n",
      "Iteration 24, loss = 0.31811124\n",
      "Iteration 25, loss = 0.30638777\n",
      "Iteration 26, loss = 0.31178418\n",
      "Iteration 27, loss = 0.29536101\n",
      "Iteration 28, loss = 0.28813295\n",
      "Iteration 29, loss = 0.28710053\n",
      "Iteration 30, loss = 0.29033810\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.18199351\n",
      "Iteration 2, loss = 1.29386091\n",
      "Iteration 3, loss = 0.68607473\n",
      "Iteration 4, loss = 0.63687672\n",
      "Iteration 5, loss = 0.54253101\n",
      "Iteration 6, loss = 0.43588894\n",
      "Iteration 7, loss = 0.41896111\n",
      "Iteration 8, loss = 0.41819026\n",
      "Iteration 9, loss = 0.39023503\n",
      "Iteration 10, loss = 0.35239004\n",
      "Iteration 11, loss = 0.34111911\n",
      "Iteration 12, loss = 0.33372799\n",
      "Iteration 13, loss = 0.33173578\n",
      "Iteration 14, loss = 0.32776291\n",
      "Iteration 15, loss = 0.31888941\n",
      "Iteration 16, loss = 0.31139137\n",
      "Iteration 17, loss = 0.30099703\n",
      "Iteration 18, loss = 0.29571012\n",
      "Iteration 19, loss = 0.28754164\n",
      "Iteration 20, loss = 0.28382091\n",
      "Iteration 21, loss = 0.28350399\n",
      "Iteration 22, loss = 0.28053250\n",
      "Iteration 23, loss = 0.28189708\n",
      "Iteration 24, loss = 0.29956767\n",
      "Iteration 25, loss = 0.31225098\n",
      "Iteration 26, loss = 0.29624562\n",
      "Iteration 27, loss = 0.27154558\n",
      "Iteration 28, loss = 0.28806584\n",
      "Iteration 29, loss = 0.29560547\n",
      "Iteration 30, loss = 0.29702794\n",
      "Iteration 31, loss = 0.30140261\n",
      "Iteration 32, loss = 0.29190287\n",
      "Iteration 33, loss = 0.30359635\n",
      "Iteration 34, loss = 0.30415668\n",
      "Iteration 35, loss = 0.30590575\n",
      "Iteration 36, loss = 0.29783392\n",
      "Iteration 37, loss = 0.29667636\n",
      "Iteration 38, loss = 0.29046724\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.18762317\n",
      "Iteration 2, loss = 1.26841330\n",
      "Iteration 3, loss = 0.70784481\n",
      "Iteration 4, loss = 0.67616196\n",
      "Iteration 5, loss = 0.54953207\n",
      "Iteration 6, loss = 0.44456805\n",
      "Iteration 7, loss = 0.43421309\n",
      "Iteration 8, loss = 0.43626215\n",
      "Iteration 9, loss = 0.40442547\n",
      "Iteration 10, loss = 0.36341546\n",
      "Iteration 11, loss = 0.34508402\n",
      "Iteration 12, loss = 0.33974597\n",
      "Iteration 13, loss = 0.33018857\n",
      "Iteration 14, loss = 0.31608999\n",
      "Iteration 15, loss = 0.31862387\n",
      "Iteration 16, loss = 0.31315259\n",
      "Iteration 17, loss = 0.30028427\n",
      "Iteration 18, loss = 0.29581502\n",
      "Iteration 19, loss = 0.29429330\n",
      "Iteration 20, loss = 0.29835212\n",
      "Iteration 21, loss = 0.28498305\n",
      "Iteration 22, loss = 0.29807476\n",
      "Iteration 23, loss = 0.30274584\n",
      "Iteration 24, loss = 0.29884711\n",
      "Iteration 25, loss = 0.31075769\n",
      "Iteration 26, loss = 0.31003670\n",
      "Iteration 27, loss = 0.29200995\n",
      "Iteration 28, loss = 0.30923513\n",
      "Iteration 29, loss = 0.30135250\n",
      "Iteration 30, loss = 0.31172309\n",
      "Iteration 31, loss = 0.33503270\n",
      "Iteration 32, loss = 0.30852515\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.19757192\n",
      "Iteration 2, loss = 1.27198411\n",
      "Iteration 3, loss = 0.70825824\n",
      "Iteration 4, loss = 0.68269198\n",
      "Iteration 5, loss = 0.57721461\n",
      "Iteration 6, loss = 0.46690159\n",
      "Iteration 7, loss = 0.43928318\n",
      "Iteration 8, loss = 0.43912063\n",
      "Iteration 9, loss = 0.41607195\n",
      "Iteration 10, loss = 0.38092339\n",
      "Iteration 11, loss = 0.35880378\n",
      "Iteration 12, loss = 0.34740205\n",
      "Iteration 13, loss = 0.33871910\n",
      "Iteration 14, loss = 0.32832103\n",
      "Iteration 15, loss = 0.33379947\n",
      "Iteration 16, loss = 0.33280425\n",
      "Iteration 17, loss = 0.31911474\n",
      "Iteration 18, loss = 0.31013455\n",
      "Iteration 19, loss = 0.30745561\n",
      "Iteration 20, loss = 0.33999425\n",
      "Iteration 21, loss = 0.32744439\n",
      "Iteration 22, loss = 0.31846016\n",
      "Iteration 23, loss = 0.32191905\n",
      "Iteration 24, loss = 0.31712194\n",
      "Iteration 25, loss = 0.33139472\n",
      "Iteration 26, loss = 0.32738006\n",
      "Iteration 27, loss = 0.30185911\n",
      "Iteration 28, loss = 0.30204664\n",
      "Iteration 29, loss = 0.30478928\n",
      "Iteration 30, loss = 0.31179984\n",
      "Iteration 31, loss = 0.33324174\n",
      "Iteration 32, loss = 0.32619768\n",
      "Iteration 33, loss = 0.31489980\n",
      "Iteration 34, loss = 0.29941304\n",
      "Iteration 35, loss = 0.30163890\n",
      "Iteration 36, loss = 0.31506585\n",
      "Iteration 37, loss = 0.30896918\n",
      "Iteration 38, loss = 0.30806066\n",
      "Iteration 39, loss = 0.31557921\n",
      "Iteration 40, loss = 0.32247804\n",
      "Iteration 41, loss = 0.32294372\n",
      "Iteration 42, loss = 0.32030588\n",
      "Iteration 43, loss = 0.31184637\n",
      "Iteration 44, loss = 0.29799526\n",
      "Iteration 45, loss = 0.30405392\n",
      "Iteration 46, loss = 0.30978033\n",
      "Iteration 47, loss = 0.31842908\n",
      "Iteration 48, loss = 0.34814382\n",
      "Iteration 49, loss = 0.34381625\n",
      "Iteration 50, loss = 0.34326758\n",
      "Iteration 51, loss = 0.34490970\n",
      "Iteration 52, loss = 0.31846456\n",
      "Iteration 53, loss = 0.30063706\n",
      "Iteration 54, loss = 0.29963011\n",
      "Iteration 55, loss = 0.29854815\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.20493718\n",
      "Iteration 2, loss = 1.24874055\n",
      "Iteration 3, loss = 0.68848596\n",
      "Iteration 4, loss = 0.65431934\n",
      "Iteration 5, loss = 0.55234845\n",
      "Iteration 6, loss = 0.44776078\n",
      "Iteration 7, loss = 0.41949532\n",
      "Iteration 8, loss = 0.42678617\n",
      "Iteration 9, loss = 0.41404491\n",
      "Iteration 10, loss = 0.38038650\n",
      "Iteration 11, loss = 0.35349940\n",
      "Iteration 12, loss = 0.33875485\n",
      "Iteration 13, loss = 0.33006761\n",
      "Iteration 14, loss = 0.32319232\n",
      "Iteration 15, loss = 0.33327784\n",
      "Iteration 16, loss = 0.32767605\n",
      "Iteration 17, loss = 0.31053308\n",
      "Iteration 18, loss = 0.30046424\n",
      "Iteration 19, loss = 0.30194549\n",
      "Iteration 20, loss = 0.32541545\n",
      "Iteration 21, loss = 0.31433573\n",
      "Iteration 22, loss = 0.31075971\n",
      "Iteration 23, loss = 0.31143736\n",
      "Iteration 24, loss = 0.31446788\n",
      "Iteration 25, loss = 0.32752265\n",
      "Iteration 26, loss = 0.33232438\n",
      "Iteration 27, loss = 0.29920685\n",
      "Iteration 28, loss = 0.29979258\n",
      "Iteration 29, loss = 0.31110913\n",
      "Iteration 30, loss = 0.31278585\n",
      "Iteration 31, loss = 0.34289716\n",
      "Iteration 32, loss = 0.33179404\n",
      "Iteration 33, loss = 0.29877321\n",
      "Iteration 34, loss = 0.29979535\n",
      "Iteration 35, loss = 0.30285564\n",
      "Iteration 36, loss = 0.30287045\n",
      "Iteration 37, loss = 0.29986969\n",
      "Iteration 38, loss = 0.29628559\n",
      "Iteration 39, loss = 0.29271968\n",
      "Iteration 40, loss = 0.30047610\n",
      "Iteration 41, loss = 0.31544998\n",
      "Iteration 42, loss = 0.32299088\n",
      "Iteration 43, loss = 0.31201259\n",
      "Iteration 44, loss = 0.28746262\n",
      "Iteration 45, loss = 0.29783793\n",
      "Iteration 46, loss = 0.31310570\n",
      "Iteration 47, loss = 0.32517372\n",
      "Iteration 48, loss = 0.34843054\n",
      "Iteration 49, loss = 0.34102874\n",
      "Iteration 50, loss = 0.35402883\n",
      "Iteration 51, loss = 0.35279135\n",
      "Iteration 52, loss = 0.32086353\n",
      "Iteration 53, loss = 0.29328794\n",
      "Iteration 54, loss = 0.30673335\n",
      "Iteration 55, loss = 0.29120812\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.20154204\n",
      "Iteration 2, loss = 1.25125905\n",
      "Iteration 3, loss = 0.68541072\n",
      "Iteration 4, loss = 0.64061706\n",
      "Iteration 5, loss = 0.52487653\n",
      "Iteration 6, loss = 0.42851303\n",
      "Iteration 7, loss = 0.41356512\n",
      "Iteration 8, loss = 0.42124142\n",
      "Iteration 9, loss = 0.40293577\n",
      "Iteration 10, loss = 0.37286046\n",
      "Iteration 11, loss = 0.34581315\n",
      "Iteration 12, loss = 0.33233073\n",
      "Iteration 13, loss = 0.32997143\n",
      "Iteration 14, loss = 0.32154188\n",
      "Iteration 15, loss = 0.32008951\n",
      "Iteration 16, loss = 0.31473671\n",
      "Iteration 17, loss = 0.30450755\n",
      "Iteration 18, loss = 0.30151587\n",
      "Iteration 19, loss = 0.30489663\n",
      "Iteration 20, loss = 0.31605739\n",
      "Iteration 21, loss = 0.30373881\n",
      "Iteration 22, loss = 0.31293319\n",
      "Iteration 23, loss = 0.30574640\n",
      "Iteration 24, loss = 0.29551762\n",
      "Iteration 25, loss = 0.33057927\n",
      "Iteration 26, loss = 0.33596411\n",
      "Iteration 27, loss = 0.30117859\n",
      "Iteration 28, loss = 0.29148742\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 29, loss = 0.30817291\n",
      "Iteration 30, loss = 0.30407885\n",
      "Iteration 31, loss = 0.31755282\n",
      "Iteration 32, loss = 0.33528483\n",
      "Iteration 33, loss = 0.31556895\n",
      "Iteration 34, loss = 0.29261476\n",
      "Iteration 35, loss = 0.29249384\n",
      "Iteration 36, loss = 0.31825724\n",
      "Iteration 37, loss = 0.29846044\n",
      "Iteration 38, loss = 0.29146685\n",
      "Iteration 39, loss = 0.29774704\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.39855318\n",
      "Iteration 2, loss = 1.50666333\n",
      "Iteration 3, loss = 0.91590031\n",
      "Iteration 4, loss = 0.66031982\n",
      "Iteration 5, loss = 0.60101576\n",
      "Iteration 6, loss = 0.53864455\n",
      "Iteration 7, loss = 0.46376427\n",
      "Iteration 8, loss = 0.44215499\n",
      "Iteration 9, loss = 0.45032962\n",
      "Iteration 10, loss = 0.42550351\n",
      "Iteration 11, loss = 0.38385050\n",
      "Iteration 12, loss = 0.36819227\n",
      "Iteration 13, loss = 0.37713290\n",
      "Iteration 14, loss = 0.35909593\n",
      "Iteration 15, loss = 0.33112434\n",
      "Iteration 16, loss = 0.34942242\n",
      "Iteration 17, loss = 0.34178357\n",
      "Iteration 18, loss = 0.30831844\n",
      "Iteration 19, loss = 0.30708296\n",
      "Iteration 20, loss = 0.31501007\n",
      "Iteration 21, loss = 0.30584844\n",
      "Iteration 22, loss = 0.29927329\n",
      "Iteration 23, loss = 0.30778564\n",
      "Iteration 24, loss = 0.31232952\n",
      "Iteration 25, loss = 0.33029089\n",
      "Iteration 26, loss = 0.35111089\n",
      "Iteration 27, loss = 0.35525040\n",
      "Iteration 28, loss = 0.33772144\n",
      "Iteration 29, loss = 0.32087788\n",
      "Iteration 30, loss = 0.29120605\n",
      "Iteration 31, loss = 0.30406250\n",
      "Iteration 32, loss = 0.34662818\n",
      "Iteration 33, loss = 0.34807156\n",
      "Iteration 34, loss = 0.32262387\n",
      "Iteration 35, loss = 0.30402470\n",
      "Iteration 36, loss = 0.30775103\n",
      "Iteration 37, loss = 0.29130886\n",
      "Iteration 38, loss = 0.32448431\n",
      "Iteration 39, loss = 0.37044910\n",
      "Iteration 40, loss = 0.35787709\n",
      "Iteration 41, loss = 0.31079605\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.38657485\n",
      "Iteration 2, loss = 1.51203544\n",
      "Iteration 3, loss = 0.90932782\n",
      "Iteration 4, loss = 0.72011683\n",
      "Iteration 5, loss = 0.60600702\n",
      "Iteration 6, loss = 0.50922468\n",
      "Iteration 7, loss = 0.46653665\n",
      "Iteration 8, loss = 0.45681629\n",
      "Iteration 9, loss = 0.44000611\n",
      "Iteration 10, loss = 0.42335780\n",
      "Iteration 11, loss = 0.40260421\n",
      "Iteration 12, loss = 0.37861026\n",
      "Iteration 13, loss = 0.38698408\n",
      "Iteration 14, loss = 0.36895164\n",
      "Iteration 15, loss = 0.34193989\n",
      "Iteration 16, loss = 0.35555352\n",
      "Iteration 17, loss = 0.35540545\n",
      "Iteration 18, loss = 0.33906717\n",
      "Iteration 19, loss = 0.34539544\n",
      "Iteration 20, loss = 0.32697876\n",
      "Iteration 21, loss = 0.31233380\n",
      "Iteration 22, loss = 0.30597185\n",
      "Iteration 23, loss = 0.30531934\n",
      "Iteration 24, loss = 0.29195059\n",
      "Iteration 25, loss = 0.33947231\n",
      "Iteration 26, loss = 0.36613068\n",
      "Iteration 27, loss = 0.32630474\n",
      "Iteration 28, loss = 0.35508569\n",
      "Iteration 29, loss = 0.38392469\n",
      "Iteration 30, loss = 0.36422760\n",
      "Iteration 31, loss = 0.33132801\n",
      "Iteration 32, loss = 0.32317155\n",
      "Iteration 33, loss = 0.33218330\n",
      "Iteration 34, loss = 0.32542961\n",
      "Iteration 35, loss = 0.30199815\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.39261357\n",
      "Iteration 2, loss = 1.60837314\n",
      "Iteration 3, loss = 0.86662929\n",
      "Iteration 4, loss = 0.74887089\n",
      "Iteration 5, loss = 0.61779149\n",
      "Iteration 6, loss = 0.49403168\n",
      "Iteration 7, loss = 0.45742621\n",
      "Iteration 8, loss = 0.45197128\n",
      "Iteration 9, loss = 0.43026904\n",
      "Iteration 10, loss = 0.41189986\n",
      "Iteration 11, loss = 0.39428414\n",
      "Iteration 12, loss = 0.36747316\n",
      "Iteration 13, loss = 0.36812560\n",
      "Iteration 14, loss = 0.36511559\n",
      "Iteration 15, loss = 0.35169388\n",
      "Iteration 16, loss = 0.33910399\n",
      "Iteration 17, loss = 0.33974923\n",
      "Iteration 18, loss = 0.33934608\n",
      "Iteration 19, loss = 0.34138114\n",
      "Iteration 20, loss = 0.32176571\n",
      "Iteration 21, loss = 0.31382501\n",
      "Iteration 22, loss = 0.30683651\n",
      "Iteration 23, loss = 0.30749906\n",
      "Iteration 24, loss = 0.28590717\n",
      "Iteration 25, loss = 0.31765504\n",
      "Iteration 26, loss = 0.33783537\n",
      "Iteration 27, loss = 0.31272886\n",
      "Iteration 28, loss = 0.34169577\n",
      "Iteration 29, loss = 0.37259093\n",
      "Iteration 30, loss = 0.33863647\n",
      "Iteration 31, loss = 0.31944349\n",
      "Iteration 32, loss = 0.33406229\n",
      "Iteration 33, loss = 0.33522981\n",
      "Iteration 34, loss = 0.31473447\n",
      "Iteration 35, loss = 0.28812022\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.38841041\n",
      "Iteration 2, loss = 1.53042885\n",
      "Iteration 3, loss = 0.93538700\n",
      "Iteration 4, loss = 0.70790246\n",
      "Iteration 5, loss = 0.59780194\n",
      "Iteration 6, loss = 0.52007682\n",
      "Iteration 7, loss = 0.47330378\n",
      "Iteration 8, loss = 0.45794128\n",
      "Iteration 9, loss = 0.43245848\n",
      "Iteration 10, loss = 0.40432366\n",
      "Iteration 11, loss = 0.38540825\n",
      "Iteration 12, loss = 0.37008634\n",
      "Iteration 13, loss = 0.36715438\n",
      "Iteration 14, loss = 0.34866386\n",
      "Iteration 15, loss = 0.34349746\n",
      "Iteration 16, loss = 0.34435826\n",
      "Iteration 17, loss = 0.33514755\n",
      "Iteration 18, loss = 0.33839891\n",
      "Iteration 19, loss = 0.35365528\n",
      "Iteration 20, loss = 0.33410874\n",
      "Iteration 21, loss = 0.31614786\n",
      "Iteration 22, loss = 0.30387201\n",
      "Iteration 23, loss = 0.30959444\n",
      "Iteration 24, loss = 0.29676232\n",
      "Iteration 25, loss = 0.30028895\n",
      "Iteration 26, loss = 0.31458830\n",
      "Iteration 27, loss = 0.28585482\n",
      "Iteration 28, loss = 0.29106421\n",
      "Iteration 29, loss = 0.32700437\n",
      "Iteration 30, loss = 0.33785676\n",
      "Iteration 31, loss = 0.31853315\n",
      "Iteration 32, loss = 0.30177143\n",
      "Iteration 33, loss = 0.28886853\n",
      "Iteration 34, loss = 0.28505868\n",
      "Iteration 35, loss = 0.28880033\n",
      "Iteration 36, loss = 0.27932360\n",
      "Iteration 37, loss = 0.28669186\n",
      "Iteration 38, loss = 0.30007679\n",
      "Iteration 39, loss = 0.27916823\n",
      "Iteration 40, loss = 0.28800526\n",
      "Iteration 41, loss = 0.28679744\n",
      "Iteration 42, loss = 0.28550062\n",
      "Iteration 43, loss = 0.29536831\n",
      "Iteration 44, loss = 0.28483044\n",
      "Iteration 45, loss = 0.28802415\n",
      "Iteration 46, loss = 0.27835660\n",
      "Iteration 47, loss = 0.28646177\n",
      "Iteration 48, loss = 0.28764761\n",
      "Iteration 49, loss = 0.30083066\n",
      "Iteration 50, loss = 0.31198573\n",
      "Iteration 51, loss = 0.30553968\n",
      "Iteration 52, loss = 0.29505859\n",
      "Iteration 53, loss = 0.29118184\n",
      "Iteration 54, loss = 0.30377793\n",
      "Iteration 55, loss = 0.30043361\n",
      "Iteration 56, loss = 0.28983331\n",
      "Iteration 57, loss = 0.28116493\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.39283573\n",
      "Iteration 2, loss = 1.47908281\n",
      "Iteration 3, loss = 0.88567130\n",
      "Iteration 4, loss = 0.68601457\n",
      "Iteration 5, loss = 0.57985556\n",
      "Iteration 6, loss = 0.49987306\n",
      "Iteration 7, loss = 0.46460556\n",
      "Iteration 8, loss = 0.44739136\n",
      "Iteration 9, loss = 0.41554621\n",
      "Iteration 10, loss = 0.38998689\n",
      "Iteration 11, loss = 0.37301974\n",
      "Iteration 12, loss = 0.35766533\n",
      "Iteration 13, loss = 0.35526537\n",
      "Iteration 14, loss = 0.33839828\n",
      "Iteration 15, loss = 0.33680776\n",
      "Iteration 16, loss = 0.34033242\n",
      "Iteration 17, loss = 0.33250704\n",
      "Iteration 18, loss = 0.32900773\n",
      "Iteration 19, loss = 0.33278032\n",
      "Iteration 20, loss = 0.33758350\n",
      "Iteration 21, loss = 0.32265437\n",
      "Iteration 22, loss = 0.30339975\n",
      "Iteration 23, loss = 0.30221921\n",
      "Iteration 24, loss = 0.29749246\n",
      "Iteration 25, loss = 0.31717897\n",
      "Iteration 26, loss = 0.32449314\n",
      "Iteration 27, loss = 0.28677080\n",
      "Iteration 28, loss = 0.30263697\n",
      "Iteration 29, loss = 0.31722441\n",
      "Iteration 30, loss = 0.31051927\n",
      "Iteration 31, loss = 0.29458882\n",
      "Iteration 32, loss = 0.29401516\n",
      "Iteration 33, loss = 0.30753442\n",
      "Iteration 34, loss = 0.30286176\n",
      "Iteration 35, loss = 0.29779546\n",
      "Iteration 36, loss = 0.30117298\n",
      "Iteration 37, loss = 0.30859218\n",
      "Iteration 38, loss = 0.31034710\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.40104024\n",
      "Iteration 2, loss = 1.54487750\n",
      "Iteration 3, loss = 0.93900872\n",
      "Iteration 4, loss = 0.72544494\n",
      "Iteration 5, loss = 0.60710231\n",
      "Iteration 6, loss = 0.51261215\n",
      "Iteration 7, loss = 0.46626884\n",
      "Iteration 8, loss = 0.45158030\n",
      "Iteration 9, loss = 0.41882685\n",
      "Iteration 10, loss = 0.39146375\n",
      "Iteration 11, loss = 0.37816829\n",
      "Iteration 12, loss = 0.35677957\n",
      "Iteration 13, loss = 0.34915068\n",
      "Iteration 14, loss = 0.33239603\n",
      "Iteration 15, loss = 0.32937888\n",
      "Iteration 16, loss = 0.33861058\n",
      "Iteration 17, loss = 0.33195570\n",
      "Iteration 18, loss = 0.32242093\n",
      "Iteration 19, loss = 0.31347316\n",
      "Iteration 20, loss = 0.31304097\n",
      "Iteration 21, loss = 0.30818192\n",
      "Iteration 22, loss = 0.29750739\n",
      "Iteration 23, loss = 0.29706231\n",
      "Iteration 24, loss = 0.30062267\n",
      "Iteration 25, loss = 0.30104424\n",
      "Iteration 26, loss = 0.30198348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 27, loss = 0.28799326\n",
      "Iteration 28, loss = 0.28889240\n",
      "Iteration 29, loss = 0.28520103\n",
      "Iteration 30, loss = 0.27824381\n",
      "Iteration 31, loss = 0.27560186\n",
      "Iteration 32, loss = 0.28182801\n",
      "Iteration 33, loss = 0.28192231\n",
      "Iteration 34, loss = 0.27749733\n",
      "Iteration 35, loss = 0.28637511\n",
      "Iteration 36, loss = 0.29020044\n",
      "Iteration 37, loss = 0.29834340\n",
      "Iteration 38, loss = 0.30537714\n",
      "Iteration 39, loss = 0.29043641\n",
      "Iteration 40, loss = 0.29346659\n",
      "Iteration 41, loss = 0.30792004\n",
      "Iteration 42, loss = 0.31566123\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.39659522\n",
      "Iteration 2, loss = 1.42998033\n",
      "Iteration 3, loss = 0.81199985\n",
      "Iteration 4, loss = 0.65886285\n",
      "Iteration 5, loss = 0.56378693\n",
      "Iteration 6, loss = 0.47661439\n",
      "Iteration 7, loss = 0.43185546\n",
      "Iteration 8, loss = 0.40741790\n",
      "Iteration 9, loss = 0.38405581\n",
      "Iteration 10, loss = 0.37109909\n",
      "Iteration 11, loss = 0.36490149\n",
      "Iteration 12, loss = 0.34780984\n",
      "Iteration 13, loss = 0.33751883\n",
      "Iteration 14, loss = 0.32245852\n",
      "Iteration 15, loss = 0.32215698\n",
      "Iteration 16, loss = 0.32407074\n",
      "Iteration 17, loss = 0.31112137\n",
      "Iteration 18, loss = 0.29847132\n",
      "Iteration 19, loss = 0.29734027\n",
      "Iteration 20, loss = 0.30409491\n",
      "Iteration 21, loss = 0.30447093\n",
      "Iteration 22, loss = 0.29331212\n",
      "Iteration 23, loss = 0.29297865\n",
      "Iteration 24, loss = 0.33106530\n",
      "Iteration 25, loss = 0.32051471\n",
      "Iteration 26, loss = 0.30429261\n",
      "Iteration 27, loss = 0.29765146\n",
      "Iteration 28, loss = 0.30704903\n",
      "Iteration 29, loss = 0.30068430\n",
      "Iteration 30, loss = 0.29441579\n",
      "Iteration 31, loss = 0.29224295\n",
      "Iteration 32, loss = 0.29516477\n",
      "Iteration 33, loss = 0.29012329\n",
      "Iteration 34, loss = 0.29173292\n",
      "Iteration 35, loss = 0.30870271\n",
      "Iteration 36, loss = 0.29481064\n",
      "Iteration 37, loss = 0.29152840\n",
      "Iteration 38, loss = 0.30768329\n",
      "Iteration 39, loss = 0.28954080\n",
      "Iteration 40, loss = 0.30099274\n",
      "Iteration 41, loss = 0.31498427\n",
      "Iteration 42, loss = 0.33641634\n",
      "Iteration 43, loss = 0.32575161\n",
      "Iteration 44, loss = 0.30011593\n",
      "Iteration 45, loss = 0.30901725\n",
      "Iteration 46, loss = 0.31758846\n",
      "Iteration 47, loss = 0.31765065\n",
      "Iteration 48, loss = 0.30684882\n",
      "Iteration 49, loss = 0.32121271\n",
      "Iteration 50, loss = 0.32159896\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.39207604\n",
      "Iteration 2, loss = 1.40746700\n",
      "Iteration 3, loss = 0.83261961\n",
      "Iteration 4, loss = 0.66738725\n",
      "Iteration 5, loss = 0.56232317\n",
      "Iteration 6, loss = 0.48372415\n",
      "Iteration 7, loss = 0.44702756\n",
      "Iteration 8, loss = 0.42623231\n",
      "Iteration 9, loss = 0.40020997\n",
      "Iteration 10, loss = 0.38837292\n",
      "Iteration 11, loss = 0.37820252\n",
      "Iteration 12, loss = 0.35552985\n",
      "Iteration 13, loss = 0.34727262\n",
      "Iteration 14, loss = 0.33754025\n",
      "Iteration 15, loss = 0.33840438\n",
      "Iteration 16, loss = 0.33638843\n",
      "Iteration 17, loss = 0.32080135\n",
      "Iteration 18, loss = 0.31392725\n",
      "Iteration 19, loss = 0.32066832\n",
      "Iteration 20, loss = 0.32299218\n",
      "Iteration 21, loss = 0.32621154\n",
      "Iteration 22, loss = 0.31890478\n",
      "Iteration 23, loss = 0.30874997\n",
      "Iteration 24, loss = 0.33168795\n",
      "Iteration 25, loss = 0.32842860\n",
      "Iteration 26, loss = 0.31957561\n",
      "Iteration 27, loss = 0.31938227\n",
      "Iteration 28, loss = 0.33053035\n",
      "Iteration 29, loss = 0.30887111\n",
      "Iteration 30, loss = 0.30359373\n",
      "Iteration 31, loss = 0.31469422\n",
      "Iteration 32, loss = 0.31612246\n",
      "Iteration 33, loss = 0.30026268\n",
      "Iteration 34, loss = 0.30805138\n",
      "Iteration 35, loss = 0.34104582\n",
      "Iteration 36, loss = 0.31747150\n",
      "Iteration 37, loss = 0.32764644\n",
      "Iteration 38, loss = 0.35078923\n",
      "Iteration 39, loss = 0.31917918\n",
      "Iteration 40, loss = 0.32083586\n",
      "Iteration 41, loss = 0.35307662\n",
      "Iteration 42, loss = 0.38209015\n",
      "Iteration 43, loss = 0.33575040\n",
      "Iteration 44, loss = 0.29629768\n",
      "Iteration 45, loss = 0.30271525\n",
      "Iteration 46, loss = 0.32119325\n",
      "Iteration 47, loss = 0.32736305\n",
      "Iteration 48, loss = 0.31069948\n",
      "Iteration 49, loss = 0.33615385\n",
      "Iteration 50, loss = 0.33349389\n",
      "Iteration 51, loss = 0.29305699\n",
      "Iteration 52, loss = 0.31177828\n",
      "Iteration 53, loss = 0.35253784\n",
      "Iteration 54, loss = 0.32677792\n",
      "Iteration 55, loss = 0.29776386\n",
      "Iteration 56, loss = 0.29924757\n",
      "Iteration 57, loss = 0.31088863\n",
      "Iteration 58, loss = 0.30581548\n",
      "Iteration 59, loss = 0.30287774\n",
      "Iteration 60, loss = 0.29359325\n",
      "Iteration 61, loss = 0.30715992\n",
      "Iteration 62, loss = 0.31815510\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.39400769\n",
      "Iteration 2, loss = 1.35239230\n",
      "Iteration 3, loss = 0.85469783\n",
      "Iteration 4, loss = 0.62432879\n",
      "Iteration 5, loss = 0.56373201\n",
      "Iteration 6, loss = 0.49560009\n",
      "Iteration 7, loss = 0.44060501\n",
      "Iteration 8, loss = 0.42031167\n",
      "Iteration 9, loss = 0.40458375\n",
      "Iteration 10, loss = 0.38698665\n",
      "Iteration 11, loss = 0.36454837\n",
      "Iteration 12, loss = 0.34875433\n",
      "Iteration 13, loss = 0.34641264\n",
      "Iteration 14, loss = 0.33368880\n",
      "Iteration 15, loss = 0.33225930\n",
      "Iteration 16, loss = 0.32273871\n",
      "Iteration 17, loss = 0.30705860\n",
      "Iteration 18, loss = 0.30459098\n",
      "Iteration 19, loss = 0.30476319\n",
      "Iteration 20, loss = 0.30960589\n",
      "Iteration 21, loss = 0.31371475\n",
      "Iteration 22, loss = 0.30949329\n",
      "Iteration 23, loss = 0.30116827\n",
      "Iteration 24, loss = 0.31269752\n",
      "Iteration 25, loss = 0.30478293\n",
      "Iteration 26, loss = 0.31011211\n",
      "Iteration 27, loss = 0.30364342\n",
      "Iteration 28, loss = 0.30287629\n",
      "Iteration 29, loss = 0.29826915\n",
      "Iteration 30, loss = 0.30409689\n",
      "Iteration 31, loss = 0.31702678\n",
      "Iteration 32, loss = 0.30830564\n",
      "Iteration 33, loss = 0.29019871\n",
      "Iteration 34, loss = 0.30221278\n",
      "Iteration 35, loss = 0.32271234\n",
      "Iteration 36, loss = 0.31279264\n",
      "Iteration 37, loss = 0.32996844\n",
      "Iteration 38, loss = 0.34022035\n",
      "Iteration 39, loss = 0.31066857\n",
      "Iteration 40, loss = 0.31998349\n",
      "Iteration 41, loss = 0.34361095\n",
      "Iteration 42, loss = 0.35666914\n",
      "Iteration 43, loss = 0.31638680\n",
      "Iteration 44, loss = 0.29798401\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.39876648\n",
      "Iteration 2, loss = 1.36710168\n",
      "Iteration 3, loss = 0.80943342\n",
      "Iteration 4, loss = 0.61667982\n",
      "Iteration 5, loss = 0.55458047\n",
      "Iteration 6, loss = 0.46957319\n",
      "Iteration 7, loss = 0.42357705\n",
      "Iteration 8, loss = 0.40376662\n",
      "Iteration 9, loss = 0.38090695\n",
      "Iteration 10, loss = 0.36047100\n",
      "Iteration 11, loss = 0.34711269\n",
      "Iteration 12, loss = 0.33619792\n",
      "Iteration 13, loss = 0.32065388\n",
      "Iteration 14, loss = 0.31609116\n",
      "Iteration 15, loss = 0.32788679\n",
      "Iteration 16, loss = 0.31329056\n",
      "Iteration 17, loss = 0.29707414\n",
      "Iteration 18, loss = 0.30018840\n",
      "Iteration 19, loss = 0.30143211\n",
      "Iteration 20, loss = 0.30201151\n",
      "Iteration 21, loss = 0.30853224\n",
      "Iteration 22, loss = 0.30324709\n",
      "Iteration 23, loss = 0.30410754\n",
      "Iteration 24, loss = 0.31023817\n",
      "Iteration 25, loss = 0.30471155\n",
      "Iteration 26, loss = 0.31870442\n",
      "Iteration 27, loss = 0.31497281\n",
      "Iteration 28, loss = 0.31817669\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.25555115\n",
      "Iteration 2, loss = 1.27343040\n",
      "Iteration 3, loss = 0.82068291\n",
      "Iteration 4, loss = 0.70495914\n",
      "Iteration 5, loss = 0.54732311\n",
      "Iteration 6, loss = 0.49743783\n",
      "Iteration 7, loss = 0.49207162\n",
      "Iteration 8, loss = 0.46451370\n",
      "Iteration 9, loss = 0.43743788\n",
      "Iteration 10, loss = 0.40520355\n",
      "Iteration 11, loss = 0.37573799\n",
      "Iteration 12, loss = 0.38478591\n",
      "Iteration 13, loss = 0.38470650\n",
      "Iteration 14, loss = 0.35359577\n",
      "Iteration 15, loss = 0.34749226\n",
      "Iteration 16, loss = 0.33539283\n",
      "Iteration 17, loss = 0.31338616\n",
      "Iteration 18, loss = 0.30265221\n",
      "Iteration 19, loss = 0.31953538\n",
      "Iteration 20, loss = 0.32956377\n",
      "Iteration 21, loss = 0.32191907\n",
      "Iteration 22, loss = 0.31695007\n",
      "Iteration 23, loss = 0.31973926\n",
      "Iteration 24, loss = 0.31096379\n",
      "Iteration 25, loss = 0.31186640\n",
      "Iteration 26, loss = 0.33396969\n",
      "Iteration 27, loss = 0.34313278\n",
      "Iteration 28, loss = 0.32403742\n",
      "Iteration 29, loss = 0.31541944\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.22514677\n",
      "Iteration 2, loss = 1.24931976\n",
      "Iteration 3, loss = 0.79526893\n",
      "Iteration 4, loss = 0.64532207\n",
      "Iteration 5, loss = 0.51895087\n",
      "Iteration 6, loss = 0.47793616\n",
      "Iteration 7, loss = 0.43466793\n",
      "Iteration 8, loss = 0.40635559\n",
      "Iteration 9, loss = 0.38390196\n",
      "Iteration 10, loss = 0.39015682\n",
      "Iteration 11, loss = 0.38506295\n",
      "Iteration 12, loss = 0.36849046\n",
      "Iteration 13, loss = 0.35330658\n",
      "Iteration 14, loss = 0.33892262\n",
      "Iteration 15, loss = 0.32266125\n",
      "Iteration 16, loss = 0.31925078\n",
      "Iteration 17, loss = 0.32863341\n",
      "Iteration 18, loss = 0.34060792\n",
      "Iteration 19, loss = 0.33680881\n",
      "Iteration 20, loss = 0.30781257\n",
      "Iteration 21, loss = 0.31021198\n",
      "Iteration 22, loss = 0.30511263\n",
      "Iteration 23, loss = 0.29588884\n",
      "Iteration 24, loss = 0.31825821\n",
      "Iteration 25, loss = 0.34187217\n",
      "Iteration 26, loss = 0.34278392\n",
      "Iteration 27, loss = 0.32149405\n",
      "Iteration 28, loss = 0.33731050\n",
      "Iteration 29, loss = 0.32730942\n",
      "Iteration 30, loss = 0.28756854\n",
      "Iteration 31, loss = 0.32179862\n",
      "Iteration 32, loss = 0.34453545\n",
      "Iteration 33, loss = 0.32128252\n",
      "Iteration 34, loss = 0.31198824\n",
      "Iteration 35, loss = 0.31853450\n",
      "Iteration 36, loss = 0.30604957\n",
      "Iteration 37, loss = 0.30195053\n",
      "Iteration 38, loss = 0.29603460\n",
      "Iteration 39, loss = 0.38678166\n",
      "Iteration 40, loss = 0.38839078\n",
      "Iteration 41, loss = 0.33716753\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.23661880\n",
      "Iteration 2, loss = 1.27937827\n",
      "Iteration 3, loss = 0.81801156\n",
      "Iteration 4, loss = 0.66393466\n",
      "Iteration 5, loss = 0.51375081\n",
      "Iteration 6, loss = 0.46488417\n",
      "Iteration 7, loss = 0.43521677\n",
      "Iteration 8, loss = 0.41337414\n",
      "Iteration 9, loss = 0.38733970\n",
      "Iteration 10, loss = 0.38946880\n",
      "Iteration 11, loss = 0.38519913\n",
      "Iteration 12, loss = 0.37041045\n",
      "Iteration 13, loss = 0.34896526\n",
      "Iteration 14, loss = 0.33861379\n",
      "Iteration 15, loss = 0.33824633\n",
      "Iteration 16, loss = 0.33581928\n",
      "Iteration 17, loss = 0.31721693\n",
      "Iteration 18, loss = 0.34684883\n",
      "Iteration 19, loss = 0.36012894\n",
      "Iteration 20, loss = 0.32143616\n",
      "Iteration 21, loss = 0.30837231\n",
      "Iteration 22, loss = 0.31971851\n",
      "Iteration 23, loss = 0.31633495\n",
      "Iteration 24, loss = 0.31881988\n",
      "Iteration 25, loss = 0.34195474\n",
      "Iteration 26, loss = 0.36537176\n",
      "Iteration 27, loss = 0.32005615\n",
      "Iteration 28, loss = 0.31141123\n",
      "Iteration 29, loss = 0.34635619\n",
      "Iteration 30, loss = 0.30829381\n",
      "Iteration 31, loss = 0.29178783\n",
      "Iteration 32, loss = 0.32295531\n",
      "Iteration 33, loss = 0.32467378\n",
      "Iteration 34, loss = 0.30869186\n",
      "Iteration 35, loss = 0.31574211\n",
      "Iteration 36, loss = 0.32392303\n",
      "Iteration 37, loss = 0.31959829\n",
      "Iteration 38, loss = 0.28991824\n",
      "Iteration 39, loss = 0.37013712\n",
      "Iteration 40, loss = 0.37928585\n",
      "Iteration 41, loss = 0.32580869\n",
      "Iteration 42, loss = 0.31391495\n",
      "Iteration 43, loss = 0.34287952\n",
      "Iteration 44, loss = 0.37867733\n",
      "Iteration 45, loss = 0.37716582\n",
      "Iteration 46, loss = 0.33594959\n",
      "Iteration 47, loss = 0.29713923\n",
      "Iteration 48, loss = 0.31420531\n",
      "Iteration 49, loss = 0.33568555\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.22718006\n",
      "Iteration 2, loss = 1.23577913\n",
      "Iteration 3, loss = 0.77480411\n",
      "Iteration 4, loss = 0.65397233\n",
      "Iteration 5, loss = 0.50708928\n",
      "Iteration 6, loss = 0.45996718\n",
      "Iteration 7, loss = 0.43408922\n",
      "Iteration 8, loss = 0.40830044\n",
      "Iteration 9, loss = 0.38901295\n",
      "Iteration 10, loss = 0.39129927\n",
      "Iteration 11, loss = 0.39149554\n",
      "Iteration 12, loss = 0.37316130\n",
      "Iteration 13, loss = 0.34034886\n",
      "Iteration 14, loss = 0.32311702\n",
      "Iteration 15, loss = 0.32924279\n",
      "Iteration 16, loss = 0.33253155\n",
      "Iteration 17, loss = 0.31680640\n",
      "Iteration 18, loss = 0.32973234\n",
      "Iteration 19, loss = 0.34812993\n",
      "Iteration 20, loss = 0.30652577\n",
      "Iteration 21, loss = 0.29317575\n",
      "Iteration 22, loss = 0.32361823\n",
      "Iteration 23, loss = 0.32185536\n",
      "Iteration 24, loss = 0.29886390\n",
      "Iteration 25, loss = 0.30769569\n",
      "Iteration 26, loss = 0.34263055\n",
      "Iteration 27, loss = 0.30257766\n",
      "Iteration 28, loss = 0.30932369\n",
      "Iteration 29, loss = 0.34704696\n",
      "Iteration 30, loss = 0.30413175\n",
      "Iteration 31, loss = 0.29085434\n",
      "Iteration 32, loss = 0.31260921\n",
      "Iteration 33, loss = 0.31337772\n",
      "Iteration 34, loss = 0.29788844\n",
      "Iteration 35, loss = 0.29738045\n",
      "Iteration 36, loss = 0.30069967\n",
      "Iteration 37, loss = 0.30304434\n",
      "Iteration 38, loss = 0.28301422\n",
      "Iteration 39, loss = 0.34036501\n",
      "Iteration 40, loss = 0.34441321\n",
      "Iteration 41, loss = 0.29868803\n",
      "Iteration 42, loss = 0.29763817\n",
      "Iteration 43, loss = 0.32608917\n",
      "Iteration 44, loss = 0.35785143\n",
      "Iteration 45, loss = 0.36532909\n",
      "Iteration 46, loss = 0.32578143\n",
      "Iteration 47, loss = 0.29159733\n",
      "Iteration 48, loss = 0.30080108\n",
      "Iteration 49, loss = 0.32171044\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.24208910\n",
      "Iteration 2, loss = 1.31211755\n",
      "Iteration 3, loss = 0.79699245\n",
      "Iteration 4, loss = 0.69144528\n",
      "Iteration 5, loss = 0.54724589\n",
      "Iteration 6, loss = 0.48829586\n",
      "Iteration 7, loss = 0.45324176\n",
      "Iteration 8, loss = 0.42744692\n",
      "Iteration 9, loss = 0.40417120\n",
      "Iteration 10, loss = 0.39853527\n",
      "Iteration 11, loss = 0.38305259\n",
      "Iteration 12, loss = 0.36720377\n",
      "Iteration 13, loss = 0.34753639\n",
      "Iteration 14, loss = 0.32695815\n",
      "Iteration 15, loss = 0.33566080\n",
      "Iteration 16, loss = 0.34936090\n",
      "Iteration 17, loss = 0.32315615\n",
      "Iteration 18, loss = 0.32609327\n",
      "Iteration 19, loss = 0.33575772\n",
      "Iteration 20, loss = 0.30747618\n",
      "Iteration 21, loss = 0.29926993\n",
      "Iteration 22, loss = 0.33237514\n",
      "Iteration 23, loss = 0.33688842\n",
      "Iteration 24, loss = 0.30458497\n",
      "Iteration 25, loss = 0.30632631\n",
      "Iteration 26, loss = 0.35422095\n",
      "Iteration 27, loss = 0.32360144\n",
      "Iteration 28, loss = 0.31182959\n",
      "Iteration 29, loss = 0.34132341\n",
      "Iteration 30, loss = 0.32877310\n",
      "Iteration 31, loss = 0.32057156\n",
      "Iteration 32, loss = 0.30315249\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.23463535\n",
      "Iteration 2, loss = 1.35992355\n",
      "Iteration 3, loss = 0.78590716\n",
      "Iteration 4, loss = 0.69248402\n",
      "Iteration 5, loss = 0.55830158\n",
      "Iteration 6, loss = 0.48154905\n",
      "Iteration 7, loss = 0.44126272\n",
      "Iteration 8, loss = 0.41609213\n",
      "Iteration 9, loss = 0.40006451\n",
      "Iteration 10, loss = 0.38549525\n",
      "Iteration 11, loss = 0.36035445\n",
      "Iteration 12, loss = 0.35007629\n",
      "Iteration 13, loss = 0.34563354\n",
      "Iteration 14, loss = 0.32460722\n",
      "Iteration 15, loss = 0.32766333\n",
      "Iteration 16, loss = 0.33825893\n",
      "Iteration 17, loss = 0.30980149\n",
      "Iteration 18, loss = 0.30554841\n",
      "Iteration 19, loss = 0.31586665\n",
      "Iteration 20, loss = 0.31497779\n",
      "Iteration 21, loss = 0.29741619\n",
      "Iteration 22, loss = 0.29376345\n",
      "Iteration 23, loss = 0.33005957\n",
      "Iteration 24, loss = 0.33058911\n",
      "Iteration 25, loss = 0.30129196\n",
      "Iteration 26, loss = 0.32940632\n",
      "Iteration 27, loss = 0.29963074\n",
      "Iteration 28, loss = 0.29562225\n",
      "Iteration 29, loss = 0.33677399\n",
      "Iteration 30, loss = 0.31286900\n",
      "Iteration 31, loss = 0.29034582\n",
      "Iteration 32, loss = 0.29789291\n",
      "Iteration 33, loss = 0.31759021\n",
      "Iteration 34, loss = 0.32087839\n",
      "Iteration 35, loss = 0.31543492\n",
      "Iteration 36, loss = 0.31415035\n",
      "Iteration 37, loss = 0.31194537\n",
      "Iteration 38, loss = 0.29231368\n",
      "Iteration 39, loss = 0.32080219\n",
      "Iteration 40, loss = 0.33857720\n",
      "Iteration 41, loss = 0.30587596\n",
      "Iteration 42, loss = 0.30637051\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.26015049\n",
      "Iteration 2, loss = 1.38780278\n",
      "Iteration 3, loss = 0.82561735\n",
      "Iteration 4, loss = 0.73509208\n",
      "Iteration 5, loss = 0.55623498\n",
      "Iteration 6, loss = 0.48694120\n",
      "Iteration 7, loss = 0.45943444\n",
      "Iteration 8, loss = 0.42480882\n",
      "Iteration 9, loss = 0.40016950\n",
      "Iteration 10, loss = 0.38991383\n",
      "Iteration 11, loss = 0.37875610\n",
      "Iteration 12, loss = 0.37082446\n",
      "Iteration 13, loss = 0.35704897\n",
      "Iteration 14, loss = 0.33314158\n",
      "Iteration 15, loss = 0.33240735\n",
      "Iteration 16, loss = 0.34269230\n",
      "Iteration 17, loss = 0.32567243\n",
      "Iteration 18, loss = 0.32448789\n",
      "Iteration 19, loss = 0.33123591\n",
      "Iteration 20, loss = 0.31754352\n",
      "Iteration 21, loss = 0.30039968\n",
      "Iteration 22, loss = 0.31077596\n",
      "Iteration 23, loss = 0.37508844\n",
      "Iteration 24, loss = 0.32829367\n",
      "Iteration 25, loss = 0.30928143\n",
      "Iteration 26, loss = 0.37320180\n",
      "Iteration 27, loss = 0.35719341\n",
      "Iteration 28, loss = 0.30585486\n",
      "Iteration 29, loss = 0.32395784\n",
      "Iteration 30, loss = 0.32902829\n",
      "Iteration 31, loss = 0.30369641\n",
      "Iteration 32, loss = 0.30389197\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.26102510\n",
      "Iteration 2, loss = 1.49100867\n",
      "Iteration 3, loss = 0.86351990\n",
      "Iteration 4, loss = 0.76441282\n",
      "Iteration 5, loss = 0.61224817\n",
      "Iteration 6, loss = 0.53317381\n",
      "Iteration 7, loss = 0.49198593\n",
      "Iteration 8, loss = 0.45713002\n",
      "Iteration 9, loss = 0.43207064\n",
      "Iteration 10, loss = 0.41381251\n",
      "Iteration 11, loss = 0.39338181\n",
      "Iteration 12, loss = 0.38341659\n",
      "Iteration 13, loss = 0.37201952\n",
      "Iteration 14, loss = 0.35532957\n",
      "Iteration 15, loss = 0.35878512\n",
      "Iteration 16, loss = 0.36563668\n",
      "Iteration 17, loss = 0.34279655\n",
      "Iteration 18, loss = 0.34082543\n",
      "Iteration 19, loss = 0.34389902\n",
      "Iteration 20, loss = 0.32668064\n",
      "Iteration 21, loss = 0.31418362\n",
      "Iteration 22, loss = 0.32326040\n",
      "Iteration 23, loss = 0.37803148\n",
      "Iteration 24, loss = 0.34007293\n",
      "Iteration 25, loss = 0.31298496\n",
      "Iteration 26, loss = 0.36508224\n",
      "Iteration 27, loss = 0.36799871\n",
      "Iteration 28, loss = 0.31513148\n",
      "Iteration 29, loss = 0.34438157\n",
      "Iteration 30, loss = 0.37974399\n",
      "Iteration 31, loss = 0.34063008\n",
      "Iteration 32, loss = 0.31771884\n",
      "Iteration 33, loss = 0.33470273\n",
      "Iteration 34, loss = 0.34807319\n",
      "Iteration 35, loss = 0.34020187\n",
      "Iteration 36, loss = 0.34461991\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.25270164\n",
      "Iteration 2, loss = 1.46559126\n",
      "Iteration 3, loss = 0.82530232\n",
      "Iteration 4, loss = 0.74489218\n",
      "Iteration 5, loss = 0.60440954\n",
      "Iteration 6, loss = 0.53315000\n",
      "Iteration 7, loss = 0.49061348\n",
      "Iteration 8, loss = 0.45582678\n",
      "Iteration 9, loss = 0.43223742\n",
      "Iteration 10, loss = 0.41556561\n",
      "Iteration 11, loss = 0.39138847\n",
      "Iteration 12, loss = 0.38315806\n",
      "Iteration 13, loss = 0.38098532\n",
      "Iteration 14, loss = 0.35918439\n",
      "Iteration 15, loss = 0.35172264\n",
      "Iteration 16, loss = 0.35591026\n",
      "Iteration 17, loss = 0.35080724\n",
      "Iteration 18, loss = 0.34213409\n",
      "Iteration 19, loss = 0.33247994\n",
      "Iteration 20, loss = 0.32116090\n",
      "Iteration 21, loss = 0.31978822\n",
      "Iteration 22, loss = 0.32826648\n",
      "Iteration 23, loss = 0.37377073\n",
      "Iteration 24, loss = 0.35492108\n",
      "Iteration 25, loss = 0.33209672\n",
      "Iteration 26, loss = 0.36641174\n",
      "Iteration 27, loss = 0.37582313\n",
      "Iteration 28, loss = 0.31646482\n",
      "Iteration 29, loss = 0.33907342\n",
      "Iteration 30, loss = 0.38302967\n",
      "Iteration 31, loss = 0.33526386\n",
      "Iteration 32, loss = 0.31326267\n",
      "Iteration 33, loss = 0.33910004\n",
      "Iteration 34, loss = 0.35042496\n",
      "Iteration 35, loss = 0.32913741\n",
      "Iteration 36, loss = 0.32823524\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 37, loss = 0.34651160\n",
      "Iteration 38, loss = 0.34098889\n",
      "Iteration 39, loss = 0.33174646\n",
      "Iteration 40, loss = 0.34226258\n",
      "Iteration 41, loss = 0.31754720\n",
      "Iteration 42, loss = 0.29725080\n",
      "Iteration 43, loss = 0.30451392\n",
      "Iteration 44, loss = 0.31043615\n",
      "Iteration 45, loss = 0.31937289\n",
      "Iteration 46, loss = 0.31799229\n",
      "Iteration 47, loss = 0.30332038\n",
      "Iteration 48, loss = 0.30588184\n",
      "Iteration 49, loss = 0.30562018\n",
      "Iteration 50, loss = 0.31147020\n",
      "Iteration 51, loss = 0.30298746\n",
      "Iteration 52, loss = 0.29569083\n",
      "Iteration 53, loss = 0.32685208\n",
      "Iteration 54, loss = 0.29914651\n",
      "Iteration 55, loss = 0.29830497\n",
      "Iteration 56, loss = 0.32532310\n",
      "Iteration 57, loss = 0.32350017\n",
      "Iteration 58, loss = 0.33838808\n",
      "Iteration 59, loss = 0.32558575\n",
      "Iteration 60, loss = 0.29275273\n",
      "Iteration 61, loss = 0.29202718\n",
      "Iteration 62, loss = 0.29644100\n",
      "Iteration 63, loss = 0.29355458\n",
      "Iteration 64, loss = 0.30588160\n",
      "Iteration 65, loss = 0.29801489\n",
      "Iteration 66, loss = 0.28850366\n",
      "Iteration 67, loss = 0.30717440\n",
      "Iteration 68, loss = 0.30877060\n",
      "Iteration 69, loss = 0.29199266\n",
      "Iteration 70, loss = 0.27516013\n",
      "Iteration 71, loss = 0.29021151\n",
      "Iteration 72, loss = 0.31617392\n",
      "Iteration 73, loss = 0.31057074\n",
      "Iteration 74, loss = 0.29531486\n",
      "Iteration 75, loss = 0.30420437\n",
      "Iteration 76, loss = 0.29605486\n",
      "Iteration 77, loss = 0.31110741\n",
      "Iteration 78, loss = 0.32361889\n",
      "Iteration 79, loss = 0.31259312\n",
      "Iteration 80, loss = 0.32460284\n",
      "Iteration 81, loss = 0.33473482\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.25208927\n",
      "Iteration 2, loss = 1.47236931\n",
      "Iteration 3, loss = 0.83573541\n",
      "Iteration 4, loss = 0.77088230\n",
      "Iteration 5, loss = 0.60490981\n",
      "Iteration 6, loss = 0.50751436\n",
      "Iteration 7, loss = 0.46116262\n",
      "Iteration 8, loss = 0.43599904\n",
      "Iteration 9, loss = 0.41869872\n",
      "Iteration 10, loss = 0.40295568\n",
      "Iteration 11, loss = 0.38204557\n",
      "Iteration 12, loss = 0.37634607\n",
      "Iteration 13, loss = 0.37217488\n",
      "Iteration 14, loss = 0.35617336\n",
      "Iteration 15, loss = 0.34819872\n",
      "Iteration 16, loss = 0.34335966\n",
      "Iteration 17, loss = 0.33184315\n",
      "Iteration 18, loss = 0.33016898\n",
      "Iteration 19, loss = 0.31705770\n",
      "Iteration 20, loss = 0.30602446\n",
      "Iteration 21, loss = 0.31042554\n",
      "Iteration 22, loss = 0.30900430\n",
      "Iteration 23, loss = 0.32584406\n",
      "Iteration 24, loss = 0.31215321\n",
      "Iteration 25, loss = 0.31341672\n",
      "Iteration 26, loss = 0.34610127\n",
      "Iteration 27, loss = 0.37540857\n",
      "Iteration 28, loss = 0.31505226\n",
      "Iteration 29, loss = 0.30539566\n",
      "Iteration 30, loss = 0.35081207\n",
      "Iteration 31, loss = 0.32468169\n",
      "Iteration 32, loss = 0.31662985\n",
      "Iteration 33, loss = 0.33248495\n",
      "Iteration 34, loss = 0.33275427\n",
      "Iteration 35, loss = 0.31725414\n",
      "Iteration 36, loss = 0.30560904\n",
      "Iteration 37, loss = 0.32686982\n",
      "Iteration 38, loss = 0.33523834\n",
      "Iteration 39, loss = 0.32720192\n",
      "Iteration 40, loss = 0.31384478\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.50702099\n",
      "Iteration 2, loss = 1.70754175\n",
      "Iteration 3, loss = 0.99885296\n",
      "Iteration 4, loss = 0.64903355\n",
      "Iteration 5, loss = 0.71207024\n",
      "Iteration 6, loss = 0.62723847\n",
      "Iteration 7, loss = 0.49602478\n",
      "Iteration 8, loss = 0.44092318\n",
      "Iteration 9, loss = 0.43835388\n",
      "Iteration 10, loss = 0.44379839\n",
      "Iteration 11, loss = 0.42527962\n",
      "Iteration 12, loss = 0.38765677\n",
      "Iteration 13, loss = 0.36568973\n",
      "Iteration 14, loss = 0.35799546\n",
      "Iteration 15, loss = 0.34682244\n",
      "Iteration 16, loss = 0.33244914\n",
      "Iteration 17, loss = 0.32368680\n",
      "Iteration 18, loss = 0.33603486\n",
      "Iteration 19, loss = 0.33661794\n",
      "Iteration 20, loss = 0.31381057\n",
      "Iteration 21, loss = 0.30082277\n",
      "Iteration 22, loss = 0.29961087\n",
      "Iteration 23, loss = 0.31120614\n",
      "Iteration 24, loss = 0.32485838\n",
      "Iteration 25, loss = 0.32215715\n",
      "Iteration 26, loss = 0.31973588\n",
      "Iteration 27, loss = 0.32083944\n",
      "Iteration 28, loss = 0.32675515\n",
      "Iteration 29, loss = 0.32547830\n",
      "Iteration 30, loss = 0.32004074\n",
      "Iteration 31, loss = 0.33065565\n",
      "Iteration 32, loss = 0.32043744\n",
      "Iteration 33, loss = 0.31238270\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.52098313\n",
      "Iteration 2, loss = 1.44631164\n",
      "Iteration 3, loss = 0.78670717\n",
      "Iteration 4, loss = 0.61996028\n",
      "Iteration 5, loss = 0.61778101\n",
      "Iteration 6, loss = 0.53402572\n",
      "Iteration 7, loss = 0.46586929\n",
      "Iteration 8, loss = 0.42739025\n",
      "Iteration 9, loss = 0.40852626\n",
      "Iteration 10, loss = 0.39632093\n",
      "Iteration 11, loss = 0.38053588\n",
      "Iteration 12, loss = 0.36950532\n",
      "Iteration 13, loss = 0.35225550\n",
      "Iteration 14, loss = 0.35459585\n",
      "Iteration 15, loss = 0.35238369\n",
      "Iteration 16, loss = 0.33915876\n",
      "Iteration 17, loss = 0.32213891\n",
      "Iteration 18, loss = 0.30477394\n",
      "Iteration 19, loss = 0.30068667\n",
      "Iteration 20, loss = 0.30347503\n",
      "Iteration 21, loss = 0.31068512\n",
      "Iteration 22, loss = 0.32682946\n",
      "Iteration 23, loss = 0.32431446\n",
      "Iteration 24, loss = 0.29980237\n",
      "Iteration 25, loss = 0.30712464\n",
      "Iteration 26, loss = 0.32018786\n",
      "Iteration 27, loss = 0.30442994\n",
      "Iteration 28, loss = 0.30103183\n",
      "Iteration 29, loss = 0.30500034\n",
      "Iteration 30, loss = 0.30627650\n",
      "Iteration 31, loss = 0.29874596\n",
      "Iteration 32, loss = 0.31270929\n",
      "Iteration 33, loss = 0.32680599\n",
      "Iteration 34, loss = 0.32761785\n",
      "Iteration 35, loss = 0.31770965\n",
      "Iteration 36, loss = 0.31181806\n",
      "Iteration 37, loss = 0.31811415\n",
      "Iteration 38, loss = 0.29495239\n",
      "Iteration 39, loss = 0.29635018\n",
      "Iteration 40, loss = 0.30089602\n",
      "Iteration 41, loss = 0.28092282\n",
      "Iteration 42, loss = 0.29132080\n",
      "Iteration 43, loss = 0.30175212\n",
      "Iteration 44, loss = 0.31326285\n",
      "Iteration 45, loss = 0.29989447\n",
      "Iteration 46, loss = 0.30567778\n",
      "Iteration 47, loss = 0.29952355\n",
      "Iteration 48, loss = 0.27750960\n",
      "Iteration 49, loss = 0.29479271\n",
      "Iteration 50, loss = 0.29856743\n",
      "Iteration 51, loss = 0.29208770\n",
      "Iteration 52, loss = 0.29506604\n",
      "Iteration 53, loss = 0.29179811\n",
      "Iteration 54, loss = 0.32904780\n",
      "Iteration 55, loss = 0.32853724\n",
      "Iteration 56, loss = 0.30689336\n",
      "Iteration 57, loss = 0.32030648\n",
      "Iteration 58, loss = 0.33196590\n",
      "Iteration 59, loss = 0.30912843\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.51981122\n",
      "Iteration 2, loss = 1.43459151\n",
      "Iteration 3, loss = 0.77329231\n",
      "Iteration 4, loss = 0.60167579\n",
      "Iteration 5, loss = 0.58600596\n",
      "Iteration 6, loss = 0.52423885\n",
      "Iteration 7, loss = 0.46150647\n",
      "Iteration 8, loss = 0.42155615\n",
      "Iteration 9, loss = 0.40820648\n",
      "Iteration 10, loss = 0.39810494\n",
      "Iteration 11, loss = 0.38665050\n",
      "Iteration 12, loss = 0.38053714\n",
      "Iteration 13, loss = 0.36327227\n",
      "Iteration 14, loss = 0.35401738\n",
      "Iteration 15, loss = 0.34657857\n",
      "Iteration 16, loss = 0.32500393\n",
      "Iteration 17, loss = 0.30938260\n",
      "Iteration 18, loss = 0.30198470\n",
      "Iteration 19, loss = 0.30016398\n",
      "Iteration 20, loss = 0.29869024\n",
      "Iteration 21, loss = 0.30448183\n",
      "Iteration 22, loss = 0.32518793\n",
      "Iteration 23, loss = 0.33133684\n",
      "Iteration 24, loss = 0.29317159\n",
      "Iteration 25, loss = 0.29613840\n",
      "Iteration 26, loss = 0.31571940\n",
      "Iteration 27, loss = 0.30153519\n",
      "Iteration 28, loss = 0.30044560\n",
      "Iteration 29, loss = 0.30571799\n",
      "Iteration 30, loss = 0.30731983\n",
      "Iteration 31, loss = 0.30832129\n",
      "Iteration 32, loss = 0.31451504\n",
      "Iteration 33, loss = 0.31707133\n",
      "Iteration 34, loss = 0.31162301\n",
      "Iteration 35, loss = 0.31116689\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.52125917\n",
      "Iteration 2, loss = 1.38349827\n",
      "Iteration 3, loss = 0.70809736\n",
      "Iteration 4, loss = 0.57471886\n",
      "Iteration 5, loss = 0.57877495\n",
      "Iteration 6, loss = 0.49736536\n",
      "Iteration 7, loss = 0.42912457\n",
      "Iteration 8, loss = 0.41074653\n",
      "Iteration 9, loss = 0.40802078\n",
      "Iteration 10, loss = 0.39032627\n",
      "Iteration 11, loss = 0.37392732\n",
      "Iteration 12, loss = 0.38297063\n",
      "Iteration 13, loss = 0.36713782\n",
      "Iteration 14, loss = 0.35122197\n",
      "Iteration 15, loss = 0.33603223\n",
      "Iteration 16, loss = 0.31725980\n",
      "Iteration 17, loss = 0.30619453\n",
      "Iteration 18, loss = 0.30095398\n",
      "Iteration 19, loss = 0.29080891\n",
      "Iteration 20, loss = 0.28751117\n",
      "Iteration 21, loss = 0.29897641\n",
      "Iteration 22, loss = 0.30972477\n",
      "Iteration 23, loss = 0.31483591\n",
      "Iteration 24, loss = 0.28512319\n",
      "Iteration 25, loss = 0.28440275\n",
      "Iteration 26, loss = 0.29080560\n",
      "Iteration 27, loss = 0.28430944\n",
      "Iteration 28, loss = 0.28489726\n",
      "Iteration 29, loss = 0.29023191\n",
      "Iteration 30, loss = 0.29445420\n",
      "Iteration 31, loss = 0.29831980\n",
      "Iteration 32, loss = 0.30215025\n",
      "Iteration 33, loss = 0.29450382\n",
      "Iteration 34, loss = 0.29591494\n",
      "Iteration 35, loss = 0.29378646\n",
      "Iteration 36, loss = 0.30425083\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.52002150\n",
      "Iteration 2, loss = 1.36698563\n",
      "Iteration 3, loss = 0.70077182\n",
      "Iteration 4, loss = 0.60944499\n",
      "Iteration 5, loss = 0.58403861\n",
      "Iteration 6, loss = 0.49237708\n",
      "Iteration 7, loss = 0.42878955\n",
      "Iteration 8, loss = 0.41327820\n",
      "Iteration 9, loss = 0.41016252\n",
      "Iteration 10, loss = 0.39232856\n",
      "Iteration 11, loss = 0.36881374\n",
      "Iteration 12, loss = 0.36349933\n",
      "Iteration 13, loss = 0.34897678\n",
      "Iteration 14, loss = 0.34758128\n",
      "Iteration 15, loss = 0.34843686\n",
      "Iteration 16, loss = 0.33440000\n",
      "Iteration 17, loss = 0.31494496\n",
      "Iteration 18, loss = 0.29982095\n",
      "Iteration 19, loss = 0.29227776\n",
      "Iteration 20, loss = 0.29356303\n",
      "Iteration 21, loss = 0.29730436\n",
      "Iteration 22, loss = 0.32608561\n",
      "Iteration 23, loss = 0.32434613\n",
      "Iteration 24, loss = 0.28496857\n",
      "Iteration 25, loss = 0.28355367\n",
      "Iteration 26, loss = 0.29290909\n",
      "Iteration 27, loss = 0.29697233\n",
      "Iteration 28, loss = 0.30233321\n",
      "Iteration 29, loss = 0.29546580\n",
      "Iteration 30, loss = 0.31380492\n",
      "Iteration 31, loss = 0.32351577\n",
      "Iteration 32, loss = 0.31498951\n",
      "Iteration 33, loss = 0.28848671\n",
      "Iteration 34, loss = 0.28293291\n",
      "Iteration 35, loss = 0.28692622\n",
      "Iteration 36, loss = 0.31269380\n",
      "Iteration 37, loss = 0.29240993\n",
      "Iteration 38, loss = 0.29984841\n",
      "Iteration 39, loss = 0.31077834\n",
      "Iteration 40, loss = 0.29413970\n",
      "Iteration 41, loss = 0.29630681\n",
      "Iteration 42, loss = 0.31483264\n",
      "Iteration 43, loss = 0.31361257\n",
      "Iteration 44, loss = 0.30836763\n",
      "Iteration 45, loss = 0.29060956\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.50702219\n",
      "Iteration 2, loss = 1.45264616\n",
      "Iteration 3, loss = 0.75200634\n",
      "Iteration 4, loss = 0.60473144\n",
      "Iteration 5, loss = 0.60766171\n",
      "Iteration 6, loss = 0.49870970\n",
      "Iteration 7, loss = 0.41472354\n",
      "Iteration 8, loss = 0.40525400\n",
      "Iteration 9, loss = 0.41663689\n",
      "Iteration 10, loss = 0.40171905\n",
      "Iteration 11, loss = 0.36955614\n",
      "Iteration 12, loss = 0.34815349\n",
      "Iteration 13, loss = 0.33726890\n",
      "Iteration 14, loss = 0.33157377\n",
      "Iteration 15, loss = 0.32650742\n",
      "Iteration 16, loss = 0.31475936\n",
      "Iteration 17, loss = 0.30187498\n",
      "Iteration 18, loss = 0.29317751\n",
      "Iteration 19, loss = 0.28532289\n",
      "Iteration 20, loss = 0.28664973\n",
      "Iteration 21, loss = 0.28166291\n",
      "Iteration 22, loss = 0.28030957\n",
      "Iteration 23, loss = 0.29885484\n",
      "Iteration 24, loss = 0.28560381\n",
      "Iteration 25, loss = 0.27587374\n",
      "Iteration 26, loss = 0.28244942\n",
      "Iteration 27, loss = 0.28969342\n",
      "Iteration 28, loss = 0.27890492\n",
      "Iteration 29, loss = 0.28095556\n",
      "Iteration 30, loss = 0.30441627\n",
      "Iteration 31, loss = 0.31455776\n",
      "Iteration 32, loss = 0.29920354\n",
      "Iteration 33, loss = 0.28511974\n",
      "Iteration 34, loss = 0.28897633\n",
      "Iteration 35, loss = 0.30000194\n",
      "Iteration 36, loss = 0.30173115\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.50601636\n",
      "Iteration 2, loss = 1.46532646\n",
      "Iteration 3, loss = 0.76845543\n",
      "Iteration 4, loss = 0.63895454\n",
      "Iteration 5, loss = 0.62880977\n",
      "Iteration 6, loss = 0.50114597\n",
      "Iteration 7, loss = 0.42549908\n",
      "Iteration 8, loss = 0.42093292\n",
      "Iteration 9, loss = 0.41944367\n",
      "Iteration 10, loss = 0.39659100\n",
      "Iteration 11, loss = 0.35804063\n",
      "Iteration 12, loss = 0.33834343\n",
      "Iteration 13, loss = 0.33551788\n",
      "Iteration 14, loss = 0.33153507\n",
      "Iteration 15, loss = 0.32169034\n",
      "Iteration 16, loss = 0.31602912\n",
      "Iteration 17, loss = 0.31120070\n",
      "Iteration 18, loss = 0.30494161\n",
      "Iteration 19, loss = 0.29569812\n",
      "Iteration 20, loss = 0.29034779\n",
      "Iteration 21, loss = 0.29459111\n",
      "Iteration 22, loss = 0.28189415\n",
      "Iteration 23, loss = 0.30546754\n",
      "Iteration 24, loss = 0.29980297\n",
      "Iteration 25, loss = 0.28191551\n",
      "Iteration 26, loss = 0.28110754\n",
      "Iteration 27, loss = 0.29748089\n",
      "Iteration 28, loss = 0.28789656\n",
      "Iteration 29, loss = 0.28976713\n",
      "Iteration 30, loss = 0.30675031\n",
      "Iteration 31, loss = 0.31763024\n",
      "Iteration 32, loss = 0.31867493\n",
      "Iteration 33, loss = 0.30187547\n",
      "Iteration 34, loss = 0.29671470\n",
      "Iteration 35, loss = 0.31339909\n",
      "Iteration 36, loss = 0.31186988\n",
      "Iteration 37, loss = 0.30440585\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.50493774\n",
      "Iteration 2, loss = 1.48430507\n",
      "Iteration 3, loss = 0.79125445\n",
      "Iteration 4, loss = 0.63742908\n",
      "Iteration 5, loss = 0.66086126\n",
      "Iteration 6, loss = 0.53079273\n",
      "Iteration 7, loss = 0.44714224\n",
      "Iteration 8, loss = 0.44023559\n",
      "Iteration 9, loss = 0.43574638\n",
      "Iteration 10, loss = 0.40488313\n",
      "Iteration 11, loss = 0.36322163\n",
      "Iteration 12, loss = 0.34762981\n",
      "Iteration 13, loss = 0.34000826\n",
      "Iteration 14, loss = 0.33807142\n",
      "Iteration 15, loss = 0.33581767\n",
      "Iteration 16, loss = 0.32605324\n",
      "Iteration 17, loss = 0.31713425\n",
      "Iteration 18, loss = 0.31373645\n",
      "Iteration 19, loss = 0.30982203\n",
      "Iteration 20, loss = 0.29896980\n",
      "Iteration 21, loss = 0.30122299\n",
      "Iteration 22, loss = 0.29148834\n",
      "Iteration 23, loss = 0.30568619\n",
      "Iteration 24, loss = 0.29750579\n",
      "Iteration 25, loss = 0.29368606\n",
      "Iteration 26, loss = 0.30422462\n",
      "Iteration 27, loss = 0.31898253\n",
      "Iteration 28, loss = 0.31759912\n",
      "Iteration 29, loss = 0.30861835\n",
      "Iteration 30, loss = 0.31867839\n",
      "Iteration 31, loss = 0.32970704\n",
      "Iteration 32, loss = 0.33260743\n",
      "Iteration 33, loss = 0.31456328\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.50080673\n",
      "Iteration 2, loss = 1.46737157\n",
      "Iteration 3, loss = 0.77185024\n",
      "Iteration 4, loss = 0.62400177\n",
      "Iteration 5, loss = 0.65866694\n",
      "Iteration 6, loss = 0.53354424\n",
      "Iteration 7, loss = 0.43822457\n",
      "Iteration 8, loss = 0.42873287\n",
      "Iteration 9, loss = 0.43854168\n",
      "Iteration 10, loss = 0.40871499\n",
      "Iteration 11, loss = 0.36845040\n",
      "Iteration 12, loss = 0.35048149\n",
      "Iteration 13, loss = 0.33927176\n",
      "Iteration 14, loss = 0.34628703\n",
      "Iteration 15, loss = 0.34637761\n",
      "Iteration 16, loss = 0.33202937\n",
      "Iteration 17, loss = 0.31561735\n",
      "Iteration 18, loss = 0.30998295\n",
      "Iteration 19, loss = 0.30975095\n",
      "Iteration 20, loss = 0.30368911\n",
      "Iteration 21, loss = 0.30868198\n",
      "Iteration 22, loss = 0.30010441\n",
      "Iteration 23, loss = 0.30889896\n",
      "Iteration 24, loss = 0.29786953\n",
      "Iteration 25, loss = 0.28051238\n",
      "Iteration 26, loss = 0.28197006\n",
      "Iteration 27, loss = 0.28938451\n",
      "Iteration 28, loss = 0.29642945\n",
      "Iteration 29, loss = 0.30098268\n",
      "Iteration 30, loss = 0.31855116\n",
      "Iteration 31, loss = 0.30958038\n",
      "Iteration 32, loss = 0.29354766\n",
      "Iteration 33, loss = 0.29454805\n",
      "Iteration 34, loss = 0.31645429\n",
      "Iteration 35, loss = 0.31409932\n",
      "Iteration 36, loss = 0.30179239\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.50338947\n",
      "Iteration 2, loss = 1.51282085\n",
      "Iteration 3, loss = 0.79641402\n",
      "Iteration 4, loss = 0.63047091\n",
      "Iteration 5, loss = 0.67659912\n",
      "Iteration 6, loss = 0.53297329\n",
      "Iteration 7, loss = 0.43417872\n",
      "Iteration 8, loss = 0.42839063\n",
      "Iteration 9, loss = 0.43773533\n",
      "Iteration 10, loss = 0.40618149\n",
      "Iteration 11, loss = 0.35502655\n",
      "Iteration 12, loss = 0.32813048\n",
      "Iteration 13, loss = 0.32421578\n",
      "Iteration 14, loss = 0.33702433\n",
      "Iteration 15, loss = 0.33658695\n",
      "Iteration 16, loss = 0.32018468\n",
      "Iteration 17, loss = 0.30621591\n",
      "Iteration 18, loss = 0.30559023\n",
      "Iteration 19, loss = 0.30203772\n",
      "Iteration 20, loss = 0.29258470\n",
      "Iteration 21, loss = 0.28383051\n",
      "Iteration 22, loss = 0.28505483\n",
      "Iteration 23, loss = 0.28573633\n",
      "Iteration 24, loss = 0.28736912\n",
      "Iteration 25, loss = 0.27924041\n",
      "Iteration 26, loss = 0.28479058\n",
      "Iteration 27, loss = 0.28650222\n",
      "Iteration 28, loss = 0.31292897\n",
      "Iteration 29, loss = 0.31188414\n",
      "Iteration 30, loss = 0.30913353\n",
      "Iteration 31, loss = 0.29878218\n",
      "Iteration 32, loss = 0.28592002\n",
      "Iteration 33, loss = 0.28716766\n",
      "Iteration 34, loss = 0.31566735\n",
      "Iteration 35, loss = 0.31099265\n",
      "Iteration 36, loss = 0.29481004\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.07648302\n",
      "Iteration 2, loss = 1.43590485\n",
      "Iteration 3, loss = 0.72769040\n",
      "Iteration 4, loss = 0.78821197\n",
      "Iteration 5, loss = 0.64091946\n",
      "Iteration 6, loss = 0.46792060\n",
      "Iteration 7, loss = 0.47412418\n",
      "Iteration 8, loss = 0.48761147\n",
      "Iteration 9, loss = 0.43012487\n",
      "Iteration 10, loss = 0.37660836\n",
      "Iteration 11, loss = 0.34928735\n",
      "Iteration 12, loss = 0.35373069\n",
      "Iteration 13, loss = 0.36167011\n",
      "Iteration 14, loss = 0.35689070\n",
      "Iteration 15, loss = 0.34065000\n",
      "Iteration 16, loss = 0.32734195\n",
      "Iteration 17, loss = 0.33682027\n",
      "Iteration 18, loss = 0.35214307\n",
      "Iteration 19, loss = 0.35403623\n",
      "Iteration 20, loss = 0.33100769\n",
      "Iteration 21, loss = 0.30041842\n",
      "Iteration 22, loss = 0.29147734\n",
      "Iteration 23, loss = 0.30135532\n",
      "Iteration 24, loss = 0.31104815\n",
      "Iteration 25, loss = 0.31816852\n",
      "Iteration 26, loss = 0.32863170\n",
      "Iteration 27, loss = 0.31964242\n",
      "Iteration 28, loss = 0.30641750\n",
      "Iteration 29, loss = 0.32910248\n",
      "Iteration 30, loss = 0.32618395\n",
      "Iteration 31, loss = 0.31822599\n",
      "Iteration 32, loss = 0.33333083\n",
      "Iteration 33, loss = 0.32156428\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.09622413\n",
      "Iteration 2, loss = 1.10849896\n",
      "Iteration 3, loss = 0.66621755\n",
      "Iteration 4, loss = 0.58836389\n",
      "Iteration 5, loss = 0.44559092\n",
      "Iteration 6, loss = 0.46826705\n",
      "Iteration 7, loss = 0.45552316\n",
      "Iteration 8, loss = 0.39463964\n",
      "Iteration 9, loss = 0.39173633\n",
      "Iteration 10, loss = 0.39982833\n",
      "Iteration 11, loss = 0.36895768\n",
      "Iteration 12, loss = 0.34129320\n",
      "Iteration 13, loss = 0.33952044\n",
      "Iteration 14, loss = 0.34130358\n",
      "Iteration 15, loss = 0.32531800\n",
      "Iteration 16, loss = 0.33669151\n",
      "Iteration 17, loss = 0.34999921\n",
      "Iteration 18, loss = 0.35785994\n",
      "Iteration 19, loss = 0.33511360\n",
      "Iteration 20, loss = 0.31217475\n",
      "Iteration 21, loss = 0.31616309\n",
      "Iteration 22, loss = 0.34152260\n",
      "Iteration 23, loss = 0.33325303\n",
      "Iteration 24, loss = 0.33359507\n",
      "Iteration 25, loss = 0.31223912\n",
      "Iteration 26, loss = 0.33496039\n",
      "Iteration 27, loss = 0.36439852\n",
      "Iteration 28, loss = 0.32708446\n",
      "Iteration 29, loss = 0.31681336\n",
      "Iteration 30, loss = 0.35193843\n",
      "Iteration 31, loss = 0.32967501\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.10742135\n",
      "Iteration 2, loss = 1.14057587\n",
      "Iteration 3, loss = 0.71670759\n",
      "Iteration 4, loss = 0.58922249\n",
      "Iteration 5, loss = 0.45154094\n",
      "Iteration 6, loss = 0.46677016\n",
      "Iteration 7, loss = 0.46341195\n",
      "Iteration 8, loss = 0.39841394\n",
      "Iteration 9, loss = 0.37386612\n",
      "Iteration 10, loss = 0.38442881\n",
      "Iteration 11, loss = 0.36953318\n",
      "Iteration 12, loss = 0.34788534\n",
      "Iteration 13, loss = 0.34521848\n",
      "Iteration 14, loss = 0.34181327\n",
      "Iteration 15, loss = 0.33339761\n",
      "Iteration 16, loss = 0.34943515\n",
      "Iteration 17, loss = 0.34910914\n",
      "Iteration 18, loss = 0.33236732\n",
      "Iteration 19, loss = 0.32638666\n",
      "Iteration 20, loss = 0.32690430\n",
      "Iteration 21, loss = 0.32056068\n",
      "Iteration 22, loss = 0.32831334\n",
      "Iteration 23, loss = 0.30846004\n",
      "Iteration 24, loss = 0.32163369\n",
      "Iteration 25, loss = 0.33498615\n",
      "Iteration 26, loss = 0.33383368\n",
      "Iteration 27, loss = 0.33986594\n",
      "Iteration 28, loss = 0.32553786\n",
      "Iteration 29, loss = 0.29972758\n",
      "Iteration 30, loss = 0.30835563\n",
      "Iteration 31, loss = 0.30707043\n",
      "Iteration 32, loss = 0.29067115\n",
      "Iteration 33, loss = 0.29906202\n",
      "Iteration 34, loss = 0.31918938\n",
      "Iteration 35, loss = 0.32417055\n",
      "Iteration 36, loss = 0.29750571\n",
      "Iteration 37, loss = 0.29582448\n",
      "Iteration 38, loss = 0.31188074\n",
      "Iteration 39, loss = 0.29460710\n",
      "Iteration 40, loss = 0.29632969\n",
      "Iteration 41, loss = 0.30454403\n",
      "Iteration 42, loss = 0.29803418\n",
      "Iteration 43, loss = 0.29681566\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.09797713\n",
      "Iteration 2, loss = 1.20786347\n",
      "Iteration 3, loss = 0.66244100\n",
      "Iteration 4, loss = 0.59977118\n",
      "Iteration 5, loss = 0.46467522\n",
      "Iteration 6, loss = 0.43651588\n",
      "Iteration 7, loss = 0.45069867\n",
      "Iteration 8, loss = 0.41263806\n",
      "Iteration 9, loss = 0.36938243\n",
      "Iteration 10, loss = 0.35694657\n",
      "Iteration 11, loss = 0.35369291\n",
      "Iteration 12, loss = 0.33970265\n",
      "Iteration 13, loss = 0.32368706\n",
      "Iteration 14, loss = 0.33718097\n",
      "Iteration 15, loss = 0.32639775\n",
      "Iteration 16, loss = 0.32788083\n",
      "Iteration 17, loss = 0.33021233\n",
      "Iteration 18, loss = 0.31304736\n",
      "Iteration 19, loss = 0.30725378\n",
      "Iteration 20, loss = 0.31030922\n",
      "Iteration 21, loss = 0.29557134\n",
      "Iteration 22, loss = 0.30258693\n",
      "Iteration 23, loss = 0.30019080\n",
      "Iteration 24, loss = 0.31818146\n",
      "Iteration 25, loss = 0.32634892\n",
      "Iteration 26, loss = 0.33172274\n",
      "Iteration 27, loss = 0.31911644\n",
      "Iteration 28, loss = 0.31427564\n",
      "Iteration 29, loss = 0.31113577\n",
      "Iteration 30, loss = 0.31991689\n",
      "Iteration 31, loss = 0.31160293\n",
      "Iteration 32, loss = 0.28735883\n",
      "Iteration 33, loss = 0.28849588\n",
      "Iteration 34, loss = 0.29200688\n",
      "Iteration 35, loss = 0.28732879\n",
      "Iteration 36, loss = 0.28046140\n",
      "Iteration 37, loss = 0.28355773\n",
      "Iteration 38, loss = 0.28949948\n",
      "Iteration 39, loss = 0.27540711\n",
      "Iteration 40, loss = 0.27275391\n",
      "Iteration 41, loss = 0.27712766\n",
      "Iteration 42, loss = 0.27694324\n",
      "Iteration 43, loss = 0.26957534\n",
      "Iteration 44, loss = 0.27395155\n",
      "Iteration 45, loss = 0.29035500\n",
      "Iteration 46, loss = 0.27808925\n",
      "Iteration 47, loss = 0.26870747\n",
      "Iteration 48, loss = 0.28721269\n",
      "Iteration 49, loss = 0.28453602\n",
      "Iteration 50, loss = 0.27719578\n",
      "Iteration 51, loss = 0.26586348\n",
      "Iteration 52, loss = 0.28918769\n",
      "Iteration 53, loss = 0.28457906\n",
      "Iteration 54, loss = 0.32885417\n",
      "Iteration 55, loss = 0.32238055\n",
      "Iteration 56, loss = 0.34960258\n",
      "Iteration 57, loss = 0.34982152\n",
      "Iteration 58, loss = 0.36130365\n",
      "Iteration 59, loss = 0.34483316\n",
      "Iteration 60, loss = 0.31081298\n",
      "Iteration 61, loss = 0.32958527\n",
      "Iteration 62, loss = 0.32294781\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.12331457\n",
      "Iteration 2, loss = 1.18477615\n",
      "Iteration 3, loss = 0.70247629\n",
      "Iteration 4, loss = 0.57055455\n",
      "Iteration 5, loss = 0.45592642\n",
      "Iteration 6, loss = 0.44977879\n",
      "Iteration 7, loss = 0.41758677\n",
      "Iteration 8, loss = 0.38146295\n",
      "Iteration 9, loss = 0.36039886\n",
      "Iteration 10, loss = 0.35353861\n",
      "Iteration 11, loss = 0.34208867\n",
      "Iteration 12, loss = 0.33089603\n",
      "Iteration 13, loss = 0.32951817\n",
      "Iteration 14, loss = 0.34766884\n",
      "Iteration 15, loss = 0.32836259\n",
      "Iteration 16, loss = 0.31522291\n",
      "Iteration 17, loss = 0.31491607\n",
      "Iteration 18, loss = 0.30808982\n",
      "Iteration 19, loss = 0.30181786\n",
      "Iteration 20, loss = 0.29664981\n",
      "Iteration 21, loss = 0.30312527\n",
      "Iteration 22, loss = 0.30497773\n",
      "Iteration 23, loss = 0.29794982\n",
      "Iteration 24, loss = 0.30831182\n",
      "Iteration 25, loss = 0.30924537\n",
      "Iteration 26, loss = 0.31271722\n",
      "Iteration 27, loss = 0.30018247\n",
      "Iteration 28, loss = 0.32783246\n",
      "Iteration 29, loss = 0.31252920\n",
      "Iteration 30, loss = 0.30487632\n",
      "Iteration 31, loss = 0.32304923\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.11368457\n",
      "Iteration 2, loss = 1.16090410\n",
      "Iteration 3, loss = 0.70414328\n",
      "Iteration 4, loss = 0.56395994\n",
      "Iteration 5, loss = 0.44754190\n",
      "Iteration 6, loss = 0.42274534\n",
      "Iteration 7, loss = 0.40890923\n",
      "Iteration 8, loss = 0.37397377\n",
      "Iteration 9, loss = 0.34643247\n",
      "Iteration 10, loss = 0.34141115\n",
      "Iteration 11, loss = 0.32947971\n",
      "Iteration 12, loss = 0.36413825\n",
      "Iteration 13, loss = 0.33934743\n",
      "Iteration 14, loss = 0.32520782\n",
      "Iteration 15, loss = 0.31358790\n",
      "Iteration 16, loss = 0.29554561\n",
      "Iteration 17, loss = 0.28751441\n",
      "Iteration 18, loss = 0.29239804\n",
      "Iteration 19, loss = 0.29843687\n",
      "Iteration 20, loss = 0.30766032\n",
      "Iteration 21, loss = 0.30544601\n",
      "Iteration 22, loss = 0.29839966\n",
      "Iteration 23, loss = 0.30745494\n",
      "Iteration 24, loss = 0.29800960\n",
      "Iteration 25, loss = 0.30443991\n",
      "Iteration 26, loss = 0.31795961\n",
      "Iteration 27, loss = 0.31011031\n",
      "Iteration 28, loss = 0.28976813\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.09988216\n",
      "Iteration 2, loss = 1.12584354\n",
      "Iteration 3, loss = 0.66346475\n",
      "Iteration 4, loss = 0.56312927\n",
      "Iteration 5, loss = 0.45372704\n",
      "Iteration 6, loss = 0.42373157\n",
      "Iteration 7, loss = 0.38590509\n",
      "Iteration 8, loss = 0.35279454\n",
      "Iteration 9, loss = 0.33142442\n",
      "Iteration 10, loss = 0.35145637\n",
      "Iteration 11, loss = 0.33230831\n",
      "Iteration 12, loss = 0.35613517\n",
      "Iteration 13, loss = 0.36101188\n",
      "Iteration 14, loss = 0.33066079\n",
      "Iteration 15, loss = 0.29935372\n",
      "Iteration 16, loss = 0.30387627\n",
      "Iteration 17, loss = 0.31012017\n",
      "Iteration 18, loss = 0.31898446\n",
      "Iteration 19, loss = 0.31424746\n",
      "Iteration 20, loss = 0.32457509\n",
      "Iteration 21, loss = 0.33709383\n",
      "Iteration 22, loss = 0.32994594\n",
      "Iteration 23, loss = 0.34205006\n",
      "Iteration 24, loss = 0.31798128\n",
      "Iteration 25, loss = 0.29747279\n",
      "Iteration 26, loss = 0.31539238\n",
      "Iteration 27, loss = 0.30029565\n",
      "Iteration 28, loss = 0.30447627\n",
      "Iteration 29, loss = 0.28963939\n",
      "Iteration 30, loss = 0.29922132\n",
      "Iteration 31, loss = 0.29807980\n",
      "Iteration 32, loss = 0.29273178\n",
      "Iteration 33, loss = 0.29688235\n",
      "Iteration 34, loss = 0.29384006\n",
      "Iteration 35, loss = 0.28208033\n",
      "Iteration 36, loss = 0.27891819\n",
      "Iteration 37, loss = 0.30645153\n",
      "Iteration 38, loss = 0.30778987\n",
      "Iteration 39, loss = 0.30213154\n",
      "Iteration 40, loss = 0.32172479\n",
      "Iteration 41, loss = 0.31030790\n",
      "Iteration 42, loss = 0.30591513\n",
      "Iteration 43, loss = 0.29556032\n",
      "Iteration 44, loss = 0.28588056\n",
      "Iteration 45, loss = 0.31944926\n",
      "Iteration 46, loss = 0.29605715\n",
      "Iteration 47, loss = 0.27026933\n",
      "Iteration 48, loss = 0.30650045\n",
      "Iteration 49, loss = 0.30130958\n",
      "Iteration 50, loss = 0.27261996\n",
      "Iteration 51, loss = 0.29657397\n",
      "Iteration 52, loss = 0.33335013\n",
      "Iteration 53, loss = 0.29334468\n",
      "Iteration 54, loss = 0.37529997\n",
      "Iteration 55, loss = 0.36737247\n",
      "Iteration 56, loss = 0.36136609\n",
      "Iteration 57, loss = 0.33798684\n",
      "Iteration 58, loss = 0.31856499\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.10008627\n",
      "Iteration 2, loss = 1.13712052\n",
      "Iteration 3, loss = 0.68235110\n",
      "Iteration 4, loss = 0.58871546\n",
      "Iteration 5, loss = 0.47965078\n",
      "Iteration 6, loss = 0.45897739\n",
      "Iteration 7, loss = 0.40370779\n",
      "Iteration 8, loss = 0.36732034\n",
      "Iteration 9, loss = 0.34535917\n",
      "Iteration 10, loss = 0.34667705\n",
      "Iteration 11, loss = 0.33197569\n",
      "Iteration 12, loss = 0.35241862\n",
      "Iteration 13, loss = 0.36607609\n",
      "Iteration 14, loss = 0.35637845\n",
      "Iteration 15, loss = 0.31634976\n",
      "Iteration 16, loss = 0.34257246\n",
      "Iteration 17, loss = 0.33148407\n",
      "Iteration 18, loss = 0.34169231\n",
      "Iteration 19, loss = 0.34890325\n",
      "Iteration 20, loss = 0.38058338\n",
      "Iteration 21, loss = 0.38614047\n",
      "Iteration 22, loss = 0.35867730\n",
      "Iteration 23, loss = 0.35730226\n",
      "Iteration 24, loss = 0.33682360\n",
      "Iteration 25, loss = 0.32076823\n",
      "Iteration 26, loss = 0.32981216\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.09078119\n",
      "Iteration 2, loss = 1.16215643\n",
      "Iteration 3, loss = 0.65095707\n",
      "Iteration 4, loss = 0.60171146\n",
      "Iteration 5, loss = 0.47506705\n",
      "Iteration 6, loss = 0.43660690\n",
      "Iteration 7, loss = 0.40642462\n",
      "Iteration 8, loss = 0.36387073\n",
      "Iteration 9, loss = 0.33813033\n",
      "Iteration 10, loss = 0.33325145\n",
      "Iteration 11, loss = 0.32348732\n",
      "Iteration 12, loss = 0.33679933\n",
      "Iteration 13, loss = 0.34594557\n",
      "Iteration 14, loss = 0.34324465\n",
      "Iteration 15, loss = 0.31414302\n",
      "Iteration 16, loss = 0.35125945\n",
      "Iteration 17, loss = 0.33539669\n",
      "Iteration 18, loss = 0.33928929\n",
      "Iteration 19, loss = 0.35229803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 20, loss = 0.37211689\n",
      "Iteration 21, loss = 0.36374924\n",
      "Iteration 22, loss = 0.34628596\n",
      "Iteration 23, loss = 0.36792210\n",
      "Iteration 24, loss = 0.33547620\n",
      "Iteration 25, loss = 0.29990233\n",
      "Iteration 26, loss = 0.31882681\n",
      "Iteration 27, loss = 0.31627384\n",
      "Iteration 28, loss = 0.32753028\n",
      "Iteration 29, loss = 0.32047451\n",
      "Iteration 30, loss = 0.31187832\n",
      "Iteration 31, loss = 0.30548515\n",
      "Iteration 32, loss = 0.31330403\n",
      "Iteration 33, loss = 0.30463417\n",
      "Iteration 34, loss = 0.29127245\n",
      "Iteration 35, loss = 0.28412283\n",
      "Iteration 36, loss = 0.28672452\n",
      "Iteration 37, loss = 0.31363920\n",
      "Iteration 38, loss = 0.31766683\n",
      "Iteration 39, loss = 0.30835422\n",
      "Iteration 40, loss = 0.33065790\n",
      "Iteration 41, loss = 0.33777829\n",
      "Iteration 42, loss = 0.33856081\n",
      "Iteration 43, loss = 0.31386633\n",
      "Iteration 44, loss = 0.29280415\n",
      "Iteration 45, loss = 0.30302985\n",
      "Iteration 46, loss = 0.29701546\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.09815501\n",
      "Iteration 2, loss = 1.20085736\n",
      "Iteration 3, loss = 0.68273102\n",
      "Iteration 4, loss = 0.56999766\n",
      "Iteration 5, loss = 0.47066712\n",
      "Iteration 6, loss = 0.41690609\n",
      "Iteration 7, loss = 0.39372153\n",
      "Iteration 8, loss = 0.35719855\n",
      "Iteration 9, loss = 0.33463064\n",
      "Iteration 10, loss = 0.32506196\n",
      "Iteration 11, loss = 0.32378956\n",
      "Iteration 12, loss = 0.35757922\n",
      "Iteration 13, loss = 0.33485314\n",
      "Iteration 14, loss = 0.35465402\n",
      "Iteration 15, loss = 0.34341049\n",
      "Iteration 16, loss = 0.33328180\n",
      "Iteration 17, loss = 0.30748251\n",
      "Iteration 18, loss = 0.32438912\n",
      "Iteration 19, loss = 0.32149375\n",
      "Iteration 20, loss = 0.33142344\n",
      "Iteration 21, loss = 0.34152559\n",
      "Iteration 22, loss = 0.33077521\n",
      "Iteration 23, loss = 0.33645476\n",
      "Iteration 24, loss = 0.30406877\n",
      "Iteration 25, loss = 0.29467651\n",
      "Iteration 26, loss = 0.29977188\n",
      "Iteration 27, loss = 0.29430175\n",
      "Iteration 28, loss = 0.32500458\n",
      "Iteration 29, loss = 0.30793980\n",
      "Iteration 30, loss = 0.29584909\n",
      "Iteration 31, loss = 0.30083469\n",
      "Iteration 32, loss = 0.29179764\n",
      "Iteration 33, loss = 0.28568216\n",
      "Iteration 34, loss = 0.29813576\n",
      "Iteration 35, loss = 0.29250585\n",
      "Iteration 36, loss = 0.27670813\n",
      "Iteration 37, loss = 0.28718520\n",
      "Iteration 38, loss = 0.30052036\n",
      "Iteration 39, loss = 0.30847282\n",
      "Iteration 40, loss = 0.31019890\n",
      "Iteration 41, loss = 0.30687630\n",
      "Iteration 42, loss = 0.30887653\n",
      "Iteration 43, loss = 0.28975340\n",
      "Iteration 44, loss = 0.29357124\n",
      "Iteration 45, loss = 0.29874867\n",
      "Iteration 46, loss = 0.27660682\n",
      "Iteration 47, loss = 0.28825125\n",
      "Iteration 48, loss = 0.28320459\n",
      "Iteration 49, loss = 0.27153746\n",
      "Iteration 50, loss = 0.28809187\n",
      "Iteration 51, loss = 0.30565877\n",
      "Iteration 52, loss = 0.34356207\n",
      "Iteration 53, loss = 0.33647225\n",
      "Iteration 54, loss = 0.34835180\n",
      "Iteration 55, loss = 0.33704663\n",
      "Iteration 56, loss = 0.30978329\n",
      "Iteration 57, loss = 0.29654140\n",
      "Iteration 58, loss = 0.30881104\n",
      "Iteration 59, loss = 0.32582111\n",
      "Iteration 60, loss = 0.31880305\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.35408356\n",
      "Iteration 2, loss = 1.34894450\n",
      "Iteration 3, loss = 0.78893752\n",
      "Iteration 4, loss = 0.67966883\n",
      "Iteration 5, loss = 0.52040318\n",
      "Iteration 6, loss = 0.45226585\n",
      "Iteration 7, loss = 0.45680280\n",
      "Iteration 8, loss = 0.43751351\n",
      "Iteration 9, loss = 0.39770555\n",
      "Iteration 10, loss = 0.38259942\n",
      "Iteration 11, loss = 0.38686023\n",
      "Iteration 12, loss = 0.38404200\n",
      "Iteration 13, loss = 0.37344459\n",
      "Iteration 14, loss = 0.34505505\n",
      "Iteration 15, loss = 0.32165549\n",
      "Iteration 16, loss = 0.33208569\n",
      "Iteration 17, loss = 0.34593978\n",
      "Iteration 18, loss = 0.34427684\n",
      "Iteration 19, loss = 0.34452739\n",
      "Iteration 20, loss = 0.32997786\n",
      "Iteration 21, loss = 0.31830341\n",
      "Iteration 22, loss = 0.33063647\n",
      "Iteration 23, loss = 0.30730685\n",
      "Iteration 24, loss = 0.28703710\n",
      "Iteration 25, loss = 0.32388888\n",
      "Iteration 26, loss = 0.35481170\n",
      "Iteration 27, loss = 0.34035259\n",
      "Iteration 28, loss = 0.29310557\n",
      "Iteration 29, loss = 0.35550403\n",
      "Iteration 30, loss = 0.36244470\n",
      "Iteration 31, loss = 0.39136034\n",
      "Iteration 32, loss = 0.41545852\n",
      "Iteration 33, loss = 0.38145473\n",
      "Iteration 34, loss = 0.35228871\n",
      "Iteration 35, loss = 0.37724281\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.33578276\n",
      "Iteration 2, loss = 1.46331794\n",
      "Iteration 3, loss = 0.86890344\n",
      "Iteration 4, loss = 0.67865883\n",
      "Iteration 5, loss = 0.57838008\n",
      "Iteration 6, loss = 0.47716909\n",
      "Iteration 7, loss = 0.45063883\n",
      "Iteration 8, loss = 0.44030925\n",
      "Iteration 9, loss = 0.41111882\n",
      "Iteration 10, loss = 0.37892175\n",
      "Iteration 11, loss = 0.36235260\n",
      "Iteration 12, loss = 0.35236209\n",
      "Iteration 13, loss = 0.34367681\n",
      "Iteration 14, loss = 0.34303677\n",
      "Iteration 15, loss = 0.33487883\n",
      "Iteration 16, loss = 0.32017121\n",
      "Iteration 17, loss = 0.30193993\n",
      "Iteration 18, loss = 0.30166451\n",
      "Iteration 19, loss = 0.32099837\n",
      "Iteration 20, loss = 0.31746609\n",
      "Iteration 21, loss = 0.30807294\n",
      "Iteration 22, loss = 0.30587278\n",
      "Iteration 23, loss = 0.31221897\n",
      "Iteration 24, loss = 0.31798019\n",
      "Iteration 25, loss = 0.32811236\n",
      "Iteration 26, loss = 0.31300446\n",
      "Iteration 27, loss = 0.34377329\n",
      "Iteration 28, loss = 0.32947611\n",
      "Iteration 29, loss = 0.32792739\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.32684777\n",
      "Iteration 2, loss = 1.41497747\n",
      "Iteration 3, loss = 0.76686584\n",
      "Iteration 4, loss = 0.64781226\n",
      "Iteration 5, loss = 0.51904526\n",
      "Iteration 6, loss = 0.44173526\n",
      "Iteration 7, loss = 0.45231170\n",
      "Iteration 8, loss = 0.43450574\n",
      "Iteration 9, loss = 0.39251039\n",
      "Iteration 10, loss = 0.36443377\n",
      "Iteration 11, loss = 0.36299907\n",
      "Iteration 12, loss = 0.35946512\n",
      "Iteration 13, loss = 0.34781995\n",
      "Iteration 14, loss = 0.32935701\n",
      "Iteration 15, loss = 0.32724575\n",
      "Iteration 16, loss = 0.35464745\n",
      "Iteration 17, loss = 0.31897601\n",
      "Iteration 18, loss = 0.29459011\n",
      "Iteration 19, loss = 0.32279006\n",
      "Iteration 20, loss = 0.34167929\n",
      "Iteration 21, loss = 0.34388749\n",
      "Iteration 22, loss = 0.33378517\n",
      "Iteration 23, loss = 0.34162450\n",
      "Iteration 24, loss = 0.34043145\n",
      "Iteration 25, loss = 0.33654273\n",
      "Iteration 26, loss = 0.31112181\n",
      "Iteration 27, loss = 0.33786190\n",
      "Iteration 28, loss = 0.34324831\n",
      "Iteration 29, loss = 0.31704154\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.34087329\n",
      "Iteration 2, loss = 1.40106870\n",
      "Iteration 3, loss = 0.82796497\n",
      "Iteration 4, loss = 0.61562971\n",
      "Iteration 5, loss = 0.54415809\n",
      "Iteration 6, loss = 0.45970432\n",
      "Iteration 7, loss = 0.43232075\n",
      "Iteration 8, loss = 0.42783390\n",
      "Iteration 9, loss = 0.41136638\n",
      "Iteration 10, loss = 0.37713099\n",
      "Iteration 11, loss = 0.35405251\n",
      "Iteration 12, loss = 0.35178287\n",
      "Iteration 13, loss = 0.35292142\n",
      "Iteration 14, loss = 0.34059992\n",
      "Iteration 15, loss = 0.30997369\n",
      "Iteration 16, loss = 0.31879322\n",
      "Iteration 17, loss = 0.30396287\n",
      "Iteration 18, loss = 0.29141351\n",
      "Iteration 19, loss = 0.31040889\n",
      "Iteration 20, loss = 0.32190996\n",
      "Iteration 21, loss = 0.32217289\n",
      "Iteration 22, loss = 0.31330416\n",
      "Iteration 23, loss = 0.32442068\n",
      "Iteration 24, loss = 0.33311062\n",
      "Iteration 25, loss = 0.32668864\n",
      "Iteration 26, loss = 0.29369082\n",
      "Iteration 27, loss = 0.31153266\n",
      "Iteration 28, loss = 0.33306058\n",
      "Iteration 29, loss = 0.33695354\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.33342346\n",
      "Iteration 2, loss = 1.37019357\n",
      "Iteration 3, loss = 0.79035687\n",
      "Iteration 4, loss = 0.66163367\n",
      "Iteration 5, loss = 0.52465796\n",
      "Iteration 6, loss = 0.44759375\n",
      "Iteration 7, loss = 0.44821626\n",
      "Iteration 8, loss = 0.44121915\n",
      "Iteration 9, loss = 0.40221162\n",
      "Iteration 10, loss = 0.36758169\n",
      "Iteration 11, loss = 0.36164487\n",
      "Iteration 12, loss = 0.36604237\n",
      "Iteration 13, loss = 0.35442126\n",
      "Iteration 14, loss = 0.33391527\n",
      "Iteration 15, loss = 0.30860901\n",
      "Iteration 16, loss = 0.32669597\n",
      "Iteration 17, loss = 0.31035631\n",
      "Iteration 18, loss = 0.30761140\n",
      "Iteration 19, loss = 0.33102264\n",
      "Iteration 20, loss = 0.31952918\n",
      "Iteration 21, loss = 0.30834402\n",
      "Iteration 22, loss = 0.34500253\n",
      "Iteration 23, loss = 0.37806849\n",
      "Iteration 24, loss = 0.35874735\n",
      "Iteration 25, loss = 0.32849588\n",
      "Iteration 26, loss = 0.30322467\n",
      "Iteration 27, loss = 0.31120962\n",
      "Iteration 28, loss = 0.31957568\n",
      "Iteration 29, loss = 0.31001001\n",
      "Iteration 30, loss = 0.29438448\n",
      "Iteration 31, loss = 0.28908545\n",
      "Iteration 32, loss = 0.29198919\n",
      "Iteration 33, loss = 0.31467684\n",
      "Iteration 34, loss = 0.30342952\n",
      "Iteration 35, loss = 0.29694134\n",
      "Iteration 36, loss = 0.29181116\n",
      "Iteration 37, loss = 0.31546744\n",
      "Iteration 38, loss = 0.31171375\n",
      "Iteration 39, loss = 0.29381822\n",
      "Iteration 40, loss = 0.30745852\n",
      "Iteration 41, loss = 0.32759137\n",
      "Iteration 42, loss = 0.31763413\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.31040758\n",
      "Iteration 2, loss = 1.42952197\n",
      "Iteration 3, loss = 0.79064934\n",
      "Iteration 4, loss = 0.66436993\n",
      "Iteration 5, loss = 0.53872248\n",
      "Iteration 6, loss = 0.43673108\n",
      "Iteration 7, loss = 0.42082796\n",
      "Iteration 8, loss = 0.42332656\n",
      "Iteration 9, loss = 0.39593649\n",
      "Iteration 10, loss = 0.36045706\n",
      "Iteration 11, loss = 0.33642296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12, loss = 0.33279503\n",
      "Iteration 13, loss = 0.33480400\n",
      "Iteration 14, loss = 0.32590262\n",
      "Iteration 15, loss = 0.30347752\n",
      "Iteration 16, loss = 0.30932530\n",
      "Iteration 17, loss = 0.29614789\n",
      "Iteration 18, loss = 0.30571536\n",
      "Iteration 19, loss = 0.31792577\n",
      "Iteration 20, loss = 0.30080397\n",
      "Iteration 21, loss = 0.29108343\n",
      "Iteration 22, loss = 0.33347058\n",
      "Iteration 23, loss = 0.37174977\n",
      "Iteration 24, loss = 0.36832849\n",
      "Iteration 25, loss = 0.34252961\n",
      "Iteration 26, loss = 0.30652668\n",
      "Iteration 27, loss = 0.29645300\n",
      "Iteration 28, loss = 0.30950659\n",
      "Iteration 29, loss = 0.31380741\n",
      "Iteration 30, loss = 0.29773365\n",
      "Iteration 31, loss = 0.28030183\n",
      "Iteration 32, loss = 0.28749911\n",
      "Iteration 33, loss = 0.29424383\n",
      "Iteration 34, loss = 0.29753231\n",
      "Iteration 35, loss = 0.28196690\n",
      "Iteration 36, loss = 0.28042097\n",
      "Iteration 37, loss = 0.30640443\n",
      "Iteration 38, loss = 0.29981051\n",
      "Iteration 39, loss = 0.28312934\n",
      "Iteration 40, loss = 0.29514505\n",
      "Iteration 41, loss = 0.32786929\n",
      "Iteration 42, loss = 0.30768859\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.31673603\n",
      "Iteration 2, loss = 1.41736331\n",
      "Iteration 3, loss = 0.81165565\n",
      "Iteration 4, loss = 0.68354311\n",
      "Iteration 5, loss = 0.54809188\n",
      "Iteration 6, loss = 0.45432407\n",
      "Iteration 7, loss = 0.44623969\n",
      "Iteration 8, loss = 0.45227404\n",
      "Iteration 9, loss = 0.42812785\n",
      "Iteration 10, loss = 0.38593472\n",
      "Iteration 11, loss = 0.35195051\n",
      "Iteration 12, loss = 0.34342890\n",
      "Iteration 13, loss = 0.34977477\n",
      "Iteration 14, loss = 0.34504460\n",
      "Iteration 15, loss = 0.31654208\n",
      "Iteration 16, loss = 0.31468400\n",
      "Iteration 17, loss = 0.30517049\n",
      "Iteration 18, loss = 0.30348783\n",
      "Iteration 19, loss = 0.30228467\n",
      "Iteration 20, loss = 0.29461542\n",
      "Iteration 21, loss = 0.28420447\n",
      "Iteration 22, loss = 0.30578388\n",
      "Iteration 23, loss = 0.32181152\n",
      "Iteration 24, loss = 0.32595565\n",
      "Iteration 25, loss = 0.33231272\n",
      "Iteration 26, loss = 0.30891894\n",
      "Iteration 27, loss = 0.29282290\n",
      "Iteration 28, loss = 0.29638079\n",
      "Iteration 29, loss = 0.29151040\n",
      "Iteration 30, loss = 0.28402849\n",
      "Iteration 31, loss = 0.27386337\n",
      "Iteration 32, loss = 0.28584841\n",
      "Iteration 33, loss = 0.30403916\n",
      "Iteration 34, loss = 0.30129815\n",
      "Iteration 35, loss = 0.28067587\n",
      "Iteration 36, loss = 0.28140325\n",
      "Iteration 37, loss = 0.31437716\n",
      "Iteration 38, loss = 0.30614541\n",
      "Iteration 39, loss = 0.28005738\n",
      "Iteration 40, loss = 0.29389100\n",
      "Iteration 41, loss = 0.29998580\n",
      "Iteration 42, loss = 0.29524529\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.29075765\n",
      "Iteration 2, loss = 1.51246942\n",
      "Iteration 3, loss = 0.84727829\n",
      "Iteration 4, loss = 0.70546948\n",
      "Iteration 5, loss = 0.57754367\n",
      "Iteration 6, loss = 0.47610061\n",
      "Iteration 7, loss = 0.45564419\n",
      "Iteration 8, loss = 0.45119654\n",
      "Iteration 9, loss = 0.42246984\n",
      "Iteration 10, loss = 0.38437787\n",
      "Iteration 11, loss = 0.36325498\n",
      "Iteration 12, loss = 0.36128634\n",
      "Iteration 13, loss = 0.35643295\n",
      "Iteration 14, loss = 0.34613572\n",
      "Iteration 15, loss = 0.32086893\n",
      "Iteration 16, loss = 0.31488199\n",
      "Iteration 17, loss = 0.30435866\n",
      "Iteration 18, loss = 0.31957340\n",
      "Iteration 19, loss = 0.32512882\n",
      "Iteration 20, loss = 0.30901592\n",
      "Iteration 21, loss = 0.29168432\n",
      "Iteration 22, loss = 0.33061902\n",
      "Iteration 23, loss = 0.34722105\n",
      "Iteration 24, loss = 0.33822360\n",
      "Iteration 25, loss = 0.33447433\n",
      "Iteration 26, loss = 0.32028978\n",
      "Iteration 27, loss = 0.32057914\n",
      "Iteration 28, loss = 0.31240089\n",
      "Iteration 29, loss = 0.29551180\n",
      "Iteration 30, loss = 0.29213178\n",
      "Iteration 31, loss = 0.29307299\n",
      "Iteration 32, loss = 0.29928924\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.29929361\n",
      "Iteration 2, loss = 1.45708575\n",
      "Iteration 3, loss = 0.80496582\n",
      "Iteration 4, loss = 0.65508969\n",
      "Iteration 5, loss = 0.57903169\n",
      "Iteration 6, loss = 0.47396777\n",
      "Iteration 7, loss = 0.43906469\n",
      "Iteration 8, loss = 0.43972035\n",
      "Iteration 9, loss = 0.43243953\n",
      "Iteration 10, loss = 0.39327910\n",
      "Iteration 11, loss = 0.35448487\n",
      "Iteration 12, loss = 0.34799690\n",
      "Iteration 13, loss = 0.35223619\n",
      "Iteration 14, loss = 0.33994729\n",
      "Iteration 15, loss = 0.31216097\n",
      "Iteration 16, loss = 0.30767627\n",
      "Iteration 17, loss = 0.30025028\n",
      "Iteration 18, loss = 0.32098896\n",
      "Iteration 19, loss = 0.32453600\n",
      "Iteration 20, loss = 0.30462050\n",
      "Iteration 21, loss = 0.28541828\n",
      "Iteration 22, loss = 0.32012075\n",
      "Iteration 23, loss = 0.32839212\n",
      "Iteration 24, loss = 0.32597198\n",
      "Iteration 25, loss = 0.31535272\n",
      "Iteration 26, loss = 0.29658319\n",
      "Iteration 27, loss = 0.28980548\n",
      "Iteration 28, loss = 0.30160541\n",
      "Iteration 29, loss = 0.28891103\n",
      "Iteration 30, loss = 0.28331041\n",
      "Iteration 31, loss = 0.27905680\n",
      "Iteration 32, loss = 0.29275188\n",
      "Iteration 33, loss = 0.31221601\n",
      "Iteration 34, loss = 0.29782581\n",
      "Iteration 35, loss = 0.28564301\n",
      "Iteration 36, loss = 0.30456724\n",
      "Iteration 37, loss = 0.34271608\n",
      "Iteration 38, loss = 0.32350158\n",
      "Iteration 39, loss = 0.29309502\n",
      "Iteration 40, loss = 0.32071617\n",
      "Iteration 41, loss = 0.31357032\n",
      "Iteration 42, loss = 0.32038093\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.31805932\n",
      "Iteration 2, loss = 1.34132065\n",
      "Iteration 3, loss = 0.76450060\n",
      "Iteration 4, loss = 0.65226694\n",
      "Iteration 5, loss = 0.51577615\n",
      "Iteration 6, loss = 0.44893460\n",
      "Iteration 7, loss = 0.45095568\n",
      "Iteration 8, loss = 0.42915664\n",
      "Iteration 9, loss = 0.40212412\n",
      "Iteration 10, loss = 0.37887429\n",
      "Iteration 11, loss = 0.36372783\n",
      "Iteration 12, loss = 0.35739783\n",
      "Iteration 13, loss = 0.33795016\n",
      "Iteration 14, loss = 0.31682012\n",
      "Iteration 15, loss = 0.30524892\n",
      "Iteration 16, loss = 0.30830748\n",
      "Iteration 17, loss = 0.29184022\n",
      "Iteration 18, loss = 0.32843495\n",
      "Iteration 19, loss = 0.35543745\n",
      "Iteration 20, loss = 0.33017432\n",
      "Iteration 21, loss = 0.29511170\n",
      "Iteration 22, loss = 0.32947481\n",
      "Iteration 23, loss = 0.34109489\n",
      "Iteration 24, loss = 0.32915797\n",
      "Iteration 25, loss = 0.33059087\n",
      "Iteration 26, loss = 0.33174427\n",
      "Iteration 27, loss = 0.31947735\n",
      "Iteration 28, loss = 0.30604505\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.31350035\n",
      "Iteration 2, loss = 1.55889390\n",
      "Iteration 3, loss = 0.90685706\n",
      "Iteration 4, loss = 0.74212425\n",
      "Iteration 5, loss = 0.64782562\n",
      "Iteration 6, loss = 0.51200995\n",
      "Iteration 7, loss = 0.47459230\n",
      "Iteration 8, loss = 0.48186003\n",
      "Iteration 9, loss = 0.47844824\n",
      "Iteration 10, loss = 0.42141797\n",
      "Iteration 11, loss = 0.37661184\n",
      "Iteration 12, loss = 0.39856139\n",
      "Iteration 13, loss = 0.40704070\n",
      "Iteration 14, loss = 0.37124305\n",
      "Iteration 15, loss = 0.35681128\n",
      "Iteration 16, loss = 0.34403869\n",
      "Iteration 17, loss = 0.33469339\n",
      "Iteration 18, loss = 0.32092879\n",
      "Iteration 19, loss = 0.30510809\n",
      "Iteration 20, loss = 0.32252632\n",
      "Iteration 21, loss = 0.33865958\n",
      "Iteration 22, loss = 0.32596322\n",
      "Iteration 23, loss = 0.33271474\n",
      "Iteration 24, loss = 0.32262343\n",
      "Iteration 25, loss = 0.31010982\n",
      "Iteration 26, loss = 0.31809615\n",
      "Iteration 27, loss = 0.31208234\n",
      "Iteration 28, loss = 0.30830696\n",
      "Iteration 29, loss = 0.30978987\n",
      "Iteration 30, loss = 0.30968588\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.29285976\n",
      "Iteration 2, loss = 1.52343253\n",
      "Iteration 3, loss = 0.92831604\n",
      "Iteration 4, loss = 0.64683259\n",
      "Iteration 5, loss = 0.62853469\n",
      "Iteration 6, loss = 0.54158374\n",
      "Iteration 7, loss = 0.47985340\n",
      "Iteration 8, loss = 0.45838698\n",
      "Iteration 9, loss = 0.43082292\n",
      "Iteration 10, loss = 0.39044453\n",
      "Iteration 11, loss = 0.36698804\n",
      "Iteration 12, loss = 0.36613658\n",
      "Iteration 13, loss = 0.35318188\n",
      "Iteration 14, loss = 0.33133193\n",
      "Iteration 15, loss = 0.31532391\n",
      "Iteration 16, loss = 0.31188325\n",
      "Iteration 17, loss = 0.31326964\n",
      "Iteration 18, loss = 0.30952229\n",
      "Iteration 19, loss = 0.31355751\n",
      "Iteration 20, loss = 0.32603045\n",
      "Iteration 21, loss = 0.29739895\n",
      "Iteration 22, loss = 0.32874749\n",
      "Iteration 23, loss = 0.34677243\n",
      "Iteration 24, loss = 0.30333087\n",
      "Iteration 25, loss = 0.32408789\n",
      "Iteration 26, loss = 0.34178418\n",
      "Iteration 27, loss = 0.31231600\n",
      "Iteration 28, loss = 0.29883849\n",
      "Iteration 29, loss = 0.30955464\n",
      "Iteration 30, loss = 0.32185337\n",
      "Iteration 31, loss = 0.31864018\n",
      "Iteration 32, loss = 0.30028392\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.31453638\n",
      "Iteration 2, loss = 1.65787957\n",
      "Iteration 3, loss = 1.11006061\n",
      "Iteration 4, loss = 0.70492750\n",
      "Iteration 5, loss = 0.66191413\n",
      "Iteration 6, loss = 0.56254680\n",
      "Iteration 7, loss = 0.48579623\n",
      "Iteration 8, loss = 0.46812304\n",
      "Iteration 9, loss = 0.45270254\n",
      "Iteration 10, loss = 0.40992403\n",
      "Iteration 11, loss = 0.36954447\n",
      "Iteration 12, loss = 0.36523672\n",
      "Iteration 13, loss = 0.36709768\n",
      "Iteration 14, loss = 0.34533463\n",
      "Iteration 15, loss = 0.32320684\n",
      "Iteration 16, loss = 0.30989635\n",
      "Iteration 17, loss = 0.30684657\n",
      "Iteration 18, loss = 0.30585927\n",
      "Iteration 19, loss = 0.29924916\n",
      "Iteration 20, loss = 0.30691319\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 21, loss = 0.29886384\n",
      "Iteration 22, loss = 0.30838214\n",
      "Iteration 23, loss = 0.32030354\n",
      "Iteration 24, loss = 0.30922302\n",
      "Iteration 25, loss = 0.31374089\n",
      "Iteration 26, loss = 0.33399256\n",
      "Iteration 27, loss = 0.31954810\n",
      "Iteration 28, loss = 0.29931434\n",
      "Iteration 29, loss = 0.30779053\n",
      "Iteration 30, loss = 0.32722685\n",
      "Iteration 31, loss = 0.32159582\n",
      "Iteration 32, loss = 0.29670148\n",
      "Iteration 33, loss = 0.27568026\n",
      "Iteration 34, loss = 0.28857304\n",
      "Iteration 35, loss = 0.29270288\n",
      "Iteration 36, loss = 0.30617260\n",
      "Iteration 37, loss = 0.31217710\n",
      "Iteration 38, loss = 0.31538607\n",
      "Iteration 39, loss = 0.29851781\n",
      "Iteration 40, loss = 0.30835551\n",
      "Iteration 41, loss = 0.32050280\n",
      "Iteration 42, loss = 0.32234654\n",
      "Iteration 43, loss = 0.30451313\n",
      "Iteration 44, loss = 0.28000416\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.30025713\n",
      "Iteration 2, loss = 1.58150458\n",
      "Iteration 3, loss = 1.11564503\n",
      "Iteration 4, loss = 0.62903412\n",
      "Iteration 5, loss = 0.61379009\n",
      "Iteration 6, loss = 0.60967418\n",
      "Iteration 7, loss = 0.51420113\n",
      "Iteration 8, loss = 0.44577507\n",
      "Iteration 9, loss = 0.40935531\n",
      "Iteration 10, loss = 0.38892874\n",
      "Iteration 11, loss = 0.36111191\n",
      "Iteration 12, loss = 0.34588635\n",
      "Iteration 13, loss = 0.33580438\n",
      "Iteration 14, loss = 0.32227163\n",
      "Iteration 15, loss = 0.31253940\n",
      "Iteration 16, loss = 0.30126331\n",
      "Iteration 17, loss = 0.29869779\n",
      "Iteration 18, loss = 0.30061245\n",
      "Iteration 19, loss = 0.29372888\n",
      "Iteration 20, loss = 0.28094808\n",
      "Iteration 21, loss = 0.27266631\n",
      "Iteration 22, loss = 0.28463975\n",
      "Iteration 23, loss = 0.28325838\n",
      "Iteration 24, loss = 0.27865784\n",
      "Iteration 25, loss = 0.30232791\n",
      "Iteration 26, loss = 0.31932318\n",
      "Iteration 27, loss = 0.29724336\n",
      "Iteration 28, loss = 0.28524353\n",
      "Iteration 29, loss = 0.31460998\n",
      "Iteration 30, loss = 0.32213961\n",
      "Iteration 31, loss = 0.29770238\n",
      "Iteration 32, loss = 0.28098931\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.31844115\n",
      "Iteration 2, loss = 1.72576704\n",
      "Iteration 3, loss = 1.13749993\n",
      "Iteration 4, loss = 0.70600447\n",
      "Iteration 5, loss = 0.69418686\n",
      "Iteration 6, loss = 0.60725320\n",
      "Iteration 7, loss = 0.50746148\n",
      "Iteration 8, loss = 0.45978971\n",
      "Iteration 9, loss = 0.43972857\n",
      "Iteration 10, loss = 0.41141868\n",
      "Iteration 11, loss = 0.37007065\n",
      "Iteration 12, loss = 0.35356721\n",
      "Iteration 13, loss = 0.34942376\n",
      "Iteration 14, loss = 0.33807023\n",
      "Iteration 15, loss = 0.32566186\n",
      "Iteration 16, loss = 0.30990484\n",
      "Iteration 17, loss = 0.30107758\n",
      "Iteration 18, loss = 0.30775716\n",
      "Iteration 19, loss = 0.30942987\n",
      "Iteration 20, loss = 0.29722260\n",
      "Iteration 21, loss = 0.28324729\n",
      "Iteration 22, loss = 0.29489641\n",
      "Iteration 23, loss = 0.28944315\n",
      "Iteration 24, loss = 0.29329242\n",
      "Iteration 25, loss = 0.32355464\n",
      "Iteration 26, loss = 0.33443177\n",
      "Iteration 27, loss = 0.31217330\n",
      "Iteration 28, loss = 0.30535738\n",
      "Iteration 29, loss = 0.32771210\n",
      "Iteration 30, loss = 0.33584834\n",
      "Iteration 31, loss = 0.31644338\n",
      "Iteration 32, loss = 0.30276081\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.33255530\n",
      "Iteration 2, loss = 1.76375689\n",
      "Iteration 3, loss = 1.08289178\n",
      "Iteration 4, loss = 0.74225255\n",
      "Iteration 5, loss = 0.69849330\n",
      "Iteration 6, loss = 0.55990995\n",
      "Iteration 7, loss = 0.47923746\n",
      "Iteration 8, loss = 0.45515473\n",
      "Iteration 9, loss = 0.42702811\n",
      "Iteration 10, loss = 0.38915607\n",
      "Iteration 11, loss = 0.35514738\n",
      "Iteration 12, loss = 0.33937312\n",
      "Iteration 13, loss = 0.33231022\n",
      "Iteration 14, loss = 0.32385676\n",
      "Iteration 15, loss = 0.31541120\n",
      "Iteration 16, loss = 0.30510876\n",
      "Iteration 17, loss = 0.29811378\n",
      "Iteration 18, loss = 0.30720544\n",
      "Iteration 19, loss = 0.30578014\n",
      "Iteration 20, loss = 0.28615956\n",
      "Iteration 21, loss = 0.27476793\n",
      "Iteration 22, loss = 0.30055664\n",
      "Iteration 23, loss = 0.29436207\n",
      "Iteration 24, loss = 0.28982974\n",
      "Iteration 25, loss = 0.32192789\n",
      "Iteration 26, loss = 0.32935829\n",
      "Iteration 27, loss = 0.30226649\n",
      "Iteration 28, loss = 0.32952304\n",
      "Iteration 29, loss = 0.33505688\n",
      "Iteration 30, loss = 0.31267846\n",
      "Iteration 31, loss = 0.30778715\n",
      "Iteration 32, loss = 0.30405172\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.33907060\n",
      "Iteration 2, loss = 1.77169920\n",
      "Iteration 3, loss = 1.07176088\n",
      "Iteration 4, loss = 0.77495163\n",
      "Iteration 5, loss = 0.70948671\n",
      "Iteration 6, loss = 0.55137824\n",
      "Iteration 7, loss = 0.48419414\n",
      "Iteration 8, loss = 0.46362379\n",
      "Iteration 9, loss = 0.43560185\n",
      "Iteration 10, loss = 0.39862983\n",
      "Iteration 11, loss = 0.36079012\n",
      "Iteration 12, loss = 0.35346744\n",
      "Iteration 13, loss = 0.35067237\n",
      "Iteration 14, loss = 0.32912932\n",
      "Iteration 15, loss = 0.31751304\n",
      "Iteration 16, loss = 0.31667568\n",
      "Iteration 17, loss = 0.30763032\n",
      "Iteration 18, loss = 0.31895155\n",
      "Iteration 19, loss = 0.31439402\n",
      "Iteration 20, loss = 0.28657907\n",
      "Iteration 21, loss = 0.27830379\n",
      "Iteration 22, loss = 0.30751585\n",
      "Iteration 23, loss = 0.29894810\n",
      "Iteration 24, loss = 0.28108905\n",
      "Iteration 25, loss = 0.33838594\n",
      "Iteration 26, loss = 0.36815461\n",
      "Iteration 27, loss = 0.29839120\n",
      "Iteration 28, loss = 0.35088184\n",
      "Iteration 29, loss = 0.36436473\n",
      "Iteration 30, loss = 0.33099731\n",
      "Iteration 31, loss = 0.32838227\n",
      "Iteration 32, loss = 0.31220387\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.33957543\n",
      "Iteration 2, loss = 1.78732655\n",
      "Iteration 3, loss = 1.08095025\n",
      "Iteration 4, loss = 0.80511196\n",
      "Iteration 5, loss = 0.73811577\n",
      "Iteration 6, loss = 0.57825802\n",
      "Iteration 7, loss = 0.52074235\n",
      "Iteration 8, loss = 0.50031208\n",
      "Iteration 9, loss = 0.46585788\n",
      "Iteration 10, loss = 0.42701966\n",
      "Iteration 11, loss = 0.38761831\n",
      "Iteration 12, loss = 0.37945252\n",
      "Iteration 13, loss = 0.37544358\n",
      "Iteration 14, loss = 0.34892834\n",
      "Iteration 15, loss = 0.33131398\n",
      "Iteration 16, loss = 0.33807488\n",
      "Iteration 17, loss = 0.33110253\n",
      "Iteration 18, loss = 0.32670148\n",
      "Iteration 19, loss = 0.33154252\n",
      "Iteration 20, loss = 0.30677795\n",
      "Iteration 21, loss = 0.29909938\n",
      "Iteration 22, loss = 0.34219312\n",
      "Iteration 23, loss = 0.34930680\n",
      "Iteration 24, loss = 0.30752370\n",
      "Iteration 25, loss = 0.32533352\n",
      "Iteration 26, loss = 0.36543576\n",
      "Iteration 27, loss = 0.32866392\n",
      "Iteration 28, loss = 0.36385423\n",
      "Iteration 29, loss = 0.39833288\n",
      "Iteration 30, loss = 0.36441546\n",
      "Iteration 31, loss = 0.34396541\n",
      "Iteration 32, loss = 0.33096013\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.34161762\n",
      "Iteration 2, loss = 1.73665773\n",
      "Iteration 3, loss = 1.06949093\n",
      "Iteration 4, loss = 0.77485524\n",
      "Iteration 5, loss = 0.72621570\n",
      "Iteration 6, loss = 0.58907320\n",
      "Iteration 7, loss = 0.52752131\n",
      "Iteration 8, loss = 0.49609612\n",
      "Iteration 9, loss = 0.46453867\n",
      "Iteration 10, loss = 0.43440836\n",
      "Iteration 11, loss = 0.38581143\n",
      "Iteration 12, loss = 0.36166658\n",
      "Iteration 13, loss = 0.36549139\n",
      "Iteration 14, loss = 0.35558030\n",
      "Iteration 15, loss = 0.33055386\n",
      "Iteration 16, loss = 0.32455076\n",
      "Iteration 17, loss = 0.32200317\n",
      "Iteration 18, loss = 0.31445336\n",
      "Iteration 19, loss = 0.32192332\n",
      "Iteration 20, loss = 0.31428762\n",
      "Iteration 21, loss = 0.30230944\n",
      "Iteration 22, loss = 0.31647698\n",
      "Iteration 23, loss = 0.32343051\n",
      "Iteration 24, loss = 0.30232573\n",
      "Iteration 25, loss = 0.31564807\n",
      "Iteration 26, loss = 0.34866249\n",
      "Iteration 27, loss = 0.33030990\n",
      "Iteration 28, loss = 0.35343294\n",
      "Iteration 29, loss = 0.38753524\n",
      "Iteration 30, loss = 0.34097081\n",
      "Iteration 31, loss = 0.32317026\n",
      "Iteration 32, loss = 0.32403546\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.33780546\n",
      "Iteration 2, loss = 1.57055399\n",
      "Iteration 3, loss = 0.93900346\n",
      "Iteration 4, loss = 0.76524623\n",
      "Iteration 5, loss = 0.63400020\n",
      "Iteration 6, loss = 0.51073874\n",
      "Iteration 7, loss = 0.49370997\n",
      "Iteration 8, loss = 0.47911520\n",
      "Iteration 9, loss = 0.44132756\n",
      "Iteration 10, loss = 0.40461325\n",
      "Iteration 11, loss = 0.37900418\n",
      "Iteration 12, loss = 0.36772801\n",
      "Iteration 13, loss = 0.36810608\n",
      "Iteration 14, loss = 0.33168490\n",
      "Iteration 15, loss = 0.32302226\n",
      "Iteration 16, loss = 0.32826799\n",
      "Iteration 17, loss = 0.32839123\n",
      "Iteration 18, loss = 0.32358881\n",
      "Iteration 19, loss = 0.33132614\n",
      "Iteration 20, loss = 0.29805030\n",
      "Iteration 21, loss = 0.28783955\n",
      "Iteration 22, loss = 0.33797184\n",
      "Iteration 23, loss = 0.34916671\n",
      "Iteration 24, loss = 0.31056178\n",
      "Iteration 25, loss = 0.31750150\n",
      "Iteration 26, loss = 0.33489483\n",
      "Iteration 27, loss = 0.30940262\n",
      "Iteration 28, loss = 0.36105262\n",
      "Iteration 29, loss = 0.39672451\n",
      "Iteration 30, loss = 0.33856505\n",
      "Iteration 31, loss = 0.31910154\n",
      "Iteration 32, loss = 0.33117482\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.38720212\n",
      "Iteration 2, loss = 1.64131106\n",
      "Iteration 3, loss = 0.89957855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4, loss = 0.70396828\n",
      "Iteration 5, loss = 0.68967714\n",
      "Iteration 6, loss = 0.58825549\n",
      "Iteration 7, loss = 0.50533520\n",
      "Iteration 8, loss = 0.46442501\n",
      "Iteration 9, loss = 0.45726415\n",
      "Iteration 10, loss = 0.44564632\n",
      "Iteration 11, loss = 0.39681484\n",
      "Iteration 12, loss = 0.36179066\n",
      "Iteration 13, loss = 0.35632198\n",
      "Iteration 14, loss = 0.36067187\n",
      "Iteration 15, loss = 0.34313575\n",
      "Iteration 16, loss = 0.32943427\n",
      "Iteration 17, loss = 0.33242149\n",
      "Iteration 18, loss = 0.33294716\n",
      "Iteration 19, loss = 0.31583690\n",
      "Iteration 20, loss = 0.32478588\n",
      "Iteration 21, loss = 0.32475358\n",
      "Iteration 22, loss = 0.31032441\n",
      "Iteration 23, loss = 0.32392199\n",
      "Iteration 24, loss = 0.32273362\n",
      "Iteration 25, loss = 0.34472870\n",
      "Iteration 26, loss = 0.35844731\n",
      "Iteration 27, loss = 0.32979406\n",
      "Iteration 28, loss = 0.32941860\n",
      "Iteration 29, loss = 0.32333488\n",
      "Iteration 30, loss = 0.31415649\n",
      "Iteration 31, loss = 0.32492413\n",
      "Iteration 32, loss = 0.35619884\n",
      "Iteration 33, loss = 0.34270321\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.39050930\n",
      "Iteration 2, loss = 1.32792556\n",
      "Iteration 3, loss = 0.68955345\n",
      "Iteration 4, loss = 0.63613561\n",
      "Iteration 5, loss = 0.57116572\n",
      "Iteration 6, loss = 0.46725392\n",
      "Iteration 7, loss = 0.44942504\n",
      "Iteration 8, loss = 0.44400896\n",
      "Iteration 9, loss = 0.39998172\n",
      "Iteration 10, loss = 0.37413906\n",
      "Iteration 11, loss = 0.36279118\n",
      "Iteration 12, loss = 0.36052198\n",
      "Iteration 13, loss = 0.35417681\n",
      "Iteration 14, loss = 0.35936838\n",
      "Iteration 15, loss = 0.34936066\n",
      "Iteration 16, loss = 0.32196753\n",
      "Iteration 17, loss = 0.32800869\n",
      "Iteration 18, loss = 0.32750601\n",
      "Iteration 19, loss = 0.31604654\n",
      "Iteration 20, loss = 0.32640337\n",
      "Iteration 21, loss = 0.31808399\n",
      "Iteration 22, loss = 0.31021663\n",
      "Iteration 23, loss = 0.30813088\n",
      "Iteration 24, loss = 0.30356407\n",
      "Iteration 25, loss = 0.30764398\n",
      "Iteration 26, loss = 0.29584882\n",
      "Iteration 27, loss = 0.29109176\n",
      "Iteration 28, loss = 0.29325062\n",
      "Iteration 29, loss = 0.29639968\n",
      "Iteration 30, loss = 0.30298993\n",
      "Iteration 31, loss = 0.34285163\n",
      "Iteration 32, loss = 0.32834446\n",
      "Iteration 33, loss = 0.29618262\n",
      "Iteration 34, loss = 0.32917566\n",
      "Iteration 35, loss = 0.35721876\n",
      "Iteration 36, loss = 0.31093201\n",
      "Iteration 37, loss = 0.28876564\n",
      "Iteration 38, loss = 0.31118868\n",
      "Iteration 39, loss = 0.31684905\n",
      "Iteration 40, loss = 0.28272855\n",
      "Iteration 41, loss = 0.31970232\n",
      "Iteration 42, loss = 0.35180459\n",
      "Iteration 43, loss = 0.32456588\n",
      "Iteration 44, loss = 0.33496425\n",
      "Iteration 45, loss = 0.31045542\n",
      "Iteration 46, loss = 0.31278254\n",
      "Iteration 47, loss = 0.32806989\n",
      "Iteration 48, loss = 0.32040652\n",
      "Iteration 49, loss = 0.36356454\n",
      "Iteration 50, loss = 0.37673588\n",
      "Iteration 51, loss = 0.32885304\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.40913630\n",
      "Iteration 2, loss = 1.28519858\n",
      "Iteration 3, loss = 0.70821603\n",
      "Iteration 4, loss = 0.63249941\n",
      "Iteration 5, loss = 0.54837747\n",
      "Iteration 6, loss = 0.47582534\n",
      "Iteration 7, loss = 0.45365076\n",
      "Iteration 8, loss = 0.43830602\n",
      "Iteration 9, loss = 0.40906717\n",
      "Iteration 10, loss = 0.38956754\n",
      "Iteration 11, loss = 0.36727736\n",
      "Iteration 12, loss = 0.35312112\n",
      "Iteration 13, loss = 0.35503129\n",
      "Iteration 14, loss = 0.37820861\n",
      "Iteration 15, loss = 0.36838670\n",
      "Iteration 16, loss = 0.32571097\n",
      "Iteration 17, loss = 0.31810639\n",
      "Iteration 18, loss = 0.33146809\n",
      "Iteration 19, loss = 0.32226829\n",
      "Iteration 20, loss = 0.31273252\n",
      "Iteration 21, loss = 0.31456684\n",
      "Iteration 22, loss = 0.31319852\n",
      "Iteration 23, loss = 0.29603076\n",
      "Iteration 24, loss = 0.31277565\n",
      "Iteration 25, loss = 0.36318391\n",
      "Iteration 26, loss = 0.31334359\n",
      "Iteration 27, loss = 0.29734030\n",
      "Iteration 28, loss = 0.33820626\n",
      "Iteration 29, loss = 0.32347917\n",
      "Iteration 30, loss = 0.31236297\n",
      "Iteration 31, loss = 0.35709270\n",
      "Iteration 32, loss = 0.39921451\n",
      "Iteration 33, loss = 0.32280296\n",
      "Iteration 34, loss = 0.30571052\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.40062517\n",
      "Iteration 2, loss = 1.29393873\n",
      "Iteration 3, loss = 0.68758448\n",
      "Iteration 4, loss = 0.63741765\n",
      "Iteration 5, loss = 0.55170812\n",
      "Iteration 6, loss = 0.45664322\n",
      "Iteration 7, loss = 0.45061508\n",
      "Iteration 8, loss = 0.45306973\n",
      "Iteration 9, loss = 0.42220044\n",
      "Iteration 10, loss = 0.38697765\n",
      "Iteration 11, loss = 0.37415460\n",
      "Iteration 12, loss = 0.36105844\n",
      "Iteration 13, loss = 0.35099531\n",
      "Iteration 14, loss = 0.37171081\n",
      "Iteration 15, loss = 0.36960867\n",
      "Iteration 16, loss = 0.33237287\n",
      "Iteration 17, loss = 0.31780075\n",
      "Iteration 18, loss = 0.31607844\n",
      "Iteration 19, loss = 0.31342718\n",
      "Iteration 20, loss = 0.31171905\n",
      "Iteration 21, loss = 0.30303214\n",
      "Iteration 22, loss = 0.29249386\n",
      "Iteration 23, loss = 0.28169176\n",
      "Iteration 24, loss = 0.28635105\n",
      "Iteration 25, loss = 0.33608074\n",
      "Iteration 26, loss = 0.30851390\n",
      "Iteration 27, loss = 0.28262084\n",
      "Iteration 28, loss = 0.28758725\n",
      "Iteration 29, loss = 0.29062720\n",
      "Iteration 30, loss = 0.31257021\n",
      "Iteration 31, loss = 0.33396064\n",
      "Iteration 32, loss = 0.32641804\n",
      "Iteration 33, loss = 0.30074297\n",
      "Iteration 34, loss = 0.29499742\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.40883096\n",
      "Iteration 2, loss = 1.26760372\n",
      "Iteration 3, loss = 0.73015961\n",
      "Iteration 4, loss = 0.63368658\n",
      "Iteration 5, loss = 0.53644991\n",
      "Iteration 6, loss = 0.47338617\n",
      "Iteration 7, loss = 0.45287465\n",
      "Iteration 8, loss = 0.43759240\n",
      "Iteration 9, loss = 0.41419901\n",
      "Iteration 10, loss = 0.39435538\n",
      "Iteration 11, loss = 0.38001327\n",
      "Iteration 12, loss = 0.36921349\n",
      "Iteration 13, loss = 0.37101417\n",
      "Iteration 14, loss = 0.36233738\n",
      "Iteration 15, loss = 0.32745377\n",
      "Iteration 16, loss = 0.32021478\n",
      "Iteration 17, loss = 0.34630019\n",
      "Iteration 18, loss = 0.34382036\n",
      "Iteration 19, loss = 0.32959045\n",
      "Iteration 20, loss = 0.32141352\n",
      "Iteration 21, loss = 0.30325867\n",
      "Iteration 22, loss = 0.29491277\n",
      "Iteration 23, loss = 0.30105078\n",
      "Iteration 24, loss = 0.31195333\n",
      "Iteration 25, loss = 0.33037490\n",
      "Iteration 26, loss = 0.31594722\n",
      "Iteration 27, loss = 0.30285363\n",
      "Iteration 28, loss = 0.30222408\n",
      "Iteration 29, loss = 0.28727683\n",
      "Iteration 30, loss = 0.30509976\n",
      "Iteration 31, loss = 0.32351647\n",
      "Iteration 32, loss = 0.31385181\n",
      "Iteration 33, loss = 0.28778601\n",
      "Iteration 34, loss = 0.29938928\n",
      "Iteration 35, loss = 0.32149772\n",
      "Iteration 36, loss = 0.30646216\n",
      "Iteration 37, loss = 0.28016019\n",
      "Iteration 38, loss = 0.28148509\n",
      "Iteration 39, loss = 0.28665260\n",
      "Iteration 40, loss = 0.30871702\n",
      "Iteration 41, loss = 0.31969535\n",
      "Iteration 42, loss = 0.32541125\n",
      "Iteration 43, loss = 0.31876790\n",
      "Iteration 44, loss = 0.28987945\n",
      "Iteration 45, loss = 0.28452902\n",
      "Iteration 46, loss = 0.30283539\n",
      "Iteration 47, loss = 0.28648159\n",
      "Iteration 48, loss = 0.29621593\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.40552262\n",
      "Iteration 2, loss = 1.35849558\n",
      "Iteration 3, loss = 0.71172315\n",
      "Iteration 4, loss = 0.64026674\n",
      "Iteration 5, loss = 0.58039372\n",
      "Iteration 6, loss = 0.46692624\n",
      "Iteration 7, loss = 0.43238726\n",
      "Iteration 8, loss = 0.44158766\n",
      "Iteration 9, loss = 0.42074328\n",
      "Iteration 10, loss = 0.38154758\n",
      "Iteration 11, loss = 0.36745582\n",
      "Iteration 12, loss = 0.35942165\n",
      "Iteration 13, loss = 0.34730243\n",
      "Iteration 14, loss = 0.35099505\n",
      "Iteration 15, loss = 0.33346756\n",
      "Iteration 16, loss = 0.31413194\n",
      "Iteration 17, loss = 0.31196009\n",
      "Iteration 18, loss = 0.32139210\n",
      "Iteration 19, loss = 0.31814356\n",
      "Iteration 20, loss = 0.30776963\n",
      "Iteration 21, loss = 0.29852631\n",
      "Iteration 22, loss = 0.29706618\n",
      "Iteration 23, loss = 0.29359681\n",
      "Iteration 24, loss = 0.29683854\n",
      "Iteration 25, loss = 0.30368561\n",
      "Iteration 26, loss = 0.29775134\n",
      "Iteration 27, loss = 0.28826254\n",
      "Iteration 28, loss = 0.28577366\n",
      "Iteration 29, loss = 0.27907397\n",
      "Iteration 30, loss = 0.27001304\n",
      "Iteration 31, loss = 0.29640922\n",
      "Iteration 32, loss = 0.31051456\n",
      "Iteration 33, loss = 0.28633273\n",
      "Iteration 34, loss = 0.31013482\n",
      "Iteration 35, loss = 0.31986934\n",
      "Iteration 36, loss = 0.30161040\n",
      "Iteration 37, loss = 0.28968171\n",
      "Iteration 38, loss = 0.28784197\n",
      "Iteration 39, loss = 0.28609300\n",
      "Iteration 40, loss = 0.29368362\n",
      "Iteration 41, loss = 0.30569852\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.39875029\n",
      "Iteration 2, loss = 1.34951051\n",
      "Iteration 3, loss = 0.74102339\n",
      "Iteration 4, loss = 0.66622179\n",
      "Iteration 5, loss = 0.57400139\n",
      "Iteration 6, loss = 0.49320480\n",
      "Iteration 7, loss = 0.47001401\n",
      "Iteration 8, loss = 0.46572456\n",
      "Iteration 9, loss = 0.43591681\n",
      "Iteration 10, loss = 0.40717630\n",
      "Iteration 11, loss = 0.40320427\n",
      "Iteration 12, loss = 0.38903754\n",
      "Iteration 13, loss = 0.37227263\n",
      "Iteration 14, loss = 0.38225169\n",
      "Iteration 15, loss = 0.35058810\n",
      "Iteration 16, loss = 0.31869252\n",
      "Iteration 17, loss = 0.33051569\n",
      "Iteration 18, loss = 0.36035390\n",
      "Iteration 19, loss = 0.34976776\n",
      "Iteration 20, loss = 0.32401078\n",
      "Iteration 21, loss = 0.31039498\n",
      "Iteration 22, loss = 0.30035202\n",
      "Iteration 23, loss = 0.28546437\n",
      "Iteration 24, loss = 0.29962116\n",
      "Iteration 25, loss = 0.32005108\n",
      "Iteration 26, loss = 0.31550142\n",
      "Iteration 27, loss = 0.29508503\n",
      "Iteration 28, loss = 0.30278982\n",
      "Iteration 29, loss = 0.30096608\n",
      "Iteration 30, loss = 0.28721025\n",
      "Iteration 31, loss = 0.29571794\n",
      "Iteration 32, loss = 0.30606875\n",
      "Iteration 33, loss = 0.29122632\n",
      "Iteration 34, loss = 0.30629044\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.39555710\n",
      "Iteration 2, loss = 1.34225427\n",
      "Iteration 3, loss = 0.76133007\n",
      "Iteration 4, loss = 0.67421696\n",
      "Iteration 5, loss = 0.59511077\n",
      "Iteration 6, loss = 0.53053031\n",
      "Iteration 7, loss = 0.49687954\n",
      "Iteration 8, loss = 0.47116414\n",
      "Iteration 9, loss = 0.44503363\n",
      "Iteration 10, loss = 0.42180707\n",
      "Iteration 11, loss = 0.41424110\n",
      "Iteration 12, loss = 0.40328696\n",
      "Iteration 13, loss = 0.39358757\n",
      "Iteration 14, loss = 0.39104913\n",
      "Iteration 15, loss = 0.35934484\n",
      "Iteration 16, loss = 0.33591788\n",
      "Iteration 17, loss = 0.33835269\n",
      "Iteration 18, loss = 0.35860038\n",
      "Iteration 19, loss = 0.34319711\n",
      "Iteration 20, loss = 0.33886855\n",
      "Iteration 21, loss = 0.33354725\n",
      "Iteration 22, loss = 0.31117730\n",
      "Iteration 23, loss = 0.30891052\n",
      "Iteration 24, loss = 0.33525066\n",
      "Iteration 25, loss = 0.32276038\n",
      "Iteration 26, loss = 0.32083511\n",
      "Iteration 27, loss = 0.32026290\n",
      "Iteration 28, loss = 0.34072015\n",
      "Iteration 29, loss = 0.34646821\n",
      "Iteration 30, loss = 0.31065815\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 31, loss = 0.32023952\n",
      "Iteration 32, loss = 0.32411734\n",
      "Iteration 33, loss = 0.31971908\n",
      "Iteration 34, loss = 0.33989450\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.38587034\n",
      "Iteration 2, loss = 1.33415664\n",
      "Iteration 3, loss = 0.73006523\n",
      "Iteration 4, loss = 0.65056409\n",
      "Iteration 5, loss = 0.61088060\n",
      "Iteration 6, loss = 0.52664320\n",
      "Iteration 7, loss = 0.47113167\n",
      "Iteration 8, loss = 0.45913683\n",
      "Iteration 9, loss = 0.45083456\n",
      "Iteration 10, loss = 0.43369319\n",
      "Iteration 11, loss = 0.41913258\n",
      "Iteration 12, loss = 0.38877083\n",
      "Iteration 13, loss = 0.37539383\n",
      "Iteration 14, loss = 0.36943016\n",
      "Iteration 15, loss = 0.34876062\n",
      "Iteration 16, loss = 0.34121765\n",
      "Iteration 17, loss = 0.33748952\n",
      "Iteration 18, loss = 0.34267847\n",
      "Iteration 19, loss = 0.32013227\n",
      "Iteration 20, loss = 0.30002892\n",
      "Iteration 21, loss = 0.30754747\n",
      "Iteration 22, loss = 0.30211246\n",
      "Iteration 23, loss = 0.29691558\n",
      "Iteration 24, loss = 0.31901106\n",
      "Iteration 25, loss = 0.31193572\n",
      "Iteration 26, loss = 0.31587737\n",
      "Iteration 27, loss = 0.31853468\n",
      "Iteration 28, loss = 0.34071848\n",
      "Iteration 29, loss = 0.34870755\n",
      "Iteration 30, loss = 0.31037238\n",
      "Iteration 31, loss = 0.30532391\n",
      "Iteration 32, loss = 0.31779055\n",
      "Iteration 33, loss = 0.32542936\n",
      "Iteration 34, loss = 0.32982227\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.39677936\n",
      "Iteration 2, loss = 1.31512366\n",
      "Iteration 3, loss = 0.73674909\n",
      "Iteration 4, loss = 0.64234643\n",
      "Iteration 5, loss = 0.57989288\n",
      "Iteration 6, loss = 0.51743405\n",
      "Iteration 7, loss = 0.47556847\n",
      "Iteration 8, loss = 0.44563805\n",
      "Iteration 9, loss = 0.42760497\n",
      "Iteration 10, loss = 0.40524546\n",
      "Iteration 11, loss = 0.39535390\n",
      "Iteration 12, loss = 0.39445993\n",
      "Iteration 13, loss = 0.39168001\n",
      "Iteration 14, loss = 0.36137864\n",
      "Iteration 15, loss = 0.30854654\n",
      "Iteration 16, loss = 0.30708850\n",
      "Iteration 17, loss = 0.32408117\n",
      "Iteration 18, loss = 0.33209304\n",
      "Iteration 19, loss = 0.32296090\n",
      "Iteration 20, loss = 0.31367271\n",
      "Iteration 21, loss = 0.29212150\n",
      "Iteration 22, loss = 0.29917048\n",
      "Iteration 23, loss = 0.31196437\n",
      "Iteration 24, loss = 0.32939120\n",
      "Iteration 25, loss = 0.30788271\n",
      "Iteration 26, loss = 0.30334302\n",
      "Iteration 27, loss = 0.30898744\n",
      "Iteration 28, loss = 0.33919139\n",
      "Iteration 29, loss = 0.34748444\n",
      "Iteration 30, loss = 0.31370106\n",
      "Iteration 31, loss = 0.28834047\n",
      "Iteration 32, loss = 0.28449578\n",
      "Iteration 33, loss = 0.29522970\n",
      "Iteration 34, loss = 0.30162438\n",
      "Iteration 35, loss = 0.29878568\n",
      "Iteration 36, loss = 0.29894939\n",
      "Iteration 37, loss = 0.28358686\n",
      "Iteration 38, loss = 0.27229105\n",
      "Iteration 39, loss = 0.28331763\n",
      "Iteration 40, loss = 0.31122892\n",
      "Iteration 41, loss = 0.32905676\n",
      "Iteration 42, loss = 0.31929491\n",
      "Iteration 43, loss = 0.29147198\n",
      "Iteration 44, loss = 0.27725779\n",
      "Iteration 45, loss = 0.29712718\n",
      "Iteration 46, loss = 0.32955990\n",
      "Iteration 47, loss = 0.29330699\n",
      "Iteration 48, loss = 0.29792839\n",
      "Iteration 49, loss = 0.28800069\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.50195939\n",
      "Iteration 2, loss = 1.43922204\n",
      "Iteration 3, loss = 0.79613958\n",
      "Iteration 4, loss = 0.68259327\n",
      "Iteration 5, loss = 0.64864587\n",
      "Iteration 6, loss = 0.51236930\n",
      "Iteration 7, loss = 0.44697864\n",
      "Iteration 8, loss = 0.47290685\n",
      "Iteration 9, loss = 0.47869604\n",
      "Iteration 10, loss = 0.41455682\n",
      "Iteration 11, loss = 0.36727790\n",
      "Iteration 12, loss = 0.38774648\n",
      "Iteration 13, loss = 0.39060861\n",
      "Iteration 14, loss = 0.35766988\n",
      "Iteration 15, loss = 0.32762921\n",
      "Iteration 16, loss = 0.32910677\n",
      "Iteration 17, loss = 0.34123366\n",
      "Iteration 18, loss = 0.33627881\n",
      "Iteration 19, loss = 0.32579206\n",
      "Iteration 20, loss = 0.33310553\n",
      "Iteration 21, loss = 0.37516720\n",
      "Iteration 22, loss = 0.37712870\n",
      "Iteration 23, loss = 0.34784289\n",
      "Iteration 24, loss = 0.33863706\n",
      "Iteration 25, loss = 0.32562318\n",
      "Iteration 26, loss = 0.32390277\n",
      "Iteration 27, loss = 0.33935685\n",
      "Iteration 28, loss = 0.34196584\n",
      "Iteration 29, loss = 0.34803643\n",
      "Iteration 30, loss = 0.39949893\n",
      "Iteration 31, loss = 0.37610871\n",
      "Iteration 32, loss = 0.33139807\n",
      "Iteration 33, loss = 0.32780733\n",
      "Iteration 34, loss = 0.33088928\n",
      "Iteration 35, loss = 0.36319744\n",
      "Iteration 36, loss = 0.33282948\n",
      "Iteration 37, loss = 0.33455646\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.54234980\n",
      "Iteration 2, loss = 1.44881162\n",
      "Iteration 3, loss = 0.86392804\n",
      "Iteration 4, loss = 0.64593195\n",
      "Iteration 5, loss = 0.64724693\n",
      "Iteration 6, loss = 0.52827177\n",
      "Iteration 7, loss = 0.45005506\n",
      "Iteration 8, loss = 0.44160074\n",
      "Iteration 9, loss = 0.45076750\n",
      "Iteration 10, loss = 0.42421651\n",
      "Iteration 11, loss = 0.39009851\n",
      "Iteration 12, loss = 0.37798021\n",
      "Iteration 13, loss = 0.36964168\n",
      "Iteration 14, loss = 0.36198114\n",
      "Iteration 15, loss = 0.34498624\n",
      "Iteration 16, loss = 0.31998046\n",
      "Iteration 17, loss = 0.32149631\n",
      "Iteration 18, loss = 0.32420436\n",
      "Iteration 19, loss = 0.32153163\n",
      "Iteration 20, loss = 0.31209413\n",
      "Iteration 21, loss = 0.31637714\n",
      "Iteration 22, loss = 0.35200085\n",
      "Iteration 23, loss = 0.35284068\n",
      "Iteration 24, loss = 0.35715369\n",
      "Iteration 25, loss = 0.33522308\n",
      "Iteration 26, loss = 0.33527983\n",
      "Iteration 27, loss = 0.35608684\n",
      "Iteration 28, loss = 0.33596872\n",
      "Iteration 29, loss = 0.31909843\n",
      "Iteration 30, loss = 0.31088497\n",
      "Iteration 31, loss = 0.30685613\n",
      "Iteration 32, loss = 0.31133738\n",
      "Iteration 33, loss = 0.32297428\n",
      "Iteration 34, loss = 0.31113973\n",
      "Iteration 35, loss = 0.30003443\n",
      "Iteration 36, loss = 0.30345468\n",
      "Iteration 37, loss = 0.31515809\n",
      "Iteration 38, loss = 0.32137139\n",
      "Iteration 39, loss = 0.30602192\n",
      "Iteration 40, loss = 0.31230556\n",
      "Iteration 41, loss = 0.32360244\n",
      "Iteration 42, loss = 0.31417423\n",
      "Iteration 43, loss = 0.32762673\n",
      "Iteration 44, loss = 0.32137014\n",
      "Iteration 45, loss = 0.29206360\n",
      "Iteration 46, loss = 0.28479287\n",
      "Iteration 47, loss = 0.31142118\n",
      "Iteration 48, loss = 0.30899406\n",
      "Iteration 49, loss = 0.28062325\n",
      "Iteration 50, loss = 0.33778330\n",
      "Iteration 51, loss = 0.33938974\n",
      "Iteration 52, loss = 0.32557995\n",
      "Iteration 53, loss = 0.32076207\n",
      "Iteration 54, loss = 0.31322743\n",
      "Iteration 55, loss = 0.30465396\n",
      "Iteration 56, loss = 0.31546120\n",
      "Iteration 57, loss = 0.31400572\n",
      "Iteration 58, loss = 0.32911661\n",
      "Iteration 59, loss = 0.32214455\n",
      "Iteration 60, loss = 0.30991607\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.54681155\n",
      "Iteration 2, loss = 1.38609166\n",
      "Iteration 3, loss = 0.81159082\n",
      "Iteration 4, loss = 0.65551632\n",
      "Iteration 5, loss = 0.59678561\n",
      "Iteration 6, loss = 0.49241086\n",
      "Iteration 7, loss = 0.45026517\n",
      "Iteration 8, loss = 0.43637237\n",
      "Iteration 9, loss = 0.43910777\n",
      "Iteration 10, loss = 0.42285341\n",
      "Iteration 11, loss = 0.39034764\n",
      "Iteration 12, loss = 0.37267305\n",
      "Iteration 13, loss = 0.36497602\n",
      "Iteration 14, loss = 0.35983569\n",
      "Iteration 15, loss = 0.34903210\n",
      "Iteration 16, loss = 0.32921286\n",
      "Iteration 17, loss = 0.32133599\n",
      "Iteration 18, loss = 0.32074426\n",
      "Iteration 19, loss = 0.32290394\n",
      "Iteration 20, loss = 0.32677840\n",
      "Iteration 21, loss = 0.32079042\n",
      "Iteration 22, loss = 0.34580432\n",
      "Iteration 23, loss = 0.35720374\n",
      "Iteration 24, loss = 0.37512764\n",
      "Iteration 25, loss = 0.35925720\n",
      "Iteration 26, loss = 0.33247459\n",
      "Iteration 27, loss = 0.32017212\n",
      "Iteration 28, loss = 0.31517841\n",
      "Iteration 29, loss = 0.31753789\n",
      "Iteration 30, loss = 0.30405837\n",
      "Iteration 31, loss = 0.30647146\n",
      "Iteration 32, loss = 0.31600017\n",
      "Iteration 33, loss = 0.32049721\n",
      "Iteration 34, loss = 0.30572583\n",
      "Iteration 35, loss = 0.29684030\n",
      "Iteration 36, loss = 0.29736772\n",
      "Iteration 37, loss = 0.30889877\n",
      "Iteration 38, loss = 0.31722715\n",
      "Iteration 39, loss = 0.31074647\n",
      "Iteration 40, loss = 0.29456802\n",
      "Iteration 41, loss = 0.30389275\n",
      "Iteration 42, loss = 0.31103897\n",
      "Iteration 43, loss = 0.34616351\n",
      "Iteration 44, loss = 0.32827265\n",
      "Iteration 45, loss = 0.29346201\n",
      "Iteration 46, loss = 0.29303179\n",
      "Iteration 47, loss = 0.31168807\n",
      "Iteration 48, loss = 0.29216355\n",
      "Iteration 49, loss = 0.28710645\n",
      "Iteration 50, loss = 0.33685128\n",
      "Iteration 51, loss = 0.36606395\n",
      "Iteration 52, loss = 0.34308640\n",
      "Iteration 53, loss = 0.29026546\n",
      "Iteration 54, loss = 0.31045231\n",
      "Iteration 55, loss = 0.33207427\n",
      "Iteration 56, loss = 0.32991037\n",
      "Iteration 57, loss = 0.32492806\n",
      "Iteration 58, loss = 0.34167834\n",
      "Iteration 59, loss = 0.31941851\n",
      "Iteration 60, loss = 0.30735251\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.54518582\n",
      "Iteration 2, loss = 1.45675394\n",
      "Iteration 3, loss = 1.01949759\n",
      "Iteration 4, loss = 0.63485107\n",
      "Iteration 5, loss = 0.59557608\n",
      "Iteration 6, loss = 0.57020828\n",
      "Iteration 7, loss = 0.49656948\n",
      "Iteration 8, loss = 0.43975290\n",
      "Iteration 9, loss = 0.42579306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10, loss = 0.41052682\n",
      "Iteration 11, loss = 0.37666434\n",
      "Iteration 12, loss = 0.36084088\n",
      "Iteration 13, loss = 0.35790846\n",
      "Iteration 14, loss = 0.35655994\n",
      "Iteration 15, loss = 0.33777191\n",
      "Iteration 16, loss = 0.31401233\n",
      "Iteration 17, loss = 0.31109555\n",
      "Iteration 18, loss = 0.31701737\n",
      "Iteration 19, loss = 0.31311320\n",
      "Iteration 20, loss = 0.31205745\n",
      "Iteration 21, loss = 0.29976547\n",
      "Iteration 22, loss = 0.32970764\n",
      "Iteration 23, loss = 0.34474923\n",
      "Iteration 24, loss = 0.34620095\n",
      "Iteration 25, loss = 0.33800327\n",
      "Iteration 26, loss = 0.32063004\n",
      "Iteration 27, loss = 0.29329071\n",
      "Iteration 28, loss = 0.28704711\n",
      "Iteration 29, loss = 0.28538206\n",
      "Iteration 30, loss = 0.28554761\n",
      "Iteration 31, loss = 0.30305392\n",
      "Iteration 32, loss = 0.30878762\n",
      "Iteration 33, loss = 0.30483972\n",
      "Iteration 34, loss = 0.28967607\n",
      "Iteration 35, loss = 0.28129514\n",
      "Iteration 36, loss = 0.29002136\n",
      "Iteration 37, loss = 0.29137151\n",
      "Iteration 38, loss = 0.28829201\n",
      "Iteration 39, loss = 0.28922137\n",
      "Iteration 40, loss = 0.29242502\n",
      "Iteration 41, loss = 0.30162788\n",
      "Iteration 42, loss = 0.29448359\n",
      "Iteration 43, loss = 0.31632400\n",
      "Iteration 44, loss = 0.31797903\n",
      "Iteration 45, loss = 0.30488570\n",
      "Iteration 46, loss = 0.27933012\n",
      "Iteration 47, loss = 0.29128727\n",
      "Iteration 48, loss = 0.30054477\n",
      "Iteration 49, loss = 0.29707277\n",
      "Iteration 50, loss = 0.34113268\n",
      "Iteration 51, loss = 0.35853489\n",
      "Iteration 52, loss = 0.33835677\n",
      "Iteration 53, loss = 0.30624743\n",
      "Iteration 54, loss = 0.30672967\n",
      "Iteration 55, loss = 0.32820992\n",
      "Iteration 56, loss = 0.31499716\n",
      "Iteration 57, loss = 0.34435414\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.49308230\n",
      "Iteration 2, loss = 1.45768782\n",
      "Iteration 3, loss = 0.89857733\n",
      "Iteration 4, loss = 0.62574093\n",
      "Iteration 5, loss = 0.62177448\n",
      "Iteration 6, loss = 0.53889300\n",
      "Iteration 7, loss = 0.46813693\n",
      "Iteration 8, loss = 0.44196874\n",
      "Iteration 9, loss = 0.43053647\n",
      "Iteration 10, loss = 0.40694010\n",
      "Iteration 11, loss = 0.36845609\n",
      "Iteration 12, loss = 0.36099124\n",
      "Iteration 13, loss = 0.36448038\n",
      "Iteration 14, loss = 0.34621282\n",
      "Iteration 15, loss = 0.31997683\n",
      "Iteration 16, loss = 0.31290797\n",
      "Iteration 17, loss = 0.32005619\n",
      "Iteration 18, loss = 0.32182790\n",
      "Iteration 19, loss = 0.31465005\n",
      "Iteration 20, loss = 0.31464256\n",
      "Iteration 21, loss = 0.31076539\n",
      "Iteration 22, loss = 0.32545731\n",
      "Iteration 23, loss = 0.32663072\n",
      "Iteration 24, loss = 0.31869748\n",
      "Iteration 25, loss = 0.31890552\n",
      "Iteration 26, loss = 0.31659111\n",
      "Iteration 27, loss = 0.30532356\n",
      "Iteration 28, loss = 0.30185958\n",
      "Iteration 29, loss = 0.30073571\n",
      "Iteration 30, loss = 0.29779420\n",
      "Iteration 31, loss = 0.31356858\n",
      "Iteration 32, loss = 0.32654824\n",
      "Iteration 33, loss = 0.31867852\n",
      "Iteration 34, loss = 0.28642923\n",
      "Iteration 35, loss = 0.27739366\n",
      "Iteration 36, loss = 0.28524100\n",
      "Iteration 37, loss = 0.28579999\n",
      "Iteration 38, loss = 0.28086512\n",
      "Iteration 39, loss = 0.28969061\n",
      "Iteration 40, loss = 0.30461505\n",
      "Iteration 41, loss = 0.30709801\n",
      "Iteration 42, loss = 0.30275546\n",
      "Iteration 43, loss = 0.33890316\n",
      "Iteration 44, loss = 0.32579118\n",
      "Iteration 45, loss = 0.31730460\n",
      "Iteration 46, loss = 0.32343957\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.49323212\n",
      "Iteration 2, loss = 1.48333600\n",
      "Iteration 3, loss = 0.90176803\n",
      "Iteration 4, loss = 0.62161494\n",
      "Iteration 5, loss = 0.60269410\n",
      "Iteration 6, loss = 0.53210413\n",
      "Iteration 7, loss = 0.47773493\n",
      "Iteration 8, loss = 0.44210065\n",
      "Iteration 9, loss = 0.41021943\n",
      "Iteration 10, loss = 0.39879116\n",
      "Iteration 11, loss = 0.36996277\n",
      "Iteration 12, loss = 0.34461550\n",
      "Iteration 13, loss = 0.33679586\n",
      "Iteration 14, loss = 0.32911995\n",
      "Iteration 15, loss = 0.30487775\n",
      "Iteration 16, loss = 0.30306565\n",
      "Iteration 17, loss = 0.32217484\n",
      "Iteration 18, loss = 0.32835160\n",
      "Iteration 19, loss = 0.31534200\n",
      "Iteration 20, loss = 0.31030520\n",
      "Iteration 21, loss = 0.30383577\n",
      "Iteration 22, loss = 0.31154170\n",
      "Iteration 23, loss = 0.30874050\n",
      "Iteration 24, loss = 0.30199025\n",
      "Iteration 25, loss = 0.31436760\n",
      "Iteration 26, loss = 0.31475187\n",
      "Iteration 27, loss = 0.29662366\n",
      "Iteration 28, loss = 0.30721512\n",
      "Iteration 29, loss = 0.30729035\n",
      "Iteration 30, loss = 0.30264279\n",
      "Iteration 31, loss = 0.31511229\n",
      "Iteration 32, loss = 0.30807750\n",
      "Iteration 33, loss = 0.29774797\n",
      "Iteration 34, loss = 0.29286463\n",
      "Iteration 35, loss = 0.28825227\n",
      "Iteration 36, loss = 0.27970674\n",
      "Iteration 37, loss = 0.26813345\n",
      "Iteration 38, loss = 0.27444848\n",
      "Iteration 39, loss = 0.29981243\n",
      "Iteration 40, loss = 0.30565096\n",
      "Iteration 41, loss = 0.32734309\n",
      "Iteration 42, loss = 0.29332567\n",
      "Iteration 43, loss = 0.33553537\n",
      "Iteration 44, loss = 0.32953160\n",
      "Iteration 45, loss = 0.31673432\n",
      "Iteration 46, loss = 0.30863274\n",
      "Iteration 47, loss = 0.29899443\n",
      "Iteration 48, loss = 0.28894600\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.45374824\n",
      "Iteration 2, loss = 1.44774226\n",
      "Iteration 3, loss = 0.81738736\n",
      "Iteration 4, loss = 0.62600600\n",
      "Iteration 5, loss = 0.60044826\n",
      "Iteration 6, loss = 0.49684615\n",
      "Iteration 7, loss = 0.45039514\n",
      "Iteration 8, loss = 0.43175200\n",
      "Iteration 9, loss = 0.41420556\n",
      "Iteration 10, loss = 0.39871299\n",
      "Iteration 11, loss = 0.36624084\n",
      "Iteration 12, loss = 0.34333981\n",
      "Iteration 13, loss = 0.33585824\n",
      "Iteration 14, loss = 0.32243907\n",
      "Iteration 15, loss = 0.30663079\n",
      "Iteration 16, loss = 0.32142504\n",
      "Iteration 17, loss = 0.34361807\n",
      "Iteration 18, loss = 0.33678151\n",
      "Iteration 19, loss = 0.31753493\n",
      "Iteration 20, loss = 0.31632214\n",
      "Iteration 21, loss = 0.30742810\n",
      "Iteration 22, loss = 0.31971389\n",
      "Iteration 23, loss = 0.30803406\n",
      "Iteration 24, loss = 0.30561806\n",
      "Iteration 25, loss = 0.31880196\n",
      "Iteration 26, loss = 0.31604296\n",
      "Iteration 27, loss = 0.30777556\n",
      "Iteration 28, loss = 0.31239861\n",
      "Iteration 29, loss = 0.30253621\n",
      "Iteration 30, loss = 0.30406626\n",
      "Iteration 31, loss = 0.31088084\n",
      "Iteration 32, loss = 0.29471997\n",
      "Iteration 33, loss = 0.28781249\n",
      "Iteration 34, loss = 0.29761438\n",
      "Iteration 35, loss = 0.29398716\n",
      "Iteration 36, loss = 0.29296164\n",
      "Iteration 37, loss = 0.28704326\n",
      "Iteration 38, loss = 0.29128576\n",
      "Iteration 39, loss = 0.29839090\n",
      "Iteration 40, loss = 0.30070307\n",
      "Iteration 41, loss = 0.30498505\n",
      "Iteration 42, loss = 0.27982573\n",
      "Iteration 43, loss = 0.30063985\n",
      "Iteration 44, loss = 0.29780495\n",
      "Iteration 45, loss = 0.30215165\n",
      "Iteration 46, loss = 0.29105255\n",
      "Iteration 47, loss = 0.29403130\n",
      "Iteration 48, loss = 0.28806929\n",
      "Iteration 49, loss = 0.32688823\n",
      "Iteration 50, loss = 0.34700040\n",
      "Iteration 51, loss = 0.30523836\n",
      "Iteration 52, loss = 0.28907106\n",
      "Iteration 53, loss = 0.32059431\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.47543045\n",
      "Iteration 2, loss = 1.44715680\n",
      "Iteration 3, loss = 0.84997357\n",
      "Iteration 4, loss = 0.65798348\n",
      "Iteration 5, loss = 0.64208411\n",
      "Iteration 6, loss = 0.52129173\n",
      "Iteration 7, loss = 0.46693860\n",
      "Iteration 8, loss = 0.45550672\n",
      "Iteration 9, loss = 0.43705546\n",
      "Iteration 10, loss = 0.40807667\n",
      "Iteration 11, loss = 0.37354775\n",
      "Iteration 12, loss = 0.36703240\n",
      "Iteration 13, loss = 0.36542800\n",
      "Iteration 14, loss = 0.34122478\n",
      "Iteration 15, loss = 0.31505357\n",
      "Iteration 16, loss = 0.32548966\n",
      "Iteration 17, loss = 0.34518548\n",
      "Iteration 18, loss = 0.34289647\n",
      "Iteration 19, loss = 0.33417319\n",
      "Iteration 20, loss = 0.32465252\n",
      "Iteration 21, loss = 0.31586948\n",
      "Iteration 22, loss = 0.32484148\n",
      "Iteration 23, loss = 0.31875287\n",
      "Iteration 24, loss = 0.32682938\n",
      "Iteration 25, loss = 0.34074987\n",
      "Iteration 26, loss = 0.34301171\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.45029077\n",
      "Iteration 2, loss = 1.52818544\n",
      "Iteration 3, loss = 0.92193096\n",
      "Iteration 4, loss = 0.62447948\n",
      "Iteration 5, loss = 0.62524132\n",
      "Iteration 6, loss = 0.57579895\n",
      "Iteration 7, loss = 0.49550884\n",
      "Iteration 8, loss = 0.44080413\n",
      "Iteration 9, loss = 0.42746886\n",
      "Iteration 10, loss = 0.43360118\n",
      "Iteration 11, loss = 0.40278927\n",
      "Iteration 12, loss = 0.37149914\n",
      "Iteration 13, loss = 0.36229402\n",
      "Iteration 14, loss = 0.35021809\n",
      "Iteration 15, loss = 0.32188424\n",
      "Iteration 16, loss = 0.32117392\n",
      "Iteration 17, loss = 0.34465054\n",
      "Iteration 18, loss = 0.34261875\n",
      "Iteration 19, loss = 0.33942992\n",
      "Iteration 20, loss = 0.32467649\n",
      "Iteration 21, loss = 0.32006401\n",
      "Iteration 22, loss = 0.32793840\n",
      "Iteration 23, loss = 0.30585409\n",
      "Iteration 24, loss = 0.30740216\n",
      "Iteration 25, loss = 0.33281633\n",
      "Iteration 26, loss = 0.33625349\n",
      "Iteration 27, loss = 0.31271131\n",
      "Iteration 28, loss = 0.31425165\n",
      "Iteration 29, loss = 0.31475839\n",
      "Iteration 30, loss = 0.31095164\n",
      "Iteration 31, loss = 0.30936545\n",
      "Iteration 32, loss = 0.31454558\n",
      "Iteration 33, loss = 0.29052433\n",
      "Iteration 34, loss = 0.31223650\n",
      "Iteration 35, loss = 0.31645995\n",
      "Iteration 36, loss = 0.32442083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 37, loss = 0.32481619\n",
      "Iteration 38, loss = 0.32949971\n",
      "Iteration 39, loss = 0.32207609\n",
      "Iteration 40, loss = 0.29241518\n",
      "Iteration 41, loss = 0.30123838\n",
      "Iteration 42, loss = 0.28474568\n",
      "Iteration 43, loss = 0.30223323\n",
      "Iteration 44, loss = 0.31634571\n",
      "Iteration 45, loss = 0.30821427\n",
      "Iteration 46, loss = 0.27651589\n",
      "Iteration 47, loss = 0.28765131\n",
      "Iteration 48, loss = 0.30825323\n",
      "Iteration 49, loss = 0.33724923\n",
      "Iteration 50, loss = 0.32702830\n",
      "Iteration 51, loss = 0.29398830\n",
      "Iteration 52, loss = 0.28120126\n",
      "Iteration 53, loss = 0.31031716\n",
      "Iteration 54, loss = 0.33641231\n",
      "Iteration 55, loss = 0.30772251\n",
      "Iteration 56, loss = 0.29345837\n",
      "Iteration 57, loss = 0.31864714\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.44633315\n",
      "Iteration 2, loss = 1.53580785\n",
      "Iteration 3, loss = 0.88857761\n",
      "Iteration 4, loss = 0.62431410\n",
      "Iteration 5, loss = 0.60721449\n",
      "Iteration 6, loss = 0.53542240\n",
      "Iteration 7, loss = 0.47071042\n",
      "Iteration 8, loss = 0.43802303\n",
      "Iteration 9, loss = 0.42305668\n",
      "Iteration 10, loss = 0.41826638\n",
      "Iteration 11, loss = 0.38421580\n",
      "Iteration 12, loss = 0.35443692\n",
      "Iteration 13, loss = 0.34685717\n",
      "Iteration 14, loss = 0.34041357\n",
      "Iteration 15, loss = 0.31546338\n",
      "Iteration 16, loss = 0.31192472\n",
      "Iteration 17, loss = 0.32562634\n",
      "Iteration 18, loss = 0.31996115\n",
      "Iteration 19, loss = 0.32145063\n",
      "Iteration 20, loss = 0.32700328\n",
      "Iteration 21, loss = 0.32384242\n",
      "Iteration 22, loss = 0.31838971\n",
      "Iteration 23, loss = 0.30063950\n",
      "Iteration 24, loss = 0.29881045\n",
      "Iteration 25, loss = 0.32690550\n",
      "Iteration 26, loss = 0.34302114\n",
      "Iteration 27, loss = 0.30860421\n",
      "Iteration 28, loss = 0.30177531\n",
      "Iteration 29, loss = 0.32491174\n",
      "Iteration 30, loss = 0.31923321\n",
      "Iteration 31, loss = 0.29837764\n",
      "Iteration 32, loss = 0.31534683\n",
      "Iteration 33, loss = 0.30030283\n",
      "Iteration 34, loss = 0.32867169\n",
      "Iteration 35, loss = 0.31774611\n",
      "Iteration 36, loss = 0.30802684\n",
      "Iteration 37, loss = 0.32307899\n",
      "Iteration 38, loss = 0.33363880\n",
      "Iteration 39, loss = 0.33326985\n",
      "Iteration 40, loss = 0.29907409\n",
      "Iteration 41, loss = 0.29100228\n",
      "Iteration 42, loss = 0.28539894\n",
      "Iteration 43, loss = 0.31450721\n",
      "Iteration 44, loss = 0.33247017\n",
      "Iteration 45, loss = 0.30813985\n",
      "Iteration 46, loss = 0.27091308\n",
      "Iteration 47, loss = 0.27852816\n",
      "Iteration 48, loss = 0.29641514\n",
      "Iteration 49, loss = 0.31097042\n",
      "Iteration 50, loss = 0.31936381\n",
      "Iteration 51, loss = 0.30512347\n",
      "Iteration 52, loss = 0.28635067\n",
      "Iteration 53, loss = 0.28743240\n",
      "Iteration 54, loss = 0.31689181\n",
      "Iteration 55, loss = 0.31409169\n",
      "Iteration 56, loss = 0.29350124\n",
      "Iteration 57, loss = 0.30812118\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.27438119\n",
      "Iteration 2, loss = 1.34621739\n",
      "Iteration 3, loss = 0.72777426\n",
      "Iteration 4, loss = 0.57265726\n",
      "Iteration 5, loss = 0.55781336\n",
      "Iteration 6, loss = 0.48234814\n",
      "Iteration 7, loss = 0.43446838\n",
      "Iteration 8, loss = 0.43054928\n",
      "Iteration 9, loss = 0.41180537\n",
      "Iteration 10, loss = 0.37737233\n",
      "Iteration 11, loss = 0.36103709\n",
      "Iteration 12, loss = 0.34600973\n",
      "Iteration 13, loss = 0.34637077\n",
      "Iteration 14, loss = 0.33490312\n",
      "Iteration 15, loss = 0.31645007\n",
      "Iteration 16, loss = 0.32913175\n",
      "Iteration 17, loss = 0.33044138\n",
      "Iteration 18, loss = 0.32837528\n",
      "Iteration 19, loss = 0.33948379\n",
      "Iteration 20, loss = 0.33559237\n",
      "Iteration 21, loss = 0.32721493\n",
      "Iteration 22, loss = 0.35118433\n",
      "Iteration 23, loss = 0.36472123\n",
      "Iteration 24, loss = 0.31595976\n",
      "Iteration 25, loss = 0.34193953\n",
      "Iteration 26, loss = 0.37394660\n",
      "Iteration 27, loss = 0.35524672\n",
      "Iteration 28, loss = 0.33663474\n",
      "Iteration 29, loss = 0.31583797\n",
      "Iteration 30, loss = 0.35596167\n",
      "Iteration 31, loss = 0.38311860\n",
      "Iteration 32, loss = 0.37120580\n",
      "Iteration 33, loss = 0.35486580\n",
      "Iteration 34, loss = 0.32149871\n",
      "Iteration 35, loss = 0.32508740\n",
      "Iteration 36, loss = 0.33774058\n",
      "Iteration 37, loss = 0.32712205\n",
      "Iteration 38, loss = 0.32323884\n",
      "Iteration 39, loss = 0.32291551\n",
      "Iteration 40, loss = 0.31555933\n",
      "Iteration 41, loss = 0.29830914\n",
      "Iteration 42, loss = 0.30989541\n",
      "Iteration 43, loss = 0.34119887\n",
      "Iteration 44, loss = 0.32855562\n",
      "Iteration 45, loss = 0.33985861\n",
      "Iteration 46, loss = 0.32375914\n",
      "Iteration 47, loss = 0.31279961\n",
      "Iteration 48, loss = 0.32811923\n",
      "Iteration 49, loss = 0.31737964\n",
      "Iteration 50, loss = 0.31637788\n",
      "Iteration 51, loss = 0.32661680\n",
      "Iteration 52, loss = 0.32773939\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.30093772\n",
      "Iteration 2, loss = 1.33546929\n",
      "Iteration 3, loss = 0.81327447\n",
      "Iteration 4, loss = 0.59197667\n",
      "Iteration 5, loss = 0.51923317\n",
      "Iteration 6, loss = 0.46906090\n",
      "Iteration 7, loss = 0.45420785\n",
      "Iteration 8, loss = 0.43864226\n",
      "Iteration 9, loss = 0.41973950\n",
      "Iteration 10, loss = 0.38229382\n",
      "Iteration 11, loss = 0.36362466\n",
      "Iteration 12, loss = 0.36155338\n",
      "Iteration 13, loss = 0.34906655\n",
      "Iteration 14, loss = 0.33516151\n",
      "Iteration 15, loss = 0.33053659\n",
      "Iteration 16, loss = 0.33030934\n",
      "Iteration 17, loss = 0.32798962\n",
      "Iteration 18, loss = 0.31976512\n",
      "Iteration 19, loss = 0.32359336\n",
      "Iteration 20, loss = 0.34530674\n",
      "Iteration 21, loss = 0.32240474\n",
      "Iteration 22, loss = 0.29959472\n",
      "Iteration 23, loss = 0.33978937\n",
      "Iteration 24, loss = 0.33438648\n",
      "Iteration 25, loss = 0.30696274\n",
      "Iteration 26, loss = 0.32289439\n",
      "Iteration 27, loss = 0.34979963\n",
      "Iteration 28, loss = 0.34259055\n",
      "Iteration 29, loss = 0.29992967\n",
      "Iteration 30, loss = 0.31691204\n",
      "Iteration 31, loss = 0.32216713\n",
      "Iteration 32, loss = 0.33167227\n",
      "Iteration 33, loss = 0.34864913\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.30869431\n",
      "Iteration 2, loss = 1.45546598\n",
      "Iteration 3, loss = 0.87867172\n",
      "Iteration 4, loss = 0.66265313\n",
      "Iteration 5, loss = 0.53414607\n",
      "Iteration 6, loss = 0.48048349\n",
      "Iteration 7, loss = 0.48116346\n",
      "Iteration 8, loss = 0.45781033\n",
      "Iteration 9, loss = 0.42330069\n",
      "Iteration 10, loss = 0.39626277\n",
      "Iteration 11, loss = 0.37472896\n",
      "Iteration 12, loss = 0.36703225\n",
      "Iteration 13, loss = 0.35321228\n",
      "Iteration 14, loss = 0.33778829\n",
      "Iteration 15, loss = 0.32859567\n",
      "Iteration 16, loss = 0.32688711\n",
      "Iteration 17, loss = 0.33077854\n",
      "Iteration 18, loss = 0.33132315\n",
      "Iteration 19, loss = 0.33003896\n",
      "Iteration 20, loss = 0.33470190\n",
      "Iteration 21, loss = 0.31984671\n",
      "Iteration 22, loss = 0.30192300\n",
      "Iteration 23, loss = 0.31264780\n",
      "Iteration 24, loss = 0.31773281\n",
      "Iteration 25, loss = 0.31468087\n",
      "Iteration 26, loss = 0.31243023\n",
      "Iteration 27, loss = 0.32010259\n",
      "Iteration 28, loss = 0.32885028\n",
      "Iteration 29, loss = 0.30561742\n",
      "Iteration 30, loss = 0.30879420\n",
      "Iteration 31, loss = 0.32302771\n",
      "Iteration 32, loss = 0.31689398\n",
      "Iteration 33, loss = 0.31523424\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.33023237\n",
      "Iteration 2, loss = 1.36295682\n",
      "Iteration 3, loss = 0.88134316\n",
      "Iteration 4, loss = 0.61337101\n",
      "Iteration 5, loss = 0.54194355\n",
      "Iteration 6, loss = 0.48610784\n",
      "Iteration 7, loss = 0.46621430\n",
      "Iteration 8, loss = 0.44543933\n",
      "Iteration 9, loss = 0.42292314\n",
      "Iteration 10, loss = 0.39328881\n",
      "Iteration 11, loss = 0.35933071\n",
      "Iteration 12, loss = 0.35339178\n",
      "Iteration 13, loss = 0.33896491\n",
      "Iteration 14, loss = 0.32689690\n",
      "Iteration 15, loss = 0.31603735\n",
      "Iteration 16, loss = 0.30525608\n",
      "Iteration 17, loss = 0.30910999\n",
      "Iteration 18, loss = 0.30983294\n",
      "Iteration 19, loss = 0.31299481\n",
      "Iteration 20, loss = 0.32105653\n",
      "Iteration 21, loss = 0.31243856\n",
      "Iteration 22, loss = 0.29445237\n",
      "Iteration 23, loss = 0.28870952\n",
      "Iteration 24, loss = 0.29005477\n",
      "Iteration 25, loss = 0.30323259\n",
      "Iteration 26, loss = 0.30990564\n",
      "Iteration 27, loss = 0.30359314\n",
      "Iteration 28, loss = 0.31305335\n",
      "Iteration 29, loss = 0.30866546\n",
      "Iteration 30, loss = 0.30295995\n",
      "Iteration 31, loss = 0.31467225\n",
      "Iteration 32, loss = 0.30930604\n",
      "Iteration 33, loss = 0.28959973\n",
      "Iteration 34, loss = 0.29034475\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.31914859\n",
      "Iteration 2, loss = 1.42620885\n",
      "Iteration 3, loss = 0.91536158\n",
      "Iteration 4, loss = 0.67152461\n",
      "Iteration 5, loss = 0.55571735\n",
      "Iteration 6, loss = 0.48565903\n",
      "Iteration 7, loss = 0.48520925\n",
      "Iteration 8, loss = 0.46379470\n",
      "Iteration 9, loss = 0.42496171\n",
      "Iteration 10, loss = 0.39487242\n",
      "Iteration 11, loss = 0.36905952\n",
      "Iteration 12, loss = 0.36254075\n",
      "Iteration 13, loss = 0.34292049\n",
      "Iteration 14, loss = 0.32459035\n",
      "Iteration 15, loss = 0.31372437\n",
      "Iteration 16, loss = 0.30682902\n",
      "Iteration 17, loss = 0.30931193\n",
      "Iteration 18, loss = 0.30914083\n",
      "Iteration 19, loss = 0.31103719\n",
      "Iteration 20, loss = 0.32630937\n",
      "Iteration 21, loss = 0.31787482\n",
      "Iteration 22, loss = 0.31873757\n",
      "Iteration 23, loss = 0.32831701\n",
      "Iteration 24, loss = 0.31552187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 25, loss = 0.30711016\n",
      "Iteration 26, loss = 0.29307612\n",
      "Iteration 27, loss = 0.29533524\n",
      "Iteration 28, loss = 0.32957525\n",
      "Iteration 29, loss = 0.32994942\n",
      "Iteration 30, loss = 0.32360506\n",
      "Iteration 31, loss = 0.32218479\n",
      "Iteration 32, loss = 0.32892683\n",
      "Iteration 33, loss = 0.32562115\n",
      "Iteration 34, loss = 0.29697920\n",
      "Iteration 35, loss = 0.31216543\n",
      "Iteration 36, loss = 0.37260185\n",
      "Iteration 37, loss = 0.37581791\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.33994845\n",
      "Iteration 2, loss = 1.36029119\n",
      "Iteration 3, loss = 0.82290436\n",
      "Iteration 4, loss = 0.66732206\n",
      "Iteration 5, loss = 0.56154958\n",
      "Iteration 6, loss = 0.48101274\n",
      "Iteration 7, loss = 0.46065279\n",
      "Iteration 8, loss = 0.43248077\n",
      "Iteration 9, loss = 0.40652349\n",
      "Iteration 10, loss = 0.39228452\n",
      "Iteration 11, loss = 0.37281201\n",
      "Iteration 12, loss = 0.35867235\n",
      "Iteration 13, loss = 0.33961169\n",
      "Iteration 14, loss = 0.31792052\n",
      "Iteration 15, loss = 0.31397544\n",
      "Iteration 16, loss = 0.31053281\n",
      "Iteration 17, loss = 0.30335459\n",
      "Iteration 18, loss = 0.29409589\n",
      "Iteration 19, loss = 0.30760539\n",
      "Iteration 20, loss = 0.33187928\n",
      "Iteration 21, loss = 0.31761659\n",
      "Iteration 22, loss = 0.31983597\n",
      "Iteration 23, loss = 0.32406451\n",
      "Iteration 24, loss = 0.30592661\n",
      "Iteration 25, loss = 0.29795253\n",
      "Iteration 26, loss = 0.28588378\n",
      "Iteration 27, loss = 0.27964269\n",
      "Iteration 28, loss = 0.30479721\n",
      "Iteration 29, loss = 0.31351405\n",
      "Iteration 30, loss = 0.30001664\n",
      "Iteration 31, loss = 0.29770127\n",
      "Iteration 32, loss = 0.29078181\n",
      "Iteration 33, loss = 0.29023625\n",
      "Iteration 34, loss = 0.27805513\n",
      "Iteration 35, loss = 0.28316475\n",
      "Iteration 36, loss = 0.30097071\n",
      "Iteration 37, loss = 0.30645560\n",
      "Iteration 38, loss = 0.30432178\n",
      "Iteration 39, loss = 0.29844713\n",
      "Iteration 40, loss = 0.29427418\n",
      "Iteration 41, loss = 0.28616915\n",
      "Iteration 42, loss = 0.29428259\n",
      "Iteration 43, loss = 0.32269922\n",
      "Iteration 44, loss = 0.30922910\n",
      "Iteration 45, loss = 0.31178755\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.30472903\n",
      "Iteration 2, loss = 1.29854551\n",
      "Iteration 3, loss = 0.72524766\n",
      "Iteration 4, loss = 0.62195971\n",
      "Iteration 5, loss = 0.50940187\n",
      "Iteration 6, loss = 0.45148946\n",
      "Iteration 7, loss = 0.43768468\n",
      "Iteration 8, loss = 0.40786450\n",
      "Iteration 9, loss = 0.38604256\n",
      "Iteration 10, loss = 0.37496285\n",
      "Iteration 11, loss = 0.35755907\n",
      "Iteration 12, loss = 0.35086480\n",
      "Iteration 13, loss = 0.32268719\n",
      "Iteration 14, loss = 0.30437652\n",
      "Iteration 15, loss = 0.30318532\n",
      "Iteration 16, loss = 0.30418063\n",
      "Iteration 17, loss = 0.30278942\n",
      "Iteration 18, loss = 0.29931921\n",
      "Iteration 19, loss = 0.30815599\n",
      "Iteration 20, loss = 0.31580735\n",
      "Iteration 21, loss = 0.31164514\n",
      "Iteration 22, loss = 0.30232177\n",
      "Iteration 23, loss = 0.29789430\n",
      "Iteration 24, loss = 0.29788008\n",
      "Iteration 25, loss = 0.27956902\n",
      "Iteration 26, loss = 0.26795413\n",
      "Iteration 27, loss = 0.28322920\n",
      "Iteration 28, loss = 0.29153031\n",
      "Iteration 29, loss = 0.31625370\n",
      "Iteration 30, loss = 0.31400091\n",
      "Iteration 31, loss = 0.31064150\n",
      "Iteration 32, loss = 0.30959809\n",
      "Iteration 33, loss = 0.29119989\n",
      "Iteration 34, loss = 0.28701798\n",
      "Iteration 35, loss = 0.29220636\n",
      "Iteration 36, loss = 0.31629129\n",
      "Iteration 37, loss = 0.34481293\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.30950900\n",
      "Iteration 2, loss = 1.30245215\n",
      "Iteration 3, loss = 0.76063767\n",
      "Iteration 4, loss = 0.65068572\n",
      "Iteration 5, loss = 0.53760629\n",
      "Iteration 6, loss = 0.48303117\n",
      "Iteration 7, loss = 0.46760308\n",
      "Iteration 8, loss = 0.43377329\n",
      "Iteration 9, loss = 0.41214145\n",
      "Iteration 10, loss = 0.40281547\n",
      "Iteration 11, loss = 0.37603223\n",
      "Iteration 12, loss = 0.36894132\n",
      "Iteration 13, loss = 0.34628040\n",
      "Iteration 14, loss = 0.32734413\n",
      "Iteration 15, loss = 0.31797937\n",
      "Iteration 16, loss = 0.30929298\n",
      "Iteration 17, loss = 0.33044930\n",
      "Iteration 18, loss = 0.32535704\n",
      "Iteration 19, loss = 0.35814095\n",
      "Iteration 20, loss = 0.36776055\n",
      "Iteration 21, loss = 0.33429153\n",
      "Iteration 22, loss = 0.32856879\n",
      "Iteration 23, loss = 0.34402750\n",
      "Iteration 24, loss = 0.33503121\n",
      "Iteration 25, loss = 0.30329461\n",
      "Iteration 26, loss = 0.29033796\n",
      "Iteration 27, loss = 0.30233756\n",
      "Iteration 28, loss = 0.31189265\n",
      "Iteration 29, loss = 0.32297714\n",
      "Iteration 30, loss = 0.33340682\n",
      "Iteration 31, loss = 0.32655131\n",
      "Iteration 32, loss = 0.30843893\n",
      "Iteration 33, loss = 0.30598920\n",
      "Iteration 34, loss = 0.30585988\n",
      "Iteration 35, loss = 0.29966453\n",
      "Iteration 36, loss = 0.30865776\n",
      "Iteration 37, loss = 0.31367383\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.30885107\n",
      "Iteration 2, loss = 1.25231739\n",
      "Iteration 3, loss = 0.71317754\n",
      "Iteration 4, loss = 0.59445842\n",
      "Iteration 5, loss = 0.51701792\n",
      "Iteration 6, loss = 0.45980710\n",
      "Iteration 7, loss = 0.45523610\n",
      "Iteration 8, loss = 0.42790429\n",
      "Iteration 9, loss = 0.39673299\n",
      "Iteration 10, loss = 0.38764468\n",
      "Iteration 11, loss = 0.36730140\n",
      "Iteration 12, loss = 0.36275618\n",
      "Iteration 13, loss = 0.33724309\n",
      "Iteration 14, loss = 0.31337123\n",
      "Iteration 15, loss = 0.30336971\n",
      "Iteration 16, loss = 0.30707654\n",
      "Iteration 17, loss = 0.33805515\n",
      "Iteration 18, loss = 0.31562041\n",
      "Iteration 19, loss = 0.34093818\n",
      "Iteration 20, loss = 0.36030086\n",
      "Iteration 21, loss = 0.31841614\n",
      "Iteration 22, loss = 0.31788234\n",
      "Iteration 23, loss = 0.32913609\n",
      "Iteration 24, loss = 0.33486902\n",
      "Iteration 25, loss = 0.32151853\n",
      "Iteration 26, loss = 0.30200319\n",
      "Iteration 27, loss = 0.29500071\n",
      "Iteration 28, loss = 0.30278041\n",
      "Iteration 29, loss = 0.31020648\n",
      "Iteration 30, loss = 0.32426351\n",
      "Iteration 31, loss = 0.30874683\n",
      "Iteration 32, loss = 0.29511838\n",
      "Iteration 33, loss = 0.29301273\n",
      "Iteration 34, loss = 0.28403068\n",
      "Iteration 35, loss = 0.27600677\n",
      "Iteration 36, loss = 0.31195701\n",
      "Iteration 37, loss = 0.32542290\n",
      "Iteration 38, loss = 0.29094501\n",
      "Iteration 39, loss = 0.27310907\n",
      "Iteration 40, loss = 0.29286508\n",
      "Iteration 41, loss = 0.31115203\n",
      "Iteration 42, loss = 0.28565727\n",
      "Iteration 43, loss = 0.28894261\n",
      "Iteration 44, loss = 0.29876156\n",
      "Iteration 45, loss = 0.33389532\n",
      "Iteration 46, loss = 0.31856201\n",
      "Iteration 47, loss = 0.30208860\n",
      "Iteration 48, loss = 0.30637006\n",
      "Iteration 49, loss = 0.33093863\n",
      "Iteration 50, loss = 0.31352250\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.29155832\n",
      "Iteration 2, loss = 1.17232544\n",
      "Iteration 3, loss = 0.65096949\n",
      "Iteration 4, loss = 0.54417983\n",
      "Iteration 5, loss = 0.47225992\n",
      "Iteration 6, loss = 0.43268536\n",
      "Iteration 7, loss = 0.43740317\n",
      "Iteration 8, loss = 0.40179401\n",
      "Iteration 9, loss = 0.37359116\n",
      "Iteration 10, loss = 0.37218345\n",
      "Iteration 11, loss = 0.35255509\n",
      "Iteration 12, loss = 0.35203372\n",
      "Iteration 13, loss = 0.33005583\n",
      "Iteration 14, loss = 0.30543948\n",
      "Iteration 15, loss = 0.29967515\n",
      "Iteration 16, loss = 0.30371156\n",
      "Iteration 17, loss = 0.32763003\n",
      "Iteration 18, loss = 0.30810194\n",
      "Iteration 19, loss = 0.34352951\n",
      "Iteration 20, loss = 0.35377303\n",
      "Iteration 21, loss = 0.31080829\n",
      "Iteration 22, loss = 0.32875094\n",
      "Iteration 23, loss = 0.33509154\n",
      "Iteration 24, loss = 0.32913415\n",
      "Iteration 25, loss = 0.31726997\n",
      "Iteration 26, loss = 0.30716155\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.99909245\n",
      "Iteration 2, loss = 1.48577786\n",
      "Iteration 3, loss = 1.19396846\n",
      "Iteration 4, loss = 0.95149386\n",
      "Iteration 5, loss = 0.76070818\n",
      "Iteration 6, loss = 0.63171435\n",
      "Iteration 7, loss = 0.54491545\n",
      "Iteration 8, loss = 0.48107652\n",
      "Iteration 9, loss = 0.42788138\n",
      "Iteration 10, loss = 0.38878478\n",
      "Iteration 11, loss = 0.35950222\n",
      "Iteration 12, loss = 0.33731830\n",
      "Iteration 13, loss = 0.32160786\n",
      "Iteration 14, loss = 0.30916284\n",
      "Iteration 15, loss = 0.30053255\n",
      "Iteration 16, loss = 0.28989757\n",
      "Iteration 17, loss = 0.27849010\n",
      "Iteration 18, loss = 0.27268561\n",
      "Iteration 19, loss = 0.27221068\n",
      "Iteration 20, loss = 0.27158229\n",
      "Iteration 21, loss = 0.26641535\n",
      "Iteration 22, loss = 0.25842460\n",
      "Iteration 23, loss = 0.25312478\n",
      "Iteration 24, loss = 0.25271492\n",
      "Iteration 25, loss = 0.24755167\n",
      "Iteration 26, loss = 0.24797922\n",
      "Iteration 27, loss = 0.25126092\n",
      "Iteration 28, loss = 0.24825455\n",
      "Iteration 29, loss = 0.24538446\n",
      "Iteration 30, loss = 0.24554380\n",
      "Iteration 31, loss = 0.24632339\n",
      "Iteration 32, loss = 0.25213282\n",
      "Iteration 33, loss = 0.24909302\n",
      "Iteration 34, loss = 0.24502229\n",
      "Iteration 35, loss = 0.24401882\n",
      "Iteration 36, loss = 0.24342692\n",
      "Iteration 37, loss = 0.24476337\n",
      "Iteration 38, loss = 0.24544022\n",
      "Iteration 39, loss = 0.24529877\n",
      "Iteration 40, loss = 0.23844282\n",
      "Iteration 41, loss = 0.24226079\n",
      "Iteration 42, loss = 0.24590769\n",
      "Iteration 43, loss = 0.24373897\n",
      "Iteration 44, loss = 0.24216534\n",
      "Iteration 45, loss = 0.23901587\n",
      "Iteration 46, loss = 0.24114919\n",
      "Iteration 47, loss = 0.24461735\n",
      "Iteration 48, loss = 0.24300860\n",
      "Iteration 49, loss = 0.24377772\n",
      "Iteration 50, loss = 0.24068019\n",
      "Iteration 51, loss = 0.23554562\n",
      "Iteration 52, loss = 0.23655906\n",
      "Iteration 53, loss = 0.24272593\n",
      "Iteration 54, loss = 0.24520251\n",
      "Iteration 55, loss = 0.24315135\n",
      "Iteration 56, loss = 0.23748719\n",
      "Iteration 57, loss = 0.23582334\n",
      "Iteration 58, loss = 0.24164738\n",
      "Iteration 59, loss = 0.23480328\n",
      "Iteration 60, loss = 0.23208272\n",
      "Iteration 61, loss = 0.23535871\n",
      "Iteration 62, loss = 0.23640675\n",
      "Iteration 63, loss = 0.23491488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 64, loss = 0.23257391\n",
      "Iteration 65, loss = 0.22999822\n",
      "Iteration 66, loss = 0.23846450\n",
      "Iteration 67, loss = 0.23742541\n",
      "Iteration 68, loss = 0.23314410\n",
      "Iteration 69, loss = 0.23302847\n",
      "Iteration 70, loss = 0.23695322\n",
      "Iteration 71, loss = 0.23119024\n",
      "Iteration 72, loss = 0.23294829\n",
      "Iteration 73, loss = 0.23297858\n",
      "Iteration 74, loss = 0.23181678\n",
      "Iteration 75, loss = 0.23330975\n",
      "Iteration 76, loss = 0.23244436\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10,\n",
       "             estimator=MLPClassifier(activation='logistic', alpha=1,\n",
       "                                     learning_rate_init=0.02, max_iter=400,\n",
       "                                     random_state=1, verbose=True),\n",
       "             param_grid={'hidden_layer_sizes': [(5,), (10,), (15,), (20,),\n",
       "                                                (25,), (30,), (35,), (40,),\n",
       "                                                (45,), (50,), (55,), (60,),\n",
       "                                                (65,), (70,), (75,), (80,),\n",
       "                                                (85,), (90,), (95,), (100,),\n",
       "                                                (105,), (110,), (115,), (120,),\n",
       "                                                (125,), (130,), (135,), (140,),\n",
       "                                                (145,), (150,), ...]},\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define a range for the \"number of neurons\" in the hidden layer for a network with 1 hidden layer:\n",
    "neuron_number = [(i,) for i in range(5,250,5)]\n",
    "\n",
    "# create a dictionary for grid parameter:\n",
    "param_grid = dict(hidden_layer_sizes = neuron_number)\n",
    "print(param_grid,'\\n')\n",
    "\n",
    "# instantiate the model:\n",
    "my_ANN = MLPClassifier(activation='logistic', solver='adam', \n",
    "                                         alpha=1, random_state=1, \n",
    "                                          learning_rate_init = 0.02,\n",
    "                                          verbose=True, tol=0.0001, max_iter=400)\n",
    "\n",
    "# creat the grid, and define the metric for evaluating the model: \n",
    "grid = GridSearchCV(my_ANN, param_grid, cv=10, scoring='accuracy')\n",
    "\n",
    "# fit the grid (start the grid search):\n",
    "grid.fit(train_images_new, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9321538461538461\n",
      "{'hidden_layer_sizes': (40,)}\n"
     ]
    }
   ],
   "source": [
    "# print the best accuracy and the best numbers of neurons\n",
    "print(grid.best_score_)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.04001186\n",
      "Iteration 2, loss = 1.50255355\n",
      "Iteration 3, loss = 1.21803944\n",
      "Iteration 4, loss = 1.00083756\n",
      "Iteration 5, loss = 0.82778368\n",
      "Iteration 6, loss = 0.69994290\n",
      "Iteration 7, loss = 0.60933590\n",
      "Iteration 8, loss = 0.54423549\n",
      "Iteration 9, loss = 0.49455766\n",
      "Iteration 10, loss = 0.45481529\n",
      "Iteration 11, loss = 0.42476800\n",
      "Iteration 12, loss = 0.40158932\n",
      "Iteration 13, loss = 0.38216230\n",
      "Iteration 14, loss = 0.36196594\n",
      "Iteration 15, loss = 0.34658752\n",
      "Iteration 16, loss = 0.33696054\n",
      "Iteration 17, loss = 0.33114147\n",
      "Iteration 18, loss = 0.32789863\n",
      "Iteration 19, loss = 0.32934334\n",
      "Iteration 20, loss = 0.32405733\n",
      "Iteration 21, loss = 0.31760340\n",
      "Iteration 22, loss = 0.31368580\n",
      "Iteration 23, loss = 0.30928694\n",
      "Iteration 24, loss = 0.30275518\n",
      "Iteration 25, loss = 0.30234850\n",
      "Iteration 26, loss = 0.31173739\n",
      "Iteration 27, loss = 0.30082858\n",
      "Iteration 28, loss = 0.29293946\n",
      "Iteration 29, loss = 0.29757457\n",
      "Iteration 30, loss = 0.30503740\n",
      "Iteration 31, loss = 0.31105289\n",
      "Iteration 32, loss = 0.31632463\n",
      "Iteration 33, loss = 0.30869973\n",
      "Iteration 34, loss = 0.29871072\n",
      "Iteration 35, loss = 0.29433211\n",
      "Iteration 36, loss = 0.29865327\n",
      "Iteration 37, loss = 0.30108458\n",
      "Iteration 38, loss = 0.29968184\n",
      "Iteration 39, loss = 0.28984951\n",
      "Iteration 40, loss = 0.28157251\n",
      "Iteration 41, loss = 0.28460446\n",
      "Iteration 42, loss = 0.29989445\n",
      "Iteration 43, loss = 0.29710472\n",
      "Iteration 44, loss = 0.29888116\n",
      "Iteration 45, loss = 0.30378929\n",
      "Iteration 46, loss = 0.29709125\n",
      "Iteration 47, loss = 0.29442034\n",
      "Iteration 48, loss = 0.29733614\n",
      "Iteration 49, loss = 0.29418415\n",
      "Iteration 50, loss = 0.28875030\n",
      "Iteration 51, loss = 0.29207563\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.03199112\n",
      "Iteration 2, loss = 1.47956952\n",
      "Iteration 3, loss = 1.18814977\n",
      "Iteration 4, loss = 0.94985029\n",
      "Iteration 5, loss = 0.77529652\n",
      "Iteration 6, loss = 0.65852752\n",
      "Iteration 7, loss = 0.57470411\n",
      "Iteration 8, loss = 0.51009093\n",
      "Iteration 9, loss = 0.46266518\n",
      "Iteration 10, loss = 0.42585775\n",
      "Iteration 11, loss = 0.39621018\n",
      "Iteration 12, loss = 0.37195221\n",
      "Iteration 13, loss = 0.35798145\n",
      "Iteration 14, loss = 0.34431226\n",
      "Iteration 15, loss = 0.33265005\n",
      "Iteration 16, loss = 0.31965310\n",
      "Iteration 17, loss = 0.31437803\n",
      "Iteration 18, loss = 0.31085332\n",
      "Iteration 19, loss = 0.30846774\n",
      "Iteration 20, loss = 0.30504189\n",
      "Iteration 21, loss = 0.30422852\n",
      "Iteration 22, loss = 0.30557605\n",
      "Iteration 23, loss = 0.30082534\n",
      "Iteration 24, loss = 0.29968134\n",
      "Iteration 25, loss = 0.30642949\n",
      "Iteration 26, loss = 0.30509429\n",
      "Iteration 27, loss = 0.29977721\n",
      "Iteration 28, loss = 0.30013119\n",
      "Iteration 29, loss = 0.30512674\n",
      "Iteration 30, loss = 0.30654244\n",
      "Iteration 31, loss = 0.30226829\n",
      "Iteration 32, loss = 0.29575396\n",
      "Iteration 33, loss = 0.28866013\n",
      "Iteration 34, loss = 0.28479274\n",
      "Iteration 35, loss = 0.29135484\n",
      "Iteration 36, loss = 0.29357506\n",
      "Iteration 37, loss = 0.29651604\n",
      "Iteration 38, loss = 0.29503955\n",
      "Iteration 39, loss = 0.29070384\n",
      "Iteration 40, loss = 0.29901345\n",
      "Iteration 41, loss = 0.30868993\n",
      "Iteration 42, loss = 0.30334303\n",
      "Iteration 43, loss = 0.30068142\n",
      "Iteration 44, loss = 0.30613928\n",
      "Iteration 45, loss = 0.31209202\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.03627468\n",
      "Iteration 2, loss = 1.49891064\n",
      "Iteration 3, loss = 1.19956335\n",
      "Iteration 4, loss = 0.95473048\n",
      "Iteration 5, loss = 0.77923008\n",
      "Iteration 6, loss = 0.66141550\n",
      "Iteration 7, loss = 0.57712768\n",
      "Iteration 8, loss = 0.50954646\n",
      "Iteration 9, loss = 0.45699860\n",
      "Iteration 10, loss = 0.41743053\n",
      "Iteration 11, loss = 0.38793302\n",
      "Iteration 12, loss = 0.36560088\n",
      "Iteration 13, loss = 0.35175599\n",
      "Iteration 14, loss = 0.33879966\n",
      "Iteration 15, loss = 0.33031017\n",
      "Iteration 16, loss = 0.31641004\n",
      "Iteration 17, loss = 0.31165240\n",
      "Iteration 18, loss = 0.30903441\n",
      "Iteration 19, loss = 0.30455792\n",
      "Iteration 20, loss = 0.30024293\n",
      "Iteration 21, loss = 0.29932443\n",
      "Iteration 22, loss = 0.30189615\n",
      "Iteration 23, loss = 0.29618798\n",
      "Iteration 24, loss = 0.29475710\n",
      "Iteration 25, loss = 0.29905964\n",
      "Iteration 26, loss = 0.30321978\n",
      "Iteration 27, loss = 0.30052743\n",
      "Iteration 28, loss = 0.29738030\n",
      "Iteration 29, loss = 0.29797963\n",
      "Iteration 30, loss = 0.30161711\n",
      "Iteration 31, loss = 0.30106164\n",
      "Iteration 32, loss = 0.29387931\n",
      "Iteration 33, loss = 0.28514286\n",
      "Iteration 34, loss = 0.28271647\n",
      "Iteration 35, loss = 0.28919367\n",
      "Iteration 36, loss = 0.29536702\n",
      "Iteration 37, loss = 0.29890029\n",
      "Iteration 38, loss = 0.29311498\n",
      "Iteration 39, loss = 0.29094657\n",
      "Iteration 40, loss = 0.29889238\n",
      "Iteration 41, loss = 0.30573193\n",
      "Iteration 42, loss = 0.29886282\n",
      "Iteration 43, loss = 0.29282044\n",
      "Iteration 44, loss = 0.29453124\n",
      "Iteration 45, loss = 0.30631264\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.02337547\n",
      "Iteration 2, loss = 1.46346897\n",
      "Iteration 3, loss = 1.18153001\n",
      "Iteration 4, loss = 0.95271176\n",
      "Iteration 5, loss = 0.77491743\n",
      "Iteration 6, loss = 0.65341807\n",
      "Iteration 7, loss = 0.57652318\n",
      "Iteration 8, loss = 0.51604220\n",
      "Iteration 9, loss = 0.46834495\n",
      "Iteration 10, loss = 0.42553222\n",
      "Iteration 11, loss = 0.39389301\n",
      "Iteration 12, loss = 0.37051578\n",
      "Iteration 13, loss = 0.35758705\n",
      "Iteration 14, loss = 0.34452921\n",
      "Iteration 15, loss = 0.33156753\n",
      "Iteration 16, loss = 0.31484896\n",
      "Iteration 17, loss = 0.30871882\n",
      "Iteration 18, loss = 0.30926412\n",
      "Iteration 19, loss = 0.30879189\n",
      "Iteration 20, loss = 0.30000344\n",
      "Iteration 21, loss = 0.29381875\n",
      "Iteration 22, loss = 0.29657430\n",
      "Iteration 23, loss = 0.29411152\n",
      "Iteration 24, loss = 0.29005594\n",
      "Iteration 25, loss = 0.29001889\n",
      "Iteration 26, loss = 0.29587318\n",
      "Iteration 27, loss = 0.29773632\n",
      "Iteration 28, loss = 0.29693609\n",
      "Iteration 29, loss = 0.29560750\n",
      "Iteration 30, loss = 0.29809290\n",
      "Iteration 31, loss = 0.29999850\n",
      "Iteration 32, loss = 0.29392936\n",
      "Iteration 33, loss = 0.28475921\n",
      "Iteration 34, loss = 0.27948198\n",
      "Iteration 35, loss = 0.28131712\n",
      "Iteration 36, loss = 0.28636924\n",
      "Iteration 37, loss = 0.29088808\n",
      "Iteration 38, loss = 0.28767708\n",
      "Iteration 39, loss = 0.28665255\n",
      "Iteration 40, loss = 0.29478105\n",
      "Iteration 41, loss = 0.30345516\n",
      "Iteration 42, loss = 0.29315378\n",
      "Iteration 43, loss = 0.28875629\n",
      "Iteration 44, loss = 0.28783080\n",
      "Iteration 45, loss = 0.29794394\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.03093946\n",
      "Iteration 2, loss = 1.48657466\n",
      "Iteration 3, loss = 1.19866066\n",
      "Iteration 4, loss = 0.95856400\n",
      "Iteration 5, loss = 0.77862323\n",
      "Iteration 6, loss = 0.65729680\n",
      "Iteration 7, loss = 0.57986024\n",
      "Iteration 8, loss = 0.51508082\n",
      "Iteration 9, loss = 0.46488200\n",
      "Iteration 10, loss = 0.42098481\n",
      "Iteration 11, loss = 0.38935963\n",
      "Iteration 12, loss = 0.36640258\n",
      "Iteration 13, loss = 0.35462685\n",
      "Iteration 14, loss = 0.34344050\n",
      "Iteration 15, loss = 0.33378108\n",
      "Iteration 16, loss = 0.31859901\n",
      "Iteration 17, loss = 0.31132626\n",
      "Iteration 18, loss = 0.31281516\n",
      "Iteration 19, loss = 0.31750382\n",
      "Iteration 20, loss = 0.31034555\n",
      "Iteration 21, loss = 0.29828877\n",
      "Iteration 22, loss = 0.29773648\n",
      "Iteration 23, loss = 0.29596719\n",
      "Iteration 24, loss = 0.29661920\n",
      "Iteration 25, loss = 0.29494026\n",
      "Iteration 26, loss = 0.29445999\n",
      "Iteration 27, loss = 0.29333071\n",
      "Iteration 28, loss = 0.29404177\n",
      "Iteration 29, loss = 0.29531076\n",
      "Iteration 30, loss = 0.29827006\n",
      "Iteration 31, loss = 0.30205705\n",
      "Iteration 32, loss = 0.30000885\n",
      "Iteration 33, loss = 0.29217275\n",
      "Iteration 34, loss = 0.28591633\n",
      "Iteration 35, loss = 0.28683560\n",
      "Iteration 36, loss = 0.28987051\n",
      "Iteration 37, loss = 0.29326790\n",
      "Iteration 38, loss = 0.29191316\n",
      "Iteration 39, loss = 0.29146284\n",
      "Iteration 40, loss = 0.29846025\n",
      "Iteration 41, loss = 0.30530609\n",
      "Iteration 42, loss = 0.29444712\n",
      "Iteration 43, loss = 0.29382427\n",
      "Iteration 44, loss = 0.29718278\n",
      "Iteration 45, loss = 0.30736551\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.03385082\n",
      "Iteration 2, loss = 1.48996144\n",
      "Iteration 3, loss = 1.20893623\n",
      "Iteration 4, loss = 0.97158059\n",
      "Iteration 5, loss = 0.78668394\n",
      "Iteration 6, loss = 0.65736236\n",
      "Iteration 7, loss = 0.57864450\n",
      "Iteration 8, loss = 0.51389862\n",
      "Iteration 9, loss = 0.46463386\n",
      "Iteration 10, loss = 0.42176852\n",
      "Iteration 11, loss = 0.38943709\n",
      "Iteration 12, loss = 0.36490515\n",
      "Iteration 13, loss = 0.35075307\n",
      "Iteration 14, loss = 0.33773866\n",
      "Iteration 15, loss = 0.33032217\n",
      "Iteration 16, loss = 0.31263460\n",
      "Iteration 17, loss = 0.30498492\n",
      "Iteration 18, loss = 0.30628737\n",
      "Iteration 19, loss = 0.31164973\n",
      "Iteration 20, loss = 0.30718382\n",
      "Iteration 21, loss = 0.29570688\n",
      "Iteration 22, loss = 0.28858859\n",
      "Iteration 23, loss = 0.28978118\n",
      "Iteration 24, loss = 0.29241129\n",
      "Iteration 25, loss = 0.29090509\n",
      "Iteration 26, loss = 0.28935771\n",
      "Iteration 27, loss = 0.28893423\n",
      "Iteration 28, loss = 0.29222588\n",
      "Iteration 29, loss = 0.29344825\n",
      "Iteration 30, loss = 0.29413250\n",
      "Iteration 31, loss = 0.29378594\n",
      "Iteration 32, loss = 0.29060143\n",
      "Iteration 33, loss = 0.28588130\n",
      "Iteration 34, loss = 0.28293655\n",
      "Iteration 35, loss = 0.28590886\n",
      "Iteration 36, loss = 0.28950024\n",
      "Iteration 37, loss = 0.28896511\n",
      "Iteration 38, loss = 0.28387625\n",
      "Iteration 39, loss = 0.28466720\n",
      "Iteration 40, loss = 0.29191323\n",
      "Iteration 41, loss = 0.29890997\n",
      "Iteration 42, loss = 0.28962391\n",
      "Iteration 43, loss = 0.29067334\n",
      "Iteration 44, loss = 0.29726993\n",
      "Iteration 45, loss = 0.29999853\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.03576590\n",
      "Iteration 2, loss = 1.50379594\n",
      "Iteration 3, loss = 1.21226970\n",
      "Iteration 4, loss = 0.96910291\n",
      "Iteration 5, loss = 0.78515948\n",
      "Iteration 6, loss = 0.65738738\n",
      "Iteration 7, loss = 0.57577346\n",
      "Iteration 8, loss = 0.50778807\n",
      "Iteration 9, loss = 0.45505508\n",
      "Iteration 10, loss = 0.41113748\n",
      "Iteration 11, loss = 0.37884605\n",
      "Iteration 12, loss = 0.35797000\n",
      "Iteration 13, loss = 0.34769390\n",
      "Iteration 14, loss = 0.33571890\n",
      "Iteration 15, loss = 0.32426689\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16, loss = 0.30632937\n",
      "Iteration 17, loss = 0.30032482\n",
      "Iteration 18, loss = 0.30641421\n",
      "Iteration 19, loss = 0.30905964\n",
      "Iteration 20, loss = 0.30162867\n",
      "Iteration 21, loss = 0.29135165\n",
      "Iteration 22, loss = 0.28821469\n",
      "Iteration 23, loss = 0.29097265\n",
      "Iteration 24, loss = 0.29161416\n",
      "Iteration 25, loss = 0.28734507\n",
      "Iteration 26, loss = 0.28649780\n",
      "Iteration 27, loss = 0.29201710\n",
      "Iteration 28, loss = 0.29503049\n",
      "Iteration 29, loss = 0.29512504\n",
      "Iteration 30, loss = 0.29189853\n",
      "Iteration 31, loss = 0.28911415\n",
      "Iteration 32, loss = 0.28814644\n",
      "Iteration 33, loss = 0.29011147\n",
      "Iteration 34, loss = 0.28910615\n",
      "Iteration 35, loss = 0.28773807\n",
      "Iteration 36, loss = 0.28839319\n",
      "Iteration 37, loss = 0.28898360\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.04113788\n",
      "Iteration 2, loss = 1.50923314\n",
      "Iteration 3, loss = 1.23189854\n",
      "Iteration 4, loss = 0.99608152\n",
      "Iteration 5, loss = 0.81043151\n",
      "Iteration 6, loss = 0.67973621\n",
      "Iteration 7, loss = 0.60100758\n",
      "Iteration 8, loss = 0.53686145\n",
      "Iteration 9, loss = 0.48447517\n",
      "Iteration 10, loss = 0.43877598\n",
      "Iteration 11, loss = 0.40480700\n",
      "Iteration 12, loss = 0.38189597\n",
      "Iteration 13, loss = 0.36793828\n",
      "Iteration 14, loss = 0.35482874\n",
      "Iteration 15, loss = 0.34153410\n",
      "Iteration 16, loss = 0.32057443\n",
      "Iteration 17, loss = 0.31278460\n",
      "Iteration 18, loss = 0.31872969\n",
      "Iteration 19, loss = 0.32231906\n",
      "Iteration 20, loss = 0.31846632\n",
      "Iteration 21, loss = 0.30807911\n",
      "Iteration 22, loss = 0.30109823\n",
      "Iteration 23, loss = 0.30355527\n",
      "Iteration 24, loss = 0.30453179\n",
      "Iteration 25, loss = 0.30119843\n",
      "Iteration 26, loss = 0.30084341\n",
      "Iteration 27, loss = 0.30755245\n",
      "Iteration 28, loss = 0.31314063\n",
      "Iteration 29, loss = 0.31892051\n",
      "Iteration 30, loss = 0.30683707\n",
      "Iteration 31, loss = 0.29660929\n",
      "Iteration 32, loss = 0.29735184\n",
      "Iteration 33, loss = 0.30431096\n",
      "Iteration 34, loss = 0.30595619\n",
      "Iteration 35, loss = 0.30382940\n",
      "Iteration 36, loss = 0.30092745\n",
      "Iteration 37, loss = 0.29941530\n",
      "Iteration 38, loss = 0.29966197\n",
      "Iteration 39, loss = 0.30071772\n",
      "Iteration 40, loss = 0.29958689\n",
      "Iteration 41, loss = 0.30082524\n",
      "Iteration 42, loss = 0.29623509\n",
      "Iteration 43, loss = 0.30346397\n",
      "Iteration 44, loss = 0.30927825\n",
      "Iteration 45, loss = 0.30291972\n",
      "Iteration 46, loss = 0.29277436\n",
      "Iteration 47, loss = 0.29140717\n",
      "Iteration 48, loss = 0.29569520\n",
      "Iteration 49, loss = 0.29844637\n",
      "Iteration 50, loss = 0.30078812\n",
      "Iteration 51, loss = 0.30028358\n",
      "Iteration 52, loss = 0.29670918\n",
      "Iteration 53, loss = 0.29379999\n",
      "Iteration 54, loss = 0.29385899\n",
      "Iteration 55, loss = 0.29524229\n",
      "Iteration 56, loss = 0.29873309\n",
      "Iteration 57, loss = 0.30120473\n",
      "Iteration 58, loss = 0.29969668\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.03149758\n",
      "Iteration 2, loss = 1.48983572\n",
      "Iteration 3, loss = 1.22019344\n",
      "Iteration 4, loss = 0.99007390\n",
      "Iteration 5, loss = 0.80703606\n",
      "Iteration 6, loss = 0.67859814\n",
      "Iteration 7, loss = 0.59940948\n",
      "Iteration 8, loss = 0.53863821\n",
      "Iteration 9, loss = 0.48579609\n",
      "Iteration 10, loss = 0.44176901\n",
      "Iteration 11, loss = 0.40719985\n",
      "Iteration 12, loss = 0.38254894\n",
      "Iteration 13, loss = 0.36599600\n",
      "Iteration 14, loss = 0.35099522\n",
      "Iteration 15, loss = 0.33818552\n",
      "Iteration 16, loss = 0.31965122\n",
      "Iteration 17, loss = 0.31531409\n",
      "Iteration 18, loss = 0.32113280\n",
      "Iteration 19, loss = 0.31991747\n",
      "Iteration 20, loss = 0.31698322\n",
      "Iteration 21, loss = 0.30672059\n",
      "Iteration 22, loss = 0.29697693\n",
      "Iteration 23, loss = 0.29830227\n",
      "Iteration 24, loss = 0.30215143\n",
      "Iteration 25, loss = 0.30064776\n",
      "Iteration 26, loss = 0.30117054\n",
      "Iteration 27, loss = 0.30706133\n",
      "Iteration 28, loss = 0.31021182\n",
      "Iteration 29, loss = 0.31600216\n",
      "Iteration 30, loss = 0.30389519\n",
      "Iteration 31, loss = 0.29180470\n",
      "Iteration 32, loss = 0.29305935\n",
      "Iteration 33, loss = 0.30338483\n",
      "Iteration 34, loss = 0.30809248\n",
      "Iteration 35, loss = 0.30295172\n",
      "Iteration 36, loss = 0.29577721\n",
      "Iteration 37, loss = 0.29259228\n",
      "Iteration 38, loss = 0.29331105\n",
      "Iteration 39, loss = 0.29647910\n",
      "Iteration 40, loss = 0.29662017\n",
      "Iteration 41, loss = 0.29660548\n",
      "Iteration 42, loss = 0.29248680\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.03793230\n",
      "Iteration 2, loss = 1.50039759\n",
      "Iteration 3, loss = 1.21519244\n",
      "Iteration 4, loss = 0.97704184\n",
      "Iteration 5, loss = 0.79159861\n",
      "Iteration 6, loss = 0.66039833\n",
      "Iteration 7, loss = 0.57538518\n",
      "Iteration 8, loss = 0.51602924\n",
      "Iteration 9, loss = 0.46451276\n",
      "Iteration 10, loss = 0.42073767\n",
      "Iteration 11, loss = 0.38607251\n",
      "Iteration 12, loss = 0.36176277\n",
      "Iteration 13, loss = 0.34590857\n",
      "Iteration 14, loss = 0.33204517\n",
      "Iteration 15, loss = 0.32272193\n",
      "Iteration 16, loss = 0.30782994\n",
      "Iteration 17, loss = 0.30484607\n",
      "Iteration 18, loss = 0.30981111\n",
      "Iteration 19, loss = 0.30920518\n",
      "Iteration 20, loss = 0.30887604\n",
      "Iteration 21, loss = 0.30175868\n",
      "Iteration 22, loss = 0.28992793\n",
      "Iteration 23, loss = 0.28841630\n",
      "Iteration 24, loss = 0.28372463\n",
      "Iteration 25, loss = 0.28678123\n",
      "Iteration 26, loss = 0.29684760\n",
      "Iteration 27, loss = 0.30738348\n",
      "Iteration 28, loss = 0.30097241\n",
      "Iteration 29, loss = 0.30432466\n",
      "Iteration 30, loss = 0.29497057\n",
      "Iteration 31, loss = 0.28426634\n",
      "Iteration 32, loss = 0.28385426\n",
      "Iteration 33, loss = 0.29164599\n",
      "Iteration 34, loss = 0.29739617\n",
      "Iteration 35, loss = 0.29608578\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "\n",
      "\n",
      " accuracy:  [0.96153846 0.92       0.96       0.96       0.92       0.88\n",
      " 0.92       0.96       0.92       0.92      ]\n"
     ]
    }
   ],
   "source": [
    "# Applying 10-fold cross validation with ANN classifier:\n",
    "\n",
    "# 1 Hidden Layer with 30 neurons:\n",
    "my_ANN = MLPClassifier(hidden_layer_sizes=(40,), activation= 'logistic', \n",
    "                       solver='adam', alpha=1, random_state=1, \n",
    "                       learning_rate_init = 0.02, verbose=True, tol=0.0001)\n",
    "\n",
    "# CV:\n",
    "accuracy_list = cross_val_score(my_ANN, train_images_new, train_labels, cv=10, scoring='accuracy')\n",
    "\n",
    "print('\\n\\n','accuracy: ',accuracy_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9321538461538461\n"
     ]
    }
   ],
   "source": [
    "# use average of accuracy values as final result\n",
    "accuracy_cv = accuracy_list.mean()\n",
    "\n",
    "print(accuracy_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.99909245\n",
      "Iteration 2, loss = 1.48577786\n",
      "Iteration 3, loss = 1.19396846\n",
      "Iteration 4, loss = 0.95149386\n",
      "Iteration 5, loss = 0.76070818\n",
      "Iteration 6, loss = 0.63171435\n",
      "Iteration 7, loss = 0.54491545\n",
      "Iteration 8, loss = 0.48107652\n",
      "Iteration 9, loss = 0.42788138\n",
      "Iteration 10, loss = 0.38878478\n",
      "Iteration 11, loss = 0.35950222\n",
      "Iteration 12, loss = 0.33731830\n",
      "Iteration 13, loss = 0.32160786\n",
      "Iteration 14, loss = 0.30916284\n",
      "Iteration 15, loss = 0.30053255\n",
      "Iteration 16, loss = 0.28989757\n",
      "Iteration 17, loss = 0.27849010\n",
      "Iteration 18, loss = 0.27268561\n",
      "Iteration 19, loss = 0.27221068\n",
      "Iteration 20, loss = 0.27158229\n",
      "Iteration 21, loss = 0.26641535\n",
      "Iteration 22, loss = 0.25842460\n",
      "Iteration 23, loss = 0.25312478\n",
      "Iteration 24, loss = 0.25271492\n",
      "Iteration 25, loss = 0.24755167\n",
      "Iteration 26, loss = 0.24797922\n",
      "Iteration 27, loss = 0.25126092\n",
      "Iteration 28, loss = 0.24825455\n",
      "Iteration 29, loss = 0.24538446\n",
      "Iteration 30, loss = 0.24554380\n",
      "Iteration 31, loss = 0.24632339\n",
      "Iteration 32, loss = 0.25213282\n",
      "Iteration 33, loss = 0.24909302\n",
      "Iteration 34, loss = 0.24502229\n",
      "Iteration 35, loss = 0.24401882\n",
      "Iteration 36, loss = 0.24342692\n",
      "Iteration 37, loss = 0.24476337\n",
      "Iteration 38, loss = 0.24544022\n",
      "Iteration 39, loss = 0.24529877\n",
      "Iteration 40, loss = 0.23844282\n",
      "Iteration 41, loss = 0.24226079\n",
      "Iteration 42, loss = 0.24590769\n",
      "Iteration 43, loss = 0.24373897\n",
      "Iteration 44, loss = 0.24216534\n",
      "Iteration 45, loss = 0.23901587\n",
      "Iteration 46, loss = 0.24114919\n",
      "Iteration 47, loss = 0.24461735\n",
      "Iteration 48, loss = 0.24300860\n",
      "Iteration 49, loss = 0.24377772\n",
      "Iteration 50, loss = 0.24068019\n",
      "Iteration 51, loss = 0.23554562\n",
      "Iteration 52, loss = 0.23655906\n",
      "Iteration 53, loss = 0.24272593\n",
      "Iteration 54, loss = 0.24520251\n",
      "Iteration 55, loss = 0.24315135\n",
      "Iteration 56, loss = 0.23748719\n",
      "Iteration 57, loss = 0.23582334\n",
      "Iteration 58, loss = 0.24164738\n",
      "Iteration 59, loss = 0.23480328\n",
      "Iteration 60, loss = 0.23208272\n",
      "Iteration 61, loss = 0.23535871\n",
      "Iteration 62, loss = 0.23640675\n",
      "Iteration 63, loss = 0.23491488\n",
      "Iteration 64, loss = 0.23257391\n",
      "Iteration 65, loss = 0.22999822\n",
      "Iteration 66, loss = 0.23846450\n",
      "Iteration 67, loss = 0.23742541\n",
      "Iteration 68, loss = 0.23314410\n",
      "Iteration 69, loss = 0.23302847\n",
      "Iteration 70, loss = 0.23695322\n",
      "Iteration 71, loss = 0.23119024\n",
      "Iteration 72, loss = 0.23294829\n",
      "Iteration 73, loss = 0.23297858\n",
      "Iteration 74, loss = 0.23181678\n",
      "Iteration 75, loss = 0.23330975\n",
      "Iteration 76, loss = 0.23244436\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='logistic', alpha=1, hidden_layer_sizes=(40,),\n",
       "              learning_rate_init=0.02, random_state=1, verbose=True)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_ANN.fit(train_images_new, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_prob_rf1 = my_ANN.predict_proba(test_images_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.    0.    0.    0.    0.    0.075 0.075 0.6   0.6   1.   ]\n",
      "\n",
      "[0.         0.03846154 0.46153846 0.53846154 0.92307692 0.92307692\n",
      " 0.96153846 0.96153846 1.         1.        ]\n",
      "0.9740384615384615\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "#Now we look for the tpr, fpr, and auc of the random forest Covid result\n",
    "fpr_rf, tpr_rf, thresholds = metrics.roc_curve(y_test,y_predict_prob_rf1[:,0], pos_label=0)\n",
    "\n",
    "print(fpr_rf)\n",
    "print()\n",
    "print(tpr_rf)\n",
    "\n",
    "# AUC:\n",
    "AUC_rf1 = metrics.auc(fpr_rf, tpr_rf)\n",
    "print(AUC_rf1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Roc Curve:\n",
    "plt.plot(fpr_rf, tpr_rf, color='seagreen', lw=2, \n",
    "         label='MLP Classifier (area = %0.2f)' % AUC_rf1)\n",
    "\n",
    "# Random Guess line:\n",
    "plt.plot([0, 1], [0, 1], color='blue', lw=1, linestyle='--')\n",
    "\n",
    "# Defining The Range of X-Axis and Y-Axis:\n",
    "plt.xlim([-0.005, 1.005])\n",
    "plt.ylim([0.0, 1.01])\n",
    "\n",
    "# Labels, Title, Legend:\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# instantiate the model:\n",
    "my_SVC = svm.SVC(C=10, kernel='rbf', gamma=0.0005, random_state=1, probability=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=10, gamma=0.0005, probability=True, random_state=1)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start training\n",
    "my_SVC.fit(train_images_new, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing on the testing set:\n",
    "y_predict = my_SVC.predict(test_images_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9090909090909091\n"
     ]
    }
   ],
   "source": [
    "# print out the accuracy score\n",
    "accuracy = accuracy_score(y_test, y_predict)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_prob_rf12 = my_SVC.predict_proba(test_images_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.   0.   0.   0.   0.   0.05 0.05 0.75 0.75 1.  ]\n",
      "\n",
      "[0.         0.03846154 0.15384615 0.23076923 0.92307692 0.92307692\n",
      " 0.96153846 0.96153846 1.         1.        ]\n",
      "0.9692307692307691\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "#Now we look for the tpr, fpr, and auc of the random forest Covid result\n",
    "fpr_rf, tpr_rf, thresholds = metrics.roc_curve(y_test,y_predict_prob_rf12[:,0], pos_label=0)\n",
    "\n",
    "print(fpr_rf)\n",
    "print()\n",
    "print(tpr_rf)\n",
    "\n",
    "# AUC:\n",
    "AUC_rf2 = metrics.auc(fpr_rf, tpr_rf)\n",
    "print(AUC_rf2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAusUlEQVR4nO3dd5xU1f3/8dcHZEEFUYqKooAKNkREVMSo2ECNigUF7H7FBohGTSwx+anREIMlUSxBMWBiwILYC0YpNrpLl6KALKBuQFBQ6n5+f5y767LO7s7C3p3Zmffz8ZjHzJ175t7PXHQ+e8o9x9wdERGRkmqkOgAREUlPShAiIpKQEoSIiCSkBCEiIgkpQYiISEJKECIikpAShIiIJKQEIRnFzBaZ2U9mtsbMvjazIWZWt0SZjmb2gZn9YGarzex1MzuoRJmdzOxvZvZVdKwF0XajUs5rZtbPzGaa2VozyzOzF83skDi/r0iclCAkE53p7nWBtsBhwO2FO8zsaGAU8CqwB9ACmAZ8bGb7RGVygPeBg4FTgZ2AjsAK4MhSzvl34AagH9AAaAW8Avy6osGb2XYV/YxIHEx3UksmMbNFQC93/2+0/VfgYHf/dbT9ITDD3XuX+NzbQL67X2pmvYD7gH3dfU0S52wJfA4c7e4TSykzBvi3uz8dbV8exfmraNuBvsCNwHbAu8Aad7+l2DFeBca6+0NmtgfwKHAcsAZ42N0fKf8KiSRPNQjJWGbWFDgNWBBt70CoCbyYoPgLwCnR65OBd5JJDpGTgLzSkkMFnA0cBRwE/AfobmYGYGa7AJ2B4WZWA3idUPPZMzr/jWbWZRvPL7IFJQjJRK+Y2Q/AEuBb4P9F7zcg/De/PMFnlgOF/QsNSylTmoqWL01/d1/p7j8BHwIOHBvt6wZ86u7LgCOAxu5+j7tvcPcvgaeAHpUQg0gRJQjJRGe7ez2gE3AAP//wfwcUAE0SfKYJ8L/o9YpSypSmouVLs6TwhYe23+FAz+itC4HnotfNgD3MbFXhA7gD2K0SYhApogQhGcvdxwJDgAei7bXAp8D5CYpfQOiYBvgv0MXMdkzyVO8DTc2sfRll1gI7FNvePVHIJbaHAd3MrBmh6WlE9P4SYKG771zsUc/dT08yXpGkKEFIpvsbcIqZtY22bwMui4ak1jOzXczsXuBo4O6ozL8IP8IjzOwAM6thZg3N7A4z+8WPsLvPBx4HhplZJzPLMbM6ZtbDzG6LiuUC55rZDma2H3BleYG7+2dAPvA08K67r4p2TQS+N7NbzWx7M6tpZq3N7IiKXhyRsihBSEZz93zgWeAP0fZHQBfgXEK/wWLCUNhfRT/0uPt6Qkf158B7wPeEH+VGwIRSTtUPGAg8BqwCvgDOIXQmAzwMbAC+AYbyc3NReYZFsfyn2HfaDJxJGMa7kNA09jRQP8ljiiRFw1xFRCQh1SBERCQhJQgREUlICUJERBKKLUGY2TNm9q2ZzSxlv5nZI9EkaNPNrF1csYiISMXFOSnYEMKojmdL2X8a0DJ6HAU8ET2XqVGjRt68efPKiVBEJEtMmTLlf+7euCKfiS1BuPs4M2teRpGuwLPRHaPjzWxnM2vi7mVOWdC8eXMmT55cmaGKiGQ8M1tc0c+kclrhPSk2tQCQF71XGXPaiEg11XfE/Xy48LNUh1HtFWysxTfvns8Oey9g0bM3bdUxUtlJbQneS3hThpldbWaTzWxyfn5+zGGJSCopOWy7n5btzbyH/sqG1Q3ZcZ/Pt/o4qaxB5AF7FdtuCixLVNDdBwGDANq3b687+0SywLRbhqc6hGrn++9h7Vowg0knwJlnNgGO2erjpbIG8RpwaTSaqQOwurz+BxERSezNN6F1axgxAnbfHc48c9uPGVsNwsyGEaZbbmRmeYQ5+WsBuPuTwFvA6YTFXH4ErogrFhGRTNanD7zzDgwZAieeWHnHjXMUU89y9jvQJ67zi4hkMncYMwY6dYJLLoEBA2CHHcr7VMVocXQRkWpm6VK47jr44gsYNw46dIjnPJpqQ0SkGpkzB9q2hXbtYOpUaNgwvnNlZQ1C46xFpLr54gv46is4/nj4+GNo1Sr+c2ZlDULJQSS9HdvisFSHkDY2b4aHHoKjjoL586FGjapJDpClNYhCGmctIumub1/4/HMYPx72269qz53VCUJEJB1t2AAPPwxXXw333Qe77BJufqtqWdnEJCKSriZOhMMPh48+ComiQYPUJAdQDUJEJG18+y106wZ//St07566xFBICUJEJMVGjw4jk+68ExYsgJycVEcUqIlJRCRFVq8O/QyXXhrubYD0SQ5QTWsQuo9BRDLBwIFQsybMnAn166c6ml+qlgmiMpKDxlmLSCrk58MNN0C/fnDHHanvZyhLtUwQhXQfg4hUF+4wbBjcdFNoUmrTJr2TA1TzBCEiUh24w/r18OKL8MYb0L59qiNKjjqpRURiUlAATz4Z1mioXRtGjqw+yQFUgxARicWCBdCrF6xbB4MHp39zUiKqQYiIVKJNm8Jj0SI4++xwf8PBB6c6qq2jBCEiUkmmTQuL9wwbBiefDDfeGIaxVldKECIi28gd/vhHOOWUsNLbxRenOqLKoT4IEZFtkJ8PjRuHR24u7LFHqiOqPKpBiIhshbVrQxNShw5hCOv112dWcgAlCBGRCvvsMzjkEFi5MkzPXbt2qiOKh5qYRESS9N138NNPoabw+ONw6qmpjiheqkGIiCRh5Eho3RpeeQV22y3zkwOoBiEiUq6rr4YxY8Lw1eOOS3U0VUc1CBGRBNxh1KjwfNVV4R6HbEoOoBqEiMgvfPUVXHMNLF8O778PRxyR6ohSQzUIEZFiZs+Gdu3gV7+CSZOgYcNUR5Q6qkGIiABz58KSJWHm1QkTYN99Ux1R6qkGISJZbeNG+Mtf4JhjQtNSjRpKDoVUgxCRrNa3LyxcCJMnQ/PmqY4mvShBiEjWWbcOHngAeveG+++H+vWr53oNcVMTk4hklY8/hrZtw3QZmzfDzjsrOZRGNQgRyRrffAMXXQQPPgjnnZfqaNJfrDUIMzvVzOaa2QIzuy3B/vpm9rqZTTOzWWZ2RZzxiEh2GjUK7rorTJExf76SQ7JiSxBmVhN4DDgNOAjoaWYHlSjWB5jt7ocCnYAHzSwnrphEJLusXAlXXBGmyjj66PBerVqpjak6ibOJ6Uhggbt/CWBmw4GuwOxiZRyoZ2YG1AVWAptijElEssg//gF168KMGVCvXqqjqX7iTBB7AkuKbecBR5UoMxB4DVgG1AO6u3tBjDGJSIb7+uuweM9NN8Ftt6kDelvE2QeR6J/FS2x3AXKBPYC2wEAz2+kXBzK72swmm9nk/Pz8yo5TRDKAOwwZAm3aQKtWcNhhSg7bKs4EkQfsVWy7KaGmUNwVwMseLAAWAgeUPJC7D3L39u7evnHjxrEFLCLVU0FBWPbzzTdDh/R990GdOqmOqvqLM0FMAlqaWYuo47kHoTmpuK+AkwDMbDdgf+DLGGMSkQyyeTM88giccEJY9vPFF8M9DlI5YuuDcPdNZtYXeBeoCTzj7rPM7Npo/5PAn4AhZjaD0CR1q7v/L66YRCRzfP45XHllmDvp6afVnBSHWG+Uc/e3gLdKvPdksdfLgM5xxiAimWXjxvC8bBlceCFcd11IElL5dFlFpNqYOjUs3vP882Fa7j59lBzipEsrImnPPQxZPe00uPnmMF2GxE9zMYlIWlu+HJo0gWbNYPr0MF2GVA3VIEQkLX3/fWhCOvZY2LAh9DUoOVQtJQgRSTtTpsAhh4R7GyZNghzN0JYSamISkbSxYgX89BPsvTcMHgwnn5zqiLKbahAiknLu8MIL0Lp1uBu6cWMlh3SgGoSIpNyVV8L48fDyyz9Pyy2ppxqEiKSEe6gtuEPfvmEJUCWH9KIahIhUuS+/DIv4rFoVkkK7dqmOSBJRDUJEqtSsWXDkkdClS2hWatAg1RFJaVSDEJEqMXs2LFkCnTuHYazNmqU6IimPahAiEqsNG+Cee+D44+Gbb8Ksq0oO1YNqECISqz59wsyrU6fCXnuVX17ShxKEiFS6H3+E+++Hfv3gwQehXj2t11AdJd3EZGY7xhmIiGSGsWPh0ENh3rywvdNOSg7VVbkJwsw6mtlsYE60faiZPR57ZCJS7XzzDfzf/4Vaw7Bh0LBhqiOSbZFME9PDQBei9aTdfZqZHRdrVCJSrbz5Jnz6Kdx7L8ydC9up8TojJNXE5O5LSry1OYZYRKSayc8Pi/f06xdWeAMlh0ySzD/lEjPrCLiZ5QD9iJqbRCS7DR4Mu+8OM2bADjukOhqpbMkkiGuBvwN7AnnAKKB3nEGJSPrKywtDV2+9NSwDKpkrmSam/d39Inffzd13dfeLgQPjDkxE0ktBAQwaBIcdFh7t26c6IolbMjWIR4GSU2klek9EMtTmzbBxI4wZAx98EFZ7k8xXaoIws6OBjkBjM7up2K6dgJpxByYiqbd5M/ztb/DKKzBuHPznP6mOSKpSWTWIHKBuVKZesfe/B7rFGZSIpN6sWXDFFVC3LgwZopvdslGpCcLdxwJjzWyIuy+uwphEJIXWrw/JID8frroKevVScshWyfRB/GhmA4CDgTqFb7r7ibFFJSIpMWFCWP7z9tvD/Q2dOqU6IkmlZEYxPQd8DrQA7gYWAZNijElEqlhBAdx8M3TtCnfeCRdemOqIJB0kU4No6O6DzeyGYs1OY+MOTESqRl4eNG0KBxwAM2dCo0apjkjSRTI1iI3R83Iz+7WZHQY0jTEmEakCq1aFPoYTTgiL+lx1lZKDbCmZBHGvmdUHbgZuAZ4GbowzKBGJ16RJ0Lp1mDdpyhTIyUl1RJKOym1icvc3opergRMAzOyYOIMSkXh8+y2sWwctWsBzz4VlQEVKU2oNwsxqmllPM7vFzFpH751hZp8AA6ssQhHZZu4hIRxyCIwaFZqSlBykPGXVIAYDewETgUfMbDFwNHCbu79SBbGJSCW57DLIzQ3rNmgOJUlWWQmiPdDG3QvMrA7wP2A/d/862YOb2amEmWBrAk+7+18SlOkE/A2oBfzP3fV3jUglKCiA114LQ1dvvhkOPFB9DVIxZSWIDe5eAODu68xsXgWTQ03gMeAUwjThk8zsNXefXazMzsDjwKnu/pWZ7bo1X0JEtjRvXhiVtGFDaEo69NBURyTVUVmjmA4ws+nRY0ax7RlmNj2JYx8JLHD3L919AzAc6FqizIXAy+7+FYC7f7s1X0JEfjZzJnTsCOeeCx99BLvskuqIpLoqqwaxrWs+7AkUX6o0DziqRJlWQC0zG0OYEPDv7v7sNp5XJCtNmwbLlsGpp4b+hqa6W0m2Uak1CHdfXNYjiWMnmt7LS2xvBxwO/BroAvzBzFr94kBmV5vZZDObnJ+fn8SpRbLH+vXwhz/AKafAd9+FifWUHKQyxLm8eB5hFFShpsCyBGX+5+5rgbVmNg44FJhXvJC7DwIGAbRv3943IiKFeveGlStDrWGPPVIdjWSSZO6k3lqTgJZm1sLMcoAewGslyrwKHGtm25nZDoQmqDkxxiSSEdasCTOurlgBf/87vPyykoNUvqQShJltb2b7V+TA7r4J6Au8S/jRf8HdZ5nZtWZ2bVRmDvAOMJ1wv8XT7j6zIucRyTbvvRdueFu2DGrUCAv6aL0GiUO5TUxmdibwAGGFuRZm1ha4x93PKu+z7v4W8FaJ954ssT0AGFCBmEWy1jffQN++8PjjcNppqY5GMl0yfRB3EYasjgFw91wzax5fSCJS0siRMH483H8/zJ4NNbUqvFSBZBLEJndfbarDilS5r7+G668PQ1gHDw7vKTlIVUkmQcw0swuBmmbWEugHfBJvWCIC8OyzsN9+4Xn77VMdjWSbZDqpryesR70e+A9h2u8bY4xJJKstXhz6Fz75BH73O+jfX8lBUiOZBLG/u//e3Y+IHne6+7rYIxPJMgUF8NhjcPjhcOyxcMQRqY5Isl0yTUwPmVkT4EVguLvPijkmkayzaVN4TJwY5k864IBURySSRA3C3U8AOgH5wKBosr474w5MJBts3BiakDp1gtq1YehQJQdJH0ndKOfuX7v7I8C1QC7wxziDEskG06fDUUfBmDHw73/rZjdJP8ncKHcg0B3oBqwgTNt9c8xxiWSsdevCHdCrVsENN8Cllyo5SHpKpgbxT+A7oLO7H+/uT2jdBpGt89FHYfGeESPguOPCUqBKDpKuyq1BuHuHqghEJJMVFITawogR8OijcN55qY5IpHylJggze8HdL4hWkyu+joMB7u5tYo9OJAMsXgzNmkG7dnD33dCgQaojEklOWTWIG6LnM6oiEJFMs3Il3HRTmENp+nS44opURyRSMWWtKLc8etk7wWpyvasmPJHqafx4aN0adtoJJk+GnJxURyRSccl0Up+S4D1NNCySwPLlsGgRtGwJL74IjzwS1msQqY5KTRBmdl3U/7C/mU0v9lhIWOBHRCLu8M9/hhFKo0dDw4ZwzDGpjkpk25TVB/Ef4G2gP3Bbsfd/cPeVsUYlUs1cdBF8/jmMGgVt26Y6GpHKUVYTk7v7IqAP8EOxB2amcRiS9TZvDs1I7nDHHWEeJSUHySTl1SDOAKYQhrkWv53HgX1ijEskrc2ZA1deCdttByefHDqkRTJNqQnC3c+InltUXTgi6W/GDDjhBLjnHrj22jBthkgmSmYupmOAXHdfa2YXA+2Av7n7V7FHJ5JGpkyBZcvgjDNCkmjSJNURicQrmb99ngB+NLNDgd8Bi4F/xRqVSBr56Se47TY4/fTw2kzJQbJDMgsGbXJ3N7OuwN/dfbCZXRZ3YCLpok8fWLs23A29226pjkak6iRTg/jBzG4HLgHeNLOaQK14wxJJre+/h1tugfx8GDgQnn9eyUGyTzIJojuwHvg/d/8a2BMYEGtUIin01lthVNKqVVCrFuywQ6ojEkmNZKb7/trMngOOMLMzgInu/mz8oYlUva+/ht/+NtwVfdJJqY5GJLXKrUGY2QXAROB84AJggpl1izswkariDi+8EJqUdt89jFBSchBJrpP698ARhavImVlj4L/AS3EGJlIVli2D3r1h/nwYPDi8p/saRIJkEkSNEkuMriC5vguRtOUehqv+5z/Qpk3ohK5dO9VRiaSXZBLEO2b2LjAs2u4OvBVfSCLx+vJLuPrqcCf0LbekOhqR9FVuTcDdfwv8A2gDHAoMcvdb4w5MpLJt3gwPPwxHHgmnnhqeRaR0Za1J3RJ4ANgXmAHc4u5Lqyowkcq0cSMUFMCsWWG1t/32S3VEIumvrBrEM8AbwHmEGV0frZKIRCrRhg1w993QqVNY9vPpp5UcRJJVVh9EPXd/Kno918ymVkVAIpVl6lS47DJo1ix0QpuV/xkR+VlZCaKOmR3Gz+tAbF98292VMCQt/fgj1KwZJta7/Xbo2VPJQWRrlNXEtBx4CHgwenxdbPuBZA5uZqea2VwzW2Bmt5VR7ggz26wb8GRbjRkThq2+8kpYE/rCC5UcRLZWWQsGnbAtB44m9XsMOAXIAyaZ2WvuPjtBufuBd7flfJLdCgrCDW9vvAGPPw5nnZXqiESqvzhveDsSWODuX7r7BmA40DVBueuBEcC3CfaJlOuLL8Ldz7/6VRilpOQgUjniTBB7AkuKbedF7xUxsz2Bc4AnyzqQmV1tZpPNbHJ+fn6lByrVU35+aEI688wwjPXii6F+/VRHJZI54kwQiVp+vcT234Bb3X1zWQdy90Hu3t7d2zdu3Liy4pNq7JNP4JBDYI89YPLkMC23iFSuZNakNuAiYB93v8fM9gZ2d/eJ5Xw0D9ir2HZTYFmJMu2B4eEUNAJON7NN7v5KkvFLlsnLC/c2HHAAvPaa7oYWiVMyNYjHgaOBntH2D4TO5/JMAlqaWQszywF6AK8VL+DuLdy9ubs3J8wO21vJQRIpKIB//AMOOww++ggaNFByEIlbMpP1HeXu7czsMwB3/y76wS+Tu28ys76E0Uk1gWfcfZaZXRvtL7PfQaS4Hj1g8WIYPTqs9iYi8UsmQWyMhqI6FK0HUZDMwd39LUrM/FpaYnD3y5M5pmSPTZvCQj49eoSZV1u2DDfAiUjVSKaJ6RFgJLCrmd0HfAT8OdaoJOvNmAEdO4a5k77/PvQ5KDmIVK1k1qR+zsymACcRRiad7e5zYo9Mstb06WHJz/794cordSe0SKokM4ppb+BH4PXi77n7V3EGJtlnwgRYvhy6dg03vO26a6ojEsluyTQxvUmY9vtN4H3gS+DtOIOS7LJ2Ldx0E5x9dhitZKbkIJIOkmliOqT4tpm1A66JLSLJOn37hg7pGTOgUaNURyMihSp8J3U0zfcRMcQiWWTVKrjhBvj2W3jiCfjXv5QcRNJNMn0QNxXbrAG0AzQhkmy1V1+FPn3CpHp16oSHiKSfZO6DqFfs9SZCX8SIeMKRTLd8OfzhD/Dcc3D88amORkTKUmaCiG6Qq+vuv62ieCQDuYeEMGUKPPwwTJumoasi1UGpCcLMtoumy2hXlQFJZvnqK7j2Wli6FAYPDu8pOYhUD2XVICYS+htyzew14EVgbeFOd3855tikGnMPieCll8Id0bfeqim5RaqbZPogGgArgBMJ8zFZ9KwEIQnNmwdXXQV//nO4v0FEqqeyEsSu0QimmfycGAqVXPhHhE2b4MEHYcCA0BHdoUOqIxKRbVFWgqgJ1CW5leEky23YEJqVFi6ESZOgRYtURyQi26qsBLHc3e+pskikWlq3Du69Fz74AD7+GJ7UKh8iGaOsO6k11kTKNGlSWOFt1iwYMUKjk0QyTVk1iJOqLAqpVtasge22C30O99wD3bopOYhkolJrEO6+sioDkeph1Cg45BB4/XU4+mg4/3wlB5FMlcwwVxEKCqBXL3j/ffjHP+DUU1MdkYjErcKzuUr2mTsXatSAzp1h5kwlB5FsoQQhpfr669C/0K0bbNwIPXpAvXrlf05EMoMShCT00UfQpg20ahVGK2maDJHsoz4I2cLixaG2cPDB8M470E5TNYpkLdUgBAid0I8+CocfDhMmwC67KDmIZDvVIAQI/QzffBOalg44INXRiEg6UA0ii23cCEOHhtrD/ffDhx8qOYjIz5QgstRnn8GRR8KwYfDDD9CyZRjKKiJSSD8JWWjaNOjSBW68Ed5+G+rXT3VEIpKO1AeRRT76KPQznHsuzJkDDRumOiIRSWeqQWSBH36Avn3hggvC/QxmSg4iUj7VILLA9deH/oVZs8LwVRGRZKgGkaFWrIDevUOT0qBB8MwzSg4iUjFKEBnGHV56KUzJXasW7Lgj5OSkOioRqY7UxJRhvv4a+vcPSaJjx1RHIyLVWaw1CDM71czmmtkCM7stwf6LzGx69PjEzA6NM55M5R6akK6/Hpo0gcmTlRxEZNvFVoMws5rAY8ApQB4wycxec/fZxYotBI539+/M7DRgEHBUXDFlooUL4eqrYeVKGDw4vKcV3kSkMsRZgzgSWODuX7r7BmA40LV4AXf/xN2/izbHA01jjCejuIfnV1+FU04JE+y1bZvSkEQkw8SZIPYElhTbzoveK82VwNuJdpjZ1WY22cwm5+fnV2KI1dPs2aEJ6ZNPwt3Qv/sdbKfeJBGpZHEmiEQNHZ6woNkJhARxa6L97j7I3du7e/vGjRtXYojVy8aNcO+9cPzxcOml0KFDqiMSkUwW59+decBexbabAstKFjKzNsDTwGnuviLGeKq1devCzW7ffANTpsDee6c6IhHJdHHWICYBLc2shZnlAD2A14oXMLO9gZeBS9x9XoyxVFs//QS33gonnhjua3j0USUHEakasSUId98E9AXeBeYAL7j7LDO71syujYr9EWgIPG5muWY2Oa54qqPx48O60IsWwSuvaHSSiFStWLs23f0t4K0S7z1Z7HUvoFecMVRH33//86R6AwbA2WenOiIRyUaaaiPNvPUWtG4dno86SslBRFJHgyPTREEBXH45fPwx/POfcNJJqY5IRLKdahAp5h6m4a5RA846C6ZPV3IQkfSgBJFCS5eGJqSLLw73OHTrFmZfFRFJB0oQKTJuXJgao23bMFqpVq1URyQisiX1QVSxL76AzZvD8NX33w/PIiLpSDWIKrJ5Mzz0UBiZNHUq7LyzkoOIpDfVIKrIuefC6tWhOWm//VIdjYhI+VSDiNGGDfD002EI68MPwwcfKDmISPWhBBGTiRPh8MPDFBlr1sA++4ShrCIi1YWamGKQmxvuaXj4YejRQ3MoiUj1pARRiUaPhvx8OP98+Pzz0BEtIlJdqdGjEqxeDddcExbxqVs31BiUHESkulMNohLccAPUqQMzZ0L9+qmORkSkcihBbKX8fLj9dvjTn+Cpp3QntKSXjRs3kpeXx7p161IdilSxOnXq0LRpU2pVwo+SEkQFucOwYXDTTXDJJaHGoOQg6SYvL4969erRvHlzTKMksoa7s2LFCvLy8mjRosU2H08JooKWL4dHHoHXX4cjjkh1NCKJrVu3TskhC5kZDRs2JD8/v1KOpwSRhIKC0Iw0bRo8/jh8+qmGrkr6U3LITpX5764EUY758+Gqq2DdOhg8OLyn/+9EJBtomGspCgrC89tvQ9euYaW3gw9ObUwi1cl9993HwQcfTJs2bWjbti0TJkzgrrvu4vbbb9+iXG5uLgceeCAAzZs359hjj91if9u2bWndunXCcyxfvpwzzjgjni9QCdydfv36sd9++9GmTRumTp2asNwHH3xAu3btaN26NZdddhmbNm0CYMCAAbRt27boGtSsWZOVK1eyYcMGjjvuuKJycVGCSGD69DDr6iefQL9+8JvfQM2aqY5KpPr49NNPeeONN5g6dSrTp0/nv//9L3vttRc9e/bk+eef36Ls8OHDufDCC4u2f/jhB5YsWQLAnDlzyjzPQw89xFVXXZV0XJs3b67At9h2b7/9NvPnz2f+/PkMGjSI66677hdlCgoKuOyyyxg+fDgzZ86kWbNmDB06FIDf/va35ObmkpubS//+/Tn++ONp0KABOTk5nHTSSb+4lpVNTUzFbNgA994LTzwB/fvD0UenOiKRbXfoAz1iOe60W4aXum/58uU0atSI2rVrA9CoUaOifTvvvDMTJkzgqKOOAuCFF17g3XffLdp/wQUX8Pzzz3PLLbcwbNgwevbsyb/+9a+E5xkxYgT33nsvAIsWLeKSSy5h7dq1AAwcOJCOHTsyZswY7r77bpo0aUJubi4zZszgtttuY8yYMaxfv54+ffpwzTXXsGbNGrp27cp3333Hxo0buffee+nates2XaNXX32VSy+9FDOjQ4cOrFq1iuXLl9OkSZOiMitWrKB27dq0atUKgFNOOYX+/ftz5ZVXbnGswmtR6Oyzz+b222/noosu2qYYy6IaROTHH0Pfwpo1YS6lXr3U1yCytTp37sySJUto1aoVvXv3ZuzYsUX7evbsyfDhIbmMHz+ehg0b0rJly6L93bp14+WXXwbg9ddf58wzz0x4joULF7LLLrsUJaFdd92V9957j6lTp/L888/Tr1+/orITJ07kvvvuY/bs2QwePJj69eszadIkJk2axFNPPcXChQupU6cOI0eOZOrUqYwePZqbb74Zd//Febt3717U7FP88eyzz/6i7NKlS9lrr72Ktps2bcrSpUu3KNOoUSM2btzI5MmTAXjppZeKalCFfvzxR9555x3OO++8ovdat27NpEmTEl6bypL1NYi1a+HOO8Psqx99FBb1EckkZf2lH5e6desyZcoUPvzwQ0aPHk337t35y1/+wuWXX06PHj3o2LEjDz74IMOHD9/ir2KABg0asMsuuzB8+HAOPPBAdthhh4TnWL58OY0bNy7a3rhxI3379iU3N5eaNWsyb968on1HHnlk0X0Bo0aNYvr06bz00ksArF69mvnz59O0aVPuuOMOxo0bR40aNVi6dCnffPMNu++++xbnrUizTqIEU3KUkZkxfPhwfvOb37B+/Xo6d+7Mdttt+dP8+uuvc8wxx9CgQYOi92rWrElOTg4//PAD9erVSzqmisjqBPHxx+Fmt2OOgddeU41BpDLVrFmTTp060alTJw455BCGDh3K5Zdfzl577UXz5s0ZO3YsI0aM4NNPP/3FZ7t3706fPn0YMmRIqcfffvvtt7hT/OGHH2a33XZj2rRpFBQUUKdOnaJ9O+64Y9Frd+fRRx+lS5cuWxxvyJAh5OfnM2XKFGrVqkXz5s0T3onevXt35s6d+4v3b7rpJi699NIt3mvatOkWtYG8vDz22GOPX3z26KOP5sMPPwRCAiue3ICEiRRg/fr1W3zPypaVCWLVKqhdG3JyYOBAOP30VEckklnmzp1LjRo1ipqOcnNzadasWdH+nj178pvf/IZ9992Xpk2b/uLz55xzDsuXL6dLly4sW7Ys4TlatWrFokWLirZXr15N06ZNqVGjBkOHDi21Q7pLly488cQTnHjiidSqVYt58+ax5557snr1anbddVdq1arF6NGjWbx4ccLPV6QGcdZZZzFw4EB69OjBhAkTqF+//hb9D4W+/fZbdt11V9avX8/999/P73//+y2+19ixY/n3v/+9xWdWrFhB48aNK2VKjdJkXR/Eq69C69bwzjvhTmglB5HKt2bNGi677DIOOugg2rRpw+zZs7nrrruK9p9//vnMmjWLHj0Sd6DXq1ePW2+9lZycnFLPseOOO7LvvvuyYMECAHr37s3QoUPp0KED8+bN26LWUFyvXr046KCDioaVXnPNNWzatImLLrqIyZMn0759e5577jkOOOCArb8AkdNPP5199tmH/fbbj6uuuorHH398i32FyW/AgAEceOCBtGnThjPPPJMTTzyxqNzIkSPp3LnzL77P6NGjOT3mHzBL1EaWztq3b+8be4R1OyvStlpQABdeCFOnhmVAjzsurghFUm/OnDlF9xZkspEjRzJlypSikUzZ5Nxzz6V///7sv//+v9iX6N/fzKa4e/uKnCPjaxDuYYqMGjWgZ8/wWslBJDOcc845NG/ePNVhVLkNGzZw9tlnJ0wOlSmj+yC++gquvRa+/TbMn7SNQ5pFJA316tUr1SFUuZycnF90iMchY2sQY8bA4YeHEUqffqopuSX7VLfmY6kclfnvnnE1iHnzQrPSYYfB2LFw0EGpjkik6tWpU4cVK1bQsGFDzeqaRQrXg6isoa8ZkyA2bYIHH4QBA8JUGfvvr+U/JXs1bdqUvLy8SlsXQKqPwhXlKkPGJIhzzoH162HyZMjCPiuRLdSqVatSVhST7BZrH4SZnWpmc81sgZndlmC/mdkj0f7pZtauIsdfvx6efDIMYX3sMXj3XSUHEZHKEluCMLOawGPAacBBQE8zK9kjcBrQMnpcDTyR7PHXLmpF27YwalSYYG/vvTVVhohIZYqzielIYIG7fwlgZsOBrsDsYmW6As966HYfb2Y7m1kTd19e1oF/Wtqcxc/+hmHPwHnnKTGIiMQhzgSxJ1B8zto84KgkyuwJbJEgzOxqQg0DYA1TpsyF+xudfz7/q9yQq6VGoOuArkNxuhaBrkNQeB2alVewpDgTRKK/60sO0E2mDO4+CBi0xQfNJlf0tvFMpOsQ6Dr8TNci0HUItuU6xNlJnQfsVWy7KVByWsZkyoiISArEmSAmAS3NrIWZ5QA9gNdKlHkNuDQazdQBWF1e/4OIiFSN2JqY3H2TmfUF3gVqAs+4+ywzuzba/yTwFnA6sAD4EbiiAqcYVH6RrKDrEOg6/EzXItB1CLb6OlS76b5FRKRqZOxkfSIism2UIEREJKG0ThBxT9VRnSRxLS6KrsF0M/vEzA5NRZxxK+86FCt3hJltNrNuVRlfVUnmOphZJzPLNbNZZja2qmOsKkn8v1HfzF43s2nRtahIX2e1YGbPmNm3ZjazlP1b91vp7mn5IHRsfwHsA+QA04CDSpQ5HXibcD9FB2BCquNO4bXoCOwSvT4tE69FMtehWLkPCIMguqU67hT997AzYdaCvaPtXVMddwqvxR3A/dHrxsBKICfVsVfydTgOaAfMLGX/Vv1WpnMNomiqDnffABRO1VFc0VQd7j4e2NnMmlR1oFWg3Gvh7p+4+3fR5njCPSWZJpn/JgCuB0YA31ZlcFUometwIfCyu38F4O7ZfC0cqGdhYYy6hASxqWrDjJe7jyN8r9Js1W9lOieI0qbhqGiZTFDR73kl4a+FTFPudTCzPYFzgCerMK6qlsx/D62AXcxsjJlNMbP416dMjWSuxUDgQMJNuDOAG9y9oGrCSxtb9VuZzutBVNpUHRkg6e9pZicQEsSvYo0oNZK5Dn8DbnX3zRm8kloy12E74HDgJGB74FMzG+/u8+IOroolcy26ALnAicC+wHtm9qG7fx9zbOlkq34r0zlBaKqOnyX1Pc2sDfA0cJq7r6ii2KpSMtehPTA8Sg6NgNPNbJO7v1IlEVaNZP/f+J+7rwXWmtk44FAg0xJEMtfiCuAvHhrjF5jZQuAAYGLVhJgWtuq3Mp2bmDRVx8/KvRZmtjfwMnBJBv6VWKjc6+DuLdy9ubs3B14CemdYcoDk/t94FTjWzLYzsx0IMynPqeI4q0Iy1+IrQk0KM9sN2B/4skqjTL2t+q1M2xqExz9VR7WR5LX4I9AQeDz663mTZ9hMlkleh4yXzHVw9zlm9g4wHSgAnnb3hEMgq7Mk/5v4EzDEzGYQmlpudfeMmgbczIYBnYBGZpYH/D+gFmzbb6Wm2hARkYTSuYlJRERSSAlCREQSUoIQEZGElCBERCQhJQgREUlICULSUjQTa26xR/Myyq6phPMNMbOF0bmmmtnRW3GMp83soOj1HSX2fbKtMUbHKbwuM6MZSncup3xbMzu9Ms4t2UfDXCUtmdkad69b2WXLOMYQ4A13f8nMOgMPuHubbTjeNsdU3nHNbCgwz93vK6P85UB7d+9b2bFI5lMNQqoFM6trZu9Hf93PMLNfzOJqZk3MbFyxv7CPjd7vbGafRp990czK++EeB+wXffam6FgzzezG6L0dzezNaH2BmWbWPXp/jJm1N7O/ANtHcTwX7VsTPT9f/C/6qOZynpnVNLMBZjbJwnz91yRxWT4lmnDNzI60sA7IZ9Hz/tGdxfcA3aNYukexPxOd57NE11GkSKrnMddDj0QPYDNhgrVcYCThrv+don2NCHeEFtaA10TPNwO/j17XBOpFZccBO0bv3wr8McH5hhCtHQGcD0wgTHY3A9iRME30LOAw4DzgqWKfrR89jyH8tV4UU7EyhTGeAwyNXucQZtjcHrgauDN6vzYwGWiRIM41xb7fi8Cp0fZOwHbR65OBEdHry4GBxT7/Z+Di6PXOhLmZdkz1v7ce6flI26k2JOv95O5tCzfMrBbwZzM7jjB1xJ7AbsDXxT4zCXgmKvuKu+ea2fHAQcDH0RQkOYS/vBMZYGZ3AvmEGXFPAkZ6mPAOM3sZOBZ4B3jAzO4nNEt9WIHv9TbwiJnVBk4Fxrn7T1GzVhv7eQW8+kBLYGGJz29vZrlAc2AK8F6x8kPNrCVhls5apZy/M3CWmd0SbdcB9iYz52mSbaQEIdXFRYTVwA53941mtojw41bE3cdFCeTXwL/MbADwHfCeu/dM4hy/dfeXCjfM7OREhdx9npkdTpjbpr+ZjXL3e5L5Eu6+zszGEKag7g4MKzwdcL27v1vOIX5y97ZmVh94A+gDPEKYb2i0u58TdeiPKeXzBpzn7nOTiVeym/ogpLqoD3wbJYcTgGYlC5hZs6jMU8BgwhKM44FjzKywT2EHM2uV5DnHAWdHn9mR0Dz0oZntAfzo7v8GHojOU9LGqCaTyHDCZGnHEiaZI3q+rvAzZtYqOmdC7r4a6AfcEn2mPrA02n15saI/EJraCr0LXG9RdcrMDivtHCJKEFJdPAe0N7PJhNrE5wnKdAJyzewzQj/B3909n/CDOczMphMSxgHJnNDdpxL6JiYS+iSedvfPgEOAiVFTz++BexN8fBAwvbCTuoRRhDWE/+thmUwI63jMBqZaWHj+H5RTw49imUaY4vqvhNrMx4T+iUKjgYMKO6kJNY1aUWwzo22RhDTMVUREElINQkREElKCEBGRhJQgREQkISUIERFJSAlCREQSUoIQEZGElCBERCSh/w9lqpzcEtSnIAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Roc Curve:\n",
    "plt.plot(fpr_rf, tpr_rf, color='seagreen', lw=2, \n",
    "         label='SVM (area = %0.2f)' % AUC_rf2)\n",
    "\n",
    "# Random Guess line:\n",
    "plt.plot([0, 1], [0, 1], color='blue', lw=1, linestyle='--')\n",
    "\n",
    "# Defining The Range of X-Axis and Y-Axis:\n",
    "plt.xlim([-0.005, 1.005])\n",
    "plt.ylim([0.0, 1.01])\n",
    "\n",
    "# Labels, Title, Legend:\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "[[24  1  1]\n",
      " [ 0 18  2]\n",
      " [ 2  0 18]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "# print confusion matrix\n",
    "cm_SVC = metrics.confusion_matrix(y_test, y_predict)\n",
    "\n",
    "print(\"Confusion matrix:\")\n",
    "print(cm_SVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(317, 10000)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_search = np.concatenate((train_images, test_images))\n",
    "X_search.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(317,)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_search = np.concatenate((y_train, y_test))\n",
    "y_search.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': [0.1, 1, 10, 100, 1000.0, 5000.0, 10000.0, 50000.0, 100000.0]} \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, estimator=SVC(C=1, gamma=0.0005, random_state=1),\n",
       "             param_grid={'C': [0.1, 1, 10, 100, 1000.0, 5000.0, 10000.0,\n",
       "                               50000.0, 100000.0]},\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the list for the search\n",
    "li = [0.1, 1, 10, 100, 1e3, 5e3, 1e4, 5e4, 1e5]\n",
    "\n",
    "# create a dictionary for grid parameter:\n",
    "param_grid = dict(C = li)\n",
    "print(param_grid,'\\n')\n",
    "\n",
    "\n",
    "X_n_pca = my_pca.fit_transform(X_search)\n",
    "\n",
    "# instantiate the model:\n",
    "my_SVC = svm.SVC(C=1, kernel='rbf', gamma=0.0005, random_state=1)\n",
    "\n",
    "# creat the grid, and define the metric for evaluating the model: \n",
    "grid = GridSearchCV(my_SVC, param_grid, cv=10, scoring='accuracy')\n",
    "\n",
    "# fit the grid (start the grid search):\n",
    "grid.fit(X_n_pca, y_search)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9558467741935484\n",
      "{'C': 10}\n"
     ]
    }
   ],
   "source": [
    "# print out best score and best value for parameter C\n",
    "print(grid.best_score_)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "my_RandomForest = RandomForestClassifier(n_estimators = 50, bootstrap = True, random_state=5)\n",
    "\n",
    "my_RandomForest.fit(train_images, y_train)\n",
    "\n",
    "y_predict_rf = my_RandomForest.predict(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "score_rf = accuracy_score(y_test, y_predict_rf)\n",
    "\n",
    "print('Accuracy:', score_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.08 0.06 0.88 0.14 0.02 0.06 0.02 0.06 0.98 0.78 0.96 0.3  0.88 0.06\n",
      " 0.02 0.82 0.28 0.06 0.92 0.98 0.08 0.1  0.94 0.04 0.98 0.14 0.88 1.\n",
      " 0.1  1.   1.   0.28 0.06 0.04 1.   0.04 0.08 1.   1.   0.76 0.34 0.3\n",
      " 0.02 0.06 0.82 0.08 0.08 0.04 0.   0.   0.02 0.14 0.38 0.06 0.1  0.5\n",
      " 0.92 0.06 1.   0.02 1.   0.96 0.02 0.28 0.1  0.18]\n"
     ]
    }
   ],
   "source": [
    "# Estimating the probability (likelihood) of Each Label: \n",
    "y_predict_prob_rf = my_RandomForest.predict_proba(test_images)\n",
    "print(y_predict_prob_rf[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.025 0.025 0.05\n",
      " 0.1   0.125 0.15  0.225 0.325 0.45  0.675 0.775 0.95  1.   ]\n",
      "\n",
      "[0.         0.30769231 0.42307692 0.5        0.53846154 0.61538462\n",
      " 0.73076923 0.80769231 0.88461538 0.88461538 0.92307692 0.92307692\n",
      " 0.92307692 1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.        ]\n",
      "0.9903846153846154\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "#Now we look for the tpr, fpr, and auc of the random forest Covid result\n",
    "fpr_rf, tpr_rf, thresholds = metrics.roc_curve(y_test,y_predict_prob_rf[:,0], pos_label=0)\n",
    "\n",
    "print(fpr_rf)\n",
    "print()\n",
    "print(tpr_rf)\n",
    "\n",
    "# AUC:\n",
    "AUC_rf = metrics.auc(fpr_rf, tpr_rf)\n",
    "print(AUC_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "[[24  1  1]\n",
      " [ 0 14  6]\n",
      " [ 1  2 17]]\n"
     ]
    }
   ],
   "source": [
    "# print confusion matrix\n",
    "cm_RF = metrics.confusion_matrix(y_test, y_predict_rf)\n",
    "\n",
    "print(\"Confusion matrix:\")\n",
    "print(cm_RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Roc Curve:\n",
    "plt.plot(fpr_rf, tpr_rf, color='seagreen', lw=2, \n",
    "         label='Random Forest (area = %0.2f)' % AUC_rf)\n",
    "\n",
    "# Random Guess line:\n",
    "plt.plot([0, 1], [0, 1], color='blue', lw=1, linestyle='--')\n",
    "\n",
    "# Defining The Range of X-Axis and Y-Axis:\n",
    "plt.xlim([-0.005, 1.005])\n",
    "plt.ylim([0.0, 1.01])\n",
    "\n",
    "# Labels, Title, Legend:\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "cnn.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
